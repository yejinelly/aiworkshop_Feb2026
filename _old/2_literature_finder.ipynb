{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“š Modular Literature Finder\n",
        "\n",
        "**Multi-database academic search with API key fallback**\n",
        "\n",
        "## Supported Databases\n",
        "\n",
        "| Database | API Key | Rate Limit | Best For |\n",
        "|----------|---------|------------|----------|\n",
        "| PubMed | âŒ | Unlimited | ì˜í•™/ìƒëª…ê³¼í•™ |\n",
        "| arXiv | âŒ | Unlimited | CS/ë¬¼ë¦¬/ìˆ˜í•™ |\n",
        "| OSF | âŒ | Unlimited | ì‚¬íšŒê³¼í•™/Preprints |\n",
        "| GitHub | âš ï¸ ê¶Œìž¥ | 60/hr (free) | ì½”ë“œ/ë„êµ¬ |\n",
        "| Semantic Scholar | âš ï¸ ê¶Œìž¥ | 100/5min | í†µí•© ê²€ìƒ‰ |\n",
        "\n",
        "---"
      ],
      "metadata": {"id": "header"}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {"id": "setup"}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q requests google-generativeai\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional, Dict\n",
        "from abc import ABC, abstractmethod\n",
        "from getpass import getpass\n",
        "\n",
        "print(\"âœ“ Setup complete\")"
      ],
      "metadata": {"id": "install"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. API Key Configuration (Optional)\n",
        "\n",
        "**ëª¨ë‘ ì„ íƒì‚¬í•­ìž…ë‹ˆë‹¤!** ì—†ì–´ë„ ê¸°ë³¸ ê²€ìƒ‰ì€ ë™ìž‘í•©ë‹ˆë‹¤."
      ],
      "metadata": {"id": "api_config"}
    },
    {
      "cell_type": "code",
      "source": [
        "# API Keys (ëª¨ë‘ ì„ íƒì‚¬í•­ - ì—†ìœ¼ë©´ Enter)\n",
        "print(\"API Keys ìž…ë ¥ (ì—†ìœ¼ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "GEMINI_API_KEY = getpass(\"Gemini API Key (LLMìš©): \") or None\n",
        "GITHUB_TOKEN = getpass(\"GitHub Token (ì„ íƒ): \") or None\n",
        "SEMANTIC_SCHOLAR_KEY = getpass(\"Semantic Scholar Key (ì„ íƒ): \") or None\n",
        "\n",
        "# Status\n",
        "print(\"\\nðŸ“Š API Key Status:\")\n",
        "print(f\"  Gemini: {'âœ“ ì„¤ì •ë¨' if GEMINI_API_KEY else 'âœ— ì—†ìŒ (ìš”ì•½ ê¸°ëŠ¥ ì œí•œ)'}\")\n",
        "print(f\"  GitHub: {'âœ“ ì„¤ì •ë¨' if GITHUB_TOKEN else 'âš ï¸ ì—†ìŒ (60 req/hr ì œí•œ)'}\")\n",
        "print(f\"  Semantic Scholar: {'âœ“ ì„¤ì •ë¨' if SEMANTIC_SCHOLAR_KEY else 'âš ï¸ ì—†ìŒ (rate limited)'}\")\n",
        "print(\"\\nâœ“ PubMed, arXiv, OSFëŠ” API key ë¶ˆí•„ìš”!\")"
      ],
      "metadata": {"id": "api_keys"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Structure"
      ],
      "metadata": {"id": "data"}
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Paper:\n",
        "    \"\"\"Unified paper format across all databases\"\"\"\n",
        "    title: str\n",
        "    authors: List[str]\n",
        "    year: Optional[int]\n",
        "    abstract: str\n",
        "    url: str\n",
        "    source: str  # 'pubmed', 'arxiv', 'osf', 'github', 'semantic_scholar'\n",
        "    doi: Optional[str] = None\n",
        "    citations: Optional[int] = None\n",
        "    keywords: List[str] = field(default_factory=list)\n",
        "\n",
        "    def __repr__(self):\n",
        "        author_str = self.authors[0] if self.authors else \"Unknown\"\n",
        "        if len(self.authors) > 1:\n",
        "            author_str += \" et al.\"\n",
        "        return f\"[{self.source.upper()}] {author_str} ({self.year}): {self.title[:60]}...\"\n",
        "\n",
        "print(\"âœ“ Paper dataclass defined\")"
      ],
      "metadata": {"id": "dataclass"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Database Modules\n",
        "\n",
        "ê° ëª¨ë“ˆì€ ë…ë¦½ì ìœ¼ë¡œ ë™ìž‘í•˜ë©°, ìˆœì„œ ë³€ê²½/ì¶”ê°€/ì œê±° ê°€ëŠ¥"
      ],
      "metadata": {"id": "modules"}
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModule(ABC):\n",
        "    \"\"\"Base class for all database modules\"\"\"\n",
        "    name: str = \"base\"\n",
        "    requires_api_key: bool = False\n",
        "\n",
        "    @abstractmethod\n",
        "    def search(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        pass\n",
        "\n",
        "    def is_available(self) -> bool:\n",
        "        return True\n",
        "\n",
        "print(\"âœ“ BaseModule defined\")"
      ],
      "metadata": {"id": "base_module"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PubMedModule(BaseModule):\n",
        "    \"\"\"PubMed/NCBI E-utilities - ì˜í•™/ìƒëª…ê³¼í•™\"\"\"\n",
        "    name = \"pubmed\"\n",
        "    requires_api_key = False\n",
        "\n",
        "    def search(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        papers = []\n",
        "\n",
        "        # Step 1: Search for IDs\n",
        "        search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "        search_params = {\n",
        "            \"db\": \"pubmed\",\n",
        "            \"term\": query,\n",
        "            \"retmax\": max_results,\n",
        "            \"retmode\": \"json\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            resp = requests.get(search_url, params=search_params, timeout=10)\n",
        "            ids = resp.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
        "\n",
        "            if not ids:\n",
        "                return papers\n",
        "\n",
        "            # Step 2: Fetch details\n",
        "            fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
        "            fetch_params = {\n",
        "                \"db\": \"pubmed\",\n",
        "                \"id\": \",\".join(ids),\n",
        "                \"retmode\": \"json\"\n",
        "            }\n",
        "\n",
        "            resp = requests.get(fetch_url, params=fetch_params, timeout=10)\n",
        "            results = resp.json().get(\"result\", {})\n",
        "\n",
        "            for pmid in ids:\n",
        "                if pmid in results:\n",
        "                    item = results[pmid]\n",
        "                    authors = [a.get(\"name\", \"\") for a in item.get(\"authors\", [])]\n",
        "\n",
        "                    papers.append(Paper(\n",
        "                        title=item.get(\"title\", \"\"),\n",
        "                        authors=authors[:5],\n",
        "                        year=int(item.get(\"pubdate\", \"0\")[:4]) if item.get(\"pubdate\") else None,\n",
        "                        abstract=\"\",  # Summary doesn't include abstract\n",
        "                        url=f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\",\n",
        "                        source=\"pubmed\",\n",
        "                        doi=item.get(\"elocationid\", \"\").replace(\"doi: \", \"\")\n",
        "                    ))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸ PubMed error: {e}\")\n",
        "\n",
        "        return papers\n",
        "\n",
        "print(\"âœ“ PubMedModule defined\")"
      ],
      "metadata": {"id": "pubmed"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArXivModule(BaseModule):\n",
        "    \"\"\"arXiv API - CS/ë¬¼ë¦¬/ìˆ˜í•™\"\"\"\n",
        "    name = \"arxiv\"\n",
        "    requires_api_key = False\n",
        "\n",
        "    def search(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        papers = []\n",
        "        url = \"http://export.arxiv.org/api/query\"\n",
        "        params = {\n",
        "            \"search_query\": f\"all:{query}\",\n",
        "            \"start\": 0,\n",
        "            \"max_results\": max_results\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            resp = requests.get(url, params=params, timeout=10)\n",
        "            # Parse XML (simple regex for demo)\n",
        "            entries = re.findall(r'<entry>(.*?)</entry>', resp.text, re.DOTALL)\n",
        "\n",
        "            for entry in entries:\n",
        "                title = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)\n",
        "                abstract = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)\n",
        "                authors = re.findall(r'<name>(.*?)</name>', entry)\n",
        "                link = re.search(r'<id>(.*?)</id>', entry)\n",
        "                published = re.search(r'<published>(.*?)</published>', entry)\n",
        "\n",
        "                if title and link:\n",
        "                    year = int(published.group(1)[:4]) if published else None\n",
        "                    papers.append(Paper(\n",
        "                        title=title.group(1).strip().replace('\\n', ' '),\n",
        "                        authors=authors[:5],\n",
        "                        year=year,\n",
        "                        abstract=abstract.group(1).strip()[:500] if abstract else \"\",\n",
        "                        url=link.group(1),\n",
        "                        source=\"arxiv\"\n",
        "                    ))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸ arXiv error: {e}\")\n",
        "\n",
        "        return papers\n",
        "\n",
        "print(\"âœ“ ArXivModule defined\")"
      ],
      "metadata": {"id": "arxiv"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OSFModule(BaseModule):\n",
        "    \"\"\"OSF (Open Science Framework) - ì‚¬íšŒê³¼í•™/Preprints\"\"\"\n",
        "    name = \"osf\"\n",
        "    requires_api_key = False\n",
        "\n",
        "    def search(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        papers = []\n",
        "        url = \"https://api.osf.io/v2/preprints/\"\n",
        "        params = {\n",
        "            \"filter[title,description]\": query,\n",
        "            \"page[size]\": max_results\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            resp = requests.get(url, params=params, timeout=10)\n",
        "            data = resp.json().get(\"data\", [])\n",
        "\n",
        "            for item in data:\n",
        "                attrs = item.get(\"attributes\", {})\n",
        "                date_created = attrs.get(\"date_created\", \"\")\n",
        "\n",
        "                papers.append(Paper(\n",
        "                    title=attrs.get(\"title\", \"\"),\n",
        "                    authors=[],  # OSF API doesn't return authors in list\n",
        "                    year=int(date_created[:4]) if date_created else None,\n",
        "                    abstract=attrs.get(\"description\", \"\")[:500],\n",
        "                    url=item.get(\"links\", {}).get(\"html\", \"\"),\n",
        "                    source=\"osf\",\n",
        "                    doi=attrs.get(\"doi\", \"\")\n",
        "                ))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸ OSF error: {e}\")\n",
        "\n",
        "        return papers\n",
        "\n",
        "print(\"âœ“ OSFModule defined\")"
      ],
      "metadata": {"id": "osf"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GitHubModule(BaseModule):\n",
        "    \"\"\"GitHub Search - ì½”ë“œ/ë„êµ¬/Awesome Lists\"\"\"\n",
        "    name = \"github\"\n",
        "    requires_api_key = False  # Works without, but rate limited\n",
        "\n",
        "    def __init__(self, token: Optional[str] = None):\n",
        "        self.token = token\n",
        "\n",
        "    def search(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        papers = []\n",
        "        url = \"https://api.github.com/search/repositories\"\n",
        "\n",
        "        headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n",
        "        if self.token:\n",
        "            headers[\"Authorization\"] = f\"token {self.token}\"\n",
        "\n",
        "        params = {\n",
        "            \"q\": f\"{query} in:readme,description\",\n",
        "            \"sort\": \"stars\",\n",
        "            \"per_page\": max_results\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            resp = requests.get(url, headers=headers, params=params, timeout=10)\n",
        "\n",
        "            if resp.status_code == 403:\n",
        "                print(\"  âš ï¸ GitHub rate limit hit\")\n",
        "                return papers\n",
        "\n",
        "            items = resp.json().get(\"items\", [])\n",
        "\n",
        "            for item in items:\n",
        "                created = item.get(\"created_at\", \"\")\n",
        "                papers.append(Paper(\n",
        "                    title=item.get(\"full_name\", \"\"),\n",
        "                    authors=[item.get(\"owner\", {}).get(\"login\", \"\")],\n",
        "                    year=int(created[:4]) if created else None,\n",
        "                    abstract=item.get(\"description\", \"\") or \"\",\n",
        "                    url=item.get(\"html_url\", \"\"),\n",
        "                    source=\"github\",\n",
        "                    citations=item.get(\"stargazers_count\", 0),  # Stars as proxy\n",
        "                    keywords=item.get(\"topics\", [])\n",
        "                ))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸ GitHub error: {e}\")\n",
        "\n",
        "        return papers\n",
        "\n",
        "print(\"âœ“ GitHubModule defined\")"
      ],
      "metadata": {"id": "github"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SemanticScholarModule(BaseModule):\n",
        "    \"\"\"Semantic Scholar API - í†µí•© í•™ìˆ  ê²€ìƒ‰\"\"\"\n",
        "    name = \"semantic_scholar\"\n",
        "    requires_api_key = False  # Works without, but rate limited\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def search(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        papers = []\n",
        "        url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "        headers = {}\n",
        "        if self.api_key:\n",
        "            headers[\"x-api-key\"] = self.api_key\n",
        "\n",
        "        params = {\n",
        "            \"query\": query,\n",
        "            \"limit\": max_results,\n",
        "            \"fields\": \"title,authors,year,abstract,url,citationCount,externalIds\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            resp = requests.get(url, headers=headers, params=params, timeout=10)\n",
        "\n",
        "            if resp.status_code == 429:\n",
        "                print(\"  âš ï¸ Semantic Scholar rate limit - waiting 5s...\")\n",
        "                time.sleep(5)\n",
        "                resp = requests.get(url, headers=headers, params=params, timeout=10)\n",
        "\n",
        "            data = resp.json().get(\"data\", [])\n",
        "\n",
        "            for item in data:\n",
        "                authors = [a.get(\"name\", \"\") for a in item.get(\"authors\", [])]\n",
        "                ext_ids = item.get(\"externalIds\", {}) or {}\n",
        "\n",
        "                papers.append(Paper(\n",
        "                    title=item.get(\"title\", \"\"),\n",
        "                    authors=authors[:5],\n",
        "                    year=item.get(\"year\"),\n",
        "                    abstract=item.get(\"abstract\", \"\")[:500] if item.get(\"abstract\") else \"\",\n",
        "                    url=item.get(\"url\", \"\"),\n",
        "                    source=\"semantic_scholar\",\n",
        "                    doi=ext_ids.get(\"DOI\", \"\"),\n",
        "                    citations=item.get(\"citationCount\")\n",
        "                ))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸ Semantic Scholar error: {e}\")\n",
        "\n",
        "        return papers\n",
        "\n",
        "print(\"âœ“ SemanticScholarModule defined\")"
      ],
      "metadata": {"id": "semantic_scholar"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Literature Pipeline\n",
        "\n",
        "ëª¨ë“ˆë“¤ì„ ì¡°í•©í•˜ì—¬ í†µí•© ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ êµ¬ì„±"
      ],
      "metadata": {"id": "pipeline"}
    },
    {
      "cell_type": "code",
      "source": [
        "class LiteraturePipeline:\n",
        "    \"\"\"Modular literature search pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, modules: List[BaseModule] = None):\n",
        "        self.modules = modules or []\n",
        "\n",
        "    def add(self, module: BaseModule):\n",
        "        \"\"\"Add a module to the pipeline\"\"\"\n",
        "        self.modules.append(module)\n",
        "        return self\n",
        "\n",
        "    def remove(self, module_name: str):\n",
        "        \"\"\"Remove a module by name\"\"\"\n",
        "        self.modules = [m for m in self.modules if m.name != module_name]\n",
        "        return self\n",
        "\n",
        "    def reorder(self, order: List[str]):\n",
        "        \"\"\"Reorder modules by name list\"\"\"\n",
        "        name_to_module = {m.name: m for m in self.modules}\n",
        "        self.modules = [name_to_module[n] for n in order if n in name_to_module]\n",
        "        return self\n",
        "\n",
        "    def list_modules(self):\n",
        "        \"\"\"Show current modules\"\"\"\n",
        "        for i, m in enumerate(self.modules, 1):\n",
        "            key_status = \"ðŸ”‘\" if m.requires_api_key else \"âœ“\"\n",
        "            print(f\"  {i}. {m.name} {key_status}\")\n",
        "\n",
        "    def search(self, query: str, max_per_source: int = 5) -> List[Paper]:\n",
        "        \"\"\"Search across all modules\"\"\"\n",
        "        all_papers = []\n",
        "\n",
        "        print(f\"\\nðŸ” Searching: '{query}'\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        for module in self.modules:\n",
        "            print(f\"\\nðŸ“š {module.name.upper()}...\")\n",
        "\n",
        "            if not module.is_available():\n",
        "                print(f\"  â­ï¸ Skipped (not available)\")\n",
        "                continue\n",
        "\n",
        "            papers = module.search(query, max_per_source)\n",
        "            all_papers.extend(papers)\n",
        "            print(f\"  âœ“ Found {len(papers)} results\")\n",
        "\n",
        "            time.sleep(0.5)  # Be nice to APIs\n",
        "\n",
        "        # Deduplicate by title similarity\n",
        "        unique_papers = self._deduplicate(all_papers)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(f\"ðŸ“Š Total: {len(unique_papers)} unique papers\")\n",
        "\n",
        "        return unique_papers\n",
        "\n",
        "    def _deduplicate(self, papers: List[Paper]) -> List[Paper]:\n",
        "        \"\"\"Remove duplicates based on title similarity\"\"\"\n",
        "        seen_titles = set()\n",
        "        unique = []\n",
        "\n",
        "        for paper in papers:\n",
        "            # Normalize title for comparison\n",
        "            norm_title = paper.title.lower().strip()[:50]\n",
        "            if norm_title not in seen_titles:\n",
        "                seen_titles.add(norm_title)\n",
        "                unique.append(paper)\n",
        "\n",
        "        return unique\n",
        "\n",
        "print(\"âœ“ LiteraturePipeline defined\")"
      ],
      "metadata": {"id": "pipeline_class"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Create Your Pipeline\n",
        "\n",
        "ì›í•˜ëŠ” ëª¨ë“ˆë§Œ ì„ íƒí•˜ì—¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±"
      ],
      "metadata": {"id": "create_pipeline"}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline with selected modules\n",
        "pipeline = LiteraturePipeline([\n",
        "    PubMedModule(),                              # ì˜í•™/ìƒëª…ê³¼í•™\n",
        "    ArXivModule(),                               # CS/ë¬¼ë¦¬/ìˆ˜í•™\n",
        "    OSFModule(),                                 # ì‚¬íšŒê³¼í•™\n",
        "    GitHubModule(token=GITHUB_TOKEN),            # ì½”ë“œ/ë„êµ¬\n",
        "    SemanticScholarModule(api_key=SEMANTIC_SCHOLAR_KEY),  # í†µí•© ê²€ìƒ‰\n",
        "])\n",
        "\n",
        "print(\"ðŸ“‹ Current Pipeline:\")\n",
        "pipeline.list_modules()"
      ],
      "metadata": {"id": "create"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Customize pipeline\n",
        "\n",
        "# Remove a module\n",
        "# pipeline.remove(\"github\")\n",
        "\n",
        "# Reorder modules\n",
        "# pipeline.reorder([\"semantic_scholar\", \"pubmed\", \"arxiv\"])\n",
        "\n",
        "# Add custom module\n",
        "# pipeline.add(MyCustomModule())\n",
        "\n",
        "print(\"ðŸ“‹ Modified Pipeline:\")\n",
        "pipeline.list_modules()"
      ],
      "metadata": {"id": "customize"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Run Search"
      ],
      "metadata": {"id": "search"}
    },
    {
      "cell_type": "code",
      "source": [
        "# Your research query\n",
        "QUERY = \"large language models emotion recognition\"  # @param {type:\"string\"}\n",
        "\n",
        "# Run search\n",
        "results = pipeline.search(QUERY, max_per_source=5)"
      ],
      "metadata": {"id": "run_search"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SEARCH RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, paper in enumerate(results[:20], 1):\n",
        "    print(f\"\\n{i}. [{paper.source.upper()}] {paper.title[:70]}\")\n",
        "    if paper.authors:\n",
        "        print(f\"   Authors: {', '.join(paper.authors[:3])}\")\n",
        "    if paper.year:\n",
        "        print(f\"   Year: {paper.year}\")\n",
        "    if paper.citations:\n",
        "        print(f\"   Citations: {paper.citations}\")\n",
        "    print(f\"   URL: {paper.url}\")"
      ],
      "metadata": {"id": "display_results"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Export Results"
      ],
      "metadata": {"id": "export"}
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to JSON\n",
        "export_data = [\n",
        "    {\n",
        "        \"title\": p.title,\n",
        "        \"authors\": p.authors,\n",
        "        \"year\": p.year,\n",
        "        \"source\": p.source,\n",
        "        \"url\": p.url,\n",
        "        \"doi\": p.doi,\n",
        "        \"citations\": p.citations\n",
        "    }\n",
        "    for p in results\n",
        "]\n",
        "\n",
        "with open(\"literature_results.json\", \"w\") as f:\n",
        "    json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Export to markdown\n",
        "md_content = f\"# Literature Search: {QUERY}\\n\\n\"\n",
        "for i, p in enumerate(results, 1):\n",
        "    authors = \", \".join(p.authors[:3]) if p.authors else \"Unknown\"\n",
        "    md_content += f\"{i}. **{p.title}**\\n\"\n",
        "    md_content += f\"   - {authors} ({p.year or 'n.d.'})\\n\"\n",
        "    md_content += f\"   - Source: {p.source} | [Link]({p.url})\\n\\n\"\n",
        "\n",
        "with open(\"literature_results.md\", \"w\") as f:\n",
        "    f.write(md_content)\n",
        "\n",
        "print(\"âœ“ Exported to:\")\n",
        "print(\"  - literature_results.json\")\n",
        "print(\"  - literature_results.md\")\n",
        "\n",
        "# Download (Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(\"literature_results.json\")\n",
        "    files.download(\"literature_results.md\")\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {"id": "export_code"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 9. Create Your Own Module (Template)\n",
        "\n",
        "ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!"
      ],
      "metadata": {"id": "template"}
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCustomModule(BaseModule):\n",
        "    \"\"\"Template for creating custom database module\"\"\"\n",
        "    name = \"my_custom\"\n",
        "    requires_api_key = False\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def search(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        papers = []\n",
        "\n",
        "        # TODO: Implement your search logic here\n",
        "        # 1. Call your API\n",
        "        # 2. Parse response\n",
        "        # 3. Create Paper objects\n",
        "        # 4. Return list of Papers\n",
        "\n",
        "        # Example:\n",
        "        # resp = requests.get(\"https://api.example.com/search\", params={\"q\": query})\n",
        "        # for item in resp.json()[\"results\"]:\n",
        "        #     papers.append(Paper(\n",
        "        #         title=item[\"title\"],\n",
        "        #         authors=item[\"authors\"],\n",
        "        #         year=item[\"year\"],\n",
        "        #         abstract=item[\"abstract\"],\n",
        "        #         url=item[\"url\"],\n",
        "        #         source=\"my_custom\"\n",
        "        #     ))\n",
        "\n",
        "        return papers\n",
        "\n",
        "# Usage:\n",
        "# pipeline.add(MyCustomModule())"
      ],
      "metadata": {"id": "custom_template"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## API Reference\n",
        "\n",
        "| Database | API Docs | Rate Limit |\n",
        "|----------|----------|------------|\n",
        "| PubMed | [E-utilities](https://www.ncbi.nlm.nih.gov/books/NBK25500/) | No limit |\n",
        "| arXiv | [arXiv API](https://arxiv.org/help/api/index) | No limit |\n",
        "| OSF | [OSF API](https://developer.osf.io/) | No limit |\n",
        "| GitHub | [GitHub API](https://docs.github.com/en/rest) | 60/hr (free) |\n",
        "| Semantic Scholar | [S2 API](https://api.semanticscholar.org/) | 100/5min |\n",
        "\n",
        "---\n",
        "\n",
        "*SNU AI Psychology Workshop - February 2026*"
      ],
      "metadata": {"id": "footer"}
    }
  ]
}
