{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìù Manuscript Evaluator: 8-Agent Paper Review System\n",
        "\n",
        "**SNU AI Psychology Workshop - February 2026**\n",
        "\n",
        "This notebook evaluates academic manuscripts using Nature-level criteria with an 8-agent pipeline.\n",
        "\n",
        "## Workflow\n",
        "\n",
        "```\n",
        "Input Manuscript ‚Üí Hook Agent (30pts) ‚Üí Narrative Agent (30pts) ‚Üí Rigor Agent (20pts)\n",
        "                ‚Üí Impact Agent (20pts) ‚Üí Fatal Flaw Detector ‚Üí Title Evaluator\n",
        "                ‚Üí Reference Checker ‚Üí Improvement Planner ‚Üí Final Report (/100)\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup & Installation"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Install required packages\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "print(\"‚úì Installation complete\")"
      ],
      "metadata": {
        "id": "install"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Dict, Any, List\n",
        "from getpass import getpass\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "print(\"‚úì Libraries imported\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. API Configuration\n",
        "\n",
        "Get your API key from [Google AI Studio](https://aistudio.google.com/)"
      ],
      "metadata": {
        "id": "api_config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your Gemini API Key\n",
        "GEMINI_API_KEY = getpass(\"Enter your Gemini API Key: \")\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Model settings\n",
        "MODEL_NAME = 'gemini-2.5-flash'\n",
        "DELAY_BETWEEN_CALLS = 3  # seconds (to avoid rate limits)\n",
        "\n",
        "print(f\"‚úì API configured\")\n",
        "print(f\"‚úì Model: {MODEL_NAME}\")\n",
        "print(f\"‚úì Delay between calls: {DELAY_BETWEEN_CALLS}s\")"
      ],
      "metadata": {
        "id": "api_key"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load Your Manuscript\n",
        "\n",
        "Choose one method:\n",
        "- **Option A**: Upload a file (.md, .txt)\n",
        "- **Option B**: Paste text directly"
      ],
      "metadata": {
        "id": "load_manuscript"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: Upload file\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Upload your manuscript file (.md or .txt):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    manuscript_text = uploaded[filename].decode('utf-8')\n",
        "    print(f\"\\n‚úì Loaded: {filename}\")\n",
        "    print(f\"‚úì Length: {len(manuscript_text):,} characters\")\n",
        "    print(f\"‚úì Words: ~{len(manuscript_text.split()):,}\")"
      ],
      "metadata": {
        "id": "upload_file"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option B: Paste text directly (run this cell instead of upload)\n",
        "# Uncomment and paste your manuscript between the triple quotes\n",
        "\n",
        "# manuscript_text = \"\"\"\n",
        "# # Your Paper Title\n",
        "#\n",
        "# ## Abstract\n",
        "# Your abstract here...\n",
        "#\n",
        "# ## Introduction\n",
        "# Your introduction here...\n",
        "# \"\"\"\n",
        "#\n",
        "# print(f\"‚úì Manuscript loaded\")\n",
        "# print(f\"‚úì Length: {len(manuscript_text):,} characters\")"
      ],
      "metadata": {
        "id": "paste_text"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Data Structures & Helper Functions"
      ],
      "metadata": {
        "id": "structures"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class HookResult:\n",
        "    one_sentence_summary: str\n",
        "    opening_pattern: str\n",
        "    gap_type: str\n",
        "    opening_commentary: str\n",
        "    gap_commentary: str\n",
        "    score: int\n",
        "\n",
        "@dataclass\n",
        "class NarrativeResult:\n",
        "    logical_flow: str\n",
        "    narrative_control: str\n",
        "    flow_commentary: str\n",
        "    narrative_commentary: str\n",
        "    score: int\n",
        "\n",
        "@dataclass\n",
        "class RigorResult:\n",
        "    defensibility: str\n",
        "    claim_discipline: str\n",
        "    defensibility_commentary: str\n",
        "    claims_commentary: str\n",
        "    score: int\n",
        "\n",
        "@dataclass\n",
        "class ImpactResult:\n",
        "    conclusion_type: str\n",
        "    commentary: str\n",
        "    score: int\n",
        "\n",
        "@dataclass\n",
        "class FatalFlaws:\n",
        "    so_what: bool\n",
        "    logic_leap: bool\n",
        "    zombie_sloppy: bool\n",
        "    details: str\n",
        "\n",
        "@dataclass\n",
        "class TitleResult:\n",
        "    clarity: str\n",
        "    keyword_presence: str\n",
        "    length_assessment: str\n",
        "    engagement: str\n",
        "    commentary: str\n",
        "    score: int\n",
        "\n",
        "@dataclass\n",
        "class ReferenceCheckResult:\n",
        "    total_references: int\n",
        "    cited_in_text: List[int]\n",
        "    uncited_references: List[int]\n",
        "    missing_references: List[int]\n",
        "    hallucination_detected: bool\n",
        "    issues: str\n",
        "\n",
        "print(\"‚úì Data structures defined\")"
      ],
      "metadata": {
        "id": "dataclasses"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_api_call(model, prompt, max_retries=3):\n",
        "    \"\"\"API call with retry logic\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(\n",
        "                prompt,\n",
        "                generation_config={'temperature': 0.2}\n",
        "            )\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            if \"ResourceExhausted\" in str(e) or \"429\" in str(e):\n",
        "                wait_time = (attempt + 1) * 10\n",
        "                print(f\"   ‚ö†Ô∏è Rate limit. Waiting {wait_time}s... ({attempt+1}/{max_retries})\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise e\n",
        "    raise Exception(\"Max retries exceeded\")\n",
        "\n",
        "def parse_json_response(response_text):\n",
        "    \"\"\"Extract JSON from response\"\"\"\n",
        "    json_start = response_text.find('{')\n",
        "    json_end = response_text.rfind('}') + 1\n",
        "    if json_start == -1 or json_end == 0:\n",
        "        raise ValueError(\"No JSON found in response\")\n",
        "    return json.loads(response_text[json_start:json_end])\n",
        "\n",
        "def extract_title(manuscript_text: str) -> str:\n",
        "    \"\"\"Extract title from manuscript\"\"\"\n",
        "    lines = manuscript_text.split('\\n')\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('# '):\n",
        "            return line[2:].strip()\n",
        "    return \"\"\n",
        "\n",
        "def extract_references(manuscript_text: str) -> tuple:\n",
        "    \"\"\"Extract References section\"\"\"\n",
        "    ref_match = re.search(r'##\\s*References?\\s*\\n', manuscript_text, re.IGNORECASE)\n",
        "    if not ref_match:\n",
        "        return [], \"\"\n",
        "    ref_text = manuscript_text[ref_match.end():]\n",
        "    references = [line.strip() for line in ref_text.split('\\n') if re.match(r'^\\[\\d+\\]', line.strip())]\n",
        "    return references, ref_text\n",
        "\n",
        "def extract_citations(manuscript_text: str) -> List[int]:\n",
        "    \"\"\"Extract citation numbers from text\"\"\"\n",
        "    ref_match = re.search(r'##\\s*References?\\s*\\n', manuscript_text, re.IGNORECASE)\n",
        "    text = manuscript_text[:ref_match.start()] if ref_match else manuscript_text\n",
        "    \n",
        "    citations = set()\n",
        "    for match in re.finditer(r'\\[(\\d+(?:,\\s*\\d+|\\s*-\\s*\\d+)*)\\]', text):\n",
        "        citation_str = match.group(1)\n",
        "        if '-' in citation_str:\n",
        "            parts = citation_str.split('-')\n",
        "            citations.update(range(int(parts[0].strip()), int(parts[1].strip()) + 1))\n",
        "        else:\n",
        "            for num in citation_str.split(','):\n",
        "                citations.add(int(num.strip()))\n",
        "    return sorted(list(citations))\n",
        "\n",
        "print(\"‚úì Helper functions defined\")"
      ],
      "metadata": {
        "id": "helpers"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Define All 8 Agents"
      ],
      "metadata": {
        "id": "agents"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_hook_agent(manuscript_text: str) -> HookResult:\n",
        "    \"\"\"Agent 1: Evaluate Hook (Abstract & Introduction) - 30 points\"\"\"\n",
        "    model = genai.GenerativeModel(MODEL_NAME)\n",
        "    \n",
        "    prompt = f\"\"\"You are a Nature editor evaluating the HOOK of a manuscript.\n",
        "\n",
        "MANUSCRIPT (First 5000 chars):\n",
        "{manuscript_text[:5000]}\n",
        "\n",
        "EVALUATE:\n",
        "1. ONE-SENTENCE SUMMARY: Main take-home message\n",
        "2. OPENING PATTERN: HIGH_IMPACT | BROAD_TO_NARROW | WEAK_NARROW\n",
        "3. GAP DEFINITION: CONCEPTUAL | METHODOLOGICAL | INCREMENTAL | UNCLEAR\n",
        "4. SCORE (0-30): 27-30 Exceptional, 24-26 Strong, 20-23 Adequate, <20 Weak\n",
        "\n",
        "Return JSON:\n",
        "{{\n",
        "    \"one_sentence_summary\": \"...\",\n",
        "    \"opening_pattern\": \"...\",\n",
        "    \"gap_type\": \"...\",\n",
        "    \"opening_commentary\": \"2-3 sentences\",\n",
        "    \"gap_commentary\": \"2-3 sentences\",\n",
        "    \"score\": 0-30\n",
        "}}\n",
        "\"\"\"\n",
        "    response = safe_api_call(model, prompt)\n",
        "    return HookResult(**parse_json_response(response.text))\n",
        "\n",
        "\n",
        "def run_narrative_agent(manuscript_text: str) -> NarrativeResult:\n",
        "    \"\"\"Agent 2: Evaluate Narrative Quality - 30 points\"\"\"\n",
        "    model = genai.GenerativeModel(MODEL_NAME)\n",
        "    \n",
        "    prompt = f\"\"\"You are a Nature editor evaluating NARRATIVE quality.\n",
        "\n",
        "MANUSCRIPT (8000 chars):\n",
        "{manuscript_text[5000:13000]}\n",
        "\n",
        "EVALUATE:\n",
        "1. LOGICAL FLOW: SEAMLESS | FUNCTIONAL | CHOPPY\n",
        "2. NARRATIVE CONTROL: DIRECTOR | PASSIVE\n",
        "3. SCORE (0-30)\n",
        "\n",
        "Return JSON:\n",
        "{{\n",
        "    \"logical_flow\": \"...\",\n",
        "    \"narrative_control\": \"...\",\n",
        "    \"flow_commentary\": \"2-3 sentences\",\n",
        "    \"narrative_commentary\": \"2-3 sentences\",\n",
        "    \"score\": 0-30\n",
        "}}\n",
        "\"\"\"\n",
        "    response = safe_api_call(model, prompt)\n",
        "    return NarrativeResult(**parse_json_response(response.text))\n",
        "\n",
        "\n",
        "def run_rigor_agent(manuscript_text: str) -> RigorResult:\n",
        "    \"\"\"Agent 3: Evaluate Rigor - 20 points\"\"\"\n",
        "    model = genai.GenerativeModel(MODEL_NAME)\n",
        "    \n",
        "    prompt = f\"\"\"You are a Nature editor evaluating RIGOR.\n",
        "\n",
        "MANUSCRIPT (10000 chars):\n",
        "{manuscript_text[8000:18000]}\n",
        "\n",
        "EVALUATE:\n",
        "1. DEFENSIBILITY: BULLETPROOF | VULNERABLE\n",
        "2. CLAIM DISCIPLINE: PRECISE | OVERCLAIMING | VAGUE\n",
        "3. SCORE (0-20)\n",
        "\n",
        "Return JSON:\n",
        "{{\n",
        "    \"defensibility\": \"...\",\n",
        "    \"claim_discipline\": \"...\",\n",
        "    \"defensibility_commentary\": \"2-3 sentences\",\n",
        "    \"claims_commentary\": \"2-3 sentences\",\n",
        "    \"score\": 0-20\n",
        "}}\n",
        "\"\"\"\n",
        "    response = safe_api_call(model, prompt)\n",
        "    return RigorResult(**parse_json_response(response.text))\n",
        "\n",
        "\n",
        "def run_impact_agent(manuscript_text: str) -> ImpactResult:\n",
        "    \"\"\"Agent 4: Evaluate Impact - 20 points\"\"\"\n",
        "    model = genai.GenerativeModel(MODEL_NAME)\n",
        "    \n",
        "    prompt = f\"\"\"You are a Nature editor evaluating IMPACT.\n",
        "\n",
        "MANUSCRIPT (Last 10000 chars):\n",
        "{manuscript_text[-10000:]}\n",
        "\n",
        "EVALUATE:\n",
        "1. CONCLUSION TYPE: FULL_CIRCLE | SUMMARY_ONLY\n",
        "2. SCORE (0-20)\n",
        "\n",
        "Return JSON:\n",
        "{{\n",
        "    \"conclusion_type\": \"...\",\n",
        "    \"commentary\": \"2-3 sentences\",\n",
        "    \"score\": 0-20\n",
        "}}\n",
        "\"\"\"\n",
        "    response = safe_api_call(model, prompt)\n",
        "    return ImpactResult(**parse_json_response(response.text))\n",
        "\n",
        "\n",
        "def run_fatal_flaw_detector(manuscript_text: str, hook: HookResult, narrative: NarrativeResult) -> FatalFlaws:\n",
        "    \"\"\"Agent 5: Detect Fatal Flaws\"\"\"\n",
        "    model = genai.GenerativeModel(MODEL_NAME)\n",
        "    \n",
        "    prompt = f\"\"\"You are a Nature editor checking for FATAL FLAWS.\n",
        "\n",
        "MANUSCRIPT (15000 chars):\n",
        "{manuscript_text[:15000]}\n",
        "\n",
        "Previous: Gap={hook.gap_type}, Flow={narrative.logical_flow}\n",
        "\n",
        "CHECK:\n",
        "1. \"SO WHAT?\" - Irrelevant study?\n",
        "2. \"LOGIC LEAP\" - Unexplained jumps?\n",
        "3. \"ZOMBIE/SLOPPY\" - Typos, AI-like writing?\n",
        "\n",
        "Return JSON:\n",
        "{{\n",
        "    \"so_what\": true|false,\n",
        "    \"logic_leap\": true|false,\n",
        "    \"zombie_sloppy\": true|false,\n",
        "    \"details\": \"Explain or 'None detected'\"\n",
        "}}\n",
        "\"\"\"\n",
        "    response = safe_api_call(model, prompt)\n",
        "    return FatalFlaws(**parse_json_response(response.text))\n",
        "\n",
        "\n",
        "def run_title_evaluator(manuscript_text: str) -> TitleResult:\n",
        "    \"\"\"Agent 6: Evaluate Title\"\"\"\n",
        "    model = genai.GenerativeModel(MODEL_NAME)\n",
        "    title = extract_title(manuscript_text)\n",
        "    \n",
        "    if not title:\n",
        "        return TitleResult(\"MISSING\", \"N/A\", \"N/A\", \"N/A\", \"No title found\", 0)\n",
        "    \n",
        "    prompt = f\"\"\"Evaluate this manuscript TITLE:\n",
        "\n",
        "TITLE: {title}\n",
        "CONTEXT: {manuscript_text[:3000]}\n",
        "\n",
        "EVALUATE:\n",
        "1. CLARITY: CRYSTAL_CLEAR | ADEQUATE | VAGUE\n",
        "2. KEYWORDS: STRONG | PARTIAL | WEAK\n",
        "3. LENGTH: OPTIMAL(10-15) | ACCEPTABLE(7-9,16-20) | TOO_SHORT | TOO_LONG\n",
        "4. ENGAGEMENT: HIGH | MEDIUM | LOW\n",
        "5. SCORE (0-10)\n",
        "\n",
        "Return JSON:\n",
        "{{\n",
        "    \"clarity\": \"...\",\n",
        "    \"keyword_presence\": \"...\",\n",
        "    \"length_assessment\": \"...\",\n",
        "    \"engagement\": \"...\",\n",
        "    \"commentary\": \"2-3 sentences\",\n",
        "    \"score\": 0-10\n",
        "}}\n",
        "\"\"\"\n",
        "    response = safe_api_call(model, prompt)\n",
        "    return TitleResult(**parse_json_response(response.text))\n",
        "\n",
        "\n",
        "def run_reference_checker(manuscript_text: str) -> ReferenceCheckResult:\n",
        "    \"\"\"Agent 7: Check References\"\"\"\n",
        "    cited = extract_citations(manuscript_text)\n",
        "    refs, _ = extract_references(manuscript_text)\n",
        "    ref_nums = [int(re.match(r'^\\[(\\d+)\\]', r).group(1)) for r in refs if re.match(r'^\\[(\\d+)\\]', r)]\n",
        "    \n",
        "    uncited = [n for n in ref_nums if n not in cited]\n",
        "    missing = [n for n in cited if n not in ref_nums]\n",
        "    \n",
        "    issues = []\n",
        "    if missing:\n",
        "        issues.append(f\"‚ö†Ô∏è HALLUCINATION: Missing refs: {missing}\")\n",
        "    if uncited:\n",
        "        issues.append(f\"üìù Uncited refs: {uncited}\")\n",
        "    if not issues:\n",
        "        issues.append(\"‚úì All citations match\")\n",
        "    \n",
        "    return ReferenceCheckResult(\n",
        "        total_references=len(ref_nums),\n",
        "        cited_in_text=cited,\n",
        "        uncited_references=uncited,\n",
        "        missing_references=missing,\n",
        "        hallucination_detected=len(missing) > 0,\n",
        "        issues=\"\\n\".join(issues)\n",
        "    )\n",
        "\n",
        "\n",
        "def run_improvement_planner(manuscript_text, hook, narrative, rigor, impact, flaws, title) -> str:\n",
        "    \"\"\"Agent 8: Generate Improvement Plan\"\"\"\n",
        "    model = genai.GenerativeModel(MODEL_NAME)\n",
        "    total = hook.score + narrative.score + rigor.score + impact.score\n",
        "    \n",
        "    prompt = f\"\"\"Create an IMPROVEMENT PLAN.\n",
        "\n",
        "SCORES:\n",
        "- Total: {total}/100\n",
        "- Hook: {hook.score}/30 ({hook.gap_type})\n",
        "- Narrative: {narrative.score}/30 ({narrative.logical_flow})\n",
        "- Rigor: {rigor.score}/20 ({rigor.defensibility})\n",
        "- Impact: {impact.score}/20 ({impact.conclusion_type})\n",
        "- Flaws: {flaws.details}\n",
        "\n",
        "MANUSCRIPT: {manuscript_text[:8000]}\n",
        "\n",
        "Generate markdown with:\n",
        "1. Current Score Summary\n",
        "2. Priority fixes (if score < 90)\n",
        "3. Specific improvements for weak areas\n",
        "4. Expected outcome after fixes\n",
        "\"\"\"\n",
        "    response = safe_api_call(model, prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "print(\"‚úì All 8 agents defined\")"
      ],
      "metadata": {
        "id": "agent_definitions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Run All Agents"
      ],
      "metadata": {
        "id": "run_agents"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all 8 agents\n",
        "print(\"=\"*60)\n",
        "print(\"MANUSCRIPT EVALUATION - 8 AGENT PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "# Agent 1: Hook\n",
        "print(\"[1/8] Hook Agent...\")\n",
        "hook_result = run_hook_agent(manuscript_text)\n",
        "print(f\"      ‚úì Score: {hook_result.score}/30 | {hook_result.gap_type}\")\n",
        "time.sleep(DELAY_BETWEEN_CALLS)\n",
        "\n",
        "# Agent 2: Narrative\n",
        "print(\"[2/8] Narrative Agent...\")\n",
        "narrative_result = run_narrative_agent(manuscript_text)\n",
        "print(f\"      ‚úì Score: {narrative_result.score}/30 | {narrative_result.logical_flow}\")\n",
        "time.sleep(DELAY_BETWEEN_CALLS)\n",
        "\n",
        "# Agent 3: Rigor\n",
        "print(\"[3/8] Rigor Agent...\")\n",
        "rigor_result = run_rigor_agent(manuscript_text)\n",
        "print(f\"      ‚úì Score: {rigor_result.score}/20 | {rigor_result.defensibility}\")\n",
        "time.sleep(DELAY_BETWEEN_CALLS)\n",
        "\n",
        "# Agent 4: Impact\n",
        "print(\"[4/8] Impact Agent...\")\n",
        "impact_result = run_impact_agent(manuscript_text)\n",
        "print(f\"      ‚úì Score: {impact_result.score}/20 | {impact_result.conclusion_type}\")\n",
        "time.sleep(DELAY_BETWEEN_CALLS)\n",
        "\n",
        "# Agent 5: Fatal Flaws\n",
        "print(\"[5/8] Fatal Flaw Detector...\")\n",
        "fatal_flaws = run_fatal_flaw_detector(manuscript_text, hook_result, narrative_result)\n",
        "has_flaw = fatal_flaws.so_what or fatal_flaws.logic_leap or fatal_flaws.zombie_sloppy\n",
        "print(f\"      {'‚ö†Ô∏è FLAWS DETECTED' if has_flaw else '‚úì No fatal flaws'}\")\n",
        "time.sleep(DELAY_BETWEEN_CALLS)\n",
        "\n",
        "# Agent 6: Title\n",
        "print(\"[6/8] Title Evaluator...\")\n",
        "title_result = run_title_evaluator(manuscript_text)\n",
        "print(f\"      ‚úì Score: {title_result.score}/10 | {title_result.clarity}\")\n",
        "time.sleep(DELAY_BETWEEN_CALLS)\n",
        "\n",
        "# Agent 7: References\n",
        "print(\"[7/8] Reference Checker...\")\n",
        "ref_check = run_reference_checker(manuscript_text)\n",
        "print(f\"      {'‚ö†Ô∏è Issues found' if ref_check.hallucination_detected else '‚úì References OK'}\")\n",
        "time.sleep(DELAY_BETWEEN_CALLS)\n",
        "\n",
        "# Agent 8: Improvement Plan\n",
        "print(\"[8/8] Improvement Planner...\")\n",
        "improvement_plan = run_improvement_planner(\n",
        "    manuscript_text, hook_result, narrative_result,\n",
        "    rigor_result, impact_result, fatal_flaws, title_result\n",
        ")\n",
        "print(\"      ‚úì Plan generated\")\n",
        "\n",
        "print()\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ ALL AGENTS COMPLETE\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "run_all"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Final Report"
      ],
      "metadata": {
        "id": "final_report"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total score\n",
        "total_score = hook_result.score + narrative_result.score + rigor_result.score + impact_result.score\n",
        "\n",
        "# Tier classification\n",
        "if total_score >= 90:\n",
        "    tier = \"üèÜ TOP 5% - Nature/Science Level\"\n",
        "elif total_score >= 80:\n",
        "    tier = \"ü•á TOP 10% - Cell/PNAS Level\"\n",
        "elif total_score >= 70:\n",
        "    tier = \"ü•à SOLID - Top Specialty Journal\"\n",
        "elif total_score >= 60:\n",
        "    tier = \"ü•â ACCEPTABLE - Revision Needed\"\n",
        "else:\n",
        "    tier = \"‚≠ï WEAK - Major Revision Required\"\n",
        "\n",
        "# Display final report\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"                    FINAL EVALUATION REPORT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n{'Dimension':<35} {'Score':>15}\")\n",
        "print(\"-\"*52)\n",
        "print(f\"{'Hook (Opening & Gap)':<35} {hook_result.score:>3} / 30\")\n",
        "print(f\"{'Narrative (Flow & Voice)':<35} {narrative_result.score:>3} / 30\")\n",
        "print(f\"{'Rigor (Methods & Claims)':<35} {rigor_result.score:>3} / 20\")\n",
        "print(f\"{'Impact (Discussion)':<35} {impact_result.score:>3} / 20\")\n",
        "print(\"-\"*52)\n",
        "print(f\"{'TOTAL SCORE':<35} {total_score:>3} / 100\")\n",
        "print()\n",
        "print(f\"TIER: {tier}\")\n",
        "print()\n",
        "\n",
        "# Quality checks\n",
        "print(\"QUALITY CHECKS:\")\n",
        "print(\"-\"*52)\n",
        "title = extract_title(manuscript_text)\n",
        "if title:\n",
        "    print(f\"Title: \\\"{title[:50]}...\\\"\")\n",
        "print(f\"Title Score: {title_result.score}/10 ({title_result.clarity})\")\n",
        "print(f\"References: {'‚ö†Ô∏è ISSUES' if ref_check.hallucination_detected else '‚úì OK'}\")\n",
        "print()\n",
        "\n",
        "# Fatal flaws warning\n",
        "if fatal_flaws.so_what or fatal_flaws.logic_leap or fatal_flaws.zombie_sloppy:\n",
        "    print(\"üö® FATAL FLAW DETECTED - AUTO REJECT\")\n",
        "    print(f\"   {fatal_flaws.details[:200]}...\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "display_report"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Detailed Feedback"
      ],
      "metadata": {
        "id": "detailed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüìù ONE-SENTENCE SUMMARY:\")\n",
        "print(f\"   {hook_result.one_sentence_summary}\")\n",
        "\n",
        "print(\"\\nüéØ HOOK ANALYSIS:\")\n",
        "print(f\"   Opening: {hook_result.opening_pattern}\")\n",
        "print(f\"   {hook_result.opening_commentary}\")\n",
        "print(f\"   Gap: {hook_result.gap_type}\")\n",
        "print(f\"   {hook_result.gap_commentary}\")\n",
        "\n",
        "print(\"\\nüìñ NARRATIVE ANALYSIS:\")\n",
        "print(f\"   Flow: {narrative_result.logical_flow}\")\n",
        "print(f\"   {narrative_result.flow_commentary}\")\n",
        "print(f\"   Control: {narrative_result.narrative_control}\")\n",
        "print(f\"   {narrative_result.narrative_commentary}\")\n",
        "\n",
        "print(\"\\nüî¨ RIGOR ANALYSIS:\")\n",
        "print(f\"   Defensibility: {rigor_result.defensibility}\")\n",
        "print(f\"   {rigor_result.defensibility_commentary}\")\n",
        "print(f\"   Claims: {rigor_result.claim_discipline}\")\n",
        "print(f\"   {rigor_result.claims_commentary}\")\n",
        "\n",
        "print(\"\\nüéØ IMPACT ANALYSIS:\")\n",
        "print(f\"   Conclusion: {impact_result.conclusion_type}\")\n",
        "print(f\"   {impact_result.commentary}\")"
      ],
      "metadata": {
        "id": "detailed_feedback"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Improvement Plan"
      ],
      "metadata": {
        "id": "improvement"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"                    IMPROVEMENT PLAN\")\n",
        "print(\"=\"*70)\n",
        "print()\n",
        "\n",
        "display(Markdown(improvement_plan))"
      ],
      "metadata": {
        "id": "show_improvement"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Download Results"
      ],
      "metadata": {
        "id": "download"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to JSON\n",
        "results = {\n",
        "    \"total_score\": total_score,\n",
        "    \"tier\": tier,\n",
        "    \"hook\": asdict(hook_result),\n",
        "    \"narrative\": asdict(narrative_result),\n",
        "    \"rigor\": asdict(rigor_result),\n",
        "    \"impact\": asdict(impact_result),\n",
        "    \"fatal_flaws\": asdict(fatal_flaws),\n",
        "    \"title\": asdict(title_result),\n",
        "    \"reference_check\": asdict(ref_check),\n",
        "    \"improvement_plan\": improvement_plan\n",
        "}\n",
        "\n",
        "# Save JSON\n",
        "with open('evaluation_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Save markdown report\n",
        "report_md = f\"\"\"# Manuscript Evaluation Report\n",
        "\n",
        "## Score: {total_score}/100\n",
        "**Tier**: {tier}\n",
        "\n",
        "| Dimension | Score |\n",
        "|-----------|-------|\n",
        "| Hook | {hook_result.score}/30 |\n",
        "| Narrative | {narrative_result.score}/30 |\n",
        "| Rigor | {rigor_result.score}/20 |\n",
        "| Impact | {impact_result.score}/20 |\n",
        "\n",
        "## One-Sentence Summary\n",
        "{hook_result.one_sentence_summary}\n",
        "\n",
        "## Improvement Plan\n",
        "{improvement_plan}\n",
        "\n",
        "---\n",
        "*Generated by 8-Agent Manuscript Evaluator*\n",
        "\"\"\"\n",
        "\n",
        "with open('evaluation_report.md', 'w') as f:\n",
        "    f.write(report_md)\n",
        "\n",
        "# Download files\n",
        "from google.colab import files\n",
        "files.download('evaluation_results.json')\n",
        "files.download('evaluation_report.md')\n",
        "\n",
        "print(\"\\n‚úÖ Files ready for download!\")"
      ],
      "metadata": {
        "id": "save_download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## About This Tool\n",
        "\n",
        "This 8-agent pipeline evaluates manuscripts based on Nature editor criteria:\n",
        "\n",
        "| Agent | Focus | Points |\n",
        "|-------|-------|--------|\n",
        "| Hook | Opening & Gap | 30 |\n",
        "| Narrative | Flow & Voice | 30 |\n",
        "| Rigor | Methods & Claims | 20 |\n",
        "| Impact | Discussion | 20 |\n",
        "| Fatal Flaw | Auto-reject check | - |\n",
        "| Title | Title quality | 10 |\n",
        "| Reference | Citation check | - |\n",
        "| Improvement | Action plan | - |\n",
        "\n",
        "**Model**: Gemini 2.5 Flash\n",
        "\n",
        "---\n",
        "*SNU AI Psychology Workshop - February 2026*"
      ],
      "metadata": {
        "id": "about"
      }
    }
  ]
}
