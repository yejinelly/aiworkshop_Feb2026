{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Deep Dive - ë¬¸í—Œ ê²€ìƒ‰ (Agent Laboratory / PaSa)\n",
    "\n",
    "**SNU AI Psychology Workshop - February 2026**\n",
    "\n",
    "---\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "1. Agent Laboratoryì™€ PaSaì˜ ì½”ë“œ êµ¬ì¡° ì´í•´\n",
    "2. arXiv/Scholar ìë™ ê²€ìƒ‰ ì‹¤í–‰\n",
    "3. ê²€ìƒ‰ ë¡œì§ ì»¤ìŠ¤í…€ (ì¿¼ë¦¬, í•„í„°, ì •ë ¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: í™˜ê²½ ì„¤ì • (Colab/ë¡œì»¬ ìë™ ê°ì§€)\nimport os\nimport sys\n\n# Colab í™˜ê²½ ê°ì§€\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive/')\n    WORKSHOP_DIR = \"/content/drive/MyDrive/aiworkshop_Feb2026/\"\n    os.makedirs(WORKSHOP_DIR, exist_ok=True)\n    os.chdir(WORKSHOP_DIR)\n    IN_COLAB = True\n    print(\"ğŸŒ Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘\")\nexcept ImportError:\n    # ë¡œì»¬ í™˜ê²½: í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™\n    current_dir = os.getcwd()\n    if current_dir.endswith('notebooks'):\n        os.chdir('..')\n    WORKSHOP_DIR = os.getcwd()\n    IN_COLAB = False\n    print(\"ğŸ’» ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘\")\n\nprint(f\"ì‘ì—… í´ë”: {WORKSHOP_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: íŒ¨í‚¤ì§€ ì„¤ì¹˜ + ì €ì¥ì†Œ í´ë¡ \n# Colab: ìë™ ì„¤ì¹˜\n# ë¡œì»¬: pyproject.tomlë¡œ ë¯¸ë¦¬ ì„¤ì¹˜ í•„ìš” (uv pip install -e . ë˜ëŠ” pip install -e .)\n\nif IN_COLAB:\n    print(\"ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\n    %pip install arxiv openai python-dotenv pandas -q\n    \n    # Agent Laboratory í´ë¡ \n    !git clone https://github.com/SamuelSchmidgall/AgentLaboratory.git 2>/dev/null || echo \"  âœ“ AgentLaboratory already cloned\"\n    \n    # PaSa í´ë¡   \n    !git clone https://github.com/bytedance/pasa.git 2>/dev/null || echo \"  âœ“ PaSa already cloned\"\n    \n    print(\"âœ… ì„¤ì¹˜ ì™„ë£Œ!\")\nelse:\n    print(\"âœ… ë¡œì»¬ í™˜ê²½: pyproject.tomlë¡œ ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ ì‚¬ìš©\")\n    print(\"   (ë¯¸ì„¤ì¹˜ ì‹œ: uv pip install -e . ë˜ëŠ” pip install -e .)\")\n    \n    # ë¡œì»¬ì—ì„œ ì €ì¥ì†Œ í´ë¡  (WORKSHOP_DIR ê¸°ì¤€, ìˆìœ¼ë©´ ê±´ë„ˆëœ€)\n    import subprocess\n    repos = [\n        (\"AgentLaboratory\", \"https://github.com/SamuelSchmidgall/AgentLaboratory.git\"),\n        (\"pasa\", \"https://github.com/bytedance/pasa.git\")\n    ]\n    \n    for name, url in repos:\n        repo_path = os.path.join(WORKSHOP_DIR, name)\n        if not os.path.exists(repo_path):\n            print(f\"   ğŸ“¥ {name} í´ë¡  ì¤‘...\")\n            subprocess.run([\"git\", \"clone\", url, repo_path], capture_output=True)\n            print(f\"   âœ… {name} í´ë¡  ì™„ë£Œ\")\n        else:\n            print(f\"   âœ“ {name} already exists\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: API Key ë¡œë”©\nimport os\nfrom pathlib import Path\n\n# Colab í™˜ê²½ ì²´í¬\nif IN_COLAB:\n    # Colab userdataì—ì„œ í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°\n    try:\n        from google.colab import userdata\n        OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n        GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n        print(\"âœ… Colab Secretsì—ì„œ API Key ë¡œë”© ì™„ë£Œ\")\n    except:\n        print(\"âš ï¸ Colab Secretsì— API KEYë¥¼ ì¶”ê°€í•˜ì„¸ìš”\")\n        OPENAI_API_KEY = None\n        GEMINI_API_KEY = None\nelse:\n    # ë¡œì»¬ í™˜ê²½: dotenv ì‚¬ìš©\n    try:\n        from dotenv import load_dotenv\n        load_dotenv()\n        OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n        GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n        \n        if OPENAI_API_KEY or GEMINI_API_KEY:\n            print(\"âœ… .env íŒŒì¼ì—ì„œ API Key ë¡œë”© ì™„ë£Œ\")\n        else:\n            print(\"âš ï¸ .env íŒŒì¼ì— API KEYë¥¼ ì¶”ê°€í•˜ì„¸ìš”\")\n    except ImportError:\n        print(\"âš ï¸ python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤\")\n        OPENAI_API_KEY = None\n        GEMINI_API_KEY = None\n\n# í™˜ê²½ë³€ìˆ˜ ì„¤ì •\nos.environ['OPENAI_API_KEY'] = OPENAI_API_KEY or ''\nos.environ['GEMINI_API_KEY'] = GEMINI_API_KEY or ''\n\n# í™•ì¸\nprint(f\"\\nğŸ”‘ OpenAI: {'ì„¤ì •ë¨' if OPENAI_API_KEY else 'ì—†ìŒ (LLM ê¸°ëŠ¥ ì œí•œ)'}\")\nprint(f\"ğŸ”‘ Gemini: {'ì„¤ì •ë¨' if GEMINI_API_KEY else 'ì—†ìŒ (LLM ê¸°ëŠ¥ ì œí•œ)'}\")\nprint(\"ğŸ’¡ arXiv ê²€ìƒ‰ì€ API key ì—†ì´ë„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. íŒŒì•…í•˜ê¸°: Agent Laboratory ì½”ë“œ êµ¬ì¡°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Agent Laboratory êµ¬ì¡°\n\n```\nAgentLaboratory/\nâ”œâ”€â”€ agents/\nâ”‚   â””â”€â”€ literature_agent.py   # ë¬¸í—Œ ê²€ìƒ‰ ì—ì´ì „íŠ¸ â­\nâ”œâ”€â”€ tools/\nâ”‚   â””â”€â”€ arxiv_search.py       # arXiv API ë˜í¼ â­\nâ””â”€â”€ run.py                    # ë©”ì¸ ì‹¤í–‰\n```\nğŸ’¡ ì‹¤í—˜ ì„¤ê³„/ë³´ê³ ì„œ ê¸°ëŠ¥ë„ ìˆì§€ë§Œ, ì›Œí¬ìƒµì—ì„œëŠ” **ë¬¸í—Œ ê²€ìƒ‰**ë§Œ ì‚¬ìš©\n\n**í•µì‹¬ íë¦„:**\n1. ì—°êµ¬ ì£¼ì œ ì…ë ¥\n2. `literature_agent`ê°€ arXivì—ì„œ ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰\n3. ë…¼ë¬¸ ìš”ì•½ ë° ê´€ë ¨ì„± í‰ê°€"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Agent Laboratory í•µì‹¬ ëª¨ë“ˆ í™•ì¸\n",
    "import os\n",
    "\n",
    "# í´ë” êµ¬ì¡° í™•ì¸\n",
    "agent_lab_path = \"AgentLaboratory\"\n",
    "if os.path.exists(agent_lab_path):\n",
    "    print(\"ğŸ“ AgentLaboratory êµ¬ì¡°:\")\n",
    "    for root, dirs, files in os.walk(agent_lab_path):\n",
    "        level = root.replace(agent_lab_path, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files[:5]:  # ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ\n",
    "            if file.endswith('.py'):\n",
    "                print(f\"{subindent}{file}\")\n",
    "else:\n",
    "    print(\"AgentLaboratoryê°€ í´ë¡ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Cell 2ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: arXiv ê²€ìƒ‰ ëª¨ë“ˆ ì§ì ‘ ì‚´í´ë³´ê¸°\nimport arxiv\n\n# arXiv ê²€ìƒ‰ í•¨ìˆ˜ (Agent Laboratory ìŠ¤íƒ€ì¼)\ndef search_arxiv_papers(query, max_results=10, sort_by=\"relevance\"):\n    \"\"\"\n    Agent Laboratoryì˜ arxiv_search.py í•µì‹¬ ë¡œì§\n    \"\"\"\n    sort_criterion = {\n        \"relevance\": arxiv.SortCriterion.Relevance,\n        \"date\": arxiv.SortCriterion.SubmittedDate,\n    }.get(sort_by, arxiv.SortCriterion.Relevance)\n    \n    client = arxiv.Client()\n    search = arxiv.Search(\n        query=query,\n        max_results=max_results,\n        sort_by=sort_criterion\n    )\n    \n    papers = []\n    for result in client.results(search):\n        papers.append({\n            \"title\": result.title,\n            \"authors\": [a.name for a in result.authors],\n            \"abstract\": result.summary,\n            \"published\": result.published.strftime(\"%Y-%m-%d\"),\n            \"pdf_url\": result.pdf_url,\n            \"arxiv_id\": result.entry_id.split('/')[-1]\n        })\n    return papers\n\nprint(\"search_arxiv_papers í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\nprint(f\"ì‹œê·¸ë‹ˆì²˜: search_arxiv_papers(query, max_results=10, sort_by='relevance')\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PaSa (Paper Search Agent) êµ¬ì¡°\n",
    "\n",
    "```\n",
    "pasa/\n",
    "â”œâ”€â”€ agents/\n",
    "â”‚   â”œâ”€â”€ crawler.py     # ë…¼ë¬¸ í¬ë¡¤ë§ ì—ì´ì „íŠ¸ â­\n",
    "â”‚   â””â”€â”€ selector.py    # ê´€ë ¨ì„± í‰ê°€ (PPO í•™ìŠµë¨) â­\n",
    "â”œâ”€â”€ search/\n",
    "â”‚   â”œâ”€â”€ arxiv.py       # arXiv ê²€ìƒ‰\n",
    "â”‚   â””â”€â”€ scholar.py     # Google Scholar ê²€ìƒ‰\n",
    "â””â”€â”€ run_search.py\n",
    "```\n",
    "\n",
    "**í•µì‹¬ ì°¨ì´ì :**\n",
    "- PaSaëŠ” **PPOë¡œ í•™ìŠµëœ Selector**ê°€ ê´€ë ¨ì„± í‰ê°€\n",
    "- Agent Laboratoryë³´ë‹¤ ë” ì •êµí•œ í•„í„°ë§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ì§ˆë¬¸\n\në‘ ë„êµ¬ì˜ êµ¬ì¡°ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”:\n- Agent Laboratory: ë¬¸í—Œ ê²€ìƒ‰ ìë™í™” (ì›Œí¬ìƒµì—ì„œëŠ” ì´ ë¶€ë¶„ë§Œ ì‚¬ìš©)\n- PaSa: ê²€ìƒ‰ íŠ¹í™” (PPO ê¸°ë°˜ ê´€ë ¨ì„± í‰ê°€)\n\në³¸ì¸ ì—°êµ¬ì—ëŠ” ì–´ë–¤ ì ‘ê·¼ì´ ë” ë§ì„ê¹Œìš”?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ì¨ë³´ê¸°: arXiv ë…¼ë¬¸ ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìƒ˜í”Œ ë°ì´í„°\n",
    "\n",
    "í…ŒìŠ¤íŠ¸í•  ì—°êµ¬ ì£¼ì œ:\n",
    "- \"LLM reasoning and cognitive psychology\"\n",
    "- \"AI agent for scientific discovery\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: ìƒ˜í”Œ ê²€ìƒ‰ ì‹¤í–‰ (ë°œí‘œì ë°ëª¨)\n",
    "sample_query = \"LLM reasoning cognitive psychology\"\n",
    "\n",
    "print(f\"ğŸ” ê²€ìƒ‰ì–´: {sample_query}\\n\")\n",
    "results = search_arxiv_papers(sample_query, max_results=5)\n",
    "\n",
    "print(f\"ğŸ“š ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë…¼ë¬¸\\n\")\n",
    "for i, paper in enumerate(results, 1):\n",
    "    print(f\"{i}. {paper['title']}\")\n",
    "    print(f\"   ì €ì: {', '.join(paper['authors'][:3])}...\")\n",
    "    print(f\"   ë‚ ì§œ: {paper['published']}\")\n",
    "    print(f\"   ID: {paper['arxiv_id']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: ê²°ê³¼ ì‹œê°í™”\n",
    "import pandas as pd\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "df = pd.DataFrame(results)\n",
    "df['abstract_preview'] = df['abstract'].str[:100] + '...'\n",
    "df['author_count'] = df['authors'].apply(len)\n",
    "\n",
    "print(\"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½\")\n",
    "display(df[['title', 'published', 'author_count']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY: ë³¸ì¸ ì—°êµ¬ ì£¼ì œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIY Cell 13: ë³¸ì¸ ì—°êµ¬ ì£¼ì œ ê²€ìƒ‰\n",
    "# íŒíŠ¸: my_query ë³€ìˆ˜ë§Œ ë°”ê¾¸ë©´ ë©ë‹ˆë‹¤\n",
    "\n",
    "my_query = \"your research topic here\"  # <- ì´ ë¶€ë¶„ ìˆ˜ì •!\n",
    "\n",
    "my_results = search_arxiv_papers(my_query, max_results=10)\n",
    "\n",
    "print(f\"ğŸ” ê²€ìƒ‰ì–´: {my_query}\")\n",
    "print(f\"ğŸ“š ê²°ê³¼: {len(my_results)}ê°œ ë…¼ë¬¸\\n\")\n",
    "\n",
    "for i, paper in enumerate(my_results, 1):\n",
    "    print(f\"{i}. {paper['title']}\")\n",
    "    print(f\"   {paper['published']} | {paper['arxiv_id']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIY Cell 14: ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„\n",
    "# ì§ˆë¬¸: ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ê´€ë ¨ì„± ë†’ì€ ë…¼ë¬¸ì€ ëª‡ ê°œì¸ê°€ìš”?\n",
    "\n",
    "# ê´€ë ¨ì„± í‰ê°€ (1-5ì )\n",
    "# ë©”ëª¨:\n",
    "# ë…¼ë¬¸ 1: \n",
    "# ë…¼ë¬¸ 2:\n",
    "# ë…¼ë¬¸ 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ë°”ê¿”ë³´ê¸°: ê²€ìƒ‰ ë¡œì§ ì»¤ìŠ¤í…€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìˆ˜ì • í¬ì¸íŠ¸\n",
    "\n",
    "| í•­ëª© | ê¸°ë³¸ê°’ | ì»¤ìŠ¤í…€ ì•„ì´ë””ì–´ |\n",
    "|------|--------|----------------|\n",
    "| ê²€ìƒ‰ í•„ë“œ | ì „ì²´ | ì œëª©ë§Œ, ì´ˆë¡ë§Œ |\n",
    "| ì¹´í…Œê³ ë¦¬ | ì „ì²´ | cs.CL, cs.AI, q-bio |\n",
    "| ë‚ ì§œ ë²”ìœ„ | ì „ì²´ | ìµœê·¼ 2ë…„ë§Œ |\n",
    "| ì •ë ¬ | relevance | ìµœì‹ ìˆœ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 16: ì»¤ìŠ¤í…€ ê²€ìƒ‰ í•¨ìˆ˜ (Before/After)\n\n# Before: ê¸°ë³¸ ê²€ìƒ‰\ndef search_basic(query):\n    return search_arxiv_papers(query, max_results=10)\n\n# After: ì‹¬ë¦¬í•™ íŠ¹í™” ê²€ìƒ‰\ndef search_psychology_papers(query, max_results=10, years_back=2):\n    \"\"\"\n    ì‹¬ë¦¬í•™ ì—°êµ¬ìš© ì»¤ìŠ¤í…€ ê²€ìƒ‰\n    - q-bio.NC (Neurons and Cognition) ì¹´í…Œê³ ë¦¬ ìš°ì„ \n    - ìµœê·¼ Në…„ ë…¼ë¬¸ë§Œ\n    - ì œëª©+ì´ˆë¡ì—ì„œ ê²€ìƒ‰\n    \"\"\"\n    from datetime import datetime, timedelta\n    \n    # ë‚ ì§œ í•„í„° ê³„ì‚°\n    cutoff_date = datetime.now() - timedelta(days=years_back*365)\n    \n    # arXiv ì¿¼ë¦¬ ë¬¸ë²•: cat:ì¹´í…Œê³ ë¦¬ AND (ti:ì œëª© OR abs:ì´ˆë¡)\n    enhanced_query = f\"(ti:{query} OR abs:{query})\"\n    \n    client = arxiv.Client()\n    search = arxiv.Search(\n        query=enhanced_query,\n        max_results=max_results * 2,  # í•„í„°ë§ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰\n        sort_by=arxiv.SortCriterion.SubmittedDate\n    )\n    \n    papers = []\n    for result in client.results(search):\n        # ë‚ ì§œ í•„í„°\n        if result.published.replace(tzinfo=None) < cutoff_date:\n            continue\n        \n        papers.append({\n            \"title\": result.title,\n            \"authors\": [a.name for a in result.authors],\n            \"abstract\": result.summary,\n            \"published\": result.published.strftime(\"%Y-%m-%d\"),\n            \"categories\": result.categories,\n            \"pdf_url\": result.pdf_url\n        })\n        \n        if len(papers) >= max_results:\n            break\n    \n    return papers\n\n# ë¹„êµ ì‹¤í–‰\nquery = \"cognitive load working memory\"\nprint(\"=== ê¸°ë³¸ ê²€ìƒ‰ ===\")\nbasic = search_basic(query)\nprint(f\"ê²°ê³¼: {len(basic)}ê°œ\")\n\nprint(\"\\n=== ì‹¬ë¦¬í•™ íŠ¹í™” ê²€ìƒ‰ (ìµœê·¼ 2ë…„) ===\")\npsych = search_psychology_papers(query, years_back=2)\nprint(f\"ê²°ê³¼: {len(psych)}ê°œ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIY Cell 17: ë³¸ì¸ë§Œì˜ í•„í„° ì¶”ê°€í•˜ê¸°\n",
    "\n",
    "def my_custom_search(query, max_results=10):\n",
    "    \"\"\"\n",
    "    TODO: ë³¸ì¸ ì—°êµ¬ì— ë§ê²Œ ì»¤ìŠ¤í…€í•˜ì„¸ìš”\n",
    "    \n",
    "    ì•„ì´ë””ì–´:\n",
    "    - íŠ¹ì • ì €ì í•„í„°: if 'Author Name' in authors\n",
    "    - í‚¤ì›Œë“œ í•„í„°: if 'keyword' in abstract.lower()\n",
    "    - ì¹´í…Œê³ ë¦¬ í•„í„°: if 'cs.CL' in categories\n",
    "    \"\"\"\n",
    "    # ì—¬ê¸°ì— ì½”ë“œ ì‘ì„±\n",
    "    pass\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "# results = my_custom_search(\"your query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: LLMìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ìë™ ìƒì„±\n",
    "from openai import OpenAI\n",
    "\n",
    "def generate_search_queries(research_topic, num_queries=3):\n",
    "    \"\"\"\n",
    "    ì—°êµ¬ ì£¼ì œì—ì„œ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìë™ ìƒì„±\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    \n",
    "    prompt = f\"\"\"Generate {num_queries} different arXiv search queries for the following research topic.\n",
    "Each query should focus on a different aspect of the topic.\n",
    "Return only the queries, one per line.\n",
    "\n",
    "Research topic: {research_topic}\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    queries = response.choices[0].message.content.strip().split('\\n')\n",
    "    return [q.strip() for q in queries if q.strip()]\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ (API key ìˆì„ ë•Œë§Œ)\n",
    "if OPENAI_API_KEY:\n",
    "    topic = \"AI agents for psychological research\"\n",
    "    queries = generate_search_queries(topic)\n",
    "    print(f\"ìƒì„±ëœ ì¿¼ë¦¬:\")\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        print(f\"{i}. {q}\")\n",
    "else:\n",
    "    print(\"API key ì—†ìŒ - ì´ ê¸°ëŠ¥ì€ ê±´ë„ˆëœë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIY Cell 19: ìë™ ìƒì„±ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰\n",
    "\n",
    "my_topic = \"your research topic\"  # <- ìˆ˜ì •\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    # ì¿¼ë¦¬ ìƒì„±\n",
    "    auto_queries = generate_search_queries(my_topic, num_queries=3)\n",
    "    \n",
    "    # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰\n",
    "    all_results = {}\n",
    "    for query in auto_queries:\n",
    "        results = search_arxiv_papers(query, max_results=3)\n",
    "        all_results[query] = results\n",
    "        print(f\"\\nğŸ” '{query}' â†’ {len(results)}ê°œ\")\n",
    "        for p in results:\n",
    "            print(f\"   - {p['title'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: ê²€ìƒ‰ ê²°ê³¼ ë¹„êµ (ì›ë³¸ vs ìˆ˜ì •ë³¸)\n",
    "query = \"cognitive psychology LLM\"\n",
    "\n",
    "# ê¸°ë³¸ ê²€ìƒ‰\n",
    "basic_results = search_arxiv_papers(query, max_results=5)\n",
    "\n",
    "# ì‹¬ë¦¬í•™ íŠ¹í™” ê²€ìƒ‰\n",
    "psych_results = search_psychology_papers(query, max_results=5, years_back=2)\n",
    "\n",
    "print(\"ğŸ“Š ê²€ìƒ‰ ë°©ë²• ë¹„êµ\\n\")\n",
    "print(f\"ê¸°ë³¸ ê²€ìƒ‰: {len(basic_results)}ê°œ\")\n",
    "for p in basic_results[:3]:\n",
    "    print(f\"  - {p['title'][:50]}... ({p['published']})\")\n",
    "\n",
    "print(f\"\\nì‹¬ë¦¬í•™ íŠ¹í™” (2ë…„): {len(psych_results)}ê°œ\")\n",
    "for p in psych_results[:3]:\n",
    "    print(f\"  - {p['title'][:50]}... ({p['published']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. í† ë¡ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### í† ë¡  ì§ˆë¬¸\n\n1. **ê²€ìƒ‰ ê²°ê³¼ì˜ ì§ˆì„ ì–´ë–»ê²Œ í‰ê°€í•  ìˆ˜ ìˆì„ê¹Œ?**\n   - precision vs recall\n   - ê´€ë ¨ì„± ì ìˆ˜?\n\n2. **Agent Laboratory vs PaSa ì¤‘ ì–´ë–¤ ê²ƒì„ ë² ì´ìŠ¤ë¡œ ì“¸ê¹Œ?**\n   - Agent Lab: ë¬¸í—Œ ê²€ìƒ‰ ìë™í™” (ì›Œí¬ìƒµì—ì„œëŠ” ì´ ë¶€ë¶„ë§Œ)\n   - PaSa: ê²€ìƒ‰ íŠ¹í™” (PPO ê¸°ë°˜ ì„ íƒ)\n\n3. **ì¶”ê°€í•˜ê³  ì‹¶ì€ ê¸°ëŠ¥ì€?**\n   - PubMed í†µí•©?\n   - í•œê¸€ ë…¼ë¬¸ (DBpia, RISS)?\n   - ìë™ PDF ë‹¤ìš´ë¡œë“œ?"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 5. PaSa ìŠ¤íƒ€ì¼ Crawler + Selector (ê³µì‹ APIë§Œ ì‚¬ìš©)\n\nPaSaì˜ í•µì‹¬ ì•„í‚¤í…ì²˜ë¥¼ **Semantic Scholar ê³µì‹ API**ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.\n\n### ì™œ ì´ ë°©ì‹ì¸ê°€?\n\n| í•­ëª© | ì„¤ëª… |\n|------|------|\n| **ê³µì‹ ìŠ¤íƒ** | Semantic Scholar ìì²´ê°€ SPECTER2ë¥¼ ì‚¬ìš© (Allen AI ê³µì‹) |\n| **API ì§ì ‘ ì œê³µ** | `embedding.specterv2` í•„ë“œë¡œ ì„ë² ë”© ìš”ì²­ ê°€ëŠ¥ |\n| **ê²€ì¦ëœ ë°©ì‹** | [allenai/s2search](https://github.com/allenai/s2search) ê³µì‹ ë¦¬ë­ì»¤ |\n| **ë¬´ë£Œ** | API key ì—†ì´ë„ ì‚¬ìš© ê°€ëŠ¥ (rate limitë§Œ ìˆìŒ) |\n\nì°¸ê³ : [Semantic Scholar API](https://www.semanticscholar.org/product/api) | [SPECTER2 GitHub](https://github.com/allenai/SPECTER2)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 21: PaSa ìŠ¤íƒ€ì¼ Agent (Semantic Scholar ê³µì‹ API ì‚¬ìš©)\nimport requests\nimport numpy as np\nfrom typing import List, Dict, Optional\n\nclass PaSaStyleAgent:\n    \"\"\"\n    PaSa ì•„í‚¤í…ì²˜ë¥¼ Semantic Scholar ê³µì‹ APIë¡œ êµ¬í˜„\n    \n    - Crawler: Semantic Scholar ê²€ìƒ‰ API\n    - Selector: Semantic Scholar Recommendations API + SPECTER2 ì„ë² ë”©\n    \n    ì¶œì²˜: https://api.semanticscholar.org/api-docs/\n    \"\"\"\n    \n    def __init__(self, api_key: Optional[str] = None):\n        \"\"\"\n        Parameters:\n        - api_key: Semantic Scholar API key (ì„ íƒ)\n                   ì—†ì–´ë„ ë™ì‘í•˜ì§€ë§Œ rate limit ì œí•œ (100 req/5ë¶„)\n                   ë°œê¸‰: https://www.semanticscholar.org/product/api\n        \"\"\"\n        self.api_key = api_key\n        self.base_url = \"https://api.semanticscholar.org/graph/v1\"\n        self.rec_url = \"https://api.semanticscholar.org/recommendations/v1\"\n    \n    def _headers(self):\n        \"\"\"API í—¤ë” ìƒì„±\"\"\"\n        headers = {}\n        if self.api_key:\n            headers['x-api-key'] = self.api_key\n        return headers\n    \n    def crawl(self, query: str, limit: int = 20, year_from: int = None) -> List[Dict]:\n        \"\"\"\n        Crawler: í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ ê²€ìƒ‰\n        \n        PaSaì˜ Google Search â†’ arXiv ê³¼ì •ì„ ëŒ€ì²´\n        \"\"\"\n        url = f\"{self.base_url}/paper/search\"\n        params = {\n            \"query\": query,\n            \"limit\": limit,\n            \"fields\": \"paperId,title,authors,year,abstract,citationCount,venue,url\"\n        }\n        if year_from:\n            params[\"year\"] = f\"{year_from}-\"\n        \n        response = requests.get(url, params=params, headers=self._headers())\n        \n        if response.status_code == 429:\n            print(\"âš ï¸ Rate limit ì´ˆê³¼ - API key ë°œê¸‰ì„ ê¶Œì¥í•©ë‹ˆë‹¤\")\n            print(\"   https://www.semanticscholar.org/product/api\")\n            return []\n        elif response.status_code != 200:\n            print(f\"âš ï¸ API ì˜¤ë¥˜: {response.status_code}\")\n            return []\n        \n        papers = []\n        for p in response.json().get('data', []):\n            papers.append({\n                'paperId': p.get('paperId'),\n                'title': p.get('title', ''),\n                'abstract': p.get('abstract', ''),\n                'authors': [a.get('name', '') for a in p.get('authors', [])],\n                'year': p.get('year'),\n                'citationCount': p.get('citationCount', 0),\n                'venue': p.get('venue', ''),\n                'url': p.get('url', '')\n            })\n        \n        return papers\n    \n    def get_recommendations(self, paper_ids: List[str], limit: int = 10) -> List[Dict]:\n        \"\"\"\n        Selector: ì‹œë“œ ë…¼ë¬¸ ê¸°ë°˜ ì¶”ì²œ (Semantic Scholar ê³µì‹ Recommendations API)\n        \n        ë‚´ë¶€ì ìœ¼ë¡œ SPECTER2 ì„ë² ë”© + SVM ì‚¬ìš©\n        ì¶œì²˜: https://api.semanticscholar.org/api-docs/recommendations\n        \"\"\"\n        url = f\"{self.rec_url}/papers\"\n        \n        payload = {\n            \"positivePaperIds\": paper_ids,\n            \"negativePaperIds\": []\n        }\n        params = {\n            \"fields\": \"paperId,title,authors,year,abstract,citationCount,venue,url\",\n            \"limit\": limit\n        }\n        \n        response = requests.post(url, json=payload, params=params, headers=self._headers())\n        \n        if response.status_code != 200:\n            print(f\"âš ï¸ Recommendations API ì˜¤ë¥˜: {response.status_code}\")\n            return []\n        \n        papers = []\n        for p in response.json().get('recommendedPapers', []):\n            papers.append({\n                'paperId': p.get('paperId'),\n                'title': p.get('title', ''),\n                'abstract': p.get('abstract', ''),\n                'authors': [a.get('name', '') for a in p.get('authors', [])],\n                'year': p.get('year'),\n                'citationCount': p.get('citationCount', 0),\n                'venue': p.get('venue', ''),\n                'url': p.get('url', '')\n            })\n        \n        return papers\n    \n    def search(self, query: str, limit: int = 20, expand: bool = True, \n               expand_limit: int = 10, year_from: int = None) -> Dict:\n        \"\"\"\n        PaSa ìŠ¤íƒ€ì¼ ì „ì²´ íŒŒì´í”„ë¼ì¸: Crawl â†’ Select â†’ Expand\n        \n        Parameters:\n        - query: ê²€ìƒ‰ ì£¼ì œ\n        - limit: ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ìˆ˜\n        - expand: ì¶”ì²œ ê¸°ë°˜ í™•ì¥ ì—¬ë¶€ (PaSaì˜ citation expansion ëŒ€ì²´)\n        - expand_limit: í™•ì¥ ì‹œ ì¶”ê°€í•  ë…¼ë¬¸ ìˆ˜\n        - year_from: ìµœì†Œ ì—°ë„\n        \n        Returns:\n        - {\"crawled\": [...], \"recommended\": [...], \"all\": [...]}\n        \"\"\"\n        print(f\"ğŸ” Step 1: Crawling '{query}'\")\n        crawled = self.crawl(query, limit=limit, year_from=year_from)\n        print(f\"   â†’ {len(crawled)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘\")\n        \n        recommended = []\n        if expand and crawled:\n            # ìƒìœ„ 3ê°œ ë…¼ë¬¸ì„ ì‹œë“œë¡œ ì¶”ì²œ í™•ì¥\n            seed_ids = [p['paperId'] for p in crawled[:3] if p['paperId']]\n            if seed_ids:\n                print(f\"ğŸ“Š Step 2: Expanding from {len(seed_ids)} seed papers\")\n                recommended = self.get_recommendations(seed_ids, limit=expand_limit)\n                print(f\"   â†’ {len(recommended)}ê°œ ì¶”ì²œ ë…¼ë¬¸ ì¶”ê°€\")\n        \n        # ì¤‘ë³µ ì œê±° í›„ í•©ì¹˜ê¸°\n        seen_ids = set()\n        all_papers = []\n        \n        for p in crawled + recommended:\n            if p['paperId'] and p['paperId'] not in seen_ids:\n                seen_ids.add(p['paperId'])\n                all_papers.append(p)\n        \n        print(f\"âœ… ì´ {len(all_papers)}ê°œ ë…¼ë¬¸ (ì¤‘ë³µ ì œê±°ë¨)\")\n        \n        return {\n            \"crawled\": crawled,\n            \"recommended\": recommended,\n            \"all\": all_papers\n        }\n\nprint(\"âœ… PaSaStyleAgent í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\nprint(\"   - crawl(): í‚¤ì›Œë“œ ê²€ìƒ‰ (Crawler)\")\nprint(\"   - get_recommendations(): ì‹œë“œ ê¸°ë°˜ ì¶”ì²œ (Selector)\")\nprint(\"   - search(): ì „ì²´ íŒŒì´í”„ë¼ì¸\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 22: PaSa ìŠ¤íƒ€ì¼ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n# API key ì—†ì´ë„ ë™ì‘ (rate limit ì£¼ì˜)\n\nagent = PaSaStyleAgent(api_key=SEMANTIC_SCHOLAR_API_KEY if 'SEMANTIC_SCHOLAR_API_KEY' in dir() else None)\n\n# ê²€ìƒ‰ ì‹¤í–‰\nresults = agent.search(\n    query=\"depression detection natural language processing\",\n    limit=10,           # Crawler: 10ê°œ ê²€ìƒ‰\n    expand=True,        # Selector: ì¶”ì²œ í™•ì¥\n    expand_limit=5,     # ì¶”ì²œ 5ê°œ ì¶”ê°€\n    year_from=2022\n)\n\n# ê²°ê³¼ ì¶œë ¥\nprint(f\"\\n{'='*70}\")\nprint(\"ğŸ“š ê²€ìƒ‰ ê²°ê³¼\")\nprint(f\"{'='*70}\\n\")\n\nfor i, paper in enumerate(results['all'][:10], 1):\n    source = \"ğŸ”\" if paper in results['crawled'] else \"ğŸ“Š\"\n    print(f\"{i}. {source} {paper['title']}\")\n    print(f\"   {paper['year']} | ì¸ìš© {paper['citationCount']} | {paper['url']}\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### DIY: ë³¸ì¸ ì—°êµ¬ ì£¼ì œë¡œ ê²€ìƒ‰",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# DIY Cell 23: ë³¸ì¸ ì—°êµ¬ ì£¼ì œë¡œ ê²€ìƒ‰\nmy_query = \"your research topic\"  # <- ìˆ˜ì •!\n\nmy_results = agent.search(\n    query=my_query,\n    limit=15,\n    expand=True,\n    expand_limit=10,\n    year_from=2020\n)\n\n# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³´ê¸°\nimport pandas as pd\n\ndf = pd.DataFrame(my_results['all'])\ndf['source'] = df.apply(lambda x: 'crawled' if x['paperId'] in [p['paperId'] for p in my_results['crawled']] else 'recommended', axis=1)\ndisplay(df[['title', 'year', 'citationCount', 'source']].head(15))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### PaSa ì›ë³¸ vs ìš°ë¦¬ êµ¬í˜„ ë¹„êµ\n\n| í•­ëª© | PaSa ì›ë³¸ | ìš°ë¦¬ êµ¬í˜„ |\n|------|-----------|----------|\n| **Crawler** | Google Search API (ìœ ë£Œ) | Semantic Scholar API (ë¬´ë£Œ) |\n| **Selector** | 7B LLM fine-tuned (GPU) | SPECTER2 Recommendations (API) |\n| **Citation í™•ì¥** | arXiv ì§ì ‘ í¬ë¡¤ë§ | Recommendations API |\n| **í•™ìŠµ ë°©ì‹** | PPO ê°•í™”í•™ìŠµ | ì‚¬ì „í•™ìŠµ ì„ë² ë”© |\n| **ë¹„ìš©** | GPU + API ë¹„ìš© | **ì™„ì „ ë¬´ë£Œ** |\n| **ê³µì‹ ì—¬ë¶€** | ByteDance ì—°êµ¬ | **Allen AI ê³µì‹ API** |\n\n**ê²°ë¡ **: Semantic Scholar APIëŠ” ë‚´ë¶€ì ìœ¼ë¡œ SPECTER2ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, ìš°ë¦¬ êµ¬í˜„ì€ **ê³µì‹ ìŠ¤íƒì„ ê·¸ëŒ€ë¡œ í™œìš©**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 6. ë‹¤ìŒ ë‹¨ê³„\n\nì´ì–´ì„œ ì‹¤ìŠµí•  ë‚´ìš©:\n- **Part 3**: LitLLMìœ¼ë¡œ ê²€ìƒ‰ëœ ë…¼ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ Related Work ì´ˆì•ˆ ìƒì„±\n- **Part 4**: AgentReviewë¡œ ì‘ì„±í•œ ë…¼ë¬¸ì— í”¼ë“œë°± ë°›ê¸°\n\n### ì°¸ê³  ìë£Œ\n- [Semantic Scholar API Docs](https://api.semanticscholar.org/api-docs/)\n- [Semantic Scholar Recommendations API](https://api.semanticscholar.org/api-docs/recommendations)\n- [SPECTER2 GitHub](https://github.com/allenai/SPECTER2)\n- [PaSa ì›ë³¸ (ByteDance)](https://github.com/bytedance/pasa)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}