{
  "id": "BnQhMqDfcKG",
  "original": "8d00f_s6PIk",
  "number": 4,
  "cdate": 1632875421715,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875421715,
  "tmdate": 1697934974595,
  "ddate": null,
  "content": {
    "title": "Probabilistic Implicit Scene Completion",
    "authorids": [
      "~Dongsu_Zhang1",
      "~Changwoon_Choi1",
      "~Inbum_Park1",
      "~Young_Min_Kim1"
    ],
    "authors": [
      "Dongsu Zhang",
      "Changwoon Choi",
      "Inbum Park",
      "Young Min Kim"
    ],
    "keywords": [
      "3D shape completion",
      "3D generative model"
    ],
    "abstract": "We propose a probabilistic shape completion method extended to the continuous geometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a considerable amount of missing data cluttered with unsegmented objects. The problem of shape completion is inherently ill-posed, and high-quality result requires scalable solutions that consider multiple possible outcomes. We employ the Generative Cellular Automata that learns the multi-modal distribution and transform the formulation to process large-scale continuous geometry. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. We formally derive that our training objective for the sparse voxel embedding maximizes the variational lower bound of the complete shape distribution and therefore our progressive generation constitutes a valid generative model. Experiments show that our model successfully generates diverse plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data. We also demonstrate that our approach outperforms deterministic models even in less ambiguous cases with a small amount of missing data, which infers that probabilistic formulation is crucial for high-quality geometry completion on input scans exhibiting any levels of completeness.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "zhang|probabilistic_implicit_scene_completion",
    "pdf": "/pdf/d9aba3c3efb0ad334f7b6a755f29a0b39bc05867.pdf",
    "one-sentence_summary": "We propose a scalable generative model for multi-modal completion of 3D scenes in implicit representation.",
    "supplementary_material": "",
    "code": "",
    "data": "",
    "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.01264/code)",
    "_bibtex": "@inproceedings{\nzhang2022probabilistic,\ntitle={Probabilistic Implicit Scene Completion},\nauthor={Dongsu Zhang and Changwoon Choi and Inbum Park and Young Min Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BnQhMqDfcKG}\n}",
    "venue": "ICLR 2022 Spotlight",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "BnQhMqDfcKG",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 21,
    "directReplyCount": 7,
    "revisions": true,
    "replies": [
      {
        "id": "NIu9iOazV-",
        "original": null,
        "number": 1,
        "cdate": 1635681262247,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635681262247,
        "tmdate": 1637652421238,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces continuous Generative Cellular Automata (cGCA) that is a generative model for continuous 3D reconstruction / shape completion. cGCA directly builds on top of GCA in the generation process, but instead applied the sparse voxel embeddings proposed in ConvONet and IF-Net to overcome the limitation of low resolution in GCA. Moreover, this paper adapts the infusion training strategy and also verifies the progressive generation is valid.",
          "main_review": "### Strengths\n1. The authors tackle the problem of probabilistic scene completion with partial observations as input.\n2. The authors apply the recent state-of-the-art neural implicit model for this probabilistic scene completion task, and show the ability of generating high-quality plausible shapes. The idea itself is simple and straightforward, but indeed effective.\n3. The paper is well-written and easy to follow in most parts. Need to polish Sec 3.3 to make it more accessible.\n\n### Weakness\n**Why is the task itself important?**\n\nThis paper is trying to tackle \u201cprobabilistic scene completion\u201d. Given partial point clouds, previous works (e.g. ConvONet, IF-Net) output a single most plausible shape. If you rerun the model with a different random seed, they usually also output a slightly different shape. This is also similar to my understanding of \u201cprobabilistic scene completion\u201d. In the introduction, you did not explain why having multiple plausible shapes is interesting or important. Moreover, justify what makes your method really different from those previous works, if all methods can output slightly different outputs. \n\n**The experimental section is convincing in general, but lacks some important experiments / explanation**\n\n1. Why cGCA can produce less blurry shapes than the deterministic models as shown in Fig. 3? Please provide some explanations more than saying \u201ccoincides with the well-known phenomena\u201d, since I don\u2019t understand why different losses for the your generative model should perform better.  \n2. What is the model configuration that you use for the baseline? For example, for ConvOcc, do you use 3-plane model or grid model? What is the resolution for the planes or grids? What is the number of parameters in their model? How long do you train them? Same for IF-Net and your method. These network configurations can make a big difference for the reconstruction results, because a model with a lot of parameters and train for long time usually tends to produce better results.  \n3. For the mode seeking, why T\u2019 is set to 5? An ablation study (e.g. T\u2019 is set from 0 to 10) is necessary to show the effectiveness of mode seeking, and why you choose 5.  \n4. In ShapeNet Scene experiments, you use T=15 transitions for GCA and cGCA? For the single object completion, you use T=30 instead. Why you have different numbers for different tasks, and it is better to provide an ablation for the choices.\n5. In Table 3, please give some explanations on 1) why (w/ cond.) does not really outperform the version w/o (2 out of 3 metrics are worse). 2) why 32^3 has really similar performance than 64^3.\n6. In Table 4 in Appendix, the memory footprint for ConvOcc does not make sense. If I remember correctly, their memory usage is very constant but only the runtime increases linearly w.r.t. the number of crops because each crop is processed individually.\n\n**What are the limitations of this paper?**\n\nYou mentioned in conclusion that the results are only evaluated with synthetic scene datasets. However, that is not really a limitation. It is always necessary to specify the real limitations of your approach. For instance, to obtain a complete shape voxel embeddings, previous methods like ConvOcc & IF-Net is pretty fast because you simply need to run encoder network containing some CNNs. In your method, you requires 15-30 transitions to obtain the final embeddings, which should be at least 15-30 slower, if I am not mistaken. \n \n\n",
          "summary_of_the_review": "In general, this paper introduces an interesting way of combining neural implicit representations and generative models. The mathematical proofs seem to be sound and correct, but in the experimental part, there are some important experiments and explanations missing. I will change my score accordingly based on the reply from the authors.\n\nMoreover, the previous work GCA has not made the code public. It would be great if the authors of cGCA can open source the code to benefit the community. This is another factor for me when making the decision.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_8mES"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_8mES"
        ]
      },
      {
        "id": "y02uMIEUZwM",
        "original": null,
        "number": 2,
        "cdate": 1635873274843,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635873274843,
        "tmdate": 1637779984130,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper is about scene completion given partial observations. The main technical part is built upon on prior work Generative Celluar Automata.(Zhang et al, 2021). GCA can be considered by recurrently applying a uniform convolutional operation on the top of current state, similar to the diffusion process. This work augments GCA by introducing a latent code for each voxel, instead of only a single 0-1 occupancy value. This latent code is used to generate fine detail implicit surface by querying a coordinated-based MLP inside each voxel. By combining the neural implicit function representation and GCA, the proposed method, namely cGCA outperforms prior works in terms of both scalability and fidelity.\n\nAs both the initial state and final state of cGCA is voxel-based representation, while the scene is represented as a continuous function or dense point cloud, an encoder-decoder network is used to convert the scene into voxel representation and vice versa.\n\n\n========\nDongsu Zhang and Changwoon Choi and Jeonghwan Kim and Young Min Kim, \"Learning to Generate 3D Shapes with Generative Cellular Automata\", ICLR 2021",
          "main_review": "Strengths\n\n1. The main novelty in this paper is the combination of a discretized voxel representation and implicit function representation in scene completion. Although both of them has been exploited before, combining them for the task of scene completion is a very natural and elegant. The key idea of this paper is augmenting the voxel with features that represent detailed local structures.\n\n2. The proposed PointNet based encoder and implicit function decoder is new and inspiring as an autoencoder for dense point clouds or surface.\n\n3. This paper delivers proof and derivation for the continuous version of GCA. Although the main chunk is similar to that of GCA, extending it with local features is non-trivial.\n\nWeakness\n\nMy main concern is the choice of GCA as the operation for the voxel stage. As far as I know(maybe I'm wrong), GCA is still new and not  widely tested. However, I think the autoencoder proposed in this paper is irrelavant to the specific voxel-based operation, thus can be easily combined with other 3D object completion method that use also voxel-based operation, such as those baselines compared in this paper. While as the discussion of the limitation of GCA is out of the scope of this paper, this is not a big issue. But I think maybe the author can focus on the idea of augmenting voxel-based representation with local features when presenting the paper.  For example, I think Sec.3.1 should be much more importance then Sec.3.3. Most of the derivations in Sec3.3 can be moved to Appendix. Again, this is my personal idea.",
          "summary_of_the_review": "From my own perspective, this paper is interesting and strong. Especially, the scene local feature autoencoder presented in this paper is inspiring. This paper presents a way towards large-scale high-resolution scene generation/completion. Although I think the structuring of the paper could be improved.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_pTw1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_pTw1"
        ]
      },
      {
        "id": "o7lklEmHaaq",
        "original": null,
        "number": 3,
        "cdate": 1635921693488,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635921693488,
        "tmdate": 1635921693488,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes an approach for scene completion which allows for sampling plausible completions from a learnt probabilistic model. The idea is to learn the state transition of a Markov Chain and use this during inference. The proposed formulation is based upon the Generative Cellular Automata where the representation of the surface is a discrete occupancy grid. The extension proposed in this work is to instead of using just voxel occupancy probabilities for the voxels also learn a shape code which can be decoded into a continuous surface, which is named sparse voxel embedding. The state transition is learned using an adapted version of infusion training.",
          "main_review": "Strengths:\n- Shape completion is an important feature AR/VR, robotics and other applications \n- The method is able to produce various plausible alternatives for shape completion\n- The surfaces are continuous and quantitatively more accurate than the voxelized versions\n\n\nWeaknesses:\n- The benefit in terms of accuracy compared to the voxelized versions is coming at a cost of lesser diversity. Especially the conditioning on the initial state which helps a lot on the accuracy decreases diversity.\n- In Fig. 3 shape completions with various amounts of sparsity in the input data is shown. However, they are all in a fairly dense range. What I would be interested is how would this model behave if the input becomes really sparse. When will it break down or will it still produce something meaningful with almost no input? On the other side it would also be interesting to know what happens if the input is very dense and the completion result is not ambiguous. I feel these would be interesting evaluations to better understand the limitations and strengths of this approach.\n- From reading the submission I was not able to understand how seams between the voxels are prevented. It seems that there is a risk that such seams could be visible? Would they become visible if the input was sparser?\n- It seems like the fact that during inference 30 rounds of state transitions needs to be computed would mean this method is significantly slower than the baselines presented in Fig 3. What are the runtimes of the proposed approach?\n- One line of works I feel could be added to the related work is 3D shape representations based on octrees. I feel this would be an alternative formulation to the sparse voxel embedding to get the resolution of the shape higher than coarse voxels.",
          "summary_of_the_review": "The paper presents a method which is able to produce multiple plausible shape completions of a sparse input. The main novelty is adding a shape code to the voxel which allows for decoding continuous surfaces. The training procedure of the state transition was adjusted accordingly. The results look promising quantitatively and qualitatively. The submission has a few weaknesses but I think the approach is novel enough and shows quantitative improvement.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_iZhE"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_iZhE"
        ]
      },
      {
        "id": "bP9bBHxqbL",
        "original": null,
        "number": 4,
        "cdate": 1635922377100,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635922377100,
        "tmdate": 1635922377100,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Review",
        "content": {
          "summary_of_the_paper": "Summary: The paper presents a shape completion method for large-scale 3d scenes. The method, called Continuous Generative Cellurlar Automata (cGCA)) is probabalistic in nature and produces multiple plausible completions for a single scene. The method employs leverages previous work in the form of key-buiilding blocks - Generative Cellular Automata, sparse voxel embeddings, and neural distance fields. Combining these approaches leads to impressive performance on recent 3D scene datasets - ShapeNet Scene and  3DFront. On ShapeNet the model outperforms Generative Cellular Automata (GCA) on reconstruction metrics, but not diversity metrics.\n\nThe main contribution of the paper to me is to realize that instead using a fixed grid like GCA to create voxel embeddings  leads to n^3 cost for the volume. In constrast, the authors use a sparse voxel embedding. They decode these embeddings with the method of  Chibane et al. to create a complete 3D shape. With this change, the method can scale to large scenes like from ShapeNet Scene and 3DFront, whereas GCA was limited to only small volumes(objects) of ShapeNet.\n\n",
          "main_review": "Writing: The paper is well written and complete. The abstract lays out the structure of the paper and the text matches mostly up to the expectations set in the abstract and introduction. I did not find any obvious error. The contribution are explicitly stated and thereforce can be easily verified. The mathematical notation once introduced is static and easy to parse. The figures are sufficiently high resolution and demonstrative of the improvements due to the proposed method.\n\nImplementation Details: I believe that the implementation is sufficiently detailed for a reader to be able to implement the algorithm - the previous models and their hyperparameters are described as are the datasets and their preprocessing steps.\n\nContribution: The paper extends GCA by using sparse voxel embeddings and a Distance Field method from Chibane et al. I think the contribution is pretty significant as the formulation under the new regime is vastly different from the old GCA in terms of implementation and engineering. The authors I think conclusively show that the new method is better performing on large scenes (Tables 1 and 2), while being competitive for single instances of 3D ojects (Table 3).\n\nComparisons: One could always ask from more comparisons, but I believe that the baselines are sufficient with two large scale methods - ConvOcc and IFNet, and comparison to the baseline GCA. Although it would be interesting to see how the authors managed to train GCA on such large scenes.\n\nProofs: The mathematics in the main paper was was checked and passed my review. However, I did not check the proofs in the supplementary.\n\nQuestions:\n(Infusion Traninig): I am not entirely sure how the Infusion kernel is different from the infusion chain in GCA apart from the problem of having o_c inthe generative model itself instead of being defined on a grid. I would like to hear more explanation from the authors as the final update rule (pg 5, training procedure) is the same as the training procedure of GCA (Algorithm 1).\n\n(Other methods): The authors do not make it clear how the other methods' performance was assessed - was it from the reported number or retrained with their preprocessing.\n\n(Mode Seeking): The authors do not provide a model without mode seeking. It would be interesting to see the performance from such a model.\n\n\n\n\n",
          "summary_of_the_review": "Summary of review: The paper is very well written with clear mathematics and clear delineation of contributions. The authors present a modification that makes older work scalable and show good results on multiple datasets, competing against the baseline method as well as other methods.\n\n",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "No concerns",
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_yCG6"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_yCG6"
        ]
      },
      {
        "id": "0CMiMMdDEyB",
        "original": null,
        "number": 5,
        "cdate": 1635928564029,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635928564029,
        "tmdate": 1637650127814,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The submitted paper studies 3D scene geometry reconstruction and proposed a pipeline for predicting signed distance fields from incomplete pointcloud data. The pipeline consists of two major components: an autoencoder that maps between the latent space and the voxel representations, and a Continuous Generative Cellular Automata (cGCA) which progressively and stochastically generates more complete data at each step. The proposed method is trained and evaluated on several synthetic datasets, including ShapeNet Scene, 3DFront, as well as ShapeNet, with a controlled sub-sampled pointcloud as input (with guaranteed rate of points for each object). Under these settings, both quantitative and qualitative results outperformed prior work. \n",
          "main_review": "Strengths:\n1. The proposed method is clean and simple. The key component, cGCA, is a relatively straightforward extension of the prior work GCA to include latent embeddings and to use signed distance fields, but it is still brings enough novelty and contribution.\n2. Being able to generate stochastic outputs is a significant advantage compared to prior work.\n3. Under the experimental setup, the proposed method clearly outperforms prior work\n\nWeaknesses:\n1. Lack of experiments on real-world datasets: despite authors referred to this as one of the future directions, I think it is an important experiment to include as part of this submission. The noise statistics can be quite different, which could potentially break the proposed method.\n2. My above concern is compounded by the fact that the authors were using a specific subsampling strategy to create synthetic inputs - i.e., each object has a guaranteed rate of surface points. This is significantly different from real-world pointcloud distributions. Are the prior work also re-trained specifically for this subsampled inputs? I would like to know how the method performs under uniform subsampling.\n3. [Minor] In Equation (3), is $Q$ not defined? ",
          "summary_of_the_review": "The authors' response and Appendix F & G address my concern regarding real-world experiments, so I am raising my ratings to \"8: accept, good paper\".\n\n> [Original Review]\nMy current recommendation is \"6: marginally above the acceptance threshold\", given the simplicity of the method and its results on synthetic dataset. I am concerned about its actual performance on real-world data, and I am happy to increase my rating if the method demonstrates similar improvement over prior work on real-world datasets. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "Not applicable",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_mswD"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_mswD"
        ]
      },
      {
        "id": "eLG6E3Uzrkx",
        "original": null,
        "number": 4,
        "cdate": 1637201299295,
        "mdate": 1637201299295,
        "ddate": null,
        "tcdate": 1637201299295,
        "tmdate": 1637201299295,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "0CMiMMdDEyB",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer mswD",
          "comment": "We thank the reviewer for the detailed comments. We address the replies below:  \n&nbsp;&nbsp;\n\n>1. Lack of experiments on real-world datasets: despite authors referred to this as one of the future directions, I think it is an important experiment to include as part of this submission. The noise statistics can be quite different, which could potentially break the proposed method.\n\nDuring the discussion stage, we tested our method on the ScanNet [1] dataset, and the result is added in Appendix G. ScanNet is one of the widely used datasets for real-world indoor environments, acquired with real sensors. While our method was only trained on the 3DFront [2] dataset, our method nicely generalizes to the ScanNet dataset, given very different statistics compared to the 3DFront dataset. Our multi-modal generation creates diverse reconstructions (e.g. chair, closets) in Fig. 12 on page 26. Also, the conditioned variant of our model can reconstruct unseen objects such as trees better than the GCA or vanilla cGCA.\n\n&nbsp;&nbsp;\n\n\n>2. My above concern is compounded by the fact that the authors were using a specific subsampling strategy to create synthetic inputs - i.e., each object has a guaranteed rate of surface points. This is significantly different from real-world pointcloud distributions. Are the prior work also re-trained specifically for this subsampled inputs? I would like to know how the method performs under uniform subsampling.\n\nWe have performed additional experiments regarding sparse inputs and non-ambiguous inputs in the Appendix. F. For sparse inputs, our method is the most robust method (especially the conditioned variant). Also, the diversity decreases with increasing accuracy as the input becomes less ambiguous. The same result is shown for non-ambiguous input (uniform sampling). The results show that our model is not just making diverse inputs, but the diversity of reconstruction is dependent on the ambiguity of the data.\n\nIn the original submission, we maintain a fixed ratio of point clouds to form the task of scene completion. If there is no remaining portion of the shape, the methods will learn to generate an object without the relevant context, which is not what we intend to learn during training. This phenomenon will especially have a severe effect on the ShapeNet scene dataset, where the objects were generated at random. \nOur input point cloud reflects the scenario for scene completion for scans that suffer from complex occlusion, missing a large amount of continuous region. We demonstrate the performance with only 20% of the original shape and still produce reasonable scenes. On the other hand, previous works on shape completion, such as ConvOcc [3] uniformly subsamples 10,000 points. Our input, while we control the ratio of remaining shape, is more challenging and ambiguous than completing the uniformly sampled points, and better align to the formulation of the multi-modal generation.\n\nAll the results, including baselines, were re-trained for each dataset and each level of completeness, which was in Appendix. A.3 in the initial submission. We realize that this is an important detail and added the statement in the main paper. For the ShapeNet scene dataset, as discussed in Appendix A.4, we uniformly sample 10,000 points after the iterative removal of points. The uniform sampling is exactly the same as that of ConvOcc, but ours is preceded by the iterative removal of points.\n\n&nbsp;&nbsp;\n\n>3. [Minor] In Equation (3), is Q not defined?\n\nThank you for the correction. We have revised the manuscript. \n\n&nbsp;&nbsp;\n\n[1] Dai et al., Scannet: Richly-annotated 3D Reconstructions of Indoor Scenes, CVPR 2017  \n[2] Fu et al., 3d-front: 3D Furnished Rooms with Layouts and Semantics, Arxiv 2020  \n[3] Peng et al., Convolutional Occupancy Networks, ECCV 2020\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ]
      },
      {
        "id": "M6_2eYO1waR",
        "original": null,
        "number": 5,
        "cdate": 1637202078883,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637202078883,
        "tmdate": 1637202128606,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "bP9bBHxqbL",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer yCG6",
          "comment": "We thank the reviewer for the effort in providing a detailed review of our work. We address the questions below:\n\n&nbsp;&nbsp;\n\n> 1. (Infusion Training): I am not entirely sure how the Infusion kernel is different from the infusion chain in GCA apart from the problem of having o_c in the generative model itself instead of being defined on a grid. I would like to hear more explanation from the authors as the final update rule (pg 5, training procedure) is the same as the training procedure of GCA (Algorithm 1).\n\nBoth GCA and cGCA operate on sparse voxels, but cGCA extends the representation to contain the latent code instead of the simple binary occupancy in GCA. The latent code represents the local implicit representation, which can be decoded into a continuous shape. Our non-trivial contribution is to incorporate the latent code in the sparse formulation and derive the training algorithm for multi-modal distribution.\n\nWe agree that the clear distinction between GCA and cGCA is necessary, and we added our explanation in Appendix C. We summarize the main points below.\n\n1) Usage of latent codes. While the GCA operates on high-resolution voxels, it only generates binary occupancy (0 for empty, 1 for occupied) confined in the voxel resolution. On the other hand, cGCA uses local implicit latent code and generates continuous shape.\n\n2) Processing disconnected shapes. In GCA, the proof of convergence (Proposition 1 in [1]) assumes (partial) connectivity of the shape. However, our derivation alleviates the connectivity assumption. Instead, we introduce an auxiliary function $G_x(s)$, which can deform the current input towards the desired output regardless of connectivity. Thus our formulation can train a generative model in a more general set-up.\n\n3) Derivation of the training objective. GCA only provides the result and training procedure but does not provide the derivation of the training objective. However, our work provides the mathematical derivation of the training objective with proof that we are maximizing the lower bound of the log-likelihood of the distribution for the ground truth shape (Section 3.3 and Appendix B).\n\nThe actual implementation is similar to Algorithm 1 in GCA [1], where the input buffer accelerates the training procedure and de-correlates the gradients of sequential models. The only difference between GCA and our model is the loss function (line 7 in Algorithm 1 [1]) which is replaced by Eq. 12 of our work, the new training objective proven to approximate the lower bound.\n\n&nbsp;&nbsp;\n\n>2. (Other methods): The authors do not make it clear how the other methods' performance was assessed - was it from the reported number or retrained with their preprocessing.\n\nAll the results, including baselines, were re-trained for each dataset and each level of completeness, which was in Appendix A.3 in the initial submission. We realize that this is an important detail and added the statement in the main manuscript.\n\n&nbsp;&nbsp;\n\n>3. (Mode Seeking): The authors do not provide a model without mode seeking. It would be interesting to see the performance from such a model.\n\nWe have added the ablation of mode seeking in Appendix E. Basically we compare the quality of reconstruction with a varying number of steps of mode seeking in Fig. 9. With a single step, we can remove all the voxels with low probability and greatly improve the final result. We chose T\u2019=5 for all experiments to make sure that the shape reaches a stable mode of the probability distribution. We added a gif for the mesh reconstructions of each state in the [link](https://drive.google.com/file/d/1S8FS1Shmo5K1yOl-Yj3DPuW1FRbyEqPg/view?usp=sharing) to show the dramatic effect of the mode seeking steps.\n\n&nbsp;&nbsp;\n\n[1] Zhang et al., Learning to Generate 3D Shapes with Generative Cellular Automata, ICLR 2021\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ]
      },
      {
        "id": "eqhM0GrEgtb",
        "original": null,
        "number": 6,
        "cdate": 1637202786078,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637202786078,
        "tmdate": 1637203066514,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "o7lklEmHaaq",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Resonse to Reviewer iZhE (1)",
          "comment": "We thank the reviewer for the constructive and encouraging comments. Below, we address the concerns.\n\n&nbsp;&nbsp;\n\n>1. The benefit in terms of accuracy compared to the voxelized versions is coming at a cost of lesser diversity. Especially the conditioning on the initial state which helps a lot on the accuracy decreases diversity.\n\nThe multi-modal shape completion assumes multiple answers and it is not simple to make an assessment about the performance. When we generate completely random shapes, we can achieve high diversity regardless of how faithful the final reconstruction is. On the other hand, if the input is almost complete to start with, we should not achieve high diversity. Therefore one needs to jointly consider the diversity (TMD) and the accuracy (CD or MMD + UHD).\n\nThat said, we assume that cGCA (w/ cond.) becomes less diverse since it does not change the original initial state. For example, in Figure 10, min. rate 0.2 of page 23 ([highlighted image link](https://drive.google.com/file/d/1XNIt2bqrm6llqyQKJJF09cD1wFLd8-H5/view)), the input is the top part of a lamp. While the GCA and vanilla cGCA modify the input geometry, cGCA (w/ cond.) faithfully preserves it. Quantitatively, this results in better accuracy, but lower diversity.\n\n&nbsp;&nbsp;\n\n>2. In Fig. 3 shape completions with various amounts of sparsity in the input data is shown. However, they are all in a fairly dense range. What I would be interested is how would this model behave if the input becomes really sparse. When will it break down or will it still produce something meaningful with almost no input? On the other side it would also be interesting to know what happens if the input is very dense and the completion result is not ambiguous. I feel these would be interesting evaluations to better understand the limitations and strengths of this approach.\n\nThank you for the suggestion, and we included the results in Appendix F. We originally followed previous work [1] and sampled 10000 points to produce input. Additionally, we experimented with different input sparsity (500, 1000, 5000, 10000 points) and tested the performance of the models trained with 10000 points. \n\nWe show that our model (especially w/ cond.) is very robust against sparsity, achieving the best accuracy at all times (Table 5). While the quality of reconstruction deteriorates given only 500 points (x20 sparser than the training data) with all approaches, the probabilistic approaches (GCA, cGCA) still tries to generate the learned shapes (e.g., shades of the lamp in Fig. 10). On the other hand, the deterministic models tend to simply fill the space between points. The results agree with our assumption that modeling multimodal distribution is crucial for generating high-quality shapes in ambiguous cases.\n\nWe also experimented with non-ambiguous input. We were also curious about how our model behaves on non-ambiguous input trained with varying levels of completeness. For all approaches, the accuracy increases and the diversity decreases as the ambiguity disappears for all models (Table 6). This confirms the diversity of reconstructions is dependent on the ambiguity of input. Also, the lower the completeness for training data, the more likely it was to generate diverse (yet plausible) scenes. \n\n&nbsp;&nbsp;\n\n>3. From reading the submission I was not able to understand how seams between the voxels are prevented. It seems that there is a risk that such seams could be visible? Would they become visible if the input was sparser?\n\nWe did not observe any seams on the boundary of voxels for all the experiments, even for the sparse input experiments conducted in Appendix F. When we decode the continuous shape from sparse voxel embedding, each point observes the hierarchical context that aggregates multi-level features with sparse convolutional networks (see $f_{\\omega_1}$ in Figure 6 and Appendix A.1). Therefore each point is affected by the multi-scale interpolation of convolved features at the given location, rather than the single latent code in a specific voxel. This approach is similar to [2] whereas we use sparse convolution instead of dense one. We added a short statement about the interpolation in the main text when we refer to the hierarchical autoencoder in Appendix A.\n\nAdditionally, we can create a single coherent continuous shape with the help of mode seeking steps. The mode seeking step eliminates the spurious samples with low probability, and the output shape stably converges to a single dominant mode. We added the visualization in Fig. 9. Also, we added a gif for the mesh reconstructions of each state in the [link](https://drive.google.com/file/d/1S8FS1Shmo5K1yOl-Yj3DPuW1FRbyEqPg/view).\n\n&nbsp;&nbsp;\n\n[1] Peng et al., Convolutional Occupancy Networks, ECCV 2020  \n[2] Chibane et al., Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion, CVPR 2020  \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ]
      },
      {
        "id": "boDs3FxxIrX",
        "original": null,
        "number": 7,
        "cdate": 1637203020873,
        "mdate": 1637203020873,
        "ddate": null,
        "tcdate": 1637203020873,
        "tmdate": 1637203020873,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "eqhM0GrEgtb",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Resonse to Reviewer iZhE (2)",
          "comment": ">4. It seems like the fact that during inference 30 rounds of state transitions needs to be computed would mean this method is significantly slower than the baselines presented in Fig 3. What are the runtimes of the proposed approach?\n\nWith our current version of code, which is not optimized yet, the runtime is roughly 1.86 seconds. This is tested for a batch size of 1 on the ShapeNet sofa dataset with 64^3 resolution, 35 transitions (30 for sampling, 5 for mode seeking). The latent codes can be further decoded into a continuous 3D geometry in about 0.28 seconds, thus the run time is roughly 2.24 seconds in total. Note that we are using a batch size of 1, which can be accelerated with larger batch size and the runtime varies largely on the number of input points. \n\nAdmittedly our approach is slower than previous works formulated with a single forward pass. Nonetheless, we reconstruct high-fidelity large-scale scenes, and handle multi-modal distribution of continuous shape. This is beyond the scale of any of the previous approaches that deal with a fixed number of points or are confined within a predefined voxel resolution. \n\nIn addition, we would like to mention that the proposed number of transitions is much smaller than other 3D diffusion-based models [3, 4]. Other diffusion-based models use 200 or 1000 transitions for completing a single object, while our method uses at most 35. This is the advantage of infusion-based probabilistic models over diffusion-based models. Infusion-based models are known to generate a sample with a smaller number of time steps than the diffusion-based models, as discussed in Section 3.4 of [5].\n\n&nbsp;&nbsp;\n\n>5. One line of works I feel could be added to the related work is 3D shape representations based on octrees. I feel this would be an alternative formulation to the sparse voxel embedding to get the resolution of the shape higher than coarse voxels.\n\nWe agree that we can handle high-resolution shapes with octrees, such as octree generating networks [6] or hierarchical implicit functions [7]. But we leave it for future work.\n\n&nbsp;&nbsp;\n\n[3] Luo et al., Diffusion Probabilistic Models for 3D Point Cloud Generation, CVPR 2021  \n[4] Zhou et al., 3D Shape Generation and Completion through Point-Voxel Diffusion, ICCV 2021  \n[5] Bordes et al., Learning to Generate Samples from Noise through Infusion Training, ICLR 2017  \n[6] Tatarchenko et al., Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs, ICCV 2017  \n[7] Tang et al., OctField: Hierarchical Implicit Functions for 3D Modeling, NeurIPS 2021  \n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ]
      },
      {
        "id": "bgUv44e8frB",
        "original": null,
        "number": 8,
        "cdate": 1637203446196,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637203446196,
        "tmdate": 1637465591570,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "y02uMIEUZwM",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer pTw1",
          "comment": "We appreciate the reviewer for the constructive  comments. Below, we address the concerns.\n\n&nbsp;&nbsp;\n\n> 1. My main concern is the choice of GCA as the operation for the voxel stage. As far as I know(maybe I'm wrong), GCA is still new and not widely tested. However, I think the autoencoder proposed in this paper is irrelavant to the specific voxel-based operation, thus can be easily combined with other 3D object completion method that use also voxel-based operation, such as those baselines compared in this paper. While as the discussion of the limitation of GCA is out of the scope of this paper, this is not a big issue. \n\nWe would like to emphasize that one of the main contributions of our work is that we are the first to tackle the problem of *probabilistic* implicit scene completion. There have been a few works (ConvOcc [1], IFNet [2]) on scene completion, and they also utilize implicit function with autoencoder to encode the continuous shapes. However, they are deterministic and result in blurry completions given ambiguous inputs (see Figure 10). On the other hand, we explicitly model the multimodal distribution and achieve the state-of-the-art result. \n\nThe main reason we adopt GCA [3] is to formulate the multi-modal distribution with infusion-based training. Few recent works, such as cGAN [4], tackle the multi-modal shape completion. But the experiments are only conducted only on single shape completions with 2,048 points. GCA outperforms cGAN and is more scalable by employing sparse convolution and the local update rules of cellular automata. We further modify GCA to formulate probabilistic implicit scene completion, and the difference we made is summarized in Appendix D.\n\n&nbsp;&nbsp;\n\n> 2. But I think maybe the author can focus on the idea of augmenting voxel-based representation with local features when presenting the paper. For example, I think Sec.3.1 should be much more importance then Sec.3.3. Most of the derivations in Sec3.3 can be moved to Appendix. Again, this is my personal idea.\n\nWe argue that Section 3.3 is an essential part of our work. As you mentioned, extending GCA with local features is non-trivial. The problem becomes more challenging when designing the training objective with local features. In our work, we show that the derived training objective maximizes the variational lower bound of log-likelihood of the data. We believe that this is a significant theoretical contribution compared to GCA. GCA has not clearly shown the relationship of training objectives (what we actually do) and the log-likelihood of data (what we aim to achieve). \n\n&nbsp;&nbsp;\n\n[1] Peng et al., Convolutional Occupancy Networks, ECCV 2020  \n[2] Chibane et al., Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion, CVPR 2020  \n[3] Zhang et al., Learning to Generate 3D Shapes with Generative Cellular Automata, ICLR 2021  \n[4] Wu et al., Multimodal Shape Completion via Conditional Generative Adversarial Networks, ECCV 2020  \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ]
      },
      {
        "id": "x4yCzPqtsQW",
        "original": null,
        "number": 9,
        "cdate": 1637204280885,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637204280885,
        "tmdate": 1637204868537,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "NIu9iOazV-",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 8mES (1)",
          "comment": "We thank the reviewer for the constructive comments. Below, we address the concerns.\n\n&nbsp;&nbsp;\n\n> 1. This paper is trying to tackle \u201cprobabilistic scene completion\u201d. Given partial point clouds, previous works (e.g. ConvONet, IF-Net) output a single most plausible shape. If you rerun the model with a different random seed, they usually also output a slightly different shape. This is also similar to my understanding of \u201cprobabilistic scene completion\u201d. In the introduction, you did not explain why having multiple plausible shapes is interesting or important. Moreover, justify what makes your method really different from those previous works, if all methods can output slightly different outputs.\n\nWe claim that the probabilistic formulation is essential for high-quality 3D shape completion, especially since there exists ambiguity for the solution. This is what we want to convey with the introduction of the main paper. Real-world scans are often highly irregular, due to complex occlusion and sensor noise. For example, given only the top part of a chair, there can be multiple plausible outcomes for the bottom part (four-legged, swivel, etc.). Therefore, the capability of processing the ambiguous scenes can largely benefit many tasks that require 3D scene understanding.  Previous works also acknowledge that 3D shape completion is an ill-posed problem and can have multiple plausible outcomes [1]. We need a solution to stably find one of the modes for high-fidelity completion. This is directly connected to producing probabilistic scene completion. \n\nIn general, using a deterministic model to generate an outcome for multimodal distribution is known to generate a blurry outcome [4]. This is because deterministic models tend to converge into the average of the multiple possible outcomes. A similar trend is observable in our experiments, where the deterministic model (ConvOcc [2] or IFNet [3]) fails to reconstruct highly incomplete scans. To our understanding, ConvOcc [2] and IFNet [3] do not have any stochastic behavior in the model. Once a model is trained, it will always produce the same result. They originally demonstrate stable performance when the input is less ambiguous with uniformly sampled input points. \n\nOn the other hand, our formulation can capture the large solution space of multimodal continuous 3D scenes, and demonstrate stable performance. The result also shows very different shapes, as shown in most of our figures. For example, the chairs in Fig. 5 are completely different ones, and the diversity cannot be achieved without a probabilistic formulation like ours. Given an ambiguous input, our model can generate multiple plausible results with a single training.\n\n&nbsp;&nbsp;\n\n> 2. Why cGCA can produce less blurry shapes than the deterministic models as shown in Fig. 3? Please provide some explanations more than saying \u201ccoincides with the well-known phenomena\u201d, since I don\u2019t understand why different losses for the your generative model should perform better.\n\nThe phenomenon is related to modeling a multi-modal distribution with a generative model. For example, suppose we are to model one-dimensional data distribution, whose values are concentrated on either value 1 or 2. There are two dominant modes. A deterministic model trained with MSE loss can be interpreted as estimating a normal distribution (see [link](https://stats.stackexchange.com/a/288453)). If properly trained, it will have a mean at 1.5, which achieves the lowest training loss. However, a normal distribution with mean at 1.5 does not describe the multi-modal data well enough. This is the averaging effect by enforcing a single mode to underlying multi-modal data.\n\nThe same phenomenon can be observed in high-dimensional data, which usually causes blurry outputs in image generation. A great example can be seen in Fig. 3 of GAN tutorial by Goodfellow [4], where a deterministic model produces a blurry prediction when learning a video sequence that has a multi-modal distribution.\n\nWe can observe that a similar phenomenon also happens in the 3D completion task. For example, in the top row of Fig.3 ([highlighted image link](https://drive.google.com/file/d/19Xi-Ny9KNIxU-4jlnpgt7H3W8IP74rrP/view?usp=sharing)), the reconstructed parts of deterministic models tend to be blurry, such as the bottom of the lampshade (inside of the pink box) or top parts of the closet (inside of the blue box). In contrast, probabilistic models generate multiple shapes with crisp geometric details. As discussed, the deterministic models cannot express the multimodal distribution, and rather generate the average of multiple modes. Thus the shape will likely be not as detailed as the shapes of the individual modes. Further examples can be verified in Fig. 13 on page 27 and in Fig. 14 on page 28.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ]
      },
      {
        "id": "J18x--B-Xi",
        "original": null,
        "number": 10,
        "cdate": 1637204708498,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637204708498,
        "tmdate": 1637205138444,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "x4yCzPqtsQW",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 8mES (2)",
          "comment": "> 3. What is the model configuration that you use for the baseline? For example, for ConvOcc, do you use 3-plane model or grid model? What is the resolution for the planes or grids? What is the number of parameters in their model? How long do you train them? Same for IF-Net and your method. These network configurations can make a big difference for the reconstruction results, because a model with a lot of parameters and train for long time usually tends to produce better results.\n\nDue to the page limit, we included the model configuration for the baselines in Appendix A.3 in our original submission. We trained ConvOcc with a grid model of $64^3$ resolution, which produces the best result on the ShapeNet scene dataset in the original ConvOcc paper [2]. We trained IFNet with the ShapeNet128 model, as in the work of Siddiqui et al. [5]. We used the default hyperparameters and trained until validation IoU (or loss) converged. We also have tried other hyperparameters (e.g. learning rate, marching cubes thresholding parameters), but it did not observe any noticeable improvement.\n\n&nbsp; &nbsp;\n\n> 4. For the mode seeking, why T\u2019 is set to 5? An ablation study (e.g. T\u2019 is set from 0 to 10) is necessary to show the effectiveness of mode seeking, and why you choose 5.\n\nThank you for the suggestion. It was also suggested by reviewer 2, and we have added an ablation study on mode seeking in Appendix E. Basically we compare the quality of reconstruction in response to the number of steps of mode seeking in Fig. 9. With a single step, we can remove all the voxels with low probability and greatly improve the final result. We chose T\u2019=5 for all experiments to make sure that the shape reaches a stable mode of the probability distribution. We added a gif for the mesh reconstructions of each state in the [link](https://drive.google.com/file/d/1S8FS1Shmo5K1yOl-Yj3DPuW1FRbyEqPg/view?usp=sharing) to show the dramatic effect of the mode seeking steps.\n\n&nbsp;&nbsp;\n\n>5.  In ShapeNet Scene experiments, you use T=15 transitions for GCA and cGCA? For the single object completion, you use T=30 instead. Why you have different numbers for different tasks, and it is better to provide an ablation for the choices.\n\nThe number of transition steps is the maximum distance we can expand from the initial incomplete shape. This is because GCA only can generate around the direct neighborhood of the occupied cells in a single inference step. We tried to set the transition steps to a minimum, since the longer the transition steps were, the longer the training/inference took. During the rebuttal period, we conducted an ablation study on the effect of transition steps by differing the transition steps trained with 15 transition steps. The below are the results. \n\n|               \t| min. CD \t| avg. CD \t| TMD  \t|\n|---------------\t|---------\t|---------\t|------\t|\n| ConvOcc       \t|    1.33 \t| -       \t| -    \t|\n| cGCA (T = 15) \t|    1.16 \t|    1.49 \t| 4.92 \t|\n| cGCA (T = 20) \t|    1.19 \t|    1.56 \t| 4.29 \t|\n| cGCA (T = 25) \t|    1.24 \t|    1.61 \t| 4.34 \t|\n| cGCA (T = 30) \t|    1.26 \t|    1.64 \t| 4.51 \t|\n\nThe accuracy decreases (higher CD) as we increase T more than the trained value. Nonetheless, the accuracy is still better than ConvOcc with even twice as many transitions as the model is trained. Thus, our model is reasonably robust against T. \n\nNote that GCA also included a similar ablation study regarding the training step T (Appendix E in GCA [6]) and similarly concluded that the model is fairly robust to the number of transition steps.\n\n&nbsp;&nbsp;\n\n[1] Wu et al., Multimodal Shape Completion via Conditional Generative Adversarial Networks, ECCV 2020  \n[2] Peng et al., Convolutional Occupancy Networks, ECCV 2020  \n[3] Chibane et al., Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion, CVPR 2020  \n[4] Goodfellow, NIPS 2016 tutorial: Generative adversarial networks, Arxiv 2016  \n[5] Siddiqui et al., Retrieval-Fuse: Neural 3D Scene Reconstruction with a Database, ICCV 2021  \n[6] Zhang et al., Learning to Generate 3D Shapes with Generative Cellular Automata, ICLR 2021\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ]
      },
      {
        "id": "6SrZlpQxGUe",
        "original": null,
        "number": 11,
        "cdate": 1637205366140,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637205366140,
        "tmdate": 1637205900678,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "J18x--B-Xi",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 8mES (3)",
          "comment": "> 6. In Table 3, please give some explanations on 1) why (w/ cond.) does not really outperform the version w/o (2 out of 3 metrics are worse). 2) why 32^3 has really similar performance than 64^3.\n\n1) Our results show that the (w/ cond.) outperforms in UHD (fidelity), similar results in MMD (quality). In terms of TMD (diversity), cGCA (w/ cond.) can be less diverse since it preserves the original initial shape. For example, in Figure 14 min. rate 0.2 of page 28 ([highlighted image link](https://drive.google.com/file/d/1XNIt2bqrm6llqyQKJJF09cD1wFLd8-H5/view)), the input provides the top of the lamp. While the GCA and vanilla cGCA modify the top part of the lamp, cGCA (w/ cond.) faithfully preserves the given input. Quantitatively, this results in better accuracy, but lower diversity.  \nWe claim that the diversity metric (TMD) should be jointly considered with the accuracy metrics (CD or MMD + UHD). As an extreme example, if a model generates completely random outputs and ignores the input, it will achieve a very high TMD but low accuracy scores.\n\n2) The voxel resolution determines the level of geometric detail that a latent code needs to represent. Local implicit representation individually encodes smaller patches into latent codes, instead of modeling the entire shape with a single latent code. As discussed in previous works (Deep local shapes [7], Local implicit grid representations [8]), the patches of 3d surfaces share geometric details at some scale. The optimal resolution of a voxel is dependent on data. For single objects in ShapeNet, 32^3 appears to be a good enough resolution, capturing geometry comparable to 64^3.\n\n&nbsp;&nbsp;\n\n> 7. In Table 4 in Appendix, the memory footprint for ConvOcc does not make sense. If I remember correctly, their memory usage is very constant but only the runtime increases linearly w.r.t. the number of crops because each crop is processed individually.\n\nThis is because we did not use the sliding windows techniques for measuring memory footprint for ConvOcc. The original ConvOcc dissects the scene into smaller sections and completes them separately. However, we argue that the sliding window technique is inappropriate for scene completion. The technique assumes that the divisions contain all the information necessary to complete the geometry. However, incomplete scans of a single object might be added into different segments, or the model will be trained to ignore the context outside of the current segment. This leads to inconsistent learning, and the model will fail to complete a scene when testing. Additionally, the model cannot observe the context beyond the current segment during testing. Thus, we have measured the memory footprint for processing the entire scene as input and result in a large memory footprint. Nonetheless, our formulation can efficiently handle the large-scale scenes.\n\n&nbsp;&nbsp;\n\n> 8. You mentioned in conclusion that the results are only evaluated with synthetic scene datasets. However, that is not really a limitation. It is always necessary to specify the real limitations of your approach. For instance, to obtain a complete shape voxel embeddings, previous methods like ConvOcc & IF-Net is pretty fast because you simply need to run encoder network containing some CNNs. In your method, you requires 15-30 transitions to obtain the final embeddings, which should be at least 15-30 slower, if I am not mistaken.\n\nWith our current version of code, which is not optimized yet, the runtime is roughly 1.86 seconds. This is tested for a batch size of 1 on the ShapeNet sofa dataset with 64^3 resolution, 35 transitions (30 for sampling, 5 for mode seeking). The latent codes can be further decoded into a continuous 3D geometry in about 0.28 seconds, thus the run time is roughly 2.24 seconds in total. Note that we are using a batch size of 1, which can be accelerated with a larger batch size and the runtime varies largely on the number of input points. \n\nAdmittedly our approach is slower than previous works formulated with a single forward pass. Nonetheless, we reconstruct high-fidelity large-scale scenes and handle multi-modal distribution of continuous shape. This is beyond the scale of any of the previous approaches that deal with a fixed number of points or are confined within a predefined voxel resolution. \n\nIn addition, we would like to mention that the proposed number of transitions is much smaller than other 3D diffusion-based models [9, 10]. Other diffusion-based models use 200 or 1000 transitions for completing a single object, while our method uses at most 35. This is the advantage of infusion-based probabilistic models over diffusion-based models. Infusion-based models are known to generate a sample with a smaller number of time steps than the diffusion-based models, as discussed in Section 3.4 of [11].\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ]
      },
      {
        "id": "0dwlTRzWBP_",
        "original": null,
        "number": 12,
        "cdate": 1637205501355,
        "mdate": 1637205501355,
        "ddate": null,
        "tcdate": 1637205501355,
        "tmdate": 1637205501355,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "6SrZlpQxGUe",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 8mES (4)",
          "comment": "> 9. Moreover, the previous work GCA has not made the code public. It would be great if the authors of cGCA can open source the code to benefit the community. This is another factor for me when making the decision.\n\nWe have added the code for GCA in our Github repository.\n\n&nbsp;&nbsp;\n\n[7] Chabra et al., Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction, ECCV 2020  \n[8] Jiang et al., Local Implicit Grid Representations for 3D Scenes, CVPR 2020  \n[9] Luo et al., Diffusion Probabilistic Models for 3D Point Cloud Generation, CVPR 2021  \n[10] Zhou et al., 3D Shape Generation and Completion through Point-Voxel Diffusion, ICCV 2021  \n[11] Bordes et al., Learning to Generate Samples from Noise through Infusion Training, ICLR 2017  \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ]
      },
      {
        "id": "2mAe5Osascy",
        "original": null,
        "number": 13,
        "cdate": 1637205740311,
        "mdate": 1637205740311,
        "ddate": null,
        "tcdate": 1637205740311,
        "tmdate": 1637205740311,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Common Response to All Reviewers",
          "comment": "We first would like to appreciate the thoughtful comments by the reviewers.\n\n\nHere are the summary of the main changes and additional experiments conducted in the response of the reviews:\n- We elaborate the difference of our proposed formulation compared to GCA in Appendix C.\n- We added an ablation study of mode seeking steps in Appendix E.\n- We examined the performance analysis under both sparse and non-ambiguous input in Appendix F.\n- We provide results of our approach in real-world scans using the ScanNet dataset in Appendix G.\n\n\nAlso, we would like to emphasize that our work is not only presenting a method with superior performance but also is unique as it models the multi-modal distribution in the context of scene completion. Modeling the multi-modal distribution is crucial for high-quality generation in an under-constrained set-up, as a deterministic generative model would likely generate a blurry reconstruction given ambiguous input. Nonetheless, the field of learning multi-modal distribution is yet to be explored, to the best of our knowledge. Our contribution includes handling the high-dimensional solution space for a large-scale incomplete 3D scene.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ]
      },
      {
        "id": "5BudqK9Q_i3",
        "original": null,
        "number": 14,
        "cdate": 1637243282073,
        "mdate": 1637243282073,
        "ddate": null,
        "tcdate": 1637243282073,
        "tmdate": 1637243282073,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "0dwlTRzWBP_",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Response to the Authors' Response",
          "comment": "Thanks for the very detailed response, you addressed most of my concerns and I am in general very convince. I will upgrade my score accordingly. There are only two small points:\n\n* I don't agree with your argument about \"the sliding-window technique is inappropriate for scene completion\". In the section 6 of the [supplementary material](http://www.cvlibs.net/publications/Peng2020ECCV_supplementary.pdf) of ConvOcc, during training, although their crop size for occupancy prediction is only 25x25x25, their input crop for point cloud conditioning is much larger (88x88x88). In this way, they can incorporate enough context information to perform 3D reconstruction. During inference, they also have the input crops overlapped with each other. I argue that such sliding-window approach is the correct way of scaling up to very large scenes (e.g. in Matterport3D, a building with multiple floors) with constant memory footprint. I will not ask for this experiment of cGCA on Matterport3D since I am pretty sure cGCA can be adapted and working for such large scenes in the sliding-window manner as well.\n* I still think that it would be nice if you add a small limitation section of cGCA for readers, hence they would not waste too much time trying to figure out themselves. For example, as you also mentioned, your method is slower than other deterministic models because you need multiple transitions. The runtime will further increase if you increase the grid resolution.\n\n\n\nLast but not the least, thanks for promising to release the code, I am looking forward to trying it out!"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_8mES"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_8mES"
        ]
      },
      {
        "id": "CxFBCzz3GFA",
        "original": null,
        "number": 15,
        "cdate": 1637395953699,
        "mdate": 1637395953699,
        "ddate": null,
        "tcdate": 1637395953699,
        "tmdate": 1637395953699,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "5BudqK9Q_i3",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 8mES ",
          "comment": "Thanks for the suggestion! We have added a statement regarding the limitations of runtime in the manuscript."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ]
      },
      {
        "id": "EaZ2Vv20HGr",
        "original": null,
        "number": 16,
        "cdate": 1637650179719,
        "mdate": 1637650179719,
        "ddate": null,
        "tcdate": 1637650179719,
        "tmdate": 1637650179719,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "eLG6E3Uzrkx",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Raised my rating to \"Accept\"",
          "comment": "Thanks for your response. Appendix F & G address my previous concern regarding real-world experiments, so I am raising my ratings to \"8: accept, good paper\"."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_mswD"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_mswD"
        ]
      },
      {
        "id": "-0Ae41yE6a",
        "original": null,
        "number": 17,
        "cdate": 1637780117386,
        "mdate": 1637780117386,
        "ddate": null,
        "tcdate": 1637780117386,
        "tmdate": 1637780117386,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "bgUv44e8frB",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Raise to Accept",
          "comment": "I'm satisfied with the response. Besides, after reading the comments from other reviewers, and also the responses, I'm feeling quite positive about this paper. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_pTw1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_pTw1"
        ]
      },
      {
        "id": "S2kCAd_saQc",
        "original": null,
        "number": 19,
        "cdate": 1637972498628,
        "mdate": 1637972498628,
        "ddate": null,
        "tcdate": 1637972498628,
        "tmdate": 1637972498628,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "boDs3FxxIrX",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Unchanged positive opinion about the submission",
          "comment": "Thanks for the explanations, they answered my questions. After reading the authors answers and the other reviews I would like to keep my positive rating of this submission."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_iZhE"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_iZhE"
        ]
      },
      {
        "id": "-bEai7YVHEi",
        "original": null,
        "number": 1,
        "cdate": 1642696832460,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696832460,
        "tmdate": 1642696832460,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Spotlight)",
          "comment": "This paper introduced a probabilistic extension to a pipeline for 3D scene geometry reconstruction from large-scale point clouds. \nAll reviewers recognized the significance of the proposed approach and praised the simplicity of deriving a probabilistic version of Generative Cellular Automata that performs well in a number of reconstruction benchmarks. Authors were responsive during rebuttal and managed to clarify the concerns raised about the limited scope of the experiments and certain parameters involved, and also raise one reviewer's scores."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "NIu9iOazV-",
        "original": null,
        "number": 1,
        "cdate": 1635681262247,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635681262247,
        "tmdate": 1637652421238,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces continuous Generative Cellular Automata (cGCA) that is a generative model for continuous 3D reconstruction / shape completion. cGCA directly builds on top of GCA in the generation process, but instead applied the sparse voxel embeddings proposed in ConvONet and IF-Net to overcome the limitation of low resolution in GCA. Moreover, this paper adapts the infusion training strategy and also verifies the progressive generation is valid.",
          "main_review": "### Strengths\n1. The authors tackle the problem of probabilistic scene completion with partial observations as input.\n2. The authors apply the recent state-of-the-art neural implicit model for this probabilistic scene completion task, and show the ability of generating high-quality plausible shapes. The idea itself is simple and straightforward, but indeed effective.\n3. The paper is well-written and easy to follow in most parts. Need to polish Sec 3.3 to make it more accessible.\n\n### Weakness\n**Why is the task itself important?**\n\nThis paper is trying to tackle \u201cprobabilistic scene completion\u201d. Given partial point clouds, previous works (e.g. ConvONet, IF-Net) output a single most plausible shape. If you rerun the model with a different random seed, they usually also output a slightly different shape. This is also similar to my understanding of \u201cprobabilistic scene completion\u201d. In the introduction, you did not explain why having multiple plausible shapes is interesting or important. Moreover, justify what makes your method really different from those previous works, if all methods can output slightly different outputs. \n\n**The experimental section is convincing in general, but lacks some important experiments / explanation**\n\n1. Why cGCA can produce less blurry shapes than the deterministic models as shown in Fig. 3? Please provide some explanations more than saying \u201ccoincides with the well-known phenomena\u201d, since I don\u2019t understand why different losses for the your generative model should perform better.  \n2. What is the model configuration that you use for the baseline? For example, for ConvOcc, do you use 3-plane model or grid model? What is the resolution for the planes or grids? What is the number of parameters in their model? How long do you train them? Same for IF-Net and your method. These network configurations can make a big difference for the reconstruction results, because a model with a lot of parameters and train for long time usually tends to produce better results.  \n3. For the mode seeking, why T\u2019 is set to 5? An ablation study (e.g. T\u2019 is set from 0 to 10) is necessary to show the effectiveness of mode seeking, and why you choose 5.  \n4. In ShapeNet Scene experiments, you use T=15 transitions for GCA and cGCA? For the single object completion, you use T=30 instead. Why you have different numbers for different tasks, and it is better to provide an ablation for the choices.\n5. In Table 3, please give some explanations on 1) why (w/ cond.) does not really outperform the version w/o (2 out of 3 metrics are worse). 2) why 32^3 has really similar performance than 64^3.\n6. In Table 4 in Appendix, the memory footprint for ConvOcc does not make sense. If I remember correctly, their memory usage is very constant but only the runtime increases linearly w.r.t. the number of crops because each crop is processed individually.\n\n**What are the limitations of this paper?**\n\nYou mentioned in conclusion that the results are only evaluated with synthetic scene datasets. However, that is not really a limitation. It is always necessary to specify the real limitations of your approach. For instance, to obtain a complete shape voxel embeddings, previous methods like ConvOcc & IF-Net is pretty fast because you simply need to run encoder network containing some CNNs. In your method, you requires 15-30 transitions to obtain the final embeddings, which should be at least 15-30 slower, if I am not mistaken. \n \n\n",
          "summary_of_the_review": "In general, this paper introduces an interesting way of combining neural implicit representations and generative models. The mathematical proofs seem to be sound and correct, but in the experimental part, there are some important experiments and explanations missing. I will change my score accordingly based on the reply from the authors.\n\nMoreover, the previous work GCA has not made the code public. It would be great if the authors of cGCA can open source the code to benefit the community. This is another factor for me when making the decision.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_8mES"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_8mES"
        ]
      },
      {
        "id": "y02uMIEUZwM",
        "original": null,
        "number": 2,
        "cdate": 1635873274843,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635873274843,
        "tmdate": 1637779984130,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper is about scene completion given partial observations. The main technical part is built upon on prior work Generative Celluar Automata.(Zhang et al, 2021). GCA can be considered by recurrently applying a uniform convolutional operation on the top of current state, similar to the diffusion process. This work augments GCA by introducing a latent code for each voxel, instead of only a single 0-1 occupancy value. This latent code is used to generate fine detail implicit surface by querying a coordinated-based MLP inside each voxel. By combining the neural implicit function representation and GCA, the proposed method, namely cGCA outperforms prior works in terms of both scalability and fidelity.\n\nAs both the initial state and final state of cGCA is voxel-based representation, while the scene is represented as a continuous function or dense point cloud, an encoder-decoder network is used to convert the scene into voxel representation and vice versa.\n\n\n========\nDongsu Zhang and Changwoon Choi and Jeonghwan Kim and Young Min Kim, \"Learning to Generate 3D Shapes with Generative Cellular Automata\", ICLR 2021",
          "main_review": "Strengths\n\n1. The main novelty in this paper is the combination of a discretized voxel representation and implicit function representation in scene completion. Although both of them has been exploited before, combining them for the task of scene completion is a very natural and elegant. The key idea of this paper is augmenting the voxel with features that represent detailed local structures.\n\n2. The proposed PointNet based encoder and implicit function decoder is new and inspiring as an autoencoder for dense point clouds or surface.\n\n3. This paper delivers proof and derivation for the continuous version of GCA. Although the main chunk is similar to that of GCA, extending it with local features is non-trivial.\n\nWeakness\n\nMy main concern is the choice of GCA as the operation for the voxel stage. As far as I know(maybe I'm wrong), GCA is still new and not  widely tested. However, I think the autoencoder proposed in this paper is irrelavant to the specific voxel-based operation, thus can be easily combined with other 3D object completion method that use also voxel-based operation, such as those baselines compared in this paper. While as the discussion of the limitation of GCA is out of the scope of this paper, this is not a big issue. But I think maybe the author can focus on the idea of augmenting voxel-based representation with local features when presenting the paper.  For example, I think Sec.3.1 should be much more importance then Sec.3.3. Most of the derivations in Sec3.3 can be moved to Appendix. Again, this is my personal idea.",
          "summary_of_the_review": "From my own perspective, this paper is interesting and strong. Especially, the scene local feature autoencoder presented in this paper is inspiring. This paper presents a way towards large-scale high-resolution scene generation/completion. Although I think the structuring of the paper could be improved.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_pTw1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_pTw1"
        ]
      },
      {
        "id": "o7lklEmHaaq",
        "original": null,
        "number": 3,
        "cdate": 1635921693488,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635921693488,
        "tmdate": 1635921693488,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes an approach for scene completion which allows for sampling plausible completions from a learnt probabilistic model. The idea is to learn the state transition of a Markov Chain and use this during inference. The proposed formulation is based upon the Generative Cellular Automata where the representation of the surface is a discrete occupancy grid. The extension proposed in this work is to instead of using just voxel occupancy probabilities for the voxels also learn a shape code which can be decoded into a continuous surface, which is named sparse voxel embedding. The state transition is learned using an adapted version of infusion training.",
          "main_review": "Strengths:\n- Shape completion is an important feature AR/VR, robotics and other applications \n- The method is able to produce various plausible alternatives for shape completion\n- The surfaces are continuous and quantitatively more accurate than the voxelized versions\n\n\nWeaknesses:\n- The benefit in terms of accuracy compared to the voxelized versions is coming at a cost of lesser diversity. Especially the conditioning on the initial state which helps a lot on the accuracy decreases diversity.\n- In Fig. 3 shape completions with various amounts of sparsity in the input data is shown. However, they are all in a fairly dense range. What I would be interested is how would this model behave if the input becomes really sparse. When will it break down or will it still produce something meaningful with almost no input? On the other side it would also be interesting to know what happens if the input is very dense and the completion result is not ambiguous. I feel these would be interesting evaluations to better understand the limitations and strengths of this approach.\n- From reading the submission I was not able to understand how seams between the voxels are prevented. It seems that there is a risk that such seams could be visible? Would they become visible if the input was sparser?\n- It seems like the fact that during inference 30 rounds of state transitions needs to be computed would mean this method is significantly slower than the baselines presented in Fig 3. What are the runtimes of the proposed approach?\n- One line of works I feel could be added to the related work is 3D shape representations based on octrees. I feel this would be an alternative formulation to the sparse voxel embedding to get the resolution of the shape higher than coarse voxels.",
          "summary_of_the_review": "The paper presents a method which is able to produce multiple plausible shape completions of a sparse input. The main novelty is adding a shape code to the voxel which allows for decoding continuous surfaces. The training procedure of the state transition was adjusted accordingly. The results look promising quantitatively and qualitatively. The submission has a few weaknesses but I think the approach is novel enough and shows quantitative improvement.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_iZhE"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_iZhE"
        ]
      },
      {
        "id": "bP9bBHxqbL",
        "original": null,
        "number": 4,
        "cdate": 1635922377100,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635922377100,
        "tmdate": 1635922377100,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Review",
        "content": {
          "summary_of_the_paper": "Summary: The paper presents a shape completion method for large-scale 3d scenes. The method, called Continuous Generative Cellurlar Automata (cGCA)) is probabalistic in nature and produces multiple plausible completions for a single scene. The method employs leverages previous work in the form of key-buiilding blocks - Generative Cellular Automata, sparse voxel embeddings, and neural distance fields. Combining these approaches leads to impressive performance on recent 3D scene datasets - ShapeNet Scene and  3DFront. On ShapeNet the model outperforms Generative Cellular Automata (GCA) on reconstruction metrics, but not diversity metrics.\n\nThe main contribution of the paper to me is to realize that instead using a fixed grid like GCA to create voxel embeddings  leads to n^3 cost for the volume. In constrast, the authors use a sparse voxel embedding. They decode these embeddings with the method of  Chibane et al. to create a complete 3D shape. With this change, the method can scale to large scenes like from ShapeNet Scene and 3DFront, whereas GCA was limited to only small volumes(objects) of ShapeNet.\n\n",
          "main_review": "Writing: The paper is well written and complete. The abstract lays out the structure of the paper and the text matches mostly up to the expectations set in the abstract and introduction. I did not find any obvious error. The contribution are explicitly stated and thereforce can be easily verified. The mathematical notation once introduced is static and easy to parse. The figures are sufficiently high resolution and demonstrative of the improvements due to the proposed method.\n\nImplementation Details: I believe that the implementation is sufficiently detailed for a reader to be able to implement the algorithm - the previous models and their hyperparameters are described as are the datasets and their preprocessing steps.\n\nContribution: The paper extends GCA by using sparse voxel embeddings and a Distance Field method from Chibane et al. I think the contribution is pretty significant as the formulation under the new regime is vastly different from the old GCA in terms of implementation and engineering. The authors I think conclusively show that the new method is better performing on large scenes (Tables 1 and 2), while being competitive for single instances of 3D ojects (Table 3).\n\nComparisons: One could always ask from more comparisons, but I believe that the baselines are sufficient with two large scale methods - ConvOcc and IFNet, and comparison to the baseline GCA. Although it would be interesting to see how the authors managed to train GCA on such large scenes.\n\nProofs: The mathematics in the main paper was was checked and passed my review. However, I did not check the proofs in the supplementary.\n\nQuestions:\n(Infusion Traninig): I am not entirely sure how the Infusion kernel is different from the infusion chain in GCA apart from the problem of having o_c inthe generative model itself instead of being defined on a grid. I would like to hear more explanation from the authors as the final update rule (pg 5, training procedure) is the same as the training procedure of GCA (Algorithm 1).\n\n(Other methods): The authors do not make it clear how the other methods' performance was assessed - was it from the reported number or retrained with their preprocessing.\n\n(Mode Seeking): The authors do not provide a model without mode seeking. It would be interesting to see the performance from such a model.\n\n\n\n\n",
          "summary_of_the_review": "Summary of review: The paper is very well written with clear mathematics and clear delineation of contributions. The authors present a modification that makes older work scalable and show good results on multiple datasets, competing against the baseline method as well as other methods.\n\n",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "No concerns",
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_yCG6"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_yCG6"
        ]
      },
      {
        "id": "0CMiMMdDEyB",
        "original": null,
        "number": 5,
        "cdate": 1635928564029,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635928564029,
        "tmdate": 1637650127814,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The submitted paper studies 3D scene geometry reconstruction and proposed a pipeline for predicting signed distance fields from incomplete pointcloud data. The pipeline consists of two major components: an autoencoder that maps between the latent space and the voxel representations, and a Continuous Generative Cellular Automata (cGCA) which progressively and stochastically generates more complete data at each step. The proposed method is trained and evaluated on several synthetic datasets, including ShapeNet Scene, 3DFront, as well as ShapeNet, with a controlled sub-sampled pointcloud as input (with guaranteed rate of points for each object). Under these settings, both quantitative and qualitative results outperformed prior work. \n",
          "main_review": "Strengths:\n1. The proposed method is clean and simple. The key component, cGCA, is a relatively straightforward extension of the prior work GCA to include latent embeddings and to use signed distance fields, but it is still brings enough novelty and contribution.\n2. Being able to generate stochastic outputs is a significant advantage compared to prior work.\n3. Under the experimental setup, the proposed method clearly outperforms prior work\n\nWeaknesses:\n1. Lack of experiments on real-world datasets: despite authors referred to this as one of the future directions, I think it is an important experiment to include as part of this submission. The noise statistics can be quite different, which could potentially break the proposed method.\n2. My above concern is compounded by the fact that the authors were using a specific subsampling strategy to create synthetic inputs - i.e., each object has a guaranteed rate of surface points. This is significantly different from real-world pointcloud distributions. Are the prior work also re-trained specifically for this subsampled inputs? I would like to know how the method performs under uniform subsampling.\n3. [Minor] In Equation (3), is $Q$ not defined? ",
          "summary_of_the_review": "The authors' response and Appendix F & G address my concern regarding real-world experiments, so I am raising my ratings to \"8: accept, good paper\".\n\n> [Original Review]\nMy current recommendation is \"6: marginally above the acceptance threshold\", given the simplicity of the method and its results on synthetic dataset. I am concerned about its actual performance on real-world data, and I am happy to increase my rating if the method demonstrates similar improvement over prior work on real-world datasets. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "Not applicable",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Reviewer_mswD"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Reviewer_mswD"
        ]
      },
      {
        "id": "2mAe5Osascy",
        "original": null,
        "number": 13,
        "cdate": 1637205740311,
        "mdate": 1637205740311,
        "ddate": null,
        "tcdate": 1637205740311,
        "tmdate": 1637205740311,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Official_Comment",
        "content": {
          "title": "Common Response to All Reviewers",
          "comment": "We first would like to appreciate the thoughtful comments by the reviewers.\n\n\nHere are the summary of the main changes and additional experiments conducted in the response of the reviews:\n- We elaborate the difference of our proposed formulation compared to GCA in Appendix C.\n- We added an ablation study of mode seeking steps in Appendix E.\n- We examined the performance analysis under both sparse and non-ambiguous input in Appendix F.\n- We provide results of our approach in real-world scans using the ScanNet dataset in Appendix G.\n\n\nAlso, we would like to emphasize that our work is not only presenting a method with superior performance but also is unique as it models the multi-modal distribution in the context of scene completion. Modeling the multi-modal distribution is crucial for high-quality generation in an under-constrained set-up, as a deterministic generative model would likely generate a blurry reconstruction given ambiguous input. Nonetheless, the field of learning multi-modal distribution is yet to be explored, to the best of our knowledge. Our contribution includes handling the high-dimensional solution space for a large-scale incomplete 3D scene.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper4/Authors"
        ]
      },
      {
        "id": "-bEai7YVHEi",
        "original": null,
        "number": 1,
        "cdate": 1642696832460,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696832460,
        "tmdate": 1642696832460,
        "tddate": null,
        "forum": "BnQhMqDfcKG",
        "replyto": "BnQhMqDfcKG",
        "invitation": "ICLR.cc/2022/Conference/Paper4/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Spotlight)",
          "comment": "This paper introduced a probabilistic extension to a pipeline for 3D scene geometry reconstruction from large-scale point clouds. \nAll reviewers recognized the significance of the proposed approach and praised the simplicity of deriving a probabilistic version of Generative Cellular Automata that performs well in a number of reconstruction benchmarks. Authors were responsive during rebuttal and managed to clarify the concerns raised about the limited scope of the experiments and certain parameters involved, and also raise one reviewer's scores."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}