{
  "id": "EAy7C1cgE1L",
  "original": "ixbEe4Ra4Iv",
  "number": 64,
  "cdate": 1632875426095,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875426095,
  "tmdate": 1676330690621,
  "ddate": null,
  "content": {
    "title": "Increasing the Cost of Model Extraction with Calibrated Proof of Work",
    "authorids": [
      "~Adam_Dziedzic1",
      "~Muhammad_Ahmad_Kaleem1",
      "~Yu_Shen_Lu1",
      "~Nicolas_Papernot1"
    ],
    "authors": [
      "Adam Dziedzic",
      "Muhammad Ahmad Kaleem",
      "Yu Shen Lu",
      "Nicolas Papernot"
    ],
    "keywords": [
      "model extraction",
      "model stealing",
      "model functionality stealing",
      "proof-of-work",
      "adversarial machine learning",
      "trustworthy machine learning",
      "deep learning"
    ],
    "abstract": "In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.",
    "one-sentence_summary": "We propose to make model extraction more difficult by requiring users to complete a callibrated proof-of-work before they can read predictions from a machine learning model exposed via a public API.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "dziedzic|increasing_the_cost_of_model_extraction_with_calibrated_proof_of_work",
    "pdf": "/pdf/c75fb6727a40e11e23ec143778cd6e178a0681f3.pdf",
    "supplementary_material": "/attachment/32bc06f0c03c140a3e8513c39087e341066fd727.zip",
    "_bibtex": "@inproceedings{\ndziedzic2022increasing,\ntitle={Increasing the Cost of Model Extraction with Calibrated Proof of Work},\nauthor={Adam Dziedzic and Muhammad Ahmad Kaleem and Yu Shen Lu and Nicolas Papernot},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EAy7C1cgE1L}\n}",
    "venue": "ICLR 2022 Spotlight",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "EAy7C1cgE1L",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 26,
    "directReplyCount": 6,
    "revisions": true,
    "replies": [
      {
        "id": "SrNOizC9BuU",
        "original": null,
        "number": 1,
        "cdate": 1634845808263,
        "mdate": null,
        "ddate": null,
        "tcdate": 1634845808263,
        "tmdate": 1634845808263,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "EAy7C1cgE1L",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a novel defense to prevent model stealing by requiring users to solve proof-of-work puzzles. The authors evaluate their method against different types of model extraction attacks. The results show that the defense will result in the attacker costing higher computational time (100x) than legitimate users (2x).\n",
          "main_review": "This paper proposes a defense against model extraction attacks by forcing users to do a proof-of-work (PoW) puzzle before they receive the labels from the victim model. To minimize the impact on legitimate users, the defense tunes the per-query difficulty based on the extent of information leakage. The privacy cost is evaluated via differential privacy budget. Compared to other defense methods, this method will not reduce the accuracy of answers, and it can protect the model before the model stealing happens (unlike watermark schemes).\n\nMy comments are as follows.\n\nFirstly, I think the structure of the paper can be improved. The first two sections of the paper take up a lot of space, so that the discussion in section 3 is not clear enough. Some experimental results are placed in the appendix, and some tables interspersed in the reference list.\n\nSecondly, section 3.3 shows how to use differential privacy to evaluate the privacy cost. Though the authors explain \"We compute the privacy cost of queries using the PATE framework\",  \"The privacy cost is computed based on the consensus among teachers and the amount of privacy noise added to the histogram\" and  \"more details can be found in appendix D\", I would suggest to show the main formulas and conclusions here to help the readers to follow.\n\nThirdly, the authors create two models. The first one is used to predict query time from the privacy cost, the second one is used to predict the number of leading zeros for the desired time. So the whole mapping flow is : privacy cost \u2192 query time \u2192 the number of leading zeros. The authors find that using a simple linear regression is sufficient for both models. Firstly, Figure 2 shows the dependency between the computation time and the number of leading zeros is exponential, is it contradictory? Secondly, I think there should be a clear mapping from privacy cost to the number of leading zeros, which will help practitioners easily to tune parameters to control the planned results, such as \"Time POW\" in Table 2. Thirdly, I can't find the results about these linear regression models in the experiment part.\n\nFinally, the difference in privacy cost between attackers and legitimate users is the main evaluation metric. Figure 3 shows a higher privacy leakage for the attackers compared to a standard user, but for the CIFAR10 dataset, this distinction is not very obvious, and it causes the computational cost for attackers to increase a little bit (In Table 2, line 16, for the attack COPYCAT on CIFAR10, time changes from 148.3 to 326). This situation may be caused by the ambiguous mapping from privacy cost to computational time, and it may pose a risk of defense failure. So more analysis is needed here.\n",
          "summary_of_the_review": "see main review",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Reviewer_emnb"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Reviewer_emnb"
        ]
      },
      {
        "id": "cNcCvwkhLED",
        "original": null,
        "number": 2,
        "cdate": 1635435234860,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635435234860,
        "tmdate": 1635435234860,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "EAy7C1cgE1L",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a new defense to the model stealing problem that an attacker queries a supervised machine learning model service to have data labeled to use as training data and copy the functionality of the model. The proposed method is proactive defense that slows down the attacker in obtaining the labeled data, based on the information leakage estimator. The evaluation shows that existing attacks using out-of-distribution data can be slow down more than a regular user querying in-distribution data.",
          "main_review": "Strengths\n- S1: This paper proposes a novel defense focusing on delaying the model stealing process.\n- S2: The proposed defense is tested against 6 attacks on 3 datasets.\n\nWeaknesses\n- W1: If the goal of the proof of work is just to slow down the process, the server can simply choose to delay the response, unlike the distributed block chain. Thus, the proof of work does not look necessary.\n- W2: The defense does not work against an attacker using in-distribution data, such as a subscribed user using the model normally until enough data is accumulated.\n- W3: The use of PATE as the information leakage estimator is not justified. PATE is devised to protect sensitive data, and how sensitive data is connected to the performance of a machine learning model is unclear. This component is rather the most important part of the framework as it dictates the user experience and the attacker's success. A detailed reasoning or comparison to other options should improve the paper.",
          "summary_of_the_review": "This paper is in general well written, and the experiment was done to show the strengths and the weaknesses of the proposed defense. The defense against model stealing is inherently difficult problem where the attacker in the end can steal the model using the queried data and returned label pairs unless architecture or resource is no object (e.g., if not GPT-3). The main idea proposed in the paper against such a strong attack looks interesting, but the key technique used here is the proof of work. I am not convinced this is necessary at all since the original model is hosted and controlled centrally, and response time can be fully controlled without using the proof of work. Just delaying the response would have much better control of information leakage per time unit than the proof of work that a powerful machine can do faster, or simultaneously processing multiple queries with multiple machines reaching up to the speed of a regular user. Rather the important piece is the information leakage estimator which is somewhat overlooked in the paper, especially in terms of why PATE has to be the right choice here. I think focusing on this can lead to a better paper.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Reviewer_LLkZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Reviewer_LLkZ"
        ]
      },
      {
        "id": "h5jcDSQcnlT",
        "original": null,
        "number": 3,
        "cdate": 1635868131150,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635868131150,
        "tmdate": 1637666722699,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "EAy7C1cgE1L",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a novel defense against model extraction attacks. The proposed approach slows information leakage by asking all users to answer a puzzle before receiving the response to their query (proof-of-work). The difficulty of the puzzle allows to keep the computation overhead low for legitimate users, while rendering information leakage prohibitively expensive for attackers. The puzzle difficulty is calibrated based on an estimate of how much information each user has already acquired. This is based on PATE, a differential privacy metric. Experiments are performed on multiple datasets, opposing the proposed defense to a wide range of attacks, including adaptive adversaries.",
          "main_review": "Strengths:\n- Interesting idea of applying proof-of-work to machine learning as an entry barrier.\n- The novelty of the paper seems to reside in the application of existing concepts, like PATE for privacy cost estimation and HashCash as challenge, as components of a theft defense method. I find this novel enough and quite ingenious. Moreover, the existing concepts were appropriately adapted for the task at hand.\n- The experimental section is extensive, and the experimental protocol seems appropriate.\n- The paper is clear, well organized and pleasant to read.\n- Good quality code base allowing to reproduce the experiments.\n\nWeaknesses:\n- The proposed defense does not seem effective against attackers that have access to data from a similar distribution to the training one (e.g., MixMatch baseline).\n- The Knockoff attack does not seem to use its most effective querying strategy (Random is used instead of Adaptive).\n- Doubling computation cost for a legitimate user is not a prohibitive cost, but is also non-negligible, both from an effort and computation time perspective, as well as the CO2 emissions mentioned in the ethics statement of the paper.\n\nOther comments / questions:\n- With the proposed approach, does it mean that legitimate users would incur higher query cost when submitting many queries?\n- The paper states multiple times that it proposes the first defense to leave model accuracy intact, but I would argue that it is the first *active* defense to do so.\n\n[Update post-discussion] I am raising my rating following the exchanges below.",
          "summary_of_the_review": "Novel active strategy to defend against model extraction, good experimental results.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Reviewer_hR7i"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Reviewer_hR7i"
        ]
      },
      {
        "id": "0j6Z8UB_V7K",
        "original": null,
        "number": 4,
        "cdate": 1636121385275,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636121385275,
        "tmdate": 1636121385275,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "EAy7C1cgE1L",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper discusses ML model extraction attacks while using APIs for accessing them on the\npublic networks. Although some methods exist for preventing or making the attacks hard, all of\nthem have substantial impacts on legitimate users\u2019 experience while using the system, including\nthe slower models or lower accuracy in results. This paper proposed a method for dissuading\nthe attacker by increasing the cost of the attack. Solving a puzzle for all users before getting\nthe final response (POW) would be the solution noting that the difficulty will be increased if\nthe system identifies any adverse behaviors. This method requires no modification of the victim\nmodel and can be applied by machine learning practitioners to guard their publicly exposed\nmodels against being easily stolen.",
          "main_review": "The paper discussed the issue, and the background works well. The purpose is clear, and\nthere are a satisfying amount of experiments for validating the proposed solution. However,\nthe privacy scoring section is not clear. I cannot find out why PATE is the best option for\ncalculating privacy metrics and the exact functionality of this section is not as straightforward\nas other parts. Moreover, identification of users is essential since, as mentioned, the difficulty of\nPOW will increase gradually, but this concern is not well pointed in the paper.",
          "summary_of_the_review": "The authors should make the privacy scoring module more clear and add more discussion about identifying malicious nodes in continuous requests",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Reviewer_aHDe"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Reviewer_aHDe"
        ]
      },
      {
        "id": "SDG9E6mBUam",
        "original": null,
        "number": 1,
        "cdate": 1637003137024,
        "mdate": 1637003137024,
        "ddate": null,
        "tcdate": 1637003137024,
        "tmdate": 1637003137024,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "0j6Z8UB_V7K",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "PATE, privacy scoring, and identification of users",
          "comment": "We appreciate the positive, encouraging, and constructive feedback. We thank the reviewer for the detailed analysis of our paper and below provide a case-by-case response to the comments.\n\n>_**I cannot find out why PATE is the best option for calculating privacy metrics.**_\n\nWe have modified the manuscript and added this statement in Section 3.3:\n\nThere are two canonical approaches to obtaining privacy in deep learning: DP-SGD and PATE. DP-SGD is not applicable in our case as it is an algorithm-level mechanism used during training, which does not measure privacy leakage for individual test queries. On the other hand, PATE is an output level mechanism, which allows us to measure per-query privacy leakage. \n\n>_**The authors should make the privacy scoring module more clear. The exact functionality of this section is not as straightforward as other parts.**_\n\nThe goal of the privacy scoring module is to explain how to compute the privacy per query and the way the difficulty of a PoW puzzle is selected. We further extended the explanation of the privacy scoring in Section 3.3:\n\nPATE creates teacher models using the defender's training data and predictions from each teacher are aggregated into a histogram where $n_i$ indicates the number of teachers who voted for class $i$. The privacy cost is computed based on the consensus among teachers and it is smaller when more teachers agree. The consensus is high if we have a low probability that the label with the maximum number of votes is not returned. This probability is expressed as: $\\frac{1}{2} \\sum_{i \\ne i^*} \\text{erfc}(\\frac{n_{i^*} - n_{i}}{2\\sigma})$, where $i^*$ is an index of the correct class, $\\text{erfc}$ is the complementary error function, and $\\sigma$ is the privacy noise (we explain the details in Appendix D). Then a student model is trained by transferring knowledge acquired by the teachers in a privacy-preserving way. This student model corresponds to the attacker\u2019s model trained during extraction. [Papernot et al. (2017a)](https://arxiv.org/pdf/1610.05755.pdf) present the analysis (e.g., Table 2) of how the performance (in terms of accuracy) of the student model increases when a higher privacy loss is incurred through the answered queries.\n\n| Dataset | $\\varepsilon$ | Student\u2019s accuracy |\n|---------|---------------|--------------------|\n| MNIST   | 2.04          | 98.00%             |\n| MNIST   | 8.03          | 98.10%             |\n| SVHN    | 5.04          | 82.72%             |\n| SVHN    | 8.19          | 90.66%             |\n\nIf the reviewer has any other specific suggestions, we would be happy to further revise the manuscript.\n\n>_**The identification of users is essential since, as mentioned, the difficulty of PoW will increase gradually, but this concern is not well pointed in the paper.**_\n\nThank you for pointing this out. We modified the manuscript and added into Section 3.2 the following statement: *Since our defense works on a per-user basis, it requires the identification of users.*\n\nAt the end of Section 4.4, we also state: \"Another adaptive adversary can attack our operation model with multiple fake accounts instead of funneling all their queries through a single account or there can be multiple colluding users. Our approach not only defends against this because the attacker still needs to solve the proof-of-work for each of the user accounts, but it also simplifies the task of identifying colluding users. We note that one appealing aspect of our defense is that it relies on the privacy cost that can easily be summed across users because of the composition property of differential privacy.\u201d\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "pqRcu-v4OfY",
        "original": null,
        "number": 2,
        "cdate": 1637012503195,
        "mdate": 1637012503195,
        "ddate": null,
        "tcdate": 1637012503195,
        "tmdate": 1637012503195,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "h5jcDSQcnlT",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "The Knockoff attack",
          "comment": "We thank the reviewer for the insightful comments. We address individual points below one by one:\n\n>_**The Knockoff attack does not seem to use its most effective querying strategy (Random is used instead of Adaptive).**_\n\nThank you for the comment, we ran additional experiments following your suggestion. For context, we initially observed that the adaptive Knockoff attack shows only marginal improvements with ImageNet in the case of Caltech256, CUBS200, and Indoor67 datasets and even performs worse for Diabetic5, as shown in [the original paper (Table 2, Open ILSVRC)](https://bit.ly/3koxLLn). In addition, the code for the adaptive attack was not released in the original implementation and one implementation of it we did find was not scalable for the large number of queries we operate within our testing. Specifically, we used the adversarial-robustness-toolbox library: https://github.com/Trusted-AI/adversarial-robustness-toolbox (ART) to test the adaptive method and we faced out-of-memory errors close to 10000 queries. We can try to investigate it further but think this is a result of the requirement in the RL aspect of the algorithm to retrain the stolen model for every single query sent to the victim model. Thus, we did not include an evaluation of the adaptive method in our original submission.\n\nThat said, following the reviewer\u2019s comment, we did compare the random and adaptive methods for a smaller number of queries on the MNIST dataset (using CIFAR10 and SVHN as the attacker\u2019s datasets) and with the CIFAR10 dataset using CIFAR100 dataset as the attacker\u2019s dataset. In all cases, we observed a better attacker performance with the random method (see the results below). Furthermore, the adaptive method assumes access to the ground truth labels for the dataset used for querying. While this may be possible if a common dataset e.g. Imagenet is used, this is not necessarily true for all datasets an attacker can use. Therefore, the adaptive attack (from the ADT library) not only performs worse than random but also requires more adversarial knowledge compared to all other attacks which assume no access to the true labels for the attacker dataset used.\n\nMNIST victim model accuracy: 98.5 %. Using CIFAR10 as the OOD dataset: \n\n|Number of Queries|Accuracy of Stolen Model (Random)|Accuracy of Stolen Model (Adaptive)|\n|-----------------|---------------------------------|-----------------------------------|\n|2000             |64.70%                           |54.50%                             |\n|4000             |67.90%                           |62.20%                             |\n|6000             |58.60%                           |43.80%                             |\n|8000             |78.50%                           |70.10%                             |\n\nMNIST victim model accuracy: 98.5 %. Using SVHN as the OOD dataset:\n\n|Number of Queries|Accuracy of Stolen Model (Random)|Accuracy of Stolen Model (Adaptive)|\n|-----------------|---------------------------------|-----------------------------------|\n|2000             |24.2 %                           |8.9 %                              |\n|4000             |36.40%                           |25.80%                             |\n|6000             |53 %                             |13.2 %                             |\n|7000             |45.2 %                           |25.80%                             |\n\nCIFAR10 Victim model accuracy: 47.6 % (the initial result for a low victim's accuracy). Using CIFAR100 as the OOD dataset:\n\n|Number of Queries|Accuracy of Stolen Model (Random)|Accuracy of Stolen Model (Adaptive)|\n|-----------------|---------------------------------|-----------------------------------|\n|1000             |29.9 %                           |25.70%                             |\n|2000             |34.7 %                           |24.1 %                             |"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "7MaisyJ5ng",
        "original": null,
        "number": 3,
        "cdate": 1637012950127,
        "mdate": 1637012950127,
        "ddate": null,
        "tcdate": 1637012950127,
        "tmdate": 1637012950127,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "pqRcu-v4OfY",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "MixMatch, Computation Time & Query Cost",
          "comment": ">_**The proposed defense does not seem effective against attackers that have access to data from a similar distribution to the training one (e.g., MixMatch baseline).**_\n\nIn the case where an attacker has access to in distribution data and acts in the same way as a legitimate user, preventing model extraction is not possible. This is because we are not able to differentiate the queries from a standard user vs an attacker. Unless we negatively impact the accuracy of labels returned to standard users, the attacker in such a case cannot be defended against. Nevertheless, we evaluated the in-distribution adversary to obtain a worst-case evaluation of our approach.\n\n>**Doubling computation cost for a legitimate user is not a prohibitive cost, but is also non-negligible, both from an effort and computation time perspective, as well as the CO$_2$ emissions mentioned in the ethics statement of the paper.**\n\nTo address the problem, our approach can be adapted to rely on PoET (Proof-of-Elapsed-Time) instead of PoW. The users\u2019 resources (e.g., a CPU) would have to be occupied for a specific amount of time without executing any work. At the end of the waiting time, the user sends proof of the elapsed time instead of proof of work. Both proofs can be easily verified. [PoET](https://bit.ly/3bS7Qat) reduces the potential energy wastage in comparison with PoW but requires access to specialized hardware, namely new secure CPU instructions that are becoming widely available in consumer and enterprise processors. If a user does not have such hardware on-premise, the proof of elapsed time could be produced using a service exposed by a cloud provider (e.g., [Azure VMs that feature TEE](https://bit.ly/3D3YBzG)).\n\nAs far as the overhead factor of 2 is concerned, previous work [[1](https://openreview.net/forum?id=SyevYxHtDB),[2](https://bit.ly/3wG0YX6)] and our results suggest that there is *no free lunch*: the active defenses against model extraction either sacrifice the accuracy of the returned answers or force users to do more work. The principal strength of our active defense approach is that it does not sacrifice accuracy, which thus comes at the expense of computational overhead.\n\nWe also added the above answer to the Ethics Statement.\n\nFurthermore, we decreased the cost for legitimate users in Table 2 from 2x to 1.23x and also tested that even with more queries their cost does not exceed 2x.\n\n[1] [Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks.](https://openreview.net/forum?id=SyevYxHtDB) Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz. ICLR 2020.\n\n[2] [Defending Against Model Stealing Attacks With Adaptive Misinformation](https://bit.ly/3wG0YX6). Sanjay Kariyappa, Moinuddin K. Qureshi. CVPR 2020.\n\n>**With the proposed approach, does it mean that legitimate users would incur higher query costs when submitting many queries?**\n\nWe modified the approach to make sure that the legitimate user does not incur a higher query cost when submitting many queries. The general idea is that we treat the privacy cost of a legitimate user for a given number of queries as a baseline and the more a given user deviates from this baseline, the higher query cost is incurred. We provide a more detailed description of our approach in Section 3.3.\n\nWe do not make it impossible to extract a model, but try to create a defense so that no advantage can be gained by selecting queries in a specific way. The defense is successful when an attacker is mapped back to the standard information gain that a legitimate user has. For an adversarial user, the required PoW should be higher to dissuade such a user from an attack (or incentivize the user to act legitimately). In the standard PATE framework, we train a student model, which we can think of as an attacker\u2019s model. The teacher models return predictions to queries that are then used to train the student. After reaching a specified privacy threshold $\\varepsilon$, teachers stop answering the queries. In our defense, we measure the privacy leakage incurred by a given user. For a legitimate user, there is also a threshold $\\varepsilon$ after which, the user could easily train its own model. The provider of the ML model can define what is expected from a legitimate user by adjusting the $\\varepsilon$ value.\n\n>**The paper states multiple times that it proposes the first defense to leave model accuracy intact, but I would argue that it is the first active defense to do so.**\n\nThank you for pointing this out. We amended the paper to clarify that in the class of active methods, this is the first approach that leaves the model accuracy intact."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "MYsqgkOWqvi",
        "original": null,
        "number": 5,
        "cdate": 1637014266111,
        "mdate": 1637014266111,
        "ddate": null,
        "tcdate": 1637014266111,
        "tmdate": 1637014266111,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "sy3LpF9pqH",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Information Leakage Estimator",
          "comment": ">**Rather the important piece is the information leakage estimator which is somewhat overlooked in the paper, especially in terms of why PATE has to be the right choice here. I think focusing on this can lead to a better paper. \nW3: The use of PATE as the information leakage estimator is not justified. PATE is devised to protect sensitive data, and how sensitive data is connected to the performance of a machine learning model is unclear. This component is rather the most important part of the framework as it dictates the user experience and the attacker's success. Detailed reasoning or comparison to other options should improve the paper.**\n\nWe thank you for pointing out this lack of justification in our original submission. We have modified the manuscript and added this statement in Section 3.3:\n\nThere are two canonical approaches to obtaining privacy in deep learning: DP-SGD and PATE. DP-SGD is not applicable in our case as it is an algorithm-level mechanism used during training, which does not measure privacy leakage for individual test queries. On the other hand, PATE is an output level mechanism, which allows us to measure per-query privacy leakage. \n\nPATE creates teacher models using the defender's training data and predictions from each teacher are aggregated into a histogram where $n_i$ indicates the number of teachers who voted for class $i$. The privacy cost is computed based on the consensus among teachers and it is smaller when more teachers agree. The consensus is high if we have a low probability that the label with the maximum number of votes is not returned. This probability is expressed as: $\\frac{1}{2} \\sum_{i \\ne i^*} \\text{erfc}(\\frac{n_{i^*} - n_{i}}{2\\sigma})$, where $i^*$ is an index of the correct class, $\\text{erfc}$ is the complementary error function, and $\\sigma$ is the privacy noise (see details in Appendix D). Then a student model is trained by transferring knowledge acquired by the teachers in a privacy-preserving way. This student model corresponds to the attacker\u2019s model trained during extraction. [Papernot et al. (2017a)](https://arxiv.org/pdf/1610.05755.pdf) present the analysis (e.g., Table 2) of how the performance (in terms of accuracy) of the student model increases when a higher privacy loss is incurred through the answered queries.\n\n|Dataset|$\\varepsilon$|Student\u2019s accuracy|\n|-------|-------------|------------------|\n|MNIST  |2.04         |98.00%            |\n|MNIST  |8.03         |98.10%            |\n|SVHN   |5.04         |82.72%            |\n|SVHN   |8.19         |90.66%            |\n\nWe further added more information to Section 3.3 on the information leakage estimator. We also explored other cost metrics (for example, gap or entropy) to assess the information leakage and present them in Appendix B.6. We found that the privacy cost obtained from PATE gives us the best performance, especially in the case of the \u201cworstcase\u201d adaptive adversary, where the other metrics would allow the attacker to obtain high performance of the stolen model despite incurring a small query cost. Thus, we selected the privacy metric for our defense."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "94UFCyJfV2_",
        "original": null,
        "number": 6,
        "cdate": 1637120333074,
        "mdate": 1637120333074,
        "ddate": null,
        "tcdate": 1637120333074,
        "tmdate": 1637120333074,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "SrNOizC9BuU",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Mapping from privacy cost to PoW difficulty",
          "comment": ">**I think there should be a clear mapping from privacy cost to the number of leading zeros, which will help practitioners easily tune parameters to control the planned results, such as \"Time POW\" in Table 2. I can't find the results about these linear regression models in the experiment part.**\n\nIn our method, first, we find the total privacy cost incurred and the number of queries issued by a given user. We compare the user\u2019s privacy cost with a legitimate privacy cost for the number of queries. Second, we take the absolute difference $x$ between the privacy costs, and the difference is then used to specify how much time should the user spend on solving a new PoW puzzle. This is done using a simple exponential function $f(x) = a^x$, where $a$ is selected by the model's owner. Finally, we map from the PoW time to the required number of leading zero bits in the puzzle.\n\nA model's owner can collect data about legitimate users. We extrapolate to an arbitrary number of queries to map it to the expected legitimate privacy cost. This is done via a simple linear regression. The model's owner also selects the cost function for PoW. In our case, we use HashCash, for which we compute the expected running time given a number of required zero bits. Because we have to map from an arbitrary PoW time to the number of bits, we interpolate the second dataset by another simple linear regression. This solution is modular and gives model owners a possibility of specifying legitimate users, how much an adversary should be penalized, and the choice of the PoW method.\n\nNote that we do not assume that we know how an attacker might query the victim model. We take the absolute difference which measures a deviation of the current user from the expected behavior. If an attacker is using OOD data, the privacy cost is higher than the standard one. On the other hand, if an attacker is adaptive and tries to minimize the privacy cost, the cost is lower than expected and the attacker is penalized as well. \n\n>**Finally, the difference in privacy cost between attackers and legitimate users is the main evaluation metric. Figure 3 shows a higher privacy leakage for the attackers compared to a standard user, but for the CIFAR10 dataset, this distinction is not very obvious, and it causes the computational cost for attackers to increase a little bit (In Table 2, line 16, for the attack COPYCAT on CIFAR10, time changes from 148.3 to 326). This situation may be caused by the ambiguous mapping from privacy cost to computational time, and it may pose a risk of defense failure. So more analysis is needed here.**\n\nFor the CIFAR10 dataset, we are able to create up to 50 PATE teacher models, and for MNIST and SVHN up to 250 teachers (as described in Section 4.2). In general, the more teacher models we can create, the more accurate the estimation of the privacy cost. This causes the observed less precise privacy estimation in the case of CIFAR10 when compared to MNIST or SVHN. \n\nNext, we improved the mapping from the privacy cost to the computational time, which increased the time from 326 to 23921 seconds. We reported the new results in Table 2 and described the mapping in Section 3.3."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "z6kEdxac8N",
        "original": null,
        "number": 7,
        "cdate": 1637120612332,
        "mdate": 1637120612332,
        "ddate": null,
        "tcdate": 1637120612332,
        "tmdate": 1637120612332,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "SrNOizC9BuU",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Organization of the paper, Privacy cost, Linear models",
          "comment": "Thank you very much for your detailed and insightful feedback. We address individual points below:\n\n>**Firstly, I think the structure of the paper can be improved. The first two sections of the paper take up a lot of space, so that the discussion in section 3 is not clear enough.**\n\nWe reduced the size of the first two Sections (Introduction and especially, the Related Work) and expanded Section 3 to make it more clear. If the reviewer has any specific points they think need more detail, we would be happy to further revise the manuscript.\n\n>**Some experimental results are placed in the appendix, and some tables are interspersed in the reference list.**\n\nWe separated the reference list and the results from the appendix so that they do not overlap. \n\nRegarding the experimental results, we include more graphs in the Appendix for completeness, and for clarity so that they are formatted large enough to be readable. If the reviewer has further suggestions on which experimental results should be moved back into the main paper, we will decrease the size of other sections further to make room. \n\n>**Secondly, section 3.3 shows how to use differential privacy to evaluate the privacy cost. Though the authors explain \"We compute the privacy cost of queries using the PATE framework\", \"The privacy cost is computed based on the consensus among teachers and the amount of privacy noise added to the histogram\" and \"more details can be found in Appendix D\", I would suggest showing the main formulas and conclusions here to help the readers to follow.**\n\nWe would like to thank the reviewer for this remark. In our revised manuscript, we added more details directly in Section 3.3, specifically: \n\u201cPATE creates teacher models using the defender's training data and predictions from each teacher are aggregated into a histogram where $n_i$ indicates the number of teachers who voted for class $i$. The privacy cost is computed based on the consensus among teachers and it is smaller when more teachers agree. The consensus is high if we have a low probability that the label with the maximum number of votes is not returned. This probability is expressed as: $\\frac{1}{2} \\sum_{i \\ne i^*} \\text{erfc}(\\frac{n_{i^*} - n_{i}}{2\\sigma})$, where $i^*$ is an index of the correct class, $\\text{erfc}$ is the complementary error function, and $\\sigma$ is the privacy noise (we explain the details in Appendix D). Then a student model is trained by transferring knowledge acquired by the teachers in a privacy-preserving way. This student model corresponds to the attacker\u2019s model trained during extraction. [Papernot et al. (2017a)](https://arxiv.org/pdf/1610.05755.pdf) present the analysis (e.g., Table 2) of how the performance (in terms of accuracy) of the student model increases when a higher privacy loss is incurred through the answered queries.\u201d\n\n| Dataset | $\\varepsilon$ | Student\u2019s accuracy |\n|---------|---------------|--------------------|\n| MNIST   | 2.04          | 98.00%             |\n| MNIST   | 8.03          | 98.10%             |\n| SVHN    | 5.04          | 82.72%             |\n| SVHN    | 8.19          | 90.66%             |\n\nIf the reviewer thinks we should add other formulas to Section 3.3, we would be happy to further revise our manuscript.\n\n>**The authors find that using a simple linear regression is sufficient for both models. Firstly, Figure 2 shows the dependency between the computation time and the number of leading zeros is exponential, is it contradictory?**\n\nWe thank the reviewer for this question and have revised the manuscript in section 3.2 to make this more clear. We take the logarithm of the computation time, which transforms this exponential dependency between the computation time and the number of leading zeros to a linear dependency. In our case, modeling this dependency with a linear regression is sufficient."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "G0CafShkOb",
        "original": null,
        "number": 8,
        "cdate": 1637159582092,
        "mdate": 1637159582092,
        "ddate": null,
        "tcdate": 1637159582092,
        "tmdate": 1637159582092,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "sy3LpF9pqH",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "proof of work",
          "comment": "\"because the attacker still needs to solve the proof-of-work for each of the user accounts, but it also simplifies the task of identifying colluding users\"\n\n- How does your method identify colluding users?\n- Can proof of work be replaced by delaying the answer? You can do the same for all users using the amount of privacy leakage, which is a separate module. I do not yet understand what a delayed response cannot do compared to proof of work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Reviewer_LLkZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Reviewer_LLkZ"
        ]
      },
      {
        "id": "8__9EOaxc_g",
        "original": null,
        "number": 9,
        "cdate": 1637168365116,
        "mdate": 1637168365116,
        "ddate": null,
        "tcdate": 1637168365116,
        "tmdate": 1637168365116,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "G0CafShkOb",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Colluding users & Proof-of-Work",
          "comment": ">**How does your method identify colluding users?**\n\nWe note that since the privacy cost can be summed over multiple users, groups of users can be checked for total privacy costs and if unusual activity is detected, the users can be manually investigated by the model owner. Equivalently, Proof-of-Work over the total privacy cost can be applied to these users.\n\n>**Can proof of work be replaced by delaying the answer? You can do the same for all users using the amount of privacy leakage, which is a separate module. I do not yet understand what a delayed response cannot do compared to proof of work.**\n\nThe main motivation behind using Proof of Work is that it requires the user to do computational work rather than simply wait for some period of time. In other words, the delay in Proof of Work comes as a result of the client having to do work and the time is taken to solve the given puzzle whereas a regular delayed response requires no amount of work on the client-side. Therefore if solely a delayed response is used, an attacker can easily wait and possibly create a large number of clients which will only have to wait but not use any computational resources. Meanwhile, with our method, the client must use computational resources in order to get the predicted queries and so it is no longer as easy to create more user accounts who only wait. An added advantage of using PoW is that it directly leads to an exponential growth in time (see Section 3.2) which allows us to create a good mapping from the times required.\n\nTo implement the delayed response on the client-side, our approach can be adapted to rely on PoET (Proof-of-Elapsed-Time) instead of PoW. The users\u2019 resources (e.g., a CPU) would have to be occupied for a specific amount of time without executing any work. At the end of the waiting time, the user sends proof of the elapsed time instead of proof of work. Both proofs can be easily verified. [PoET](https://bit.ly/3bS7Qat) reduces the potential energy wastage in comparison with PoW but requires access to specialized hardware, namely new secure CPU instructions that are becoming widely available in consumer and enterprise processors. If a user does not have such hardware on-premise, the proof of elapsed time could be produced using a service exposed by a cloud provider (e.g., [Azure VMs that feature TEE](https://bit.ly/3D3YBzG))."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "vbptd7AzbG0",
        "original": null,
        "number": 10,
        "cdate": 1637533114995,
        "mdate": 1637533114995,
        "ddate": null,
        "tcdate": 1637533114995,
        "tmdate": 1637533114995,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "8__9EOaxc_g",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Have concerns been addressed?",
          "comment": "We would like to follow up on our answers, especially regarding colluding users and replacing the Proof-of-Work with delayed responses. Do our replies adequately address the reviewer's concerns?"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "aHhb3uXN7NN",
        "original": null,
        "number": 11,
        "cdate": 1637597753987,
        "mdate": 1637597753987,
        "ddate": null,
        "tcdate": 1637597753987,
        "tmdate": 1637597753987,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "vbptd7AzbG0",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Colluding users & Proof-of-Work ",
          "comment": "Thank you for the response.\n\n   > We note that since the privacy cost can be summed over multiple users, groups of users can be checked for total privacy costs and if unusual activity is detected, the users can be manually investigated by the model owner. Equivalently, Proof-of-Work over the total privacy cost can be applied to these users.\n\nIf I understand correctly, your method of finding the colluding users does not require any component from proof-of-work.\n\n\n   > an attacker can easily wait and possibly create a large number of clients ...\n\nI find this a bit contradicting to the above statement on the colluding users. Can we detect the colluding users and just refrain from directly responding to queries instead of proof of work? Or can this also cause denial of service to normal users?\n\nThis proof of work greatly reduces the practicality and usefulness of a cloud-based AI system that can does the heavy lifting for less powerful clients such as mobile phones. I am not sure how we can find a balance of work, without harming normal users, than doing better and focusing more on measuring the information leaking which is done with PATE only in the paper. I would suggest the authors more analysis on more practical respects using the information leakage estimator, and different scenarios, also measuring side effects in those scenarios (e.g., DoS attacks, side effects of proof of work vs. delayed response)."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Reviewer_LLkZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Reviewer_LLkZ"
        ]
      },
      {
        "id": "erCpHgmd-g",
        "original": null,
        "number": 12,
        "cdate": 1637636576238,
        "mdate": 1637636576238,
        "ddate": null,
        "tcdate": 1637636576238,
        "tmdate": 1637636576238,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "aHhb3uXN7NN",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Proof-of-Work, Colluding users, Less powerful clients, Information Leakage Estimator",
          "comment": "Thank you for the follow-up comments.\n\n>**Your method of finding the colluding users does not require any component from proof-of-work.**\n\nThe method of finding colluding users is based on the privacy cost of the queries. Once the users are identified, the difficulty of the proof-of-work can be increased according to their total privacy cost.\n\n>**Can we detect the colluding users and just refrain from directly responding to queries instead of proof of work? Or can this also cause a denial of service to normal users?**\n\nWe do not refrain from responding to queries because if the detection of colluding users is a false alarm then this harms legitimate users.\n\n>**This proof of work greatly reduces the practicality and usefulness of a cloud-based AI system that can do the heavy lifting for less powerful clients such as mobile phones. I am not sure how we can find a balance of work, without harming normal users.**\n\nWe show in the experimental results in Table 2 that for legitimate users the increase in the query time is up to 1.23x, which should be an acceptable impact even on less powerful clients. Then, the difficulty of proof of work is increased only if a user deviates from the legitimate behavior. The less powerful clients could also use a service exposed by a cloud provider to obtain the proof of work or the proof of elapsed time.\n\n>**Doing better and focusing more on measuring the information leaking which is done with PATE *only* in the paper.**\n\nAs we mentioned in [the previous answer](https://openreview.net/forum?id=EAy7C1cgE1L&noteId=MYsqgkOWqvi), we added more information to Section 3.3 on the information leakage estimator. Apart from PATE, we explored other cost metrics, for example, gap or entropy to assess the information leakage and presented the results in Appendix B.6 as well as in Tables 3 & 5, Figures: 5, 6, 11, 13, 14, 17, 19, 20, 21, and 22. \n\n>**I would suggest the authors more analysis on more practical respects using the information leakage estimator, and different scenarios, also measuring side effects in those scenarios (e.g., DoS attacks, side effects of proof of work vs. delayed response).**\n\nWe thank the reviewer for bringing this to our attention and have further revised the manuscript to emphasize these considerations. In DoS, the machine learning service would stop answering queries if attackers flooded it with many queries. However, using proof of work in our defense already protects the service from DoS attacks since it requires work on the user\u2019s end before the answers are released. We have added a discussion of the PoW vs delayed response via PoET in the revised manuscript. The side effect of our defense is computational work on the user\u2019s end. Compared to the server-side delayed response, our defense makes it more difficult for an attacker to steal a model."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "UKP88GOPmD",
        "original": null,
        "number": 13,
        "cdate": 1637636883705,
        "mdate": 1637636883705,
        "ddate": null,
        "tcdate": 1637636883705,
        "tmdate": 1637636883705,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "pqRcu-v4OfY",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Full results for the Knockoff Adaptive attack",
          "comment": "We solved the problem with the out-of-memory error by modifying [the ART library](https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/art/attacks/extraction/knockoff_nets.py). We replaced the explicit collection of sampled data points with their indices. This additional level of indirection helped us to save a substantial amount of memory. We updated Figure 4 and Table 2 in the manuscript with the additional results. Overall, we find that the Knockoff Random attack still works better than Knockoff Adaptive. \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "RGk8suxUY-M",
        "original": null,
        "number": 14,
        "cdate": 1637661643853,
        "mdate": 1637661643853,
        "ddate": null,
        "tcdate": 1637661643853,
        "tmdate": 1637661643853,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "UKP88GOPmD",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Thank you for the additional experimental results",
          "comment": "I appreciate the inclusion of results for Knockoff Adaptive attack here and in the paper, especially in view of the effort that seems to have gone into adapting the ART implementation. My comment on this topic has fully been addressed."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Reviewer_hR7i"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Reviewer_hR7i"
        ]
      },
      {
        "id": "hB6q4zAEWy4",
        "original": null,
        "number": 15,
        "cdate": 1637664016024,
        "mdate": 1637664016024,
        "ddate": null,
        "tcdate": 1637664016024,
        "tmdate": 1637664016024,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "RGk8suxUY-M",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Additional Experimental Results: the Knockoff Adaptive attack",
          "comment": "Thank you for the confirmation, we appreciate it."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "W1irx5it8Oj",
        "original": null,
        "number": 16,
        "cdate": 1637664654582,
        "mdate": 1637664654582,
        "ddate": null,
        "tcdate": 1637664654582,
        "tmdate": 1637664654582,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "7MaisyJ5ng",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Have concerns been addressed?",
          "comment": "We also wanted to ask: do our replies adequately address the reviewer's concerns regarding MixMatch, Computation Time, and Query Cost?"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "pnQk7oo_RwJ",
        "original": null,
        "number": 17,
        "cdate": 1637664963882,
        "mdate": 1637664963882,
        "ddate": null,
        "tcdate": 1637664963882,
        "tmdate": 1637664963882,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "SDG9E6mBUam",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Have concerns been addressed?",
          "comment": "We would like to follow up on our responses, especially on the privacy cost and the identification of users. Do they adequately address the reviewer's concerns?"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "vimYcUVB0AI",
        "original": null,
        "number": 18,
        "cdate": 1637665836276,
        "mdate": 1637665836276,
        "ddate": null,
        "tcdate": 1637665836276,
        "tmdate": 1637665836276,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "SrNOizC9BuU",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Have concerns been addressed?",
          "comment": "We would like to follow up on our answers, especially regarding the Proof-of-Work (PoW) difficulty, privacy cost, mapping from privacy to PoW, and the organization of the paper. Do our replies adequately address the reviewer's concerns?"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "GFWOqg3BcOk",
        "original": null,
        "number": 19,
        "cdate": 1637666651741,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637666651741,
        "tmdate": 1637666743958,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "W1irx5it8Oj",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Thank you for all your answers",
          "comment": "I appreciate the authors answering in detail the reviewers' questions. I think addressing these in the paper has added value. My concerns have been answered, I am thus raising my rating by one point."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Reviewer_hR7i"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Reviewer_hR7i"
        ]
      },
      {
        "id": "gCQY0Lv2QhC",
        "original": null,
        "number": 20,
        "cdate": 1637667732203,
        "mdate": 1637667732203,
        "ddate": null,
        "tcdate": 1637667732203,
        "tmdate": 1637667732203,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "GFWOqg3BcOk",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Thank you for the review & increasing the score",
          "comment": "We thank the reviewer for the discussion and appreciate the positive feedback."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "2Cg7RUNkgi",
        "original": null,
        "number": 21,
        "cdate": 1637757740419,
        "mdate": 1637757740419,
        "ddate": null,
        "tcdate": 1637757740419,
        "tmdate": 1637757740419,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "erCpHgmd-g",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Analysis using the information leakage estimator & different scenarios",
          "comment": "We would appreciate it if the reviewer could let us know if we can carry out any specific additional analysis or experiments.  "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "_MMTl8t2UBh",
        "original": null,
        "number": 22,
        "cdate": 1637846224141,
        "mdate": 1637846224141,
        "ddate": null,
        "tcdate": 1637846224141,
        "tmdate": 1637846224141,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "EAy7C1cgE1L",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Pending questions",
          "comment": "We would like to thank the reviewers for their questions and comments. The paper has definitely improved as a result. We would like to check one last time if there are any pending questions that we have not adequately addressed."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "EkTEFF6pR-F",
        "original": null,
        "number": 1,
        "cdate": 1642696835344,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696835344,
        "tmdate": 1642696835344,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "EAy7C1cgE1L",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Spotlight)",
          "comment": "the paper proposed a novel idea of  requiring users to complete a proof-of-work before they can read the model's prediction to prevent model extraction attacks. Reviewers were excited about the paper and ideas. Some misunderstanding raised by reviewers were sufficiently clarified by the authors in the rebuttal."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "SrNOizC9BuU",
        "original": null,
        "number": 1,
        "cdate": 1634845808263,
        "mdate": null,
        "ddate": null,
        "tcdate": 1634845808263,
        "tmdate": 1634845808263,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "EAy7C1cgE1L",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a novel defense to prevent model stealing by requiring users to solve proof-of-work puzzles. The authors evaluate their method against different types of model extraction attacks. The results show that the defense will result in the attacker costing higher computational time (100x) than legitimate users (2x).\n",
          "main_review": "This paper proposes a defense against model extraction attacks by forcing users to do a proof-of-work (PoW) puzzle before they receive the labels from the victim model. To minimize the impact on legitimate users, the defense tunes the per-query difficulty based on the extent of information leakage. The privacy cost is evaluated via differential privacy budget. Compared to other defense methods, this method will not reduce the accuracy of answers, and it can protect the model before the model stealing happens (unlike watermark schemes).\n\nMy comments are as follows.\n\nFirstly, I think the structure of the paper can be improved. The first two sections of the paper take up a lot of space, so that the discussion in section 3 is not clear enough. Some experimental results are placed in the appendix, and some tables interspersed in the reference list.\n\nSecondly, section 3.3 shows how to use differential privacy to evaluate the privacy cost. Though the authors explain \"We compute the privacy cost of queries using the PATE framework\",  \"The privacy cost is computed based on the consensus among teachers and the amount of privacy noise added to the histogram\" and  \"more details can be found in appendix D\", I would suggest to show the main formulas and conclusions here to help the readers to follow.\n\nThirdly, the authors create two models. The first one is used to predict query time from the privacy cost, the second one is used to predict the number of leading zeros for the desired time. So the whole mapping flow is : privacy cost \u2192 query time \u2192 the number of leading zeros. The authors find that using a simple linear regression is sufficient for both models. Firstly, Figure 2 shows the dependency between the computation time and the number of leading zeros is exponential, is it contradictory? Secondly, I think there should be a clear mapping from privacy cost to the number of leading zeros, which will help practitioners easily to tune parameters to control the planned results, such as \"Time POW\" in Table 2. Thirdly, I can't find the results about these linear regression models in the experiment part.\n\nFinally, the difference in privacy cost between attackers and legitimate users is the main evaluation metric. Figure 3 shows a higher privacy leakage for the attackers compared to a standard user, but for the CIFAR10 dataset, this distinction is not very obvious, and it causes the computational cost for attackers to increase a little bit (In Table 2, line 16, for the attack COPYCAT on CIFAR10, time changes from 148.3 to 326). This situation may be caused by the ambiguous mapping from privacy cost to computational time, and it may pose a risk of defense failure. So more analysis is needed here.\n",
          "summary_of_the_review": "see main review",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Reviewer_emnb"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Reviewer_emnb"
        ]
      },
      {
        "id": "cNcCvwkhLED",
        "original": null,
        "number": 2,
        "cdate": 1635435234860,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635435234860,
        "tmdate": 1635435234860,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "EAy7C1cgE1L",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a new defense to the model stealing problem that an attacker queries a supervised machine learning model service to have data labeled to use as training data and copy the functionality of the model. The proposed method is proactive defense that slows down the attacker in obtaining the labeled data, based on the information leakage estimator. The evaluation shows that existing attacks using out-of-distribution data can be slow down more than a regular user querying in-distribution data.",
          "main_review": "Strengths\n- S1: This paper proposes a novel defense focusing on delaying the model stealing process.\n- S2: The proposed defense is tested against 6 attacks on 3 datasets.\n\nWeaknesses\n- W1: If the goal of the proof of work is just to slow down the process, the server can simply choose to delay the response, unlike the distributed block chain. Thus, the proof of work does not look necessary.\n- W2: The defense does not work against an attacker using in-distribution data, such as a subscribed user using the model normally until enough data is accumulated.\n- W3: The use of PATE as the information leakage estimator is not justified. PATE is devised to protect sensitive data, and how sensitive data is connected to the performance of a machine learning model is unclear. This component is rather the most important part of the framework as it dictates the user experience and the attacker's success. A detailed reasoning or comparison to other options should improve the paper.",
          "summary_of_the_review": "This paper is in general well written, and the experiment was done to show the strengths and the weaknesses of the proposed defense. The defense against model stealing is inherently difficult problem where the attacker in the end can steal the model using the queried data and returned label pairs unless architecture or resource is no object (e.g., if not GPT-3). The main idea proposed in the paper against such a strong attack looks interesting, but the key technique used here is the proof of work. I am not convinced this is necessary at all since the original model is hosted and controlled centrally, and response time can be fully controlled without using the proof of work. Just delaying the response would have much better control of information leakage per time unit than the proof of work that a powerful machine can do faster, or simultaneously processing multiple queries with multiple machines reaching up to the speed of a regular user. Rather the important piece is the information leakage estimator which is somewhat overlooked in the paper, especially in terms of why PATE has to be the right choice here. I think focusing on this can lead to a better paper.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Reviewer_LLkZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Reviewer_LLkZ"
        ]
      },
      {
        "id": "h5jcDSQcnlT",
        "original": null,
        "number": 3,
        "cdate": 1635868131150,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635868131150,
        "tmdate": 1637666722699,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "EAy7C1cgE1L",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a novel defense against model extraction attacks. The proposed approach slows information leakage by asking all users to answer a puzzle before receiving the response to their query (proof-of-work). The difficulty of the puzzle allows to keep the computation overhead low for legitimate users, while rendering information leakage prohibitively expensive for attackers. The puzzle difficulty is calibrated based on an estimate of how much information each user has already acquired. This is based on PATE, a differential privacy metric. Experiments are performed on multiple datasets, opposing the proposed defense to a wide range of attacks, including adaptive adversaries.",
          "main_review": "Strengths:\n- Interesting idea of applying proof-of-work to machine learning as an entry barrier.\n- The novelty of the paper seems to reside in the application of existing concepts, like PATE for privacy cost estimation and HashCash as challenge, as components of a theft defense method. I find this novel enough and quite ingenious. Moreover, the existing concepts were appropriately adapted for the task at hand.\n- The experimental section is extensive, and the experimental protocol seems appropriate.\n- The paper is clear, well organized and pleasant to read.\n- Good quality code base allowing to reproduce the experiments.\n\nWeaknesses:\n- The proposed defense does not seem effective against attackers that have access to data from a similar distribution to the training one (e.g., MixMatch baseline).\n- The Knockoff attack does not seem to use its most effective querying strategy (Random is used instead of Adaptive).\n- Doubling computation cost for a legitimate user is not a prohibitive cost, but is also non-negligible, both from an effort and computation time perspective, as well as the CO2 emissions mentioned in the ethics statement of the paper.\n\nOther comments / questions:\n- With the proposed approach, does it mean that legitimate users would incur higher query cost when submitting many queries?\n- The paper states multiple times that it proposes the first defense to leave model accuracy intact, but I would argue that it is the first *active* defense to do so.\n\n[Update post-discussion] I am raising my rating following the exchanges below.",
          "summary_of_the_review": "Novel active strategy to defend against model extraction, good experimental results.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Reviewer_hR7i"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Reviewer_hR7i"
        ]
      },
      {
        "id": "0j6Z8UB_V7K",
        "original": null,
        "number": 4,
        "cdate": 1636121385275,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636121385275,
        "tmdate": 1636121385275,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "EAy7C1cgE1L",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper discusses ML model extraction attacks while using APIs for accessing them on the\npublic networks. Although some methods exist for preventing or making the attacks hard, all of\nthem have substantial impacts on legitimate users\u2019 experience while using the system, including\nthe slower models or lower accuracy in results. This paper proposed a method for dissuading\nthe attacker by increasing the cost of the attack. Solving a puzzle for all users before getting\nthe final response (POW) would be the solution noting that the difficulty will be increased if\nthe system identifies any adverse behaviors. This method requires no modification of the victim\nmodel and can be applied by machine learning practitioners to guard their publicly exposed\nmodels against being easily stolen.",
          "main_review": "The paper discussed the issue, and the background works well. The purpose is clear, and\nthere are a satisfying amount of experiments for validating the proposed solution. However,\nthe privacy scoring section is not clear. I cannot find out why PATE is the best option for\ncalculating privacy metrics and the exact functionality of this section is not as straightforward\nas other parts. Moreover, identification of users is essential since, as mentioned, the difficulty of\nPOW will increase gradually, but this concern is not well pointed in the paper.",
          "summary_of_the_review": "The authors should make the privacy scoring module more clear and add more discussion about identifying malicious nodes in continuous requests",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Reviewer_aHDe"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Reviewer_aHDe"
        ]
      },
      {
        "id": "_MMTl8t2UBh",
        "original": null,
        "number": 22,
        "cdate": 1637846224141,
        "mdate": 1637846224141,
        "ddate": null,
        "tcdate": 1637846224141,
        "tmdate": 1637846224141,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "EAy7C1cgE1L",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Official_Comment",
        "content": {
          "title": "Pending questions",
          "comment": "We would like to thank the reviewers for their questions and comments. The paper has definitely improved as a result. We would like to check one last time if there are any pending questions that we have not adequately addressed."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper64/Authors"
        ]
      },
      {
        "id": "EkTEFF6pR-F",
        "original": null,
        "number": 1,
        "cdate": 1642696835344,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696835344,
        "tmdate": 1642696835344,
        "tddate": null,
        "forum": "EAy7C1cgE1L",
        "replyto": "EAy7C1cgE1L",
        "invitation": "ICLR.cc/2022/Conference/Paper64/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Spotlight)",
          "comment": "the paper proposed a novel idea of  requiring users to complete a proof-of-work before they can read the model's prediction to prevent model extraction attacks. Reviewers were excited about the paper and ideas. Some misunderstanding raised by reviewers were sufficiently clarified by the authors in the rebuttal."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}