{
  "id": "H0oaWl6THa",
  "original": "a1eseyegUWI",
  "number": 158,
  "cdate": 1632875433092,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875433092,
  "tmdate": 1676330685781,
  "ddate": null,
  "content": {
    "title": "Hybrid Local SGD for Federated Learning with Heterogeneous Communications",
    "authorids": [
      "~Yuanxiong_Guo1",
      "~Ying_Sun5",
      "ruihu2017@gmail.com",
      "~Yanmin_Gong1"
    ],
    "authors": [
      "Yuanxiong Guo",
      "Ying Sun",
      "Rui Hu",
      "Yanmin Gong"
    ],
    "keywords": [
      "Federated Learning",
      "Communication Efficiency",
      "Heterogeneity",
      "Local SGD"
    ],
    "abstract": "Communication is a key bottleneck in federated learning where a large number of edge devices collaboratively learn a model under the orchestration of a central server without sharing their own training data. While local SGD has been proposed to reduce the number of FL rounds and become the algorithm of choice for FL, its total communication cost is still prohibitive when each device needs to communicate with the remote server repeatedly for many times over bandwidth-limited networks. In light of both device-to-device (D2D) and device-to-server (D2S) cooperation opportunities in modern communication networks, this paper proposes a new federated optimization algorithm dubbed hybrid local SGD (HL-SGD) in FL settings where devices are grouped into a set of disjoint clusters with high D2D communication bandwidth. HL-SGD subsumes previous proposed algorithms such as local SGD and gossip SGD and enables us to strike the best balance between model accuracy and runtime. We analyze the convergence of HL-SGD in the presence of heterogeneous data for general nonconvex settings. We also perform extensive experiments and show that the use of hybrid model aggregation via D2D and D2S communications in HL-SGD can largely speed up the training time of federated learning. ",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "guo|hybrid_local_sgd_for_federated_learning_with_heterogeneous_communications",
    "pdf": "/pdf/e765632eb816448e521a3fb0562e39c39651de07.pdf",
    "data": "",
    "_bibtex": "@inproceedings{\nguo2022hybrid,\ntitle={Hybrid Local {SGD} for Federated Learning with Heterogeneous Communications},\nauthor={Yuanxiong Guo and Ying Sun and Rui Hu and Yanmin Gong},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=H0oaWl6THa}\n}",
    "venue": "ICLR 2022 Spotlight",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "H0oaWl6THa",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 21,
    "directReplyCount": 7,
    "revisions": true,
    "replies": [
      {
        "id": "bQCxtRGvVQ",
        "original": null,
        "number": 1,
        "cdate": 1635749245465,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635749245465,
        "tmdate": 1637636729304,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes a new method to address the heterogeneous communication setting in a hierarchical distributed system.  The system consists of a central server and several disjoint clusters, and each cluster is a local network connection between multiple edge devices. The communications are heterogeneous in the sense that in cluster device to device (D2D) communication are much faster than the device to server (D2S) communication. The proposes algorithm is a hybrid method where decentralized method is applied within clusters (on edge devices) and federated average is applied between cluster and server. Convergence analysis is provided showing the benefit of variance reduction using decentralization within cluster. Empirical studies are conducted to demonstrate the improvement under synthetic federated learning environment.",
          "main_review": "The presentation of the paper is very clear and easy to follow. The strength of the paper lies in the theoretical analysis, where combining decentralization and federated average is not straightforward at all. The theoretical result is a bit hard to digest, further clarification and comparison with existing method would make it more readable. The experimental section is also less convincing. My detailed comments are as follows. \n\n1 Theoretical discussion:  it is not clear from the current presentation how to choose the parameter $\\tau$ (number of decentralized updates per rounds) Ideally, it would need to balance (i) the D2S communication cost and the D2D communication cost (ii) intra-cluster convergence and outer-loop (round) convergence. In other words, is there any tradeoff between improving the heterogeneous communication versus having better convergence rate. For the discussion on $\\rho_{max}$, how it scales with $n$ in a non-extreme situation. The current discussion mostly focus on it being 0 (fully connected) or 1 (no connection), it would be good to discuss its scaling with $n$ in a less trivial situation.\n\n2 Empirical study:  the experiments are performed with a fixed $\\tau$. However, the local SGD may have a different regime on the dependency of $\\tau$. I encourage the authors to provide more comparison on how $\\tau$ influence the performance in figure, for instance with $\\tau =5, 10, 20$. The ablation study in Figure 4& 5 is also not convincing as it seems no difference. In particular, changing $\\rho$ does not lead to any performance or run time change. This might suggest that the local problem is too simple that be efficiently solved even the connectivity is bad.  \n\n3 Privacy concern: the lack of discussion on the privacy issue is a major omission in the paper. The decentralized communication within cluster requires client to exchange information with others, raising potential concerns on leaking informations. I believe this need to be carefully addressed as privacy is one of the major difference in Federated Learning compared to traditional distributed training.\n\nOverall, the theoretical study has merit while the empirical study is less convincing,   ",
          "summary_of_the_review": "The theoretical study has merit while the empirical study is less convincing,   ",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Reviewer_y58n"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Reviewer_y58n"
        ]
      },
      {
        "id": "KgC53hbMukO",
        "original": null,
        "number": 2,
        "cdate": 1635874352127,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635874352127,
        "tmdate": 1635874352127,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a new Federated learning algorithm which can take advantage of the fast D2D (Device-to-Device) connections among the devices. In particular, it is shown that the proposed algorithm has better performance in terms of convergence rate and accuracy compared to the existing algorithms such as 'Local SGD'-based FL which rely only on the slow D2S connections for model updating. The convergence of the proposed algorithm is theoretically characterized in terms of the D2D connectivity (topology). Numerical results are provided to show the improved performance compared to the local sgd-based FL.\n",
          "main_review": "Strength:\nThe proposed algorithm takes good advantage of the fast local connectivity among the nodes to improve the convergence. It is a novel contribution. \n\nDetailed convergence analysis of the proposed algorithm is provided. \n\nWeakness:\nWhile the Theoretical results emphasize the role of the local topology in each cluster, the corresponding numerical analysis to understand the impact of topology is missing. It would be interesting to see how the connectivity parameter $\\rho$ impacts the results in Figure 1 to 5, instead of just taking a ring topology. ",
          "summary_of_the_review": "The proposed algorithm is novel and the the performance is rigorously characterized. The improvement compared to existing algorithms is significant and hence I believe it should be accepted for publication. ",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Reviewer_87Gc"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Reviewer_87Gc"
        ]
      },
      {
        "id": "Ci5HzH30-9e",
        "original": null,
        "number": 3,
        "cdate": 1635881249028,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635881249028,
        "tmdate": 1638690080167,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces HL-SGD, a method to design a hybrid federated learning method to leverage a hybrid high-speed D2D and low-speed D2S network and speed up the communication of federated learning applications. Both theoretical analysis and empirical results are provided to show the effectiveness of HL-SGD.",
          "main_review": "The paper proposes to use a hybrid model aggregation to conduct federated learning leveraging both high-speed D2D network and low-speed D2S network.\u00a0\n\nPros:\n1. The idea proposed in the paper is easy to follow, and the research direction of enhancing the communication efficiency of federated learning is potentially impactful.\u2028\n2. Both theoretical analysis and empirical results are provided to justify the effectiveness of HL-SGD.\u2028\nCons:\u00a0\n1. The main concern I have for the paper is that the proposed idea lacks novelty. Hierarchical federated learning has been studied extensively in the literature [1,6] as well as hierarchical local SGD [3]. Why is HL-SGD fundamentally novel?\u2028\n2. The theoretical analysis also seems to make sense, but it would be valid to show the comparison among the convergence rates among this work and the prior methods.\u2028\n3. The scale of the experiments is too small. What are the sizes of the CNNs used for training EMNIST and CIFAR-10 in the experiments? I won\u2019t assume the authors are using large CNNs, but for small-scale models, why will train them to incur communication bottlenecks?\u2028\n4. Apart from communication efficiency, another big motivation for federated learning is data privacy. Does HL-SGD come with any privacy guarantee? If not, can it be compatible with existing privacy-preserving methods, e.g., Secure Aggregation [4]?\u2028\nMinor comments:\nTypo: \u201cno-iid\u201d \u2014> \u201cnon-iid\u201d\n\nMissing references: [4-5]\n\n[1] https://arxiv.org/pdf/1905.06641.pdf\n\n[2] https://arxiv.org/pdf/1909.02362.pdf\n\n[3] https://arxiv.org/abs/2007.13819\n\n[4] https://eprint.iacr.org/2017/281.pdf\n\n[5] https://arxiv.org/abs/2107.06917\n\n[6] https://arxiv.org/pdf/2103.10481.pdf",
          "summary_of_the_review": "Overall I think the proposed method is a natural extension from the previous work. And the novelty is quite marginal. The experimental evaluation can be largely improved.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Reviewer_4Rc7"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Reviewer_4Rc7"
        ]
      },
      {
        "id": "rJ7TkBg9rkC",
        "original": null,
        "number": 4,
        "cdate": 1636071416557,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636071416557,
        "tmdate": 1637329072935,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper introduces a new hybrid scheme combining local and decentralized SGD by considering a network with one central node (server) and multiple worker nodes (devices) grouped into clusters with fast intra-cluster communication but slow device to server communication. The authors show that in such settings replacing local SGD with intra-cluster averaging of iterates after each SGD update can lead to faster convergence. Theoretical analysis explores the tradeoff between intra-cluster connectivity, device sampling rate, and convergence rate, while experiments on CIFAR10 and FEMNIST with non-iid splits shows that the proposed approach yields higher test accuracy than local SGD for a given runtime/number of rounds.",
          "main_review": "Strengths:\n\n1. The paper adds a new dimension to the federated learning setup by combining local and decentralized SGD approaches and showing that doing so can give more accurate models than those obtained with just local SGD.\n\n2. The theoretical results are intuitive and the convergence analysis explores and highlights the important tradeoffs between intra-cluster connectivity, device sampling rate and convergence rate.\n\nWeaknesses:\n\n1. My main concern is that the evaluation is a bit unfair to local SGD. It seems fairly apparent that averaging models within a cluster after every iteration will outperform local SGD where models are only averaged after $\\tau$ iterations. Moreover while D2D communication over LAN or across nearby devices may be cheap it is potentially more expensive than averaging computations in a single node or a datacenter network. I also find considering a single choice of $W_k$ in experiments to be a bit limiting (since it seems like the mixing matrix really depends on the setting). Therefore I would suggest the following performing the following additional experiments:\n\na) D2D communication after multiple SGD updates (the analysis for this may be complicated but it would be instructive to see the empirical performance).\n\nb) Experiments with varying $W_k$ (also do mention the value of $\\rho_{\\max}$ corresponding to the choice of $W_k$)\n\n2. A couple of claims rely on being able to tune the intra-cluster communication parameters -  $W_k$ (as in Corollary 1) and $c_{\\text{d2d}}$ (as in the results in Fig. 3) - to obtain improved performance. Is it really possible to tune these parameters in real world settings? I would suggest adding a citation to support this.",
          "summary_of_the_review": "The paper definitely adds to the discussion in Federated Learning and by introducing an interesting new setting of hybrid local SGD. Therefore I am leaning towards accepting it. However while the analysis captures the important tradeoffs, the experiments are a bit limited and seem slightly unfair to the local SGD baseline. If these can be rectified (following my suggestions above or otherwise) I would be completely convinced about accepting it\n\nComments after Rebuttal: Thank you for adequately responding to my queries. I am satisfied with the revisions made to the paper and have updated my score to reflect the same.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Reviewer_6z2f"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Reviewer_6z2f"
        ]
      },
      {
        "id": "uOK5DeuyWOr",
        "original": null,
        "number": 2,
        "cdate": 1637132594553,
        "mdate": 1637132594553,
        "ddate": null,
        "tcdate": 1637132594553,
        "tmdate": 1637132594553,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "rJ7TkBg9rkC",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 6z2f (Part 1)",
          "comment": "- **Q1: My main concern is that the evaluation is a bit unfair to local SGD. It seems fairly apparent that averaging models within a cluster after every iteration will outperform local SGD where models are only averaged after $\\tau$ iterations. Moreover, while D2D communication over LAN or across nearby devices may be cheap it is potentially more expensive than averaging computations in a single node or a datacenter network. I also find considering a single choice of $W_k$ in experiments to be a bit limiting (since it seems like the mixing matrix really depends on the setting). Therefore I would suggest performing the following additional experiments:\na) D2D communication after multiple SGD updates (the analysis for this may be complicated but it would be instructive to see the empirical performance).\nb) Experiments with varying $W_k$ (also do mention the value of $\\rho_{max}$ corresponding to the choice of $W_k$)**\n  \n  \n- **A1**: Thank you for the great suggestions. Yes, by fixing $\\tau$ to be the same for both HL-SGD and local SGD in the experiments, it is not hard to imagine that HL-SGD can outperform local SGD in terms of accuracy-round trade-off as shown in Figure 1(a) and Figure 1(c) due to more frequent communications. What is more interesting here is that although D2D communication in HL-SGD introduces additional communication overhead in each round compared with local SGD, HL-SGD can largely outperform local SGD in terms of accuracy-runtime tradeoff as shown in Figure 1(b) and Figure 1(d). That is, HL-SGD can achieve a target model accuracy within a shorter runtime than local SGD, which is a critical requirement in many FL applications.   \n\n  Following the suggestions, we have performed additional experiments on CIFAR-10 and added the following experimental results in Appendix E:    \n\n  1. In Figure 6 of Appendix E, we empirically evaluate the performance of HL-SGD with multiple steps of SGD before gossip averaging. We allow each device to perform $l = (1, 2, 5, 10)$ steps of SGD update before aggregating models with their neighbors in the same cluster. Note that $l = 1$ corresponds to the original version of HL-SGD in Algorithm 1. The figure shows that HL-SGD with $l = 1$ has the best convergence speed and can converge to the highest level of test accuracy under non-IID data distribution after 100 rounds. In terms of runtime, choosing a value of $l > 1$ might be favorable in some cases due to the reduced D2D communication delay per round. For instance, to achieve a target level of 60% test accuracy, HL-SGD with $l = 5$ needs a slightly less amount of time (5.22\\%) than $l = 1$. It is an interesting research direction to rigorously analyze the convergence properties of HL-SGD with arbitrary $l$ and find the best hyperparameter tuning method to minimize the runtime under a target level of accuracy in the future.     \n\n  2. In Figure 4 of Appendix E, we show the performance of HL-SGD on different D2D network topologies generated by Erd\u0151s\u2013R\u00e9nyi model \n  with random edge probability in $(0.2, 0,5, 0.8, 1)$, corresponding to spectral norm $\\rho_{max} = (0.9394, 0.844, 0.5357, 0)$. As shown in the figure, increasing the D2D network connectivity speeds up the convergence of HL-SGD in terms of FL rounds, matching our convergence results, but may lead to inferior performance on runtime because the D2D communication delay $c_{d2d}$ per round is typically larger under a more connected D2D network topology. When D2D network topology can be configured in some application scenarios, choosing a sparse topology could be better in terms of accuracy-runtime trade-off in HL-SGD. This result is also consistent with the literature on decentralized algorithms [1].     \n  \n  **Reference**\n\n   [1] Lian et al., \u201cCan Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic \n  Gradient Descent\u201d, NIPS 2017. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ]
      },
      {
        "id": "-H-U9X4FmRU",
        "original": null,
        "number": 3,
        "cdate": 1637133060203,
        "mdate": 1637133060203,
        "ddate": null,
        "tcdate": 1637133060203,
        "tmdate": 1637133060203,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "uOK5DeuyWOr",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 6z2f (Part 2) ",
          "comment": "- **Q2: A couple of claims rely on being able to tune the intra-cluster communication parameters - \n (as in Corollary 1) and  (as in the results in Fig. 3) - to obtain improved performance. Is it really possible to tune these parameters in real world settings? I would suggest adding a citation to support this.**\n     \n\n- **A2:** Thanks for your comments and suggestions. The conditions on $W_k$ in Theorem 2 and Corollary 1 are sufficient conditions for convergence when allowing partial device participation at the end of each round. Condition (7) as required in Theorem 2 can be satisfied for any given $\\rho_{\\max} < 1$ by choosing $\\tau$ large enough. This is a tuning parameter of the algorithm that can be set a priori. Corollary 1 strengthens the result of Theorem 2 by requiring no loss in the order of convergence rate compared to full device participation. This naturally leads to a more stringent condition on $\\rho_{\\max}$ given in (11). Condition (11) can be always satisfied by running multiple D2D gossip averaging steps per SGD update in Algorithm 1. For instance, $h$ steps of D2D gossip averaging improves $\\rho_{\\max}$ to $(\\rho_{\\max})^{h}$. According to (11) we can choose the suitable $h$ before launching the algorithm, and thus it can always be satisfied in practice. We have included this clarification in the revised manuscript. \n\n  \n  \n     The specific value of $c_{d2d}$ depends on the real-world FL applications. $c_{d2d}$ is determined by network topology and link speed, and fixed for a given D2D network. Figure 3 aims to show that the advantage of HL-SGD becomes more significant when $c_{d2d}$ is smaller, and when $c_{d2d}$ is relatively large compared with $c_{d2s}$, the additional D2D communication overhead may offset the benefit of convergence acceleration in HL-SGD and could lead to inferior runtime performance. Under the real-world measurements of $c_{d2d}$ and $c_{d2s}$ in typical FL applications, HL-SGD will largely outperform local SGD in terms of runtime to achieve a target level of accuracy as shown in Figure 1.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ]
      },
      {
        "id": "Vz9EeTakyk",
        "original": null,
        "number": 5,
        "cdate": 1637134220029,
        "mdate": 1637134220029,
        "ddate": null,
        "tcdate": 1637134220029,
        "tmdate": 1637134220029,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "9BWprOPh_n2",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 4Rc7 (Part 2) ",
          "comment": "- **Q2: The theoretical analysis also seems to make sense, but it would be valid to show the comparison among the convergence rates among this work and the prior methods.**\n\n- **A2:** Thanks for the suggestion. We have added Table 1 and expanded the discussion after Theorem 1 to compare the algorithmic complexity with local SGD and gossip SGD in the revised manuscript. Our analysis shows that by integrating the features of the two schemes, HL-SGD reduces the transient iteration complexity of local SGD, and improves the convergence speed of Gossip SGD in FL settings. For the reasons we mentioned in Part 1, we leave out the discussion on some of the algorithms that are not directly comparable because of different network architectures. \n\n- **Q3: The scale of the experiments is too small. What are the sizes of the CNNs used for training EMNIST and CIFAR-10 in the experiments? I won\u2019t assume the authors are using large CNNs, but for small-scale models, why will train them to incur communication bottlenecks?**\n\n- **A3:** We use the same CNN models and datasets that are commonly used in the FL literature as cited in our references. In the revised manuscript, we have added the detailed size information of CNNs used in the experiments. Since the considered CNN models are relatively large (i.e., millions of parameters), and D2S network bandwidth could be rather limited in practical FL settings, communication is indeed a major concern. \n\n- **Q4: Apart from communication efficiency, another big motivation for federated learning is data privacy. Does HL-SGD come with any privacy guarantee? If not, can it be compatible with existing privacy-preserving methods, e.g., Secure Aggregation [4]?**\n\n- **A4:** This is a good question. HL-SGD inherits many good properties of FL including its advantages in privacy protection. In literature, FL could enhance data privacy on two levels: (1) In FL, raw data never leaves the devices and only model parameters are exchanged. This greatly reduces the leakage of sensitive information; and (2) In order to further protect the sensitive information of exchanged model parameters in the interactive process of FL, additional mechanisms, such as differential privacy, secure aggregation, and shuffling, have been designed.  \n\n  Our proposed new FL scheme, HL-SGD, naturally inherits the first level of privacy protection of FL as raw data is always kept on device and never shared with other devices or the server. For the second level of privacy protection, HL-SGD is compatible with secure aggregation, differential privacy mechanisms, and shuffling. It could leverage secure aggregation protocols (e.g., Ref. [4]) or shuffling in the aggregation processes for both D2D aggregation and D2S aggregation as only the sum rather than individual values is needed for the model aggregation step. We can also provide differential privacy to HL-SGD by adding random noise to the exchanged model parameters using, for instance, the Gaussian mechanism. In fact, providing differential privacy protection in HL-SGD is part of our ongoing research. We have added some discussions and references on privacy in the revised manuscript. \n\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ]
      },
      {
        "id": "SIKjvc9B-xd",
        "original": null,
        "number": 6,
        "cdate": 1637134447627,
        "mdate": 1637134447627,
        "ddate": null,
        "tcdate": 1637134447627,
        "tmdate": 1637134447627,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "KgC53hbMukO",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 87Gc",
          "comment": "- **Q1: While the Theoretical results emphasize the role of the local topology in each cluster, the corresponding numerical analysis to understand the impact of topology is missing. It would be interesting to see how the connectivity parameter  impacts the results in Figure 1 to 5, instead of just taking a ring topology.**\n\n- **A1:** Thank you for the great comments. We have performed additional experiments on CIFAR-10 and added Figure 4 in Appendix E to show the impact of D2D network topology on the performance of HL-SGD. Specifically, we generate random network topologies by Erd\u0151s\u2013R\u00e9nyi model with edge probability from $(0.2, 0,5, 0.8, 1)$, corresponding to spectral norm $\\rho_{\\max} = (0.9394, 0.844, 0.5357, 0)$. As shown in the figure, increasing the D2D network connectivity generally speeds up the convergence of HL-SGD in terms of FL rounds but may lead to inferior performance on runtime since the D2D communication delay $c_{d2d}$ per round is typically larger under a more connected D2D network topology. When D2D network topology can be configured in some application scenarios, choosing a sparse topology could be better in terms of accuracy-runtime trade-off in HL-SGD. This result is also consistent with the literature on decentralized algorithms [1]. \n\n  **Reference**\n  \n  [1] Lian et al., \u201cCan Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent\u201d, NIPS 2017. \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ]
      },
      {
        "id": "VD08qzSiPF",
        "original": null,
        "number": 7,
        "cdate": 1637134840060,
        "mdate": 1637134840060,
        "ddate": null,
        "tcdate": 1637134840060,
        "tmdate": 1637134840060,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "bQCxtRGvVQ",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer y58n (Part 1)",
          "comment": "- **Q1: Theoretical discussion: it is not clear from the current presentation how to choose the parameter  $\\tau$ (number of decentralized updates per rounds) Ideally, it would need to balance (i) the D2S communication cost and the D2D communication cost (ii) intra-cluster convergence and outer-loop (round) convergence. In other words, is there any tradeoff between improving the heterogeneous communication versus having better convergence rate. For the discussion on $\\rho_{\\max}$, how it scales with $n$ in a non-extreme situation. The current discussion mostly focus on it being 0 (fully connected) or 1 (no connection), it would be good to discuss its scaling with $n$ in a less trivial situation.**\n\n- **A1:** Thanks for the interesting question. The convergence rate with full device participation stated in Theorem 1 is obtained for any choice of $\\tau$ and $\\rho_{\\max} \\in [0,1]$. As for partial device participation, $\\tau$ should be chosen so that (7) in Theorem 2 or (11) in Corollary 1 is satisfied. The impact of $\\tau$ and $\\rho_{\\max}$ on the convergence rate, however, needs to be read from the expression of the complexity. In particular, (6) shows $\\rho_{\\max}$ is associated to the intra-cluster data heterogeneity and the stochastic gradient variance $\\sigma^2$; while the number of local steps $\\tau$ is associated to the inter-cluster data heterogeneity $\\epsilon_g^2$ and the average stochastic gradient variance $\\sigma^2/n$ within each cluster. Improving the D2D network connectivity (corresponding to a smaller $\\rho_{\\max}$) thus mitigates the impact of both local data heterogeneity and stochastic noise on the convergence rate. This provides the flexibility of choosing a larger $\\tau$ so that the overall complexity is invariant. \n  \n  \n  The discussion on $\\rho_{\\max}$ in the extreme cases is to show the tightness of the analysis in the sense that it recovers the state-of-the-art convergence rate of Local SGD. The non-extreme case stays in between. The detailed discussion is provided in the **Comparison to Local SGD** part of the main text and cited below for completeness:\n  \"Comparing (6) and the complexity of local SGD, we can see the intra-cluster D2D communication provably improves the iteration complexity by reducing the transient iterations. This is reflected in the smaller coefficient associated with the $O(({\\tau R})^{-\\frac{2}{3}})$ term. In particular, improving D2D communication connectivity will lead to a smaller $\\rho_{\\max}$ and consequently, mitigate the impact of both local data heterogeneity and stochastic noise on the convergence rate.\"\n  \n  \n  The dependency of $\\rho$ on $n$, the number of devices in each cluster, has been discussed in literature and is provided in Table 2 of Appendix F in the revised manuscript.  \n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ]
      },
      {
        "id": "OIWnrPUpDGX",
        "original": null,
        "number": 8,
        "cdate": 1637135137989,
        "mdate": 1637135137989,
        "ddate": null,
        "tcdate": 1637135137989,
        "tmdate": 1637135137989,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "VD08qzSiPF",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer y58n (Part 2)",
          "comment": "- **Q2: Empirical study: the experiments are performed with a fixed $\\tau$. However, the local SGD may have a different regime on the dependency of $\\tau$. I encourage the authors to provide more comparison on how $\\tau$ influence the performance in figure, for instance with $\\tau = 5, 10, 20$. The ablation study in Figure 4& 5 is also not convincing as it seems no difference. In particular, changing $\\rho$ does not lead to any performance or run time change. This might suggest that the local problem is too simple that be efficiently solved even the connectivity is bad.**\n\n- **A2:** Thank you for the suggestion. Here we select $\\tau = 50$ because local SGD has the shortest runtime towards the target accuracy at that hyperparameter setting. By fixing $\\tau = 50$, we show that local SGD at the optimal setting can be further improved in non-IID data distribution by incorporating D2D communications as in HL-SGD. To give a more comprehensive analysis on the advantages of HL-SGD, we have performed additional experiments by varying $\\tau = (5, 10, 20, 50)$ and compared the performances of HL-SGD and local SGD in Figure 5 of Appendix E. The results show that HL-SGD can consistently outperform local SGD across a wide range of $\\tau$. \n    \n  \n  As for Figure 4 & 5 (i.e., Figure 3 in the revised manuscript), sorry for the confusion as there might be some misunderstandings. These figures actually show the impact of $p$ (i.e., sampling ratio), not $\\rho_\\max$ (i.e., the D2D network connectivity), on the performance of HL-SGD. According to Corollary 1 in the paper, the sampling ratio does not affect the order of convergence rate under condition (11). Therefore, the difference between different sampling ratios is marginal in the experiment results. In practice, we could select a small sampling ratio to save D2S communication and reduce total runtime with little impact on model accuracy.  \n    \n  \n  To see the impact of D2D network topology $\\rho$, we have performed additional experiments on CIFAR-10 and added Figure 4 in Appendix E. Specifically, we generate random network topologies by Erd\u0151s\u2013R\u00e9nyi model with edge probability from $(0.2, 0,5, 0.8, 1)$, corresponding to spectral norm $\\rho_{\\max} = (0.9394, 0.844, 0.5357, 0)$. As shown in the figure, increasing the D2D network connectivity generally speeds up the convergence of HL-SGD in terms of FL rounds but may lead to inferior performance on runtime since the D2D communication delay $c_{d2d}$ per round might be larger under a more connected D2D network topology. When D2D network topology can be configured in some application scenarios, choosing a sparse topology could be better in terms of accuracy-runtime trade-off in HL-SGD. \n\n- **Q3: Privacy concern: the lack of discussion on the privacy issue is a major omission in the paper. The decentralized communication within cluster requires client to exchange information with others, raising potential concerns on leaking informations. I believe this need to be carefully addressed as privacy is one of the major difference in Federated Learning compared to traditional distributed training.**\n\n- **A3:** Thank you for this insightful comment. Indeed, protection of user privacy has been a major motivation for FL and also our work. It is worth noting that HL-SGD inherits the privacy advantages of FL in the following perspectives: (1) As in traditional FL algorithms (e.g., local SGD), raw data never leaves the device in HL-SGD and only model parameters are exchanged. This greatly reduces the risk of sensitive information leakage; (2) HL-SGD can be integrated with advanced techniques to provide stronger and rigorous privacy protection, including secure aggregation, differential privacy, and shuffling. \n\n  As the reviewer has pointed out, HL-SGD and traditional FL algorithms do have different patterns of information exchange. In traditional FL algorithms, the central server receives model parameters from all selected devices in each communication round, which is a major cause for privacy leakage. In HL-SGD, information exchange between device and the server still exists, but the exchanged information could be less sensitive since only in-cluster aggregated models are shared with the server. From this perspective, HL-SGD may provide better privacy protection against the server. There is also device-to-device information exchange in HL-SGD, which shares individual model updates with some other devices in the same cluster. Privacy enhancement mechanisms such as secure aggregation and differential privacy could be applied over these exchanged individual models. The exact privacy guarantee of HL-SGD depends on the adopted privacy definition, threat model, and privacy-enhancing mechanisms, but intuitively we would be able to achieve comparable privacy guarantees since raw data is always kept on device both in traditional FL algorithms and HL-SGD. We have added some discussions and references on privacy in the revised manuscript. \n  "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ]
      },
      {
        "id": "BPbRJljtAwm",
        "original": null,
        "number": 1,
        "cdate": 1637177339665,
        "mdate": 1637177339665,
        "ddate": null,
        "tcdate": 1637177339665,
        "tmdate": 1637177339665,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Public_Comment",
        "content": {
          "title": "novelty",
          "comment": "\nIt looks this method is identical to \"Accelerating Gossip SGD with Periodic Global Averaging\".\n\nIf combining adjacency matrices of all clusters into a large block-diagonal matrix, these two methods are the same.\n\nCan you please comment on their difference? Thanks."
        },
        "signatures": [
          "~Eric_James1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "~Eric_James1"
        ]
      },
      {
        "id": "dkHijf_Ih2",
        "original": null,
        "number": 9,
        "cdate": 1637181724281,
        "mdate": 1637181724281,
        "ddate": null,
        "tcdate": 1637181724281,
        "tmdate": 1637181724281,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "BPbRJljtAwm",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "Response to difference",
          "comment": "Thank you for the comment. The Gossip PGA algorithm in that work assumes a single cluster while we consider multiple clusters, which is motivated by the real-world FL applications where only nearby devices or devices in the same LAN domain can communicate via fast D2D links. Indeed, it is possible to construct an augmented matrix by stacking the $W_k$'s in blocks. This block diagonal augmented matrix $W$ has spectral norm $\\rho = \\|W - (1/N) \\mathbf{1} \\mathbf{1}^\\top\\|_2 = 1$. The convergence analysis provided by Gossip PGA is applicable but does not exploit the block diagonal structure since it is done for a general consensus matrix $W$. More specifically, the complexity of Gossip GPA is \n\n$O\\left( \\frac{\\sigma}{\\sqrt{N \\tau R}} + \\frac{ C_{\\tau,\\rho}^{\\frac{1}{3} }  D_{\\tau,\\rho}^{\\frac{1}{3} } \\rho^{\\frac{2}{3}} \\epsilon^{\\frac{2}{3}}}{ (\\tau R)^{\\frac{2}{3}} } + \\frac{C_{\\tau,\\rho}^{\\frac{1}{3}} \\rho^{\\frac{2}{3}} \\sigma^{\\frac{2}{3}}}{(\\tau R)^{\\frac{2}{3}}} + \\frac{\\rho D_{\\tau, \\rho}}{\\tau R}\\right),$\n\nwhere $C_{\\tau, \\rho} \\triangleq \\sum_{k = 0}^{\\tau - 1} \\rho^k$, $D_{\\tau, \\rho} = \\min\\{1/(1 - \\rho), \\tau\\}$, and $\\rho$ is the connectivity of $W$. Letting $\\rho \\to 1$ (since the overall network is disconnected), the complexity becomes\n$$\nO \\left( \\frac{{\\sigma}}{\\sqrt{N \\tau R}}  + \\frac{\\left(  \\tau^2  \\epsilon^2  +  \\tau   \\sigma^2 \\right)^{\\frac{1}{3}}}{  {(\\tau R})^{\\frac{2}{3}}} + \\frac{\\tau}{ \\tau R} \\right).\n$$\nThis coincides with the complexity of Local SGD. The effect of D2D networks is not reflected in the expression. Our analysis, on the other hand, exploits the block diagonal structure of $W$ and provides the explicit dependency of the rate on $\\rho_{\\max}$ -- the worst-case connectivity of the D2D subgraphs. When all the  D2D networks are connected, our rate is more informative and better than the rate of Gossip GPA as described above. See Table 1 and the related comments for the comparison with Local SGD. In fact, understanding the impact of D2D networks on the convergence rate is the key question we would like to address in this work.  \n\nMoreover, considering the unique communication challenge in FL, our algorithm uses device sampling at the end of each round to reduce the uplink D2S communication cost, while Gossip PGA requires all devices to upload models to the server. Experimental results show that this technique can reduce the required runtime to achieve a target level of model accuracy in HL-SGD.  "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ]
      },
      {
        "id": "KF9vSibza8g",
        "original": null,
        "number": 2,
        "cdate": 1637184183999,
        "mdate": 1637184183999,
        "ddate": null,
        "tcdate": 1637184183999,
        "tmdate": 1637184183999,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "dkHijf_Ih2",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Public_Comment",
        "content": {
          "title": "Thanks for your reply",
          "comment": "Thanks for your reply. I have a simple follow-up question. Why is the spectral norm of the block diagonal augmented matrix equal to 1?"
        },
        "signatures": [
          "~Eric_James1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "~Eric_James1"
        ]
      },
      {
        "id": "U2ygb9_Lbkw",
        "original": null,
        "number": 10,
        "cdate": 1637192742199,
        "mdate": 1637192742199,
        "ddate": null,
        "tcdate": 1637192742199,
        "tmdate": 1637192742199,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "KF9vSibza8g",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "spectral gap of $W$",
          "comment": "Thanks. We apologize that in our previous reply the wording is not precise. The claim is: the spectral norm of matrix $W - (1/N) \\mathbf{1}_N \\mathbf{1}_N^\\top$, defined as $\\rho$, is equal to 1. \n\nFor example, consider a $2n \\times 2n$ matrix\n$$\n    W = (\n    W_1,  0;\n    0, W_2),\n$$\nwhere $W_1, W_2 \\in \\mathbb{R}^{n \\times n}$ are symmetric bi-stochastic matrices assigned to two connected D2D subgraphs satisfying the standard Assumption 3 in the paper.\nIn addition, we construct vector $x = ( \\mathbf{1}_n; -   \\mathbf{1}_n)$. \n\nThen \n$$\n    y  = ( W -  (1/N) \\mathbf{1}_{N} \\mathbf{1}_N^\\top) x =       ( \\mathbf{1}_n; -   \\mathbf{1}_n),\n$$\nwhere $N = 2n$.\n\nTherefore, by definition\n$$\n    \\|W - (1/N) \\mathbf{1}_N \\mathbf{1}_N^\\top \\|_2 = \\sup \\Big( \\frac{ \\|( W - (1/N) \\mathbf{1}_N \\mathbf{1}_N^\\top ) x\\|_2  }{ \\|x\\|_2} : x \\in \\mathbb{R}^{N}, x \\neq 0 \\Big) \\geq 1.\n$$\nThe equality follows from the properties of bi-stochastic matrix.\n\nGenerally, $\\rho = 1$ is due to the fact that the graph of all devices is disconnected even though the subgraphs are connected.\nDoes this address your concern?"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ]
      },
      {
        "id": "cjou7rvsUqg",
        "original": null,
        "number": 11,
        "cdate": 1637329125062,
        "mdate": 1637329125062,
        "ddate": null,
        "tcdate": 1637329125062,
        "tmdate": 1637329125062,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "-H-U9X4FmRU",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "Comment after rebuttal",
          "comment": "Thank you for adequately responding to my queries. I am satisfied with the revisions made to the paper and have updated my score to reflect the same."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Reviewer_6z2f"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Reviewer_6z2f"
        ]
      },
      {
        "id": "trDbVgBASwj",
        "original": null,
        "number": 12,
        "cdate": 1637342845516,
        "mdate": 1637342845516,
        "ddate": null,
        "tcdate": 1637342845516,
        "tmdate": 1637342845516,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "cjou7rvsUqg",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 6z2f",
          "comment": "Thank you for your constructive comments and great suggestions, which are really helpful to us in further improving our manuscript.   "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ]
      },
      {
        "id": "CyO3EdVj_I",
        "original": null,
        "number": 13,
        "cdate": 1637528639763,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637528639763,
        "tmdate": 1637537863211,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "Response to All Reviewers",
          "comment": "We want to thank all the reviewers for giving many constructive comments and great suggestions to our manuscript. For the most recent uploaded version, the major changes compared with the submitted original version are summarized as follows: \n\n1. We have performed additional experiments and added the following new experimental results in Appendix E: (i) Figure 4 shows the performance of HL-SGD on different D2D network topologies generated by Erd\u0151s\u2013R\u00e9nyi model; (ii) Figure 5 shows the performance of HL-SGD on different $\\tau$; and (iii) Figure 6 shows the performance of an extension of HL-SGD by allowing each device to perform multiple updates before the gossip averaging step. \n\n2. We have clarified the differences of HL-SGD with prior hierarchical FL algorithms in the \u201cbackgrounds and related work\u201d section. \n\n3. We have expanded the discussions after Theorem 1 and added Table 1 to compare HL-SGD with prior methods (local SGD, gossip SGD, and gossip GPA) relevant to our problem setting in their algorithmic complexities. The tightness of our derived bounds and insights about the hyperparameter choices in HL-SGD are further discussed. \n\n4. We have added discussions on the privacy protection properties of HL-SGD and more details about the experimental settings. \n\nWe believe that this new version of the paper has addressed the reviewers' comments. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ]
      },
      {
        "id": "nU1CfIOCVZZ",
        "original": null,
        "number": 14,
        "cdate": 1637636698886,
        "mdate": 1637636698886,
        "ddate": null,
        "tcdate": 1637636698886,
        "tmdate": 1637636698886,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "OIWnrPUpDGX",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "RE: Response to Reviewer y58n (Part 2) ",
          "comment": "Thank you for your detailed response. I appreciate the adding of comparison table, extensive experiments and further discussion on privacy. Just a quick remark, in Figure 3 (c) and (d), the legend is referring to the parameter $\\rho$ instead of $p$, that's something mislead my earlier thought. I raise my score accordingly as my concerns are addressed carefully. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Reviewer_y58n"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Reviewer_y58n"
        ]
      },
      {
        "id": "RGbKNldpa0",
        "original": null,
        "number": 15,
        "cdate": 1637768792910,
        "mdate": 1637768792910,
        "ddate": null,
        "tcdate": 1637768792910,
        "tmdate": 1637768792910,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "nU1CfIOCVZZ",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "RE: RE: Response to Reviewer y58n ",
          "comment": "Thank you for pointing out the typo and giving us good suggestions on further improving our manuscript. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ]
      },
      {
        "id": "BKWNHofI847",
        "original": null,
        "number": 16,
        "cdate": 1638690060098,
        "mdate": 1638690060098,
        "ddate": null,
        "tcdate": 1638690060098,
        "tmdate": 1638690060098,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "Vz9EeTakyk",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "Thanks for the rebuttal",
          "comment": "After carefully reading through the rebuttal, my major concerns are addressed (the authors carefully modified the draft for addressing my concerns). I thus would like to increase my overall evaluation score from 3 to 6. I believe this work could make a good contribution to the FL field. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Reviewer_4Rc7"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Reviewer_4Rc7"
        ]
      },
      {
        "id": "3ENkr32tL-",
        "original": null,
        "number": 1,
        "cdate": 1642696841562,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696841562,
        "tmdate": 1642696841562,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Spotlight)",
          "comment": "The paper contributes to the literature on federated learning by introducing a hybrid local SGD (HL-SGD) method. HL-SGD is motivated by the setups where edge devices are grouped into clusters with fast connections within the cluster, but slower connection between the devices and the server. HL-SGD uses hybrid updates: decentralized updates within the clusters and federated averaging steps between the clusters and the server. \n\nInitially, the reviews expressed concerns regarding comparison to prior work, empirical results, and privacy of the proposed scheme. However, the authors adequately addressed all of the concerns, added relevant discussions and results to the paper, and a consensus was reached that the paper should be accepted."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "bQCxtRGvVQ",
        "original": null,
        "number": 1,
        "cdate": 1635749245465,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635749245465,
        "tmdate": 1637636729304,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes a new method to address the heterogeneous communication setting in a hierarchical distributed system.  The system consists of a central server and several disjoint clusters, and each cluster is a local network connection between multiple edge devices. The communications are heterogeneous in the sense that in cluster device to device (D2D) communication are much faster than the device to server (D2S) communication. The proposes algorithm is a hybrid method where decentralized method is applied within clusters (on edge devices) and federated average is applied between cluster and server. Convergence analysis is provided showing the benefit of variance reduction using decentralization within cluster. Empirical studies are conducted to demonstrate the improvement under synthetic federated learning environment.",
          "main_review": "The presentation of the paper is very clear and easy to follow. The strength of the paper lies in the theoretical analysis, where combining decentralization and federated average is not straightforward at all. The theoretical result is a bit hard to digest, further clarification and comparison with existing method would make it more readable. The experimental section is also less convincing. My detailed comments are as follows. \n\n1 Theoretical discussion:  it is not clear from the current presentation how to choose the parameter $\\tau$ (number of decentralized updates per rounds) Ideally, it would need to balance (i) the D2S communication cost and the D2D communication cost (ii) intra-cluster convergence and outer-loop (round) convergence. In other words, is there any tradeoff between improving the heterogeneous communication versus having better convergence rate. For the discussion on $\\rho_{max}$, how it scales with $n$ in a non-extreme situation. The current discussion mostly focus on it being 0 (fully connected) or 1 (no connection), it would be good to discuss its scaling with $n$ in a less trivial situation.\n\n2 Empirical study:  the experiments are performed with a fixed $\\tau$. However, the local SGD may have a different regime on the dependency of $\\tau$. I encourage the authors to provide more comparison on how $\\tau$ influence the performance in figure, for instance with $\\tau =5, 10, 20$. The ablation study in Figure 4& 5 is also not convincing as it seems no difference. In particular, changing $\\rho$ does not lead to any performance or run time change. This might suggest that the local problem is too simple that be efficiently solved even the connectivity is bad.  \n\n3 Privacy concern: the lack of discussion on the privacy issue is a major omission in the paper. The decentralized communication within cluster requires client to exchange information with others, raising potential concerns on leaking informations. I believe this need to be carefully addressed as privacy is one of the major difference in Federated Learning compared to traditional distributed training.\n\nOverall, the theoretical study has merit while the empirical study is less convincing,   ",
          "summary_of_the_review": "The theoretical study has merit while the empirical study is less convincing,   ",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Reviewer_y58n"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Reviewer_y58n"
        ]
      },
      {
        "id": "KgC53hbMukO",
        "original": null,
        "number": 2,
        "cdate": 1635874352127,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635874352127,
        "tmdate": 1635874352127,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a new Federated learning algorithm which can take advantage of the fast D2D (Device-to-Device) connections among the devices. In particular, it is shown that the proposed algorithm has better performance in terms of convergence rate and accuracy compared to the existing algorithms such as 'Local SGD'-based FL which rely only on the slow D2S connections for model updating. The convergence of the proposed algorithm is theoretically characterized in terms of the D2D connectivity (topology). Numerical results are provided to show the improved performance compared to the local sgd-based FL.\n",
          "main_review": "Strength:\nThe proposed algorithm takes good advantage of the fast local connectivity among the nodes to improve the convergence. It is a novel contribution. \n\nDetailed convergence analysis of the proposed algorithm is provided. \n\nWeakness:\nWhile the Theoretical results emphasize the role of the local topology in each cluster, the corresponding numerical analysis to understand the impact of topology is missing. It would be interesting to see how the connectivity parameter $\\rho$ impacts the results in Figure 1 to 5, instead of just taking a ring topology. ",
          "summary_of_the_review": "The proposed algorithm is novel and the the performance is rigorously characterized. The improvement compared to existing algorithms is significant and hence I believe it should be accepted for publication. ",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Reviewer_87Gc"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Reviewer_87Gc"
        ]
      },
      {
        "id": "Ci5HzH30-9e",
        "original": null,
        "number": 3,
        "cdate": 1635881249028,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635881249028,
        "tmdate": 1638690080167,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces HL-SGD, a method to design a hybrid federated learning method to leverage a hybrid high-speed D2D and low-speed D2S network and speed up the communication of federated learning applications. Both theoretical analysis and empirical results are provided to show the effectiveness of HL-SGD.",
          "main_review": "The paper proposes to use a hybrid model aggregation to conduct federated learning leveraging both high-speed D2D network and low-speed D2S network.\u00a0\n\nPros:\n1. The idea proposed in the paper is easy to follow, and the research direction of enhancing the communication efficiency of federated learning is potentially impactful.\u2028\n2. Both theoretical analysis and empirical results are provided to justify the effectiveness of HL-SGD.\u2028\nCons:\u00a0\n1. The main concern I have for the paper is that the proposed idea lacks novelty. Hierarchical federated learning has been studied extensively in the literature [1,6] as well as hierarchical local SGD [3]. Why is HL-SGD fundamentally novel?\u2028\n2. The theoretical analysis also seems to make sense, but it would be valid to show the comparison among the convergence rates among this work and the prior methods.\u2028\n3. The scale of the experiments is too small. What are the sizes of the CNNs used for training EMNIST and CIFAR-10 in the experiments? I won\u2019t assume the authors are using large CNNs, but for small-scale models, why will train them to incur communication bottlenecks?\u2028\n4. Apart from communication efficiency, another big motivation for federated learning is data privacy. Does HL-SGD come with any privacy guarantee? If not, can it be compatible with existing privacy-preserving methods, e.g., Secure Aggregation [4]?\u2028\nMinor comments:\nTypo: \u201cno-iid\u201d \u2014> \u201cnon-iid\u201d\n\nMissing references: [4-5]\n\n[1] https://arxiv.org/pdf/1905.06641.pdf\n\n[2] https://arxiv.org/pdf/1909.02362.pdf\n\n[3] https://arxiv.org/abs/2007.13819\n\n[4] https://eprint.iacr.org/2017/281.pdf\n\n[5] https://arxiv.org/abs/2107.06917\n\n[6] https://arxiv.org/pdf/2103.10481.pdf",
          "summary_of_the_review": "Overall I think the proposed method is a natural extension from the previous work. And the novelty is quite marginal. The experimental evaluation can be largely improved.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Reviewer_4Rc7"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Reviewer_4Rc7"
        ]
      },
      {
        "id": "rJ7TkBg9rkC",
        "original": null,
        "number": 4,
        "cdate": 1636071416557,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636071416557,
        "tmdate": 1637329072935,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper introduces a new hybrid scheme combining local and decentralized SGD by considering a network with one central node (server) and multiple worker nodes (devices) grouped into clusters with fast intra-cluster communication but slow device to server communication. The authors show that in such settings replacing local SGD with intra-cluster averaging of iterates after each SGD update can lead to faster convergence. Theoretical analysis explores the tradeoff between intra-cluster connectivity, device sampling rate, and convergence rate, while experiments on CIFAR10 and FEMNIST with non-iid splits shows that the proposed approach yields higher test accuracy than local SGD for a given runtime/number of rounds.",
          "main_review": "Strengths:\n\n1. The paper adds a new dimension to the federated learning setup by combining local and decentralized SGD approaches and showing that doing so can give more accurate models than those obtained with just local SGD.\n\n2. The theoretical results are intuitive and the convergence analysis explores and highlights the important tradeoffs between intra-cluster connectivity, device sampling rate and convergence rate.\n\nWeaknesses:\n\n1. My main concern is that the evaluation is a bit unfair to local SGD. It seems fairly apparent that averaging models within a cluster after every iteration will outperform local SGD where models are only averaged after $\\tau$ iterations. Moreover while D2D communication over LAN or across nearby devices may be cheap it is potentially more expensive than averaging computations in a single node or a datacenter network. I also find considering a single choice of $W_k$ in experiments to be a bit limiting (since it seems like the mixing matrix really depends on the setting). Therefore I would suggest the following performing the following additional experiments:\n\na) D2D communication after multiple SGD updates (the analysis for this may be complicated but it would be instructive to see the empirical performance).\n\nb) Experiments with varying $W_k$ (also do mention the value of $\\rho_{\\max}$ corresponding to the choice of $W_k$)\n\n2. A couple of claims rely on being able to tune the intra-cluster communication parameters -  $W_k$ (as in Corollary 1) and $c_{\\text{d2d}}$ (as in the results in Fig. 3) - to obtain improved performance. Is it really possible to tune these parameters in real world settings? I would suggest adding a citation to support this.",
          "summary_of_the_review": "The paper definitely adds to the discussion in Federated Learning and by introducing an interesting new setting of hybrid local SGD. Therefore I am leaning towards accepting it. However while the analysis captures the important tradeoffs, the experiments are a bit limited and seem slightly unfair to the local SGD baseline. If these can be rectified (following my suggestions above or otherwise) I would be completely convinced about accepting it\n\nComments after Rebuttal: Thank you for adequately responding to my queries. I am satisfied with the revisions made to the paper and have updated my score to reflect the same.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Reviewer_6z2f"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Reviewer_6z2f"
        ]
      },
      {
        "id": "BPbRJljtAwm",
        "original": null,
        "number": 1,
        "cdate": 1637177339665,
        "mdate": 1637177339665,
        "ddate": null,
        "tcdate": 1637177339665,
        "tmdate": 1637177339665,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Public_Comment",
        "content": {
          "title": "novelty",
          "comment": "\nIt looks this method is identical to \"Accelerating Gossip SGD with Periodic Global Averaging\".\n\nIf combining adjacency matrices of all clusters into a large block-diagonal matrix, these two methods are the same.\n\nCan you please comment on their difference? Thanks."
        },
        "signatures": [
          "~Eric_James1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "~Eric_James1"
        ]
      },
      {
        "id": "CyO3EdVj_I",
        "original": null,
        "number": 13,
        "cdate": 1637528639763,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637528639763,
        "tmdate": 1637537863211,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Official_Comment",
        "content": {
          "title": "Response to All Reviewers",
          "comment": "We want to thank all the reviewers for giving many constructive comments and great suggestions to our manuscript. For the most recent uploaded version, the major changes compared with the submitted original version are summarized as follows: \n\n1. We have performed additional experiments and added the following new experimental results in Appendix E: (i) Figure 4 shows the performance of HL-SGD on different D2D network topologies generated by Erd\u0151s\u2013R\u00e9nyi model; (ii) Figure 5 shows the performance of HL-SGD on different $\\tau$; and (iii) Figure 6 shows the performance of an extension of HL-SGD by allowing each device to perform multiple updates before the gossip averaging step. \n\n2. We have clarified the differences of HL-SGD with prior hierarchical FL algorithms in the \u201cbackgrounds and related work\u201d section. \n\n3. We have expanded the discussions after Theorem 1 and added Table 1 to compare HL-SGD with prior methods (local SGD, gossip SGD, and gossip GPA) relevant to our problem setting in their algorithmic complexities. The tightness of our derived bounds and insights about the hyperparameter choices in HL-SGD are further discussed. \n\n4. We have added discussions on the privacy protection properties of HL-SGD and more details about the experimental settings. \n\nWe believe that this new version of the paper has addressed the reviewers' comments. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper158/Authors"
        ]
      },
      {
        "id": "3ENkr32tL-",
        "original": null,
        "number": 1,
        "cdate": 1642696841562,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696841562,
        "tmdate": 1642696841562,
        "tddate": null,
        "forum": "H0oaWl6THa",
        "replyto": "H0oaWl6THa",
        "invitation": "ICLR.cc/2022/Conference/Paper158/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Spotlight)",
          "comment": "The paper contributes to the literature on federated learning by introducing a hybrid local SGD (HL-SGD) method. HL-SGD is motivated by the setups where edge devices are grouped into clusters with fast connections within the cluster, but slower connection between the devices and the server. HL-SGD uses hybrid updates: decentralized updates within the clusters and federated averaging steps between the clusters and the server. \n\nInitially, the reviews expressed concerns regarding comparison to prior work, empirical results, and privacy of the proposed scheme. However, the authors adequately addressed all of the concerns, added relevant discussions and results to the paper, and a consensus was reached that the paper should be accepted."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}