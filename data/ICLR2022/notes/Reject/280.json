{
  "id": "LBv-JtAmm4P",
  "original": "1BdiWuxmdr",
  "number": 280,
  "cdate": 1632875441361,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875441361,
  "tmdate": 1676330679143,
  "ddate": null,
  "content": {
    "title": "Is Heterophily A Real Nightmare For Graph Neural Networks on Performing Node Classification?",
    "authorids": [
      "~Sitao_Luan1",
      "~Chenqing_Hua1",
      "~Qincheng_Lu1",
      "~Jiaqi_Zhu1",
      "~Mingde_Zhao1",
      "~Shuyuan_Zhang1",
      "~Xiao-Wen_Chang1",
      "~Doina_Precup1"
    ],
    "authors": [
      "Sitao Luan",
      "Chenqing Hua",
      "Qincheng Lu",
      "Jiaqi Zhu",
      "Mingde Zhao",
      "Shuyuan Zhang",
      "Xiao-Wen Chang",
      "Doina Precup"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Heterophily",
      "Filterbank",
      "Adaptive Channel Mixing"
    ],
    "abstract": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using the graph structures based on the relational inductive bias (homophily assumption). Though GNNs are believed to outperform NNs in real-world tasks, performance advantages of GNNs over graph-agnostic NNs seem not generally satisfactory. Heterophily has been considered as a main cause and numerous works have been put forward to address it. In this paper, we first show that not all cases of heterophily are harmful for GNNs with aggregation operation. Then, we propose new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNNs. The metrics demonstrate advantages over the commonly used homophily metrics in tests on synthetic graphs. From the metrics and the observations, we find that some cases of harmful heterophily can be addressed by diversification operation. By using this fact and knowledge of filterbanks, we propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer, in order to address harmful heterophily. We validate the ACM-augmented baselines with 10 real-world node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNNs on most of the tasks without incurring significant computational burden. ",
    "one-sentence_summary": "This paper analyzes heterophily problems for graph neural networks and propose adaptive channel mixing framework to boost the performance of GNNs on node classification tasks.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "luan|is_heterophily_a_real_nightmare_for_graph_neural_networks_on_performing_node_classification",
    "pdf": "/pdf/4198ccb337501d23ab79ebdf4b81472cde2c341f.pdf",
    "supplementary_material": "/attachment/26d2dcfd878c6b88f48f64161a7aa95c250e8f6e.zip",
    "_bibtex": "@misc{\nluan2022is,\ntitle={Is Heterophily A Real Nightmare For Graph Neural Networks on Performing Node Classification?},\nauthor={Sitao Luan and Chenqing Hua and Qincheng Lu and Jiaqi Zhu and Mingde Zhao and Shuyuan Zhang and Xiao-Wen Chang and Doina Precup},\nyear={2022},\nurl={https://openreview.net/forum?id=LBv-JtAmm4P}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "LBv-JtAmm4P",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 15,
    "directReplyCount": 7,
    "revisions": true,
    "replies": [
      {
        "id": "EXZsB2Ep7nN",
        "original": null,
        "number": 1,
        "cdate": 1635807109433,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635807109433,
        "tmdate": 1635807109433,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper starts from studying the question of how heterophily affects the learning effectiveness of Graph Neural Networks on node classification tasks and posits that heterophily may not be always detrimental to the task. Following this observation, it proposes a new architecture, called Adaptive Channel Mixing, which appropriately applies aggregation, diversification and identity channels in each GNN layer in order to tackle harmful heterophily. According to the experimental results, the proposed architecture is successful.",
          "main_review": "The paper addresses a well known problem in the literature in a seemingly effective way. It is really well written, offering good background on the problem and going smoothly from the exposition of the problem and the limited view of existing approaches to the proposed ACM architecture to address it. Then, it performs very comprehensive experiments that demonstrate the effectiveness of the proposed scheme.\n\nIf I need to mention a weakness, this is the fact that the paper necessarily compresses a lot of discussion and description due to the conference constraints. For that reason, it misses a thorough analysis and comparison with the state of the art, as well as a more in-depth discussion of the experimental results and conclusions. Yet, the authors make the best of the available space to cover all necessary information.",
          "summary_of_the_review": "The paper proposes a new architecture, called Adaptive Channel Mixing, with the aim to address several cases of harmful heterophily in GNN node classification settings. The paper is well written and motivated and contains convincing experimentations. It could also have a big impact on GNN node classification practice and future research.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Reviewer_o6WB"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Reviewer_o6WB"
        ]
      },
      {
        "id": "Gydl9nHqoUZ",
        "original": null,
        "number": 2,
        "cdate": 1635848151862,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635848151862,
        "tmdate": 1635848151862,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a new aggregation-based homophily metric for assessing the homophily of a graph. This metric complements the current metrics by considering unharmful heterophily cases. The second contribution is a new filterbank framework which uses the diversification operation to fight harmful heterophily information.",
          "main_review": "Pros:\n(1) solid and deep analysis of the proposed metric and filterbank-based aggregation scheme.\n(2) solid and comprehensive experiments to show the significance of the proposed ideas.\n\nCons:\n(1) some necessary justifications are missing.\n(2) presentation can be improved.\n\nDetailed comments:\n-In section 3.1, it is not reader friendly to mention \"mean aggregation\" before introducing Definition 1. The same comment can be applied to the beginning of Section 4: explicitly give the definition of diversification operation before discuss about it.\n-Figure 2: how do you get the curves of (b) (c) (d) when you only manipulate H_edge?\n-Some figures are not easily readable in b/w print.\n-The three steps of ACM lacks explanations and justification. Specifically, in Steps 2 and 3, why using such a combination weight calculation scheme? The raw weights are estimated only based on the results from the corresponding filter. I feel it would be better to decide the weight for each channel based on all the filtered results. In step 3, why using W_{mix}? How do we interpret the temperature parameter here? Is the softmax in step 2 row-wise (need to explain it in order to improve readability)?\n-Experiments: In Figure 4, how to interpret the relations between H_{agg}^M and the performance increase? It would be better to put important and necessary discussions in the main text.\n\n-Minor typos: page 7, \"called is\", \"a set of filterbank\".",
          "summary_of_the_review": "This paper is well written and novel enough for this conference. The presentation can be improved and more justification can be added to further improve the paper.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Reviewer_vrWz"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Reviewer_vrWz"
        ]
      },
      {
        "id": "4yUbO5C2UjK",
        "original": null,
        "number": 3,
        "cdate": 1635935672404,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635935672404,
        "tmdate": 1635964701135,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this paper, the authors first analyze the potential drawbacks of existing heterophily metrics, then propose an aggregated heterophily metric aiming to better estimate the \"harmful\" heterophily and utilize diversification operation to address certain harmful heterophily cases. Based on the analysis, the authors propose an adaptive channel mixing (ACM) framework to improve GNN model performance. Several experiments have also been conducted to evaluate the proposed model.",
          "main_review": "In this paper, the authors first analyze the potential drawbacks of existing heterophily metrics, then propose an aggregated heterophily metric aiming to better estimate the \"harmful\" heterophily and utilize diversification operation to address certain harmful heterophily cases. Based on the analysis, the authors propose an adaptive channel mixing (ACM) framework to improve GNN model performance. Several experiments have also been conducted to evaluate the proposed model. In general, the authors focus on an important problem and the paper is easy to follow. There are a few issues the authors need to address.\n\nMy first concern is that the heterophily analysis might be over-simplified. It is good to see that the authors introduce an example to show when traditional heterophily metrics may fail to provide a meaningful heterophily value. However, the analysis conducted has two major assumptions: binary classification and one-hop aggregation. Though I understand that certain assumption is required for most theoretical analysis, these two seem to be very strong given that multi-layer of GNNs are usually operated on graphs with multiple node labels. This reduces the significance of the analysis.\n\nSome technical terms also need to be better explained or discussed. For example, the authors introduce high-pass (HP) and low-pass (LP) filters in Sec. 4 without introducing any motivation or rationales of applying them. I suggest the authors to better connect these filters with the analysis in Sec. 3 so that readers can better understand why in certain scenarios these two filters can help alleviate the heterophily problem. The authors briefly introduce some potential implementation of HP/LP filters in Sec. 4.2 while the exact implementation selected in the experiments is missing. I suggest the authors to explicitly indicate which filter implementations are selected and whether they're deterministic or learnable.\n\nThe ACM framework seems to be a weighted aggregation of the results after frequency filters. I suggested the authors to add an ablation study by comparing the ACM framework with some multi-head self-attention mechanisms to better understand where the gains are from.",
          "summary_of_the_review": "Strength:\n+ Focusing on an important problem\n+ Paper is easy to follow\n\nWeakness:\n- Over-simplified theoretical analysis\n- Some technical terms need to be explained / discussed\n- Missing certain ablation studies",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "n/a",
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Reviewer_ZQM7"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Reviewer_ZQM7"
        ]
      },
      {
        "id": "6qJ-FgP8UIB",
        "original": null,
        "number": 4,
        "cdate": 1636000003757,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636000003757,
        "tmdate": 1636000003757,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper shows that not all cases of heterophily are harmful for GNNs with aggregation operations. Based on a backpropagation analysis on an SGC-style GNN, it provides a new metric based on similarity matrix which considers the influence of both graph structure and input features. Observing that the diversification operation is able to address some harmful heterophily cases, it proposes the Adaptive Channel Mixing GNN framework which combines a high-pass filter, a low-pass filter and an identity channel.",
          "main_review": "Heterophily is an important factor for us to understand the performance of GNN on different graphs.  I think the paper found a good point that heterophily is not always harmful for all GNNs. This paper provides a new perspective to understand the problem, and designs a new metric based on the gradient analysis, which is interesting. The filterbank based method does solve the problem to some extent and gained very good performance.\n\nHowever, 1. It is not clear why the gradient leads to the aggregation similarity score as the heterophily metric, and how does it impact the GNNs\u2019 performance. There is also no direct connection with the later diversification analysis. \n2. The diversification operation helps on heterophilous graphs also lacks of enough motivation and theoretic explanation. It seems there is only one example (Figure 3) to illustrate it. That is not enough. In addition, the diversification operation even cannot work well for all cases, especially for multi-class problems. This makes the value of this paper hurt a lot.\n3. The filterbank based method is trivial and not new. Adding that the theoretic analysis of diversification operation or new metric is also not convincing, the novelty of this paper is incremental. \n",
          "summary_of_the_review": "Good motivation and observations, but the solution is also not novel enough and lacks of theoretic justification. ",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Reviewer_bk6o"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Reviewer_bk6o"
        ]
      },
      {
        "id": "3w7-7xbWN1r",
        "original": null,
        "number": 1,
        "cdate": 1636807797774,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636807797774,
        "tmdate": 1636808206119,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Public_Comment",
        "content": {
          "title": "Related work on GNNs and mixing patterns",
          "comment": "Hi, I wanted to bring to authors notice a related work from KDD '21 [1] which is missed in the paper and discussions. The referenced paper also analyses the behavior of GNNs w.r.t mixing patterns in networks using the notion of local assortativity. While the analysis is experimental in nature I still think it is very relevant here.\n\nWanted your thoughts on the experimental observations witnessed in [1] in relation to the current submission.\n\n[1] Susheel Suresh, Vinith Budde, Jennifer Neville, Pan Li, and Jianzhu Ma. Breaking the Limit of Graph Neural Networks by Improving the Assortativity of Graphs with Local Mixing Patterns. KDD '21\n\nThanks"
        },
        "signatures": [
          "~Susheel_Suresh1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "~Susheel_Suresh1"
        ]
      },
      {
        "id": "mV5l0km9_db",
        "original": null,
        "number": 2,
        "cdate": 1636891008555,
        "mdate": 1636891008555,
        "ddate": null,
        "tcdate": 1636891008555,
        "tmdate": 1636891008555,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Public_Comment",
        "content": {
          "title": "Chameleon and Squirrel dataset ",
          "comment": "The datasets are not appropriate properly. This is the right paper:\n\n@article{rozemberczki2021multi,\n  title={Multi-scale attributed node embedding},\n  author={Rozemberczki, Benedek and Allen, Carl and Sarkar, Rik},\n  journal={Journal of Complex Networks},\n  volume={9},\n  number={2},\n  pages={cnab014},\n  year={2021},\n  publisher={Oxford University Press}\n}\n"
        },
        "signatures": [
          "~Benedek_Andras_Rozemberczki1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "~Benedek_Andras_Rozemberczki1"
        ]
      },
      {
        "id": "a2f7eZqG03",
        "original": null,
        "number": 1,
        "cdate": 1637104859976,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637104859976,
        "tmdate": 1637106725099,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "6qJ-FgP8UIB",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer bk6o",
          "comment": "Thanks for you comments.\n\n**Q1**\n(1) It is not clear why the gradient leads to the aggregation similarity score as the heterophily metric, and how does it impact the GNNs\u2019 performance. (2) There is also no direct connection with the later diversification analysis.\n\n**R1**\n(1) The performances of current NN and GNN models are almost all related to the gradient descent algorithm, and the core difference between graph-aware and graph-agnostic models is that the former leverage graph information in the backpropagation process, while the latter does not. Thus, analyzing how the graph information will influence the gradient can help us to intuitively understand how the updating directions of GNNs are different from NNs. Based on the gradient analysis, we derive the new homophily metrics.\n\n(2) We also study the effect of diversification operation from the gradient updating perspective with analysis of $S(I-\\hat{A},X)$. From the observation in Figure 3, we find that the high-pass filter can help us to treat nodes from different classes as negative samples as we mentioned in section 4.1. This provides us a slightly different angle to analyze the high-pass filter and define diversification distinguishability.\n\n**Q2**\nThe diversification operation helps on heterophilous graphs also lacks of enough motivation and theoretic explanation.\n\n**R2**\nThe motivation of using diversification operation comes from the filterbank methods and our experimental experience. In Section 4, we use the $S(I-\\hat{A},X)$ to illustrate how diversification operation works in some cases when aggregation operation does not work well. We also provide a theoretic explanation in Theorem 1 for the binary classification problem.\n\n**Q3**\nthe diversification operation even cannot work well for all cases, especially for multi-class problems\n\n**R3**\nTo our knowledge, no method can work well for all cases. We give an example of multi-class problem just to illustrate the limitation of diversification operation because we need to be honest for that. It does not mean our method cannot work well for all multi-class problems. In fact, the ablation study have shown the effectiveness of the diversification operation.\n\nOur main point is that the proposed method has advantages on dealing with heterophily problem over the uni-channel (low-pass channel) methods, which are used by almost all GNNs, although the additional diversification operation (high-pass channel) may not address all harmful heterophilous cases. In our ACM and ACMII framework, the node-wise channel mixing mechanism can make ACM-GNNs and ACMII-GNNs  perform at least as well as the uni-channel GNNs.\n\n**Q4**\nThe filterbank based method is (1) trivial and (2) not new. (3) Adding that the theoretic analysis of diversification operation or  new metric is also (4) not convincing, (5) the novelty of this paper is incremental.\n\n**R4**\n(1) In our opinion the filterbank based method is not trivial. Could you give more detailed comments to help us to understand your view?\n\n(2) \nTo our knowledge, we are the first to analyze and apply the filterbank technique in graph representation learning to deal with the heterophily problem. There are geometric scattering networks [1,2] that apply filterbanks to address over-smoothing problem. But their definition of filterbank is totally different from the one used in our paper and theirs are not designed to address heterophily problem. \n\n(3) It is not clear to us why you think our theoretic analysis of diversification operation is not convincing. If your comment is based on **Q3**, please see our response to it.\n\n(4) We have empirically and theoretically verified the advantages of the proposed metrics over the existing homophily metrics in Figure 1, Section 3.2 and Appendix B. To us the advantages over the existing homophily metrics are quite convincing. It is not clear to us why you have an opposite point of view.\n\n(5) In our paper we discuss the differences between our method and the existing methods in Section 5, explain the insights behind HP filters for the heterophily problem, generalize the definition of filterbank to get the identity channel and design a node-wise channel mixing mechanism to learn information from different channels in other sections. To our knowledge, there is no prior work that adaptively applies a 3-channel filterbank architecture to address heterophily problem. We also discussed the detailed differences of ACM and ACMII with 2 SOTA methods, FAGCN and GPRGNN in Appendix J. \n\nOur paper develops a different approach which is not a simple improvement over the existing methods. Thus, in our opinion our contribution is not incremental. \n \n[1] Gao, Feng, Guy Wolf, and Matthew Hirn. \"Geometric scattering for graph data analysis.\" International Conference on Machine Learning. PMLR, 2019.\n\n[2] Min, Yimeng, Frederik Wenkel, and Guy Wolf. \"Scattering gcn: Overcoming oversmoothness in graph convolutional networks.\" arXiv preprint arXiv:2003.08414 (2020).\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ]
      },
      {
        "id": "WR8X2tEzAzy",
        "original": null,
        "number": 2,
        "cdate": 1637105423836,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637105423836,
        "tmdate": 1637106545262,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "4yUbO5C2UjK",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer ZQM7",
          "comment": "Thanks for you constructive suggestions and here are our response to your questions.\n\n**Q1**\n\"My first concern is that the heterophily analysis might be over-simplified...the analysis conducted has two major assumptions: (1) binary classification and (2) one-hop aggregation...\" \n\n**R1**\n(1) Our heterophily analyses are not all based on the binary classification. Specifically, the binary classification is used in the examples in Figure 1 and Figure 3 just to simplify the demonstration. The binary classification assumption is indeed used in Theorem 1 to illustrate the importance of diversification operation. But for multi-class problems, we have empirically and theoretically shown the advantages of the proposed metrics over the existing homophily metrics in  Section 3.2 and Appendix B. We also verify the effectiveness of the diversification operation on multi-class classification tasks in the ablation study .\n\n(2) Since the commonly used homophily metrics and heterophily analysis are all based on one-hop neighborhood as shown in Section 2.2, to follow the definition of homophily and make a fair comparison with the existing metrics, we define and analyze the new metric with one-hop aggregator. We will consider generalizing our analysis to multi-hop neighborhood in the future.\n\n\n**Q2**\n\" ...the authors introduce high-pass (HP) and low-pass (LP) filters in Sec. 4 without introducing any motivation or rationales of applying them. I suggest the authors to better connect these filters with the analysis in Sec. 3...\"\n\n**R2**\nThanks for your suggestion. The motivation of analyzing diversification operation is based on the filterbank methods and experimental results. And in Section 4, we explain why diversification operation works for some harmful heterophily cases based on the method  developed in section 3. We will clarify the motivation and connection in the revised version.\n\n**Q3**\n\n\"The authors briefly introduce some potential implementation of HP/LP filters in Sec. 4.2 while the exact implementation selected in the experiments is missing. I suggest the authors to explicitly indicate which filter implementations are selected and whether they're deterministic or learnable.\"\n\n**R3**\nActually in Section 6.2 we use the random-walk renormalized affinity matrix $\\hat{A}_{rw}$ as low-pass filter and $I - \\hat{A}_\\{rw\\}$ as high-pass filter.\n\nThe filters are deterministic and we will say it explicitly in the revised version.\n\n**Q4**\n\"I suggested the authors to add an ablation study by comparing the ACM framework with some multi-head self-attention mechanisms to better understand where the gains are from.\"\n\n**R4**\n\nThanks for this suggestion, but we cannot see the motivation and necessity to add multi-head self-attention in ablation study to understand the gains. \nIf you meant \n\n(1) Multi-head self-attention should be used to replace the proposed node-wise channel mixing mechanism\n\nOur reply: The node-wise channel mixing mechanism is based on the argument that each node has different needs for the information from different channels as mentioned in Section 4.2. We do not find the motivation to do this ablation and how this replacement can help us understand the gains better. On the other hand, self-attention will hurt the efficiency of the algorithm and will not add any contribution to our paper.\n\n(2) Multi-head self-attention should be used to learn the edge weights in each channel\n\nOur reply: we do have the experimental results of GAT, 8-head GAT, ACM-GAT and 8-head ACM-GAT as follows, if these are what you are interested in.\n\n| Models\\ Datasets  | Cornell | Wisconsin|Texas|Film|Squirrel|Cora|Citeseer|Pubmed|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|GAT (1-head)|58.92 $\\pm$ 3.38|51.76 $\\pm$ 6.80|60.81 $\\pm$ 5.30|27.91 $\\pm$ 0.93|28.42 $\\pm$ 1.01|73.30 $\\pm$ 1.17|70.15 $\\pm$ 1.43|81.90 $\\pm$ 0.59|\n|8-head GAT|58.92 $\\pm$ 3.15|53.53 $\\pm$ 4.89|60.81 $\\pm$ 4.56|28.01 $\\pm$ 1.19|28.41 $\\pm$ 1.88|75.02 $\\pm$ 1.00|71.33 $\\pm$ 2.05|82.02 $\\pm$ 0.70|\n|ACM-GAT (1-head)|71.35 $\\pm$ 6.68|75.49 $\\pm$ 5.06|72.97 $\\pm$ 0.58|35.18 $\\pm$ 1.13|30.44 $\\pm$ 1.43|85.17 $\\pm$1.46|74.79 $\\pm$ 1.88|88.75 $\\pm$ 0.62|\n|8-head ACM-GAT| **85.41 $\\pm$ 3.06** |**83.53 $\\pm$ 3.42**|**78.92 $\\pm$ 7.91**|**35.55 $\\pm$ 1.29**|**30.69 $\\pm$ 1.23**|**86.58 $\\pm$ 1.43**|**75.23 $\\pm$ 1.63**|**89.49 $\\pm$ 0.30**|\n\nIn our opinion, the current ablation study in Section 6.1 has already clearly shown that the HP channel, identity channel and node-wise channel mixing mechanism are effective and helps us understand where the gains are from. We are not sure which part of our ablation study is unclear or hard to understand.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ]
      },
      {
        "id": "4cb0pDglrXB",
        "original": null,
        "number": 3,
        "cdate": 1637106186460,
        "mdate": 1637106186460,
        "ddate": null,
        "tcdate": 1637106186460,
        "tmdate": 1637106186460,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "Gydl9nHqoUZ",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer vrWz",
          "comment": "\nWe appreciate your positive comments and valuable suggestions. Here are our response to your concerns and questions.\n\n**Q1**\n \"In Section 3.1, it is not reader friendly to mention \"mean aggregation\" before introducing Definition 1. The same comment can be applied to the beginning of Section 4: explicitly give the definition of diversification operation before discuss about it\"\n\n**R1**\nThanks for your suggestion. We will improve the presentation as you suggested.\n\nWe mentioned mean aggregator in the last paragraph of Section 2.1 and assumed the readers can understand mean aggregation based on mean aggregator. But we will make it more clear and straight-forward in the revised version.\n\nWe will add the definition of high-pass filter in Section 2.1 and give an explanation to the diversification operation in the revised version.\n\n**Q2**\n \"Figure 2: how do you get the curves of (b) (c) (d) when you only manipulate H_edge? -Some figures are not easily readable in b/w print\"\n\n**R2**\n\nAs mentioned in section 3.2, we generate graphs with different $H_\\text{edge}$ and for each generated graph, we calculate their $H_\\text{node},H_\\text{class}, H_\\text{agg}^M$. \n\nUnfortunately, we are unable to solve the b/w print problem. The only thing we can currently do to improve the readability is to enlarge the font size in the legend in the revised version.\n\n**Q3**\n-The three steps of ACM lacks explanations and justification. Specifically, (1) in Steps 2 and 3, why using such a combination weight calculation scheme? The raw weights are estimated only based on the results from the corresponding filter. I feel it would be better to decide the weight for each channel based on all the filtered results. In step 3, why using W_{mix}? (2) How do we interpret the temperature parameter here? (3) Is the softmax in step 2 row-wise (need to explain it in order to improve readability)?\n\n\n**R3**\n(1) The first line in step 2 is to extract nonlinear information from the filtered signal for each of the 3 channels.\nThe second line in step 2 then uses $W_{mix}$ to learn which channel is\nimportant or not important for each node from the combined information, leading to the weight information for each channel and for each node, i.e., the three vectors $\\alpha_L^l$, $\\alpha_H^l$ and $\\alpha_I^l$,\nwhose $i$-th elements are the weights for the $i$-th node.\nThese three vectors are then used as weights in defining the updated channel matrix $H^l$ in step 3.\n\n\nActually, we have tried a lot of possible designs for the weight learning. The empirical evaluation on real-world tasks indicated this design performs the best.\n\n(2) The temperature $T$ is a hyperparameter and as mentioned in section 6.1, we set it to the number of channels, i.e., $T=3$. The goal is for normalization that makes $\\tilde{\\alpha}_L^l+\\tilde{\\alpha}_H^l+\\tilde{\\alpha}_I^l$ lie in [0,1].\nMathematically this normalization is not needed. \nBut due to some numerical issue with the optimization method,\nthis normalization has an impact on the final result. \nWe have tried $T=1,3,5$ and found $T=3$ works the best.\n\n(3) Yes, the softmax is row-wise. We mention it in the paragraph under equation (11).  \nWe will give explanations about the three steps in the revised version.\n\n**Q4**\n\"Experiments: In Figure 4, how to interpret the relations between H_{agg}^M and the performance increase?\"\n\n**R4**\nThere is no direct and deterministic relation between $H_\\text{agg}^M$ and the performance increase. The reason we put $H_{agg}^M$ in Figure 4 is just to show the calculated statistics for each dataset. Due to space limitation, the relation between $H_\\text{agg}^M$ (together with other metrics) and GNN performance is discussed in Appendix H.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ]
      },
      {
        "id": "2oR-nTg1Bwq",
        "original": null,
        "number": 4,
        "cdate": 1637106325275,
        "mdate": 1637106325275,
        "ddate": null,
        "tcdate": 1637106325275,
        "tmdate": 1637106325275,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "EXZsB2Ep7nN",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer o6WB",
          "comment": "Thanks so much for your positive comments. We will keep updating our paper.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ]
      },
      {
        "id": "NL2PnlONWFY",
        "original": null,
        "number": 6,
        "cdate": 1637193647401,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637193647401,
        "tmdate": 1637193664191,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "mV5l0km9_db",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Comment",
        "content": {
          "title": "References added",
          "comment": "Hi Benedek,\n\nYour papers have been added to our revised version. Please take a look.\n\nBest,\n\nAuthors\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ]
      },
      {
        "id": "dd7yvNN5dOM",
        "original": null,
        "number": 7,
        "cdate": 1637193845015,
        "mdate": 1637193845015,
        "ddate": null,
        "tcdate": 1637193845015,
        "tmdate": 1637193845015,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "3w7-7xbWN1r",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Comment",
        "content": {
          "title": "Will consider your paper",
          "comment": "Hi Susheel,\n\nWe will consider comparing with your KDD paper in the revised version.\n\nBest,\n\nAuthors"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ]
      },
      {
        "id": "qxNLZ2bQQH0",
        "original": null,
        "number": 8,
        "cdate": 1637323357032,
        "mdate": 1637323357032,
        "ddate": null,
        "tcdate": 1637323357032,
        "tmdate": 1637323357032,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "4cb0pDglrXB",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Comment",
        "content": {
          "title": "Not satisfied by some of the replies",
          "comment": "Q2: I am wondering how you generate those curves (except for  H_edge) with smooth points along the x-axis.\nQ3: I still think a better way is to decide the raw weight for each channel based on all the filtered results.  The authors' reply is not convincing by simply saying empirical results show the current design is the best choice.\nQ4: It is not good to put the main result discussion in the appendix."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Reviewer_vrWz"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Reviewer_vrWz"
        ]
      },
      {
        "id": "qwg8wouejV5",
        "original": null,
        "number": 9,
        "cdate": 1637471076506,
        "mdate": 1637471076506,
        "ddate": null,
        "tcdate": 1637471076506,
        "tmdate": 1637471076506,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "qxNLZ2bQQH0",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Comment",
        "content": {
          "title": "Response 2 to Reviewer vrWz",
          "comment": "\n\n\n**Q1**: I am wondering how you generate those curves (except for H_edge) with smooth points along the x-axis. \n\n**R1**:\nWe reorder the value of the metrics in ascend order for x-axis.\n\nHere is a simplified example. Suppose we generate graphs with $H_\\text{edge}=0.1,0.5,0.9$, the test accuracy of GCN on the synthetic graphs are $0.8,0.5,0.9$.\nFor the generated graphs, we calculate their $H_\\text{agg}^M$, and suppose we get $H_\\text{agg}^M=0.7,0.4,0.8$. Then we will draw the performance of GCN under $H_\\text{agg}^M$ with x-axis $[0.4,0.7,0.8]$ and the corresponding reordered y-axis is $[0.5,0.8,0.9]$. Other metrics use the same process.\n\n**Q2**: I still think a better way is to decide the raw weight for each channel based on all the filtered results. The authors' reply is not convincing by simply saying empirical results show the current design is the best choice. \n\n**R2**:\nWe thought that your \"raw weight\" comment in your last report was related to $W_{mix}$\nand perhaps it would not be an issue after we gave an explanation about the role of $W_{mix}$.\nWe now realize we misunderstood it. Sorry about this.\nWe have been trying to figure out what your suggestion meant exactly.\nHere is our guess:\nReplace the first line in Step 2 by the following lines:\n\nConstruct the combined feature ${H}^{l}_\\text{Comb} = [{H}^{l}_L,{H}^{l}_H,{H}^{l}_I]$ and use it in step 2  so that the raw weight for each channel is based on all the filtered results as follows,\n\n$\\tilde{\\alpha}_L^l = \\sigma\\left({H}^{l}_\\text{Comb} \\tilde{W}^{l}_L \\right),\\ \\tilde{\\alpha}_H^l = \\sigma \\left({H}^{l}_\\text{Comb}\\tilde{W}^{l}_H \\right),\\ \\tilde{\\alpha}_I^l = \\sigma \\left({H}^{l}_\\text{Comb} \\tilde{W}^{l}_I \\right),\\ \\tilde{W}_L^{l-1},\\ \\tilde{W}_H^{l-1},\\ \\tilde{W}_I^{l-1} \\in \\mathbb{R}^{3F_l \\times 1}$\n$ \\left[{\\alpha}_L^l, {\\alpha}_H^l, {\\alpha}_I^l \\right] = \\text{Softmax}\\left((\\left[\\tilde{\\alpha}_L^l,\\tilde{\\alpha}_H^l,\\tilde{\\alpha}_I^l\\right]/T) W_\\text{Mix}^l \\right) \\in \\mathbb{R}^{N\\times 3},\\ T \\in \\mathbb{R} \\text{ temperature},\\ W_\\text{Mix}^l \\in \\mathbb{R}^{3\\times 3}; $\n\nHere are the performances of new methods (ACM-GCN (new), ACMII-GCN (new)) compared with the methods in our paper (ACM-GCN, ACMII-GCN):  \n\n| Models\\ Datasets  | Cornell | Wisconsin|Texas|Film|Squirrel|Cora|Citeseer|Pubmed|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|ACM-GCN | 94.75 $\\pm$ 3.8 | 95.75 $\\pm$ 2.03 | 94.92 $\\pm$ 2.88 | 41.62 $\\pm$ 1.15 | 69.04 $\\pm$ 1.74 | 58.02 $\\pm$ 1.86 | 88.62 $\\pm$ 1.22 | 81.68 $\\pm$ 0.97 | 90.66 $\\pm$ 0.47 |\n|ACMII-GCN | 95.9 $\\pm$ 1.83 | 96.62 $\\pm$ 2.44 | 95.08 $\\pm$ 2.07 | 41.84 $\\pm$ 1.15 | 68.38 $\\pm$ 1.36 | 54.53 $\\pm$ 2.09 | 89.00 $\\pm$ 0.72 | 81.79 $\\pm$ 0.95 | 90.74 $\\pm$ 0.5 |\n|ACM-GCN (new) | 95.08 $\\pm$ 2.64 | 96.12 $\\pm$ 1.31 | 94.92 $\\pm$ 2.48 | 41.62 $\\pm$ 1.34 | 68.82 $\\pm$ 2.18 | 57.48 $\\pm$ 1.68 | 88.59 $\\pm$ 1.04 | 81.9 $\\pm$ 1.27 | 90.75 $\\pm$ 0.77  |\n|ACMII-GCN (new) | 93.93 $\\pm$ 3.52 | 96 $\\pm$ 2 | 94.59 $\\pm$ 2.94 | 41.44 $\\pm$ 1.18 | 68.53 $\\pm$ 3.08 | 53.28 $\\pm$ 1.08 | 88.75 $\\pm$ 0.83 | 81.76 $\\pm$ 1.05 | 90.58 $\\pm$ 0.64 |\n\nFrom the results, we do not find significant differences between the frameworks with combined features and the ones without combined features in the feature extraction step. The reason we think is that the necessary nonlinear information from each channel is combined in $\\left[\\tilde{\\alpha}_L^l,\\tilde{\\alpha}_H^l,\\tilde{\\alpha}_I^l\\right]$ and $W_\\text{Mix}^l$ is enough to learn to mix the combined weights from different channels. The learning of redundant information in the feature extraction step for each channel will not improve the performance.\n(A disadvantage of using the combined feature is that it increases the computational cost.)\n\nIf our guess is not what you meant, could you please describe your suggestion mathematically?\n\n**Q3**: It is not good to put the main result discussion in the appendix.\n\n**R3**:\nThanks for your advice. Due to the space limitation, we have no choice but to put some discussion to appendix and only leave the main results and conclusions in the main text. We will definitely move some of the discussion back to the main text if we are allowed to have one more page for the main paper."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Authors"
        ]
      },
      {
        "id": "YjMznrFrx4j",
        "original": null,
        "number": 1,
        "cdate": 1642696849862,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696849862,
        "tmdate": 1642696849862,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The reviewers agree that the paper is addressing an interesting problem. However, the authors analyze the effect of heterophily on GNN for node classification. The authors simplify the analysis by removing the nonlinearity in the GNN model and derive some theoretical results. However, the analysis is very specific to the simplified version of GNN, and the link to later proposed solution is also weak. Furthermore, a more significant improvement in experiments will also make the paper more convincing."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "EXZsB2Ep7nN",
        "original": null,
        "number": 1,
        "cdate": 1635807109433,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635807109433,
        "tmdate": 1635807109433,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper starts from studying the question of how heterophily affects the learning effectiveness of Graph Neural Networks on node classification tasks and posits that heterophily may not be always detrimental to the task. Following this observation, it proposes a new architecture, called Adaptive Channel Mixing, which appropriately applies aggregation, diversification and identity channels in each GNN layer in order to tackle harmful heterophily. According to the experimental results, the proposed architecture is successful.",
          "main_review": "The paper addresses a well known problem in the literature in a seemingly effective way. It is really well written, offering good background on the problem and going smoothly from the exposition of the problem and the limited view of existing approaches to the proposed ACM architecture to address it. Then, it performs very comprehensive experiments that demonstrate the effectiveness of the proposed scheme.\n\nIf I need to mention a weakness, this is the fact that the paper necessarily compresses a lot of discussion and description due to the conference constraints. For that reason, it misses a thorough analysis and comparison with the state of the art, as well as a more in-depth discussion of the experimental results and conclusions. Yet, the authors make the best of the available space to cover all necessary information.",
          "summary_of_the_review": "The paper proposes a new architecture, called Adaptive Channel Mixing, with the aim to address several cases of harmful heterophily in GNN node classification settings. The paper is well written and motivated and contains convincing experimentations. It could also have a big impact on GNN node classification practice and future research.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Reviewer_o6WB"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Reviewer_o6WB"
        ]
      },
      {
        "id": "Gydl9nHqoUZ",
        "original": null,
        "number": 2,
        "cdate": 1635848151862,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635848151862,
        "tmdate": 1635848151862,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a new aggregation-based homophily metric for assessing the homophily of a graph. This metric complements the current metrics by considering unharmful heterophily cases. The second contribution is a new filterbank framework which uses the diversification operation to fight harmful heterophily information.",
          "main_review": "Pros:\n(1) solid and deep analysis of the proposed metric and filterbank-based aggregation scheme.\n(2) solid and comprehensive experiments to show the significance of the proposed ideas.\n\nCons:\n(1) some necessary justifications are missing.\n(2) presentation can be improved.\n\nDetailed comments:\n-In section 3.1, it is not reader friendly to mention \"mean aggregation\" before introducing Definition 1. The same comment can be applied to the beginning of Section 4: explicitly give the definition of diversification operation before discuss about it.\n-Figure 2: how do you get the curves of (b) (c) (d) when you only manipulate H_edge?\n-Some figures are not easily readable in b/w print.\n-The three steps of ACM lacks explanations and justification. Specifically, in Steps 2 and 3, why using such a combination weight calculation scheme? The raw weights are estimated only based on the results from the corresponding filter. I feel it would be better to decide the weight for each channel based on all the filtered results. In step 3, why using W_{mix}? How do we interpret the temperature parameter here? Is the softmax in step 2 row-wise (need to explain it in order to improve readability)?\n-Experiments: In Figure 4, how to interpret the relations between H_{agg}^M and the performance increase? It would be better to put important and necessary discussions in the main text.\n\n-Minor typos: page 7, \"called is\", \"a set of filterbank\".",
          "summary_of_the_review": "This paper is well written and novel enough for this conference. The presentation can be improved and more justification can be added to further improve the paper.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Reviewer_vrWz"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Reviewer_vrWz"
        ]
      },
      {
        "id": "4yUbO5C2UjK",
        "original": null,
        "number": 3,
        "cdate": 1635935672404,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635935672404,
        "tmdate": 1635964701135,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this paper, the authors first analyze the potential drawbacks of existing heterophily metrics, then propose an aggregated heterophily metric aiming to better estimate the \"harmful\" heterophily and utilize diversification operation to address certain harmful heterophily cases. Based on the analysis, the authors propose an adaptive channel mixing (ACM) framework to improve GNN model performance. Several experiments have also been conducted to evaluate the proposed model.",
          "main_review": "In this paper, the authors first analyze the potential drawbacks of existing heterophily metrics, then propose an aggregated heterophily metric aiming to better estimate the \"harmful\" heterophily and utilize diversification operation to address certain harmful heterophily cases. Based on the analysis, the authors propose an adaptive channel mixing (ACM) framework to improve GNN model performance. Several experiments have also been conducted to evaluate the proposed model. In general, the authors focus on an important problem and the paper is easy to follow. There are a few issues the authors need to address.\n\nMy first concern is that the heterophily analysis might be over-simplified. It is good to see that the authors introduce an example to show when traditional heterophily metrics may fail to provide a meaningful heterophily value. However, the analysis conducted has two major assumptions: binary classification and one-hop aggregation. Though I understand that certain assumption is required for most theoretical analysis, these two seem to be very strong given that multi-layer of GNNs are usually operated on graphs with multiple node labels. This reduces the significance of the analysis.\n\nSome technical terms also need to be better explained or discussed. For example, the authors introduce high-pass (HP) and low-pass (LP) filters in Sec. 4 without introducing any motivation or rationales of applying them. I suggest the authors to better connect these filters with the analysis in Sec. 3 so that readers can better understand why in certain scenarios these two filters can help alleviate the heterophily problem. The authors briefly introduce some potential implementation of HP/LP filters in Sec. 4.2 while the exact implementation selected in the experiments is missing. I suggest the authors to explicitly indicate which filter implementations are selected and whether they're deterministic or learnable.\n\nThe ACM framework seems to be a weighted aggregation of the results after frequency filters. I suggested the authors to add an ablation study by comparing the ACM framework with some multi-head self-attention mechanisms to better understand where the gains are from.",
          "summary_of_the_review": "Strength:\n+ Focusing on an important problem\n+ Paper is easy to follow\n\nWeakness:\n- Over-simplified theoretical analysis\n- Some technical terms need to be explained / discussed\n- Missing certain ablation studies",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "n/a",
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Reviewer_ZQM7"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Reviewer_ZQM7"
        ]
      },
      {
        "id": "6qJ-FgP8UIB",
        "original": null,
        "number": 4,
        "cdate": 1636000003757,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636000003757,
        "tmdate": 1636000003757,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper shows that not all cases of heterophily are harmful for GNNs with aggregation operations. Based on a backpropagation analysis on an SGC-style GNN, it provides a new metric based on similarity matrix which considers the influence of both graph structure and input features. Observing that the diversification operation is able to address some harmful heterophily cases, it proposes the Adaptive Channel Mixing GNN framework which combines a high-pass filter, a low-pass filter and an identity channel.",
          "main_review": "Heterophily is an important factor for us to understand the performance of GNN on different graphs.  I think the paper found a good point that heterophily is not always harmful for all GNNs. This paper provides a new perspective to understand the problem, and designs a new metric based on the gradient analysis, which is interesting. The filterbank based method does solve the problem to some extent and gained very good performance.\n\nHowever, 1. It is not clear why the gradient leads to the aggregation similarity score as the heterophily metric, and how does it impact the GNNs\u2019 performance. There is also no direct connection with the later diversification analysis. \n2. The diversification operation helps on heterophilous graphs also lacks of enough motivation and theoretic explanation. It seems there is only one example (Figure 3) to illustrate it. That is not enough. In addition, the diversification operation even cannot work well for all cases, especially for multi-class problems. This makes the value of this paper hurt a lot.\n3. The filterbank based method is trivial and not new. Adding that the theoretic analysis of diversification operation or new metric is also not convincing, the novelty of this paper is incremental. \n",
          "summary_of_the_review": "Good motivation and observations, but the solution is also not novel enough and lacks of theoretic justification. ",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper280/Reviewer_bk6o"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper280/Reviewer_bk6o"
        ]
      },
      {
        "id": "3w7-7xbWN1r",
        "original": null,
        "number": 1,
        "cdate": 1636807797774,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636807797774,
        "tmdate": 1636808206119,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Public_Comment",
        "content": {
          "title": "Related work on GNNs and mixing patterns",
          "comment": "Hi, I wanted to bring to authors notice a related work from KDD '21 [1] which is missed in the paper and discussions. The referenced paper also analyses the behavior of GNNs w.r.t mixing patterns in networks using the notion of local assortativity. While the analysis is experimental in nature I still think it is very relevant here.\n\nWanted your thoughts on the experimental observations witnessed in [1] in relation to the current submission.\n\n[1] Susheel Suresh, Vinith Budde, Jennifer Neville, Pan Li, and Jianzhu Ma. Breaking the Limit of Graph Neural Networks by Improving the Assortativity of Graphs with Local Mixing Patterns. KDD '21\n\nThanks"
        },
        "signatures": [
          "~Susheel_Suresh1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "~Susheel_Suresh1"
        ]
      },
      {
        "id": "mV5l0km9_db",
        "original": null,
        "number": 2,
        "cdate": 1636891008555,
        "mdate": 1636891008555,
        "ddate": null,
        "tcdate": 1636891008555,
        "tmdate": 1636891008555,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Public_Comment",
        "content": {
          "title": "Chameleon and Squirrel dataset ",
          "comment": "The datasets are not appropriate properly. This is the right paper:\n\n@article{rozemberczki2021multi,\n  title={Multi-scale attributed node embedding},\n  author={Rozemberczki, Benedek and Allen, Carl and Sarkar, Rik},\n  journal={Journal of Complex Networks},\n  volume={9},\n  number={2},\n  pages={cnab014},\n  year={2021},\n  publisher={Oxford University Press}\n}\n"
        },
        "signatures": [
          "~Benedek_Andras_Rozemberczki1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "~Benedek_Andras_Rozemberczki1"
        ]
      },
      {
        "id": "YjMznrFrx4j",
        "original": null,
        "number": 1,
        "cdate": 1642696849862,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696849862,
        "tmdate": 1642696849862,
        "tddate": null,
        "forum": "LBv-JtAmm4P",
        "replyto": "LBv-JtAmm4P",
        "invitation": "ICLR.cc/2022/Conference/Paper280/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The reviewers agree that the paper is addressing an interesting problem. However, the authors analyze the effect of heterophily on GNN for node classification. The authors simplify the analysis by removing the nonlinearity in the GNN model and derive some theoretical results. However, the analysis is very specific to the simplified version of GNN, and the link to later proposed solution is also weak. Furthermore, a more significant improvement in experiments will also make the paper more convincing."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}