{
  "id": "KVYq2Ea90PC",
  "original": "1jNqPh457we",
  "number": 82,
  "cdate": 1632875427313,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875427313,
  "tmdate": 1697934968689,
  "ddate": null,
  "content": {
    "title": "A Study of Face Obfuscation in ImageNet",
    "authorids": [
      "~Kaiyu_Yang1",
      "~Jacqueline_Yau1",
      "~Li_Fei-Fei1",
      "~Jia_Deng1",
      "~Olga_Russakovsky1"
    ],
    "authors": [
      "Kaiyu Yang",
      "Jacqueline Yau",
      "Li Fei-Fei",
      "Jia Deng",
      "Olga Russakovsky"
    ],
    "keywords": [
      "ImageNet",
      "Privacy",
      "Face Obfuscation"
    ],
    "abstract": "Face obfuscation (blurring, mosaicing, etc.) has been shown to be effective for privacy protection; nevertheless, object recognition research typically assumes access to complete, unobfuscated images. In this paper, we explore the effects of face obfuscation on the popular ImageNet challenge visual recognition benchmark. Most categories in the ImageNet challenge are not people categories; however, many incidental people appear in the images, and their privacy is a concern. We first annotate faces in the dataset. Then we demonstrate that face blurring and overlaying---two typical obfuscation techniques---have minimal impact on the accuracy of recognition models. Concretely, we benchmark multiple deep neural networks on face-obfuscated images and observe that the overall recognition accuracy drops only slightly (<= 1.0%). Further, we experiment with transfer learning to 4 downstream tasks (object recognition, scene recognition, face attribute classification, and object detection) and show that features learned on face-obfuscated images are equally transferable. Our work demonstrates the feasibility of privacy-aware visual recognition, improves the highly-used ImageNet challenge benchmark, and suggests an important path for future visual datasets. ",
    "pdf": "/pdf/e85c08013925bcfd24615c73a5550ca4a8bc0d6e.pdf",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "yang|a_study_of_face_obfuscation_in_imagenet",
    "supplementary_material": "",
    "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2103.06191/code)",
    "_bibtex": "@misc{\nyang2022a,\ntitle={A Study of Face Obfuscation in ImageNet},\nauthor={Kaiyu Yang and Jacqueline Yau and Li Fei-Fei and Jia Deng and Olga Russakovsky},\nyear={2022},\nurl={https://openreview.net/forum?id=KVYq2Ea90PC}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "KVYq2Ea90PC",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 13,
    "directReplyCount": 5,
    "revisions": true,
    "replies": [
      {
        "id": "6m-g0uRpct-",
        "original": null,
        "number": 1,
        "cdate": 1635689628171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635689628171,
        "tmdate": 1635689628171,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "KVYq2Ea90PC",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The main concern addressed in this paper is the privacy problem that may result from images in ImageNet databases containing unexpected faces. The authors propose a two-step face filtering method. First, the authors use a detector called Amazon Rekognition to detect the ImageNet database. Then, the authors further optimize the detector output through the crowdsourcing platform Amazon Mechanical Turk (AMT) to reduce false positives and false negatives in automated detection. For the detected faces, the authors took two approaches, distinguishing between blurring and overlaying, and tested their effectiveness on different models separately. The accuracy of the two approaches was reduced by 0.9% on average compared to the original database on the ILSVRC classification challenge. And using the database that has blurred or covered the faces still maintains the transferability of the original database in the tests of downstream tasks.\nContribution\uff1a\n1. The authors perform a very time-consuming and labor-intensive task for accurate labeling and filtering of faces in the ImageNet database and statistical analysis of the classes of faces contained in ImageNet.\n2. The authors demonstrate experiments related to classification tasks and pre-trained model training using a database containing blurred or covered faces, proving that the theory is feasible and that the dropped accuracy is acceptable.\n3. In terms of ethics, using blurred or covered face data for training can reduce privacy concerns. The study of the ImageNet database in terms of privacy can provide an important reference for subsequent databases",
          "main_review": "The main weakness:\n1. I was confused by the statement in section (Part III, second paragraph) as to whether the face annotation process was manually filtered only for faces that were successfully detected (generated prediction boxes5) by Amazon Rekognition. Although I can infer from Appendix A Stage1 that only the data with successful detector predictions should be put into the AMT platform. Perhaps it would be better to include a brief description of Amazon Rekognition in the text. \n2. And if some faces are not detected by Amazon Rekognition, how do you tackle this problem? This is an important problem for the privacy issue in this paper because this paper focuses on the privacy issue.\n3. The novelty of this paper is limited reference to the proposed method in the paper.\n\nThe main strengths:\n1. The experimental part of the paper is depicted in great detail and completely. Rigorous validation of the obfuscation approach is done on two different methods on 15 different models. And in the appendix, the detailed method of blurring, and the problems that may happen in the process of labeling are under clearer explanation.\n2. The ethics of machine learning has been widely debated, with the issue of face privacy being of particular concern. There are many similar discussions, for example, there are some papers proposing to remove images associated with people from the database. The feasibility of obfuscation processing of faces proposed in this paper is a good way to minimize privacy issues without reducing the number of databases at the same time.",
          "summary_of_the_review": "This paper has some contributions in exploring the ethicality of datasets, especially in the current very popular ImageNet database, but it exists some flaws (see weakness). The solutions and results in this paper are open sources and feasible, and this work will inspire subsequent exploration of privacy protection in publicly available datasets. ",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "Yes, Privacy, security and safety"
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Reviewer_U3FW"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Reviewer_U3FW"
        ]
      },
      {
        "id": "Y-UbKxlTgxo",
        "original": null,
        "number": 2,
        "cdate": 1635918799824,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635918799824,
        "tmdate": 1635918799824,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "KVYq2Ea90PC",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper mainly discuss the privacy issue for human for the widely used ImageNet dataset and how to handle them.\nIn addition, the paper does a very detailed empirical experiments to study the performance influence for various tasks, including object recognition, scene recognition, face attribute, and object detection if all the faces in the ImageNet are obfuscated.",
          "main_review": "The main strength of the paper is to address the privacy issues of ImageNet and is to provide an alternative face obfuscated version.\nIn addition, the authors conduct very thorough experiments across different tasks and different architecture for the study of the performance influence with the obfuscated dataset. It shows new dataset still is effective for transfer learning for various vision task with few performance drop. However, the weakness is that the main part of the paper is to examine the performance influence of different settings and is of limited technical novelty. For the verification of some downstream tasks, the coverage of tasks is not enough.",
          "summary_of_the_review": "Although the paper mainly focuses on providing plenty of empirical results to evaluate the influence of using the face obfuscated ImageNet dataset and is of limited novelty, it provides a lot of insights to show the effectiveness and feasibility of privacy preserving ImageNet.\n\nI have few concerns of the selection of transferring tasks. For example, the resolution of CIFAR-10 is only 32x32 and only for 10 classes. Similarly, Pascal VOC is also relatively small and easy dataset as compared with COCO or other recently released object detection dataset.\nMost of the images in the CelebA dataset are in frontal pose and have much fewer variations than other unconstrained face dataset, like IJB-C, etc. Since these datasets are relatively simpler than others which are more close to real-world scenarios, I wonder if the same experimental results and findings are still valid for harder datasets with more variations.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Reviewer_6ZXN"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Reviewer_6ZXN"
        ]
      },
      {
        "id": "muaJz2tXtFF",
        "original": null,
        "number": 3,
        "cdate": 1635986376213,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635986376213,
        "tmdate": 1635986567001,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "KVYq2Ea90PC",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper presents an empirical study on the effect of face obfuscation in the ImageNet dataset. The main conclusion is that face obfuscation does not decrease the utility of the dataset. Specifically, the authors showed that various networks trained on the obfuscated dataset only experienced small accuracy drop on the image classification task. The authors also discussed the impact on different categories, showing that face obfuscation hurt more to the object categories that are more closely related to faces (i.e., the bounding boxes of which overlap more with faces). Last but now least, experiments has been conducted to show that face obfuscation also does not have a significant impact on the transferability of the features learned from the new dataset. All these conclusions are inline with intuitions since ImageNet is not primarily focused on human activities / faces.",
          "main_review": "Strengths:\n* The main contribution of this work is that it provided empirical evidence on the effect of face obfuscation on the ImageNet dataset. Through comprehensive experiment, the authors showed that face obfuscation does not decrease the utility of the dataset.\n* Another contribution that should not be overlooked is that the authors annotated all faces in ImageNet in a semi-automatic manner and promised that they will make the annotations publicly available to other researchers.\n* The paper is very well written and includes a lot of details on the experiment protocol. Thus It should be straightforward for other researchers to reproduce the results and to extend the study.\n\nWeaknesses:\n* Since blur and cutout are commonly used data augmentation techniques, it is to be expected that face obfuscation would not have a big impact to visions tasks that have little to do with faces. Although it is commendable that this is now shown empirically though the study, this work also does not bring interesting new insights into the topic.\n* The authors showed that categories that are closely related to faces are indeed affected more by face obfuscation. This paper would be more interesting (from a technical point of view) if the authors could additionally investigate into methods for alleviating such impact.",
          "summary_of_the_review": "This paper is very well written and it provides empirical evidences to support the intuition that face obfuscation does not decrease the utility of the ImageNet dataset. However, my main concern is that the paper has no technical novelty and it also does not bring sufficiently new insights to the community.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Reviewer_7itG"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Reviewer_7itG"
        ]
      },
      {
        "id": "QMmULX5fYXv",
        "original": null,
        "number": 2,
        "cdate": 1637610027795,
        "mdate": 1637610027795,
        "ddate": null,
        "tcdate": 1637610027795,
        "tmdate": 1637610027795,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "KVYq2Ea90PC",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Comment",
        "content": {
          "title": "Common Response",
          "comment": "We thank all reviewers for their thoughtful comments. They agree that our thorough empirical experiments (6ZXN,U3FW) provide a lot of insights (6ZXN) and that our paper is well-written (7itG). We are especially encouraged that reviewers think our work provides important reference for subsequent data collection practices (U3FW) and our face annotations is a valuable service to the community (7itG, U3FW)\n\nReviewers\u2019 concerns focus on technical novelty. They point out that our paper only analyzes existing methods/datasets without proposing any novel techniques. This is a point that we totally agree with. However, we respectfully but wholeheartedly disagree that a paper must have algorithmic contributions to be important and impactful. Many great papers published in machine learning and computer vision do not focus on algorithmic contributions. Instead, they focus on analyzing existing methods/datasets ([A], [B], [C]), or provide a service to the community (e.g., by constructing datasets [D] [E]). \n\n* [A] Torralba and Efros. \"Unbiased look at dataset bias.\" CVPR, 2011.\n* [B] Shankar et al. \"Evaluating machine accuracy on imagenet.\" ICML, 2020.\n* [C] Tsipras et al. \"From imagenet to image classification: Contextualizing progress on benchmarks.\" ICML, 2020.\n* [D] Deng et al. \"Imagenet: A large-scale hierarchical image database.\" CVPR, 2009.\n* [E] Lin et al. \"Microsoft coco: Common objects in context.\" ECCV, 2014.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ]
      },
      {
        "id": "O_pQAS3OQuo",
        "original": null,
        "number": 3,
        "cdate": 1637610084337,
        "mdate": 1637610084337,
        "ddate": null,
        "tcdate": 1637610084337,
        "tmdate": 1637610084337,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "muaJz2tXtFF",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Comment",
        "content": {
          "title": "Individual Response to Reviewer 7itG",
          "comment": "Thank you for your valuable feedback! Below we address your questions and concerns. Please feel free to post additional comments if you have further questions.\n\n## Technical novelty\n\nPlease see the common response above\n\n## Since blur and cutout are commonly used data augmentation techniques, it is expected that face obfuscation has a marginal impact on vision tasks that have little to do with faces. \n\nPrivacy-aware face obfuscation is completely different from data augmentation such as cutout. Cutout augments the training images by applying random masks. Since masks are at random locations, the masked area may become visible when the same image appears again in another epoch. Therefore, cutout does not necessarily remove any information from the training data. In contrast, face obfuscation systematically removes information in the face bounding boxes, which may reasonably lead to the accuracy drop.\n\nAlso, the results of our experiments cannot be trivially inferred. Even if a vision task has little to do with faces, it is possible that models learn to exploit spurious face-related visual cues for making predictions. For example, prior works have shown that models rely on gender cues to perform action recognition (Hendricks et al., 2018). \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ]
      },
      {
        "id": "BPCd70GZ4f",
        "original": null,
        "number": 4,
        "cdate": 1637610115436,
        "mdate": 1637610115436,
        "ddate": null,
        "tcdate": 1637610115436,
        "tmdate": 1637610115436,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "Y-UbKxlTgxo",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Comment",
        "content": {
          "title": "Individual Response to Reviewer 6ZXN",
          "comment": "Thank you for your valuable feedback! Below we address your questions and concerns. Please feel free to post additional comments if you have further questions.\n\n## Technical novelty\n\nPlease see the common response above\n\n\n## The datasets in the transfer learning experiments are too simple.\n\nWe choose simpler datasets such as CIFAR-10, PASCAL VOC, and CelebA deliberately, because they are more likely to benefit from ImageNet pretraining. When studying the effect of face obfuscation on ImageNet\u2019s utility as a pretraining dataset, we\u2019d like to use datasets on which ImageNet pretraining makes a large difference. For example, we include PASCAL VOC but not COCO since prior work has shown that ImagetNet pretraining is not necessary for obtaining good performance on COCO [F]. \n\n\n[F] He, Kaiming, Ross Girshick, and Piotr Doll\u00e1r. \"Rethinking ImageNet pre-training.\" CVPR, 2019."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ]
      },
      {
        "id": "TBl_jsly6qe",
        "original": null,
        "number": 5,
        "cdate": 1637610147963,
        "mdate": 1637610147963,
        "ddate": null,
        "tcdate": 1637610147963,
        "tmdate": 1637610147963,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "6m-g0uRpct-",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Comment",
        "content": {
          "title": "Individual Response to Reviewer U3FW",
          "comment": "Thank you for your valuable feedback! Below we address your questions and concerns. Please feel free to post additional comments if you have further questions.\n\n## Technical novelty\n\nPlease see the common response above\n\n\n## Do you only filter the faces identified by the face detector? What if some faces are not detected?\n\nThis is definitely a concern we considered. In the human annotation step, workers are able to correct any errors in the automated face detector. Concretely, the workers (1) remove any false positive detections, (2) adjust the detected boxes to correct for localization errors and (3) add new detections as necessary to correct any false negatives. Please refer to Appendix A for details. \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ]
      },
      {
        "id": "wmytw7qfx33",
        "original": null,
        "number": 6,
        "cdate": 1637740709804,
        "mdate": 1637740709804,
        "ddate": null,
        "tcdate": 1637740709804,
        "tmdate": 1637740709804,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "BPCd70GZ4f",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Comment",
        "content": {
          "title": "Thanks for the reply",
          "comment": "Thanks for the reply of the authors. I truly appreciate the efforts done by the work for addressing the privacy issue of the ImageNet dataset. However, I would still want to see the influence and difference w/ and w/o face obfuscation for  larger and more complex datasets which can get benefit from ImageNet pretraining. I will still keep my rating."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Reviewer_6ZXN"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Reviewer_6ZXN"
        ]
      },
      {
        "id": "2zWZaOHfB5h",
        "original": null,
        "number": 7,
        "cdate": 1637769440911,
        "mdate": 1637769440911,
        "ddate": null,
        "tcdate": 1637769440911,
        "tmdate": 1637769440911,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "TBl_jsly6qe",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Comment",
        "content": {
          "title": "Final vote",
          "comment": "First, thanks for your efforts on this work. It is still not well answered my question when some faces are not detected.  It still cannot clearly explain how to process the problems of the false positive/negative detections.  How do you use different detections to correct false negatives? I will still keep my rating."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Reviewer_U3FW"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Reviewer_U3FW"
        ]
      },
      {
        "id": "wh4857w61de",
        "original": null,
        "number": 8,
        "cdate": 1637770394507,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637770394507,
        "tmdate": 1637771032010,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "2zWZaOHfB5h",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Comment",
        "content": {
          "title": "Further Clarification",
          "comment": "Thanks for the question. To clarify:\n* For false positives: the crowd worker can remove face bounding boxes detected by the face detector.\n* For false negatives: the crowd worker can draw new face bounding boxes.\n\nHopefully that addresses your question. Please feel free to ask if anything remains unclear."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ]
      },
      {
        "id": "hkhY-LAudvC",
        "original": null,
        "number": 9,
        "cdate": 1637811820515,
        "mdate": 1637811820515,
        "ddate": null,
        "tcdate": 1637811820515,
        "tmdate": 1637811820515,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "wh4857w61de",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Comment",
        "content": {
          "title": " False Negatives",
          "comment": "There are millions of images in ImageNet, it's impossible to check every image to find the face which is not detected by face detectors. And how to evaluate/judge the quality of the labels from crowd workers.  \n\nMoreover, Imagenet is an impact work in computer vision. This paper is only done some little changes on it, the contribution is incremental. It cannot meet the high standard of this conference."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Reviewer_U3FW"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Reviewer_U3FW"
        ]
      },
      {
        "id": "YiErhboQMk",
        "original": null,
        "number": 10,
        "cdate": 1638134077442,
        "mdate": 1638134077442,
        "ddate": null,
        "tcdate": 1638134077442,
        "tmdate": 1638134077442,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "hkhY-LAudvC",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Comment",
        "content": {
          "title": "Clarification on the Face Annotation Process and Our Contributions",
          "comment": "Thanks for engaging with our work and providing valuable feedback! We further address the questions about the face annotation process and the contributions of the work \n\n# Face Annotation Process\n\nAppendix A includes the full details of our face annotation method. Here we selectively cover a few points related to the reviewer\u2019s question.\n\n### Overview of the process\n\n1. Given the 1.4 million images in ILSVRC, we use a face detector to detect faces automatically.\n2. We ask crowd workers to correct the face detection results. They can remove false positives and draw new bounding boxes to cover false negatives. \n\n\n### Feasibility\n\nEven though ILSVRC has more than 1.4 million images, our crowdsourcing pipeline is scalable enough to annotate faces on all images within a reasonable budget. As mentioned on page 17, we spent about $2500 on worker compensation.\n\nState-of-the-art face detectors are already quite accurate (Table 1 and Fig. 6). When checking the face detection results, the worker can quickly skip most images without editing them. Therefore, we were able to pack 50 images in each HIT (Human Intelligence Task). And annotating 1.4 million images needs less than 30K HITs. \n\n\n### Quality control in crowdsourcing\n\nThe paper discussed our quality control measures at the end of page 16. In summary, the 50 images in a HIT contain gold standard images for which we know the answer in advance. In order to finish the HIT, the worker has to achieve reasonable accuracy on gold standard images.\n\n\n### Evaluating the quality of face annotations\n\nThe paper has a dedicated subsection (page 4) on evaluating the quality of the annotations. In summary, we selected 20 categories on which the face detector makes more errors. Then we manually verified the face annotations on the validation images from these categories. Results (Table 1) show that our face annotations are of high quality. Among the 20 categories, we have on average 1.25 false positives and 0.95 false negatives per 50 images. Further, the overall accuracy on the entire ILSVRC is much higher as these categories are selected deliberately to be error-prone.\n\n\n# Our Contributions\n\nWe agree that ImageNet is a high-impact work (with > 34,000 citations to date) and that our contributions may not be on the scale of ImageNet. Nevertheless, we respectfully disagree that the contributions are incremental. Face annotations are important for studying privacy in visual data (e.g., Oh et al. 2016 and Ren et al. 2018). ImageNet is an important computer vision dataset, but we are not aware of existing large-scale face annotations on it. We fill this gap and provide useful data for the computer vision community to study the privacy aspects of their models. Further, we release ImageNte pre-trained models on our face-obfuscated data, which can be directly plugged into existing pipelines and systems that previously relied on ImageNet pre-training.  \n\nIn addition to the dataset contribution, we are the first to investigate the effects of privacy-aware face obfuscation on large-scale visual recognition. Our extensive experiments demonstrate that training on face-obfuscated images does not significantly compromise accuracy. Therefore, we advocate for face obfuscation to become a standard step in future dataset creation efforts. We will release our annotation interfaces and crowdsourcing scripts, enabling this effort. \n\n\n* Seong Joon Oh, Rodrigo Benenson, Mario Fritz, and Bernt Schiele. Faceless person recognition: Privacy implications in social media. In European Conference on Computer Vision, 2016.\n* Zhongzheng Ren, Yong Jae Lee, and Michael S Ryoo. Learning to anonymize faces for privacy preserving action detection. In European Conference on Computer Vision, 2018.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ]
      },
      {
        "id": "6NeaFqs9ZO8",
        "original": null,
        "number": 1,
        "cdate": 1642696836092,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696836092,
        "tmdate": 1642696836092,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "KVYq2Ea90PC",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "This paper received 3 quality reviews, with 2 rated 5 and 1 rated 6. While the reviewers recognize the various contributions and insights made by this work, it was also pointed out that this work lacks technical novelty. The authors agreed with this concerns and argued that this work provides a service to the community, citing imageNet and COCO papers. The AC agrees with the contribution and major concerns. Furthermore, the AC would like to point out that in term of the level of efforts, this work might not be on par with the imageNet and COCO. All things considered, the AC believes that this work is not ready for publication at its current form, and hence recommend rejection."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "6m-g0uRpct-",
        "original": null,
        "number": 1,
        "cdate": 1635689628171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635689628171,
        "tmdate": 1635689628171,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "KVYq2Ea90PC",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The main concern addressed in this paper is the privacy problem that may result from images in ImageNet databases containing unexpected faces. The authors propose a two-step face filtering method. First, the authors use a detector called Amazon Rekognition to detect the ImageNet database. Then, the authors further optimize the detector output through the crowdsourcing platform Amazon Mechanical Turk (AMT) to reduce false positives and false negatives in automated detection. For the detected faces, the authors took two approaches, distinguishing between blurring and overlaying, and tested their effectiveness on different models separately. The accuracy of the two approaches was reduced by 0.9% on average compared to the original database on the ILSVRC classification challenge. And using the database that has blurred or covered the faces still maintains the transferability of the original database in the tests of downstream tasks.\nContribution\uff1a\n1. The authors perform a very time-consuming and labor-intensive task for accurate labeling and filtering of faces in the ImageNet database and statistical analysis of the classes of faces contained in ImageNet.\n2. The authors demonstrate experiments related to classification tasks and pre-trained model training using a database containing blurred or covered faces, proving that the theory is feasible and that the dropped accuracy is acceptable.\n3. In terms of ethics, using blurred or covered face data for training can reduce privacy concerns. The study of the ImageNet database in terms of privacy can provide an important reference for subsequent databases",
          "main_review": "The main weakness:\n1. I was confused by the statement in section (Part III, second paragraph) as to whether the face annotation process was manually filtered only for faces that were successfully detected (generated prediction boxes5) by Amazon Rekognition. Although I can infer from Appendix A Stage1 that only the data with successful detector predictions should be put into the AMT platform. Perhaps it would be better to include a brief description of Amazon Rekognition in the text. \n2. And if some faces are not detected by Amazon Rekognition, how do you tackle this problem? This is an important problem for the privacy issue in this paper because this paper focuses on the privacy issue.\n3. The novelty of this paper is limited reference to the proposed method in the paper.\n\nThe main strengths:\n1. The experimental part of the paper is depicted in great detail and completely. Rigorous validation of the obfuscation approach is done on two different methods on 15 different models. And in the appendix, the detailed method of blurring, and the problems that may happen in the process of labeling are under clearer explanation.\n2. The ethics of machine learning has been widely debated, with the issue of face privacy being of particular concern. There are many similar discussions, for example, there are some papers proposing to remove images associated with people from the database. The feasibility of obfuscation processing of faces proposed in this paper is a good way to minimize privacy issues without reducing the number of databases at the same time.",
          "summary_of_the_review": "This paper has some contributions in exploring the ethicality of datasets, especially in the current very popular ImageNet database, but it exists some flaws (see weakness). The solutions and results in this paper are open sources and feasible, and this work will inspire subsequent exploration of privacy protection in publicly available datasets. ",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "Yes, Privacy, security and safety"
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Reviewer_U3FW"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Reviewer_U3FW"
        ]
      },
      {
        "id": "Y-UbKxlTgxo",
        "original": null,
        "number": 2,
        "cdate": 1635918799824,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635918799824,
        "tmdate": 1635918799824,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "KVYq2Ea90PC",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper mainly discuss the privacy issue for human for the widely used ImageNet dataset and how to handle them.\nIn addition, the paper does a very detailed empirical experiments to study the performance influence for various tasks, including object recognition, scene recognition, face attribute, and object detection if all the faces in the ImageNet are obfuscated.",
          "main_review": "The main strength of the paper is to address the privacy issues of ImageNet and is to provide an alternative face obfuscated version.\nIn addition, the authors conduct very thorough experiments across different tasks and different architecture for the study of the performance influence with the obfuscated dataset. It shows new dataset still is effective for transfer learning for various vision task with few performance drop. However, the weakness is that the main part of the paper is to examine the performance influence of different settings and is of limited technical novelty. For the verification of some downstream tasks, the coverage of tasks is not enough.",
          "summary_of_the_review": "Although the paper mainly focuses on providing plenty of empirical results to evaluate the influence of using the face obfuscated ImageNet dataset and is of limited novelty, it provides a lot of insights to show the effectiveness and feasibility of privacy preserving ImageNet.\n\nI have few concerns of the selection of transferring tasks. For example, the resolution of CIFAR-10 is only 32x32 and only for 10 classes. Similarly, Pascal VOC is also relatively small and easy dataset as compared with COCO or other recently released object detection dataset.\nMost of the images in the CelebA dataset are in frontal pose and have much fewer variations than other unconstrained face dataset, like IJB-C, etc. Since these datasets are relatively simpler than others which are more close to real-world scenarios, I wonder if the same experimental results and findings are still valid for harder datasets with more variations.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Reviewer_6ZXN"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Reviewer_6ZXN"
        ]
      },
      {
        "id": "muaJz2tXtFF",
        "original": null,
        "number": 3,
        "cdate": 1635986376213,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635986376213,
        "tmdate": 1635986567001,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "KVYq2Ea90PC",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper presents an empirical study on the effect of face obfuscation in the ImageNet dataset. The main conclusion is that face obfuscation does not decrease the utility of the dataset. Specifically, the authors showed that various networks trained on the obfuscated dataset only experienced small accuracy drop on the image classification task. The authors also discussed the impact on different categories, showing that face obfuscation hurt more to the object categories that are more closely related to faces (i.e., the bounding boxes of which overlap more with faces). Last but now least, experiments has been conducted to show that face obfuscation also does not have a significant impact on the transferability of the features learned from the new dataset. All these conclusions are inline with intuitions since ImageNet is not primarily focused on human activities / faces.",
          "main_review": "Strengths:\n* The main contribution of this work is that it provided empirical evidence on the effect of face obfuscation on the ImageNet dataset. Through comprehensive experiment, the authors showed that face obfuscation does not decrease the utility of the dataset.\n* Another contribution that should not be overlooked is that the authors annotated all faces in ImageNet in a semi-automatic manner and promised that they will make the annotations publicly available to other researchers.\n* The paper is very well written and includes a lot of details on the experiment protocol. Thus It should be straightforward for other researchers to reproduce the results and to extend the study.\n\nWeaknesses:\n* Since blur and cutout are commonly used data augmentation techniques, it is to be expected that face obfuscation would not have a big impact to visions tasks that have little to do with faces. Although it is commendable that this is now shown empirically though the study, this work also does not bring interesting new insights into the topic.\n* The authors showed that categories that are closely related to faces are indeed affected more by face obfuscation. This paper would be more interesting (from a technical point of view) if the authors could additionally investigate into methods for alleviating such impact.",
          "summary_of_the_review": "This paper is very well written and it provides empirical evidences to support the intuition that face obfuscation does not decrease the utility of the ImageNet dataset. However, my main concern is that the paper has no technical novelty and it also does not bring sufficiently new insights to the community.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Reviewer_7itG"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Reviewer_7itG"
        ]
      },
      {
        "id": "QMmULX5fYXv",
        "original": null,
        "number": 2,
        "cdate": 1637610027795,
        "mdate": 1637610027795,
        "ddate": null,
        "tcdate": 1637610027795,
        "tmdate": 1637610027795,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "KVYq2Ea90PC",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Official_Comment",
        "content": {
          "title": "Common Response",
          "comment": "We thank all reviewers for their thoughtful comments. They agree that our thorough empirical experiments (6ZXN,U3FW) provide a lot of insights (6ZXN) and that our paper is well-written (7itG). We are especially encouraged that reviewers think our work provides important reference for subsequent data collection practices (U3FW) and our face annotations is a valuable service to the community (7itG, U3FW)\n\nReviewers\u2019 concerns focus on technical novelty. They point out that our paper only analyzes existing methods/datasets without proposing any novel techniques. This is a point that we totally agree with. However, we respectfully but wholeheartedly disagree that a paper must have algorithmic contributions to be important and impactful. Many great papers published in machine learning and computer vision do not focus on algorithmic contributions. Instead, they focus on analyzing existing methods/datasets ([A], [B], [C]), or provide a service to the community (e.g., by constructing datasets [D] [E]). \n\n* [A] Torralba and Efros. \"Unbiased look at dataset bias.\" CVPR, 2011.\n* [B] Shankar et al. \"Evaluating machine accuracy on imagenet.\" ICML, 2020.\n* [C] Tsipras et al. \"From imagenet to image classification: Contextualizing progress on benchmarks.\" ICML, 2020.\n* [D] Deng et al. \"Imagenet: A large-scale hierarchical image database.\" CVPR, 2009.\n* [E] Lin et al. \"Microsoft coco: Common objects in context.\" ECCV, 2014.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper82/Authors"
        ]
      },
      {
        "id": "6NeaFqs9ZO8",
        "original": null,
        "number": 1,
        "cdate": 1642696836092,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696836092,
        "tmdate": 1642696836092,
        "tddate": null,
        "forum": "KVYq2Ea90PC",
        "replyto": "KVYq2Ea90PC",
        "invitation": "ICLR.cc/2022/Conference/Paper82/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "This paper received 3 quality reviews, with 2 rated 5 and 1 rated 6. While the reviewers recognize the various contributions and insights made by this work, it was also pointed out that this work lacks technical novelty. The authors agreed with this concerns and argued that this work provides a service to the community, citing imageNet and COCO papers. The AC agrees with the contribution and major concerns. Furthermore, the AC would like to point out that in term of the level of efforts, this work might not be on par with the imageNet and COCO. All things considered, the AC believes that this work is not ready for publication at its current form, and hence recommend rejection."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}