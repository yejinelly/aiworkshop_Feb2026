{
  "id": "IY4IsjvUhZ",
  "original": "VP30g0VB3Mg",
  "number": 144,
  "cdate": 1632875432020,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875432020,
  "tmdate": 1676330686684,
  "ddate": null,
  "content": {
    "title": "Characterising the Area Under the Curve Loss Function Landscape",
    "authorids": [
      "~Maximilian_Paul_Niroomand1",
      "~Conor_T_Cafolla1",
      "~John_William_Roger_Morgan1",
      "~David_John_Wales1"
    ],
    "authors": [
      "Maximilian Paul Niroomand",
      "Conor T Cafolla",
      "John William Roger Morgan",
      "David John Wales"
    ],
    "keywords": [
      "loss function landscape",
      "loss function",
      "AUC",
      "area under the curve",
      "alternative loss functions",
      "loss function visualisation"
    ],
    "abstract": " One of the most common metrics to evaluate neural network classifiers is the\narea under the receiver operating characteristic curve (AUC). However, \noptimisation of the AUC as the loss function during network\ntraining is not a standard procedure. Here we compare minimising the cross-entropy (CE) loss\nand optimising the AUC directly. In particular, we analyse the loss function\nlandscape (LFL) of approximate AUC (appAUC) loss functions to discover\nthe organisation of this solution space. We discuss various surrogates for AUC approximation and show their differences.\nWe find that the characteristics of the appAUC landscape are significantly\ndifferent from the CE landscape. The approximate AUC loss function improves\ntesting AUC, and the appAUC landscape has substantially more minima, but\nthese minima are less robust, with larger average Hessian eigenvalues. We provide a theoretical foundation to explain these results.\nTo generalise our results, we lastly provide an overview of how the\nLFL can help to guide loss function analysis and selection. ",
    "one-sentence_summary": "We analyse properties of the loss function landscape for surrogate AUC loss function and compare these with a standard cross entropy loss function. ",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "niroomand|characterising_the_area_under_the_curve_loss_function_landscape",
    "pdf": "/pdf/ac5d14f80a49bce4b93a4fcdfb1b449ced773b64.pdf",
    "_bibtex": "@misc{\nniroomand2022characterising,\ntitle={Characterising the Area Under the Curve Loss Function Landscape},\nauthor={Maximilian Paul Niroomand and Conor T Cafolla and John William Roger Morgan and David John Wales},\nyear={2022},\nurl={https://openreview.net/forum?id=IY4IsjvUhZ}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "IY4IsjvUhZ",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 6,
    "directReplyCount": 6,
    "revisions": true,
    "replies": [
      {
        "id": "ZzFFUojXjM1",
        "original": null,
        "number": 2,
        "cdate": 1635864985489,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635864985489,
        "tmdate": 1635864985489,
        "tddate": null,
        "forum": "IY4IsjvUhZ",
        "replyto": "IY4IsjvUhZ",
        "invitation": "ICLR.cc/2022/Conference/Paper144/-/Official_Review",
        "content": {
          "summary_of_the_paper": "Authors did empirical studies to visualize and investigate the landscape of AUC (area under curve) loss. AUC is cirtical for a wide range of applications, such as recommendation and online advertising. Thus it is important.",
          "main_review": "Pros\n1. authors compared the minimization of CE versus the direct minimization of AUC and its surrogates. \n2. authors find a way to visualize the comparisons, which is quite intuitive (Figure 2)\n3. authors correlate these losses to demosntrate the relevance between these losses. \n4. it is not difficult to understand the effectiveness of surrogates of AUC and the tightness of CE to approximate AUC\n\nCons\n1. the experiment settings are not well addressed. AUC is mostly used for recommendation and advertising. Using such datasets might be more convincing.\n2. Some Belkin's work show that there is not so many difference between least square and CE or hinger loss in over-parameterized learning regiem. I am wondering the whether the discussion on the AUC could be included as a special case of their works?\n\n[1] Classification vs regression in overparameterized regimes: Does the loss function matter?\n[2] Evaluation of Neural Architectures Trained with Square Loss vs Cross-Entropy in Classification",
          "summary_of_the_review": "It is a solid work, though the connections to [1][2] are not clear.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper144/Reviewer_ifdg"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper144/Reviewer_ifdg"
        ]
      },
      {
        "id": "xR-GLrB-ns",
        "original": null,
        "number": 3,
        "cdate": 1635907273925,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635907273925,
        "tmdate": 1635907273925,
        "tddate": null,
        "forum": "IY4IsjvUhZ",
        "replyto": "IY4IsjvUhZ",
        "invitation": "ICLR.cc/2022/Conference/Paper144/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The authors propose to minimize approximations of the AUC during training directly, rather than the usual training objectives which are then evaluated with AUC after training.\n",
          "main_review": "Strengths:\n-- The authors have an intriguing idea of analyzing prediction models through their loss landscapes with two different losses (a standard CE loss and a proposed loss that is an approximation of the AUC). This is an interesting idea for a regime to draw conclusions.\n-- The experimental setup in 3.4 is interesting, as it suggests that there are learned models with a high CE value but a good AUC that the appAUC-trained models can find. However, this experiment is not studied any further. It would be interesting to explore those models specifically to understand how they can have high CE but have a good AUC, in terms of the effect on actual points. Are these unstable, spiky solutions? Are they overfitting, such that slightly different test sets would yield significantly worse results? This hinted at interesting conclusions but left them under-explored.\nWeaknesses:\n-- This paper assumes the AUC is a perfect measurement of the ultimate goal of training these models. While acknowledging that the traditional CE loss is only an approximation of the testing metric AUC, it ignores that the testing metric AUC may only be an approximation of the actual goal in our prediction.\n-- The datasets considered are too limited. Not only is the main dataset synthetic, but there is only one synthetic dataset and one real dataset. The results are not convincing as a result of the limited experimental setting. The real credit card data is given merely a couple sentences total, meaning the paper is almost exclusively analyzing a single synthetic dataset. This raises many questions, including how it would perform on data of other modalities, as well as whether the results would even replicate on additional synthetic or simple real datasets. The single synthetic dataset is also never shown, leaving the reader unclear about how these two models differ in their learned classifications in any way other than in terms of their AUC.\n-- As briefly mentioned in the related work, this optimization is still very similar to standard prediction losses. It throws extra terms in such that an incorrect prediction contributes to making the loss go up in multiple terms, but it is still moving in the same direction. The added terms make it a more complicated, less stable formula. While studying the slightly different landscape is interesting, ultimately it is not practically useful as a loss function.\n-- The loss compared to is a simple CE loss, but the data trained on is unbalanced. While the appAUC accounts for the class imbalance, the CE does not. There are very basic balancing techniques that could be used (e.g. proportional sampling) that would be used in practice for the CE model, and I wonder if those would eliminate the minor improvement in test AUC obtained from the appAUC loss. This is just one example of the much bigger problem that the types of models analyzed are far too minimal, with a single CE model compared to the proposed model.\n-- Many of the figures are unclear. Despite briefly defining disconnectivity graphs, their implications are not explained. In fact, many conclusions are just summarily stated from informally looking at these graphs (e.g. \u201cthe CE landscape is much more funnelled\u201d). This statement is vague and qualitative and its implications are unclear.\n-- The correlation between the loss and the true AUC is not especially strong (section 3.3), which is concerning, considering this is the fundamental assumption made in designing the loss.\n",
          "summary_of_the_review": "The model is not convincingly valuable, either as one for study or for practical use. Almost exclusively using a single synthetic dataset with a brief paragraph on a small real dataset is insufficient to draw any conclusions.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper144/Reviewer_Q2LC"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper144/Reviewer_Q2LC"
        ]
      },
      {
        "id": "NLzUrGrByTO",
        "original": null,
        "number": 4,
        "cdate": 1635983775728,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635983775728,
        "tmdate": 1635983967045,
        "tddate": null,
        "forum": "IY4IsjvUhZ",
        "replyto": "IY4IsjvUhZ",
        "invitation": "ICLR.cc/2022/Conference/Paper144/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper studied the AUC loss by investigating its approximates and by comparing it with the cross-entropy loss. The landscapes of the optimization based on approximate AUC and based on the cross-entropy loss are also studied. Numerical experiments are conducted.  ",
          "main_review": "The paper is well presented and is easy to follow. My main concern about this paper is its contribution. Frankly speaking, from my view, the contribution made in this paper is quite limited, especially given that the approximate AUC is not proposed by the authors. It is true that the authors did observe something from numerical experiments. However, I'm afraid that these observations cannot be articulated as real contributions. In particular, the authors stated that they \"provide a theoretical foundation to explain\" the observations they made, which, in my view, is overclaimed. ",
          "summary_of_the_review": "See above. ",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper144/Reviewer_hg7P"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper144/Reviewer_hg7P"
        ]
      },
      {
        "id": "9MucHfLbfsA",
        "original": null,
        "number": 5,
        "cdate": 1636645871511,
        "mdate": 1636645871511,
        "ddate": null,
        "tcdate": 1636645871511,
        "tmdate": 1636645871511,
        "tddate": null,
        "forum": "IY4IsjvUhZ",
        "replyto": "IY4IsjvUhZ",
        "invitation": "ICLR.cc/2022/Conference/Paper144/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies AUC maximization in classification tasks, which aims to directly optimize the AUC metric instead of minimizing traditional classification losses such as the cross-entropy loss. The main contribution of this paper is an empirical study of an existing AUC maximization method termed approximate AUC. Specifically, this paper compares the landscape of the approximate AUC surrogate loss with that of the cross-entropy loss. The empirical results gives an explanation on why AUC maximization can achieve higher test AUC compared to the conventional method that directly minimizes the cross-entropy loss. ",
          "main_review": "Writing: In general, this paper is well-written and easy to follow.\n\nNovelty: The significance and novelty of this paper is rather limited for several reasons.\n- The approximate surrogate of AUC examined in this paper has actually been proposed in previous research. Given that this paper only gives some empirical validations on an existing method without many innovations in algorithm design or theoretical analysis, I am not sure this work is significant enough to match the ICLR\u2019s standard. \n- AppAUC was really an old metric. As far as I am concerned, there are some recently proposed surrogate functions for AUC maximization (see the Relevance part). These methods are not considered in this paper, which largely affects the significance of this work.\n\nRelevance: This paper missed several literatures on AUC maximization which are closely related with this work. \n- As commented above, besides appAUC, there are actually many other literatures investigating deep AUC maximization [1,2]. Some of them have also proposed new surrogate function of AUC. These surrogates should be discussed and compared with appAUC.\n- As a very relevant research field, online AUC maximization also aims to directly optimize the AUC metric through some computationally efficient surrogate functions for AUC [3]. I would strongly recommend the author to consider this field in the Related Work section.\n\n[1] Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification.\n[2] Stochastic auc maximization with deep neural networks.\n[3] Stochastic online AUC maximization.",
          "summary_of_the_review": "Overall, the novelty of this paper is rather limited and the relevance with previous work is not clear enough, hence I would vote for rejection. I would encourage the authors to consider more recent research on deep AUC maximization, and possibly elaborate a more systematic empirical study on the new surrogate AUC methods. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper144/Reviewer_qWyh"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper144/Reviewer_qWyh"
        ]
      },
      {
        "id": "3tByD9-5-tm",
        "original": null,
        "number": 6,
        "cdate": 1637148764639,
        "mdate": 1637148764639,
        "ddate": null,
        "tcdate": 1637148764639,
        "tmdate": 1637148764639,
        "tddate": null,
        "forum": "IY4IsjvUhZ",
        "replyto": "IY4IsjvUhZ",
        "invitation": "ICLR.cc/2022/Conference/Paper144/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper compares using the cross-entropy and approximated AUC loss functions in neural network training with AUC evaluation. The authors analyze loss function landscapes via solution space and minima properties.",
          "main_review": "Some approximated AUC loss functions show better performance than standard learning via cross-entropy. However, most cases are about empirical results.\nGetting theoretical evidence about the advantages of those loss functions is a promising idea.\n\nHowever, the evaluation part of the paper is not satisfied standard requirements. First, the paper only considers a simple architecture of a neural network, using one linear layer. It could be irrelevant because the existing models have a much more difficult architecture. Second, just two datasets (synthetic and real ones) are used. Further conclusions could be non-reliable.\n\nAlso, it is mentioned that hinge loss is not AUC-consistent, but it is used in the SOTA AUC-M loss that is top-1 in the CheXpert competition.",
          "summary_of_the_review": "The authors suggest a useful way to analyze loss function selection, but experiments are not persuasive.\nI recommend expanding the evaluation part in terms of the number of datasets and real deep learning models with a sufficient number of layers.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper144/Reviewer_yYns"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper144/Reviewer_yYns"
        ]
      },
      {
        "id": "KjAhxlicF6j",
        "original": null,
        "number": 1,
        "cdate": 1642696840216,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696840216,
        "tmdate": 1642696840216,
        "tddate": null,
        "forum": "IY4IsjvUhZ",
        "replyto": "IY4IsjvUhZ",
        "invitation": "ICLR.cc/2022/Conference/Paper144/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The paper analyses the loss landscape induced by AUC loss. Reviewers found critical issues with the paper, and the Authors have not provided feedback. As such I have to recommend rejecting the paper. I thank the Authors for submitting the paper to the ICLR conference. I hope the reviews will be helpful in improving the paper."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "ZzFFUojXjM1",
        "original": null,
        "number": 2,
        "cdate": 1635864985489,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635864985489,
        "tmdate": 1635864985489,
        "tddate": null,
        "forum": "IY4IsjvUhZ",
        "replyto": "IY4IsjvUhZ",
        "invitation": "ICLR.cc/2022/Conference/Paper144/-/Official_Review",
        "content": {
          "summary_of_the_paper": "Authors did empirical studies to visualize and investigate the landscape of AUC (area under curve) loss. AUC is cirtical for a wide range of applications, such as recommendation and online advertising. Thus it is important.",
          "main_review": "Pros\n1. authors compared the minimization of CE versus the direct minimization of AUC and its surrogates. \n2. authors find a way to visualize the comparisons, which is quite intuitive (Figure 2)\n3. authors correlate these losses to demosntrate the relevance between these losses. \n4. it is not difficult to understand the effectiveness of surrogates of AUC and the tightness of CE to approximate AUC\n\nCons\n1. the experiment settings are not well addressed. AUC is mostly used for recommendation and advertising. Using such datasets might be more convincing.\n2. Some Belkin's work show that there is not so many difference between least square and CE or hinger loss in over-parameterized learning regiem. I am wondering the whether the discussion on the AUC could be included as a special case of their works?\n\n[1] Classification vs regression in overparameterized regimes: Does the loss function matter?\n[2] Evaluation of Neural Architectures Trained with Square Loss vs Cross-Entropy in Classification",
          "summary_of_the_review": "It is a solid work, though the connections to [1][2] are not clear.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper144/Reviewer_ifdg"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper144/Reviewer_ifdg"
        ]
      },
      {
        "id": "xR-GLrB-ns",
        "original": null,
        "number": 3,
        "cdate": 1635907273925,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635907273925,
        "tmdate": 1635907273925,
        "tddate": null,
        "forum": "IY4IsjvUhZ",
        "replyto": "IY4IsjvUhZ",
        "invitation": "ICLR.cc/2022/Conference/Paper144/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The authors propose to minimize approximations of the AUC during training directly, rather than the usual training objectives which are then evaluated with AUC after training.\n",
          "main_review": "Strengths:\n-- The authors have an intriguing idea of analyzing prediction models through their loss landscapes with two different losses (a standard CE loss and a proposed loss that is an approximation of the AUC). This is an interesting idea for a regime to draw conclusions.\n-- The experimental setup in 3.4 is interesting, as it suggests that there are learned models with a high CE value but a good AUC that the appAUC-trained models can find. However, this experiment is not studied any further. It would be interesting to explore those models specifically to understand how they can have high CE but have a good AUC, in terms of the effect on actual points. Are these unstable, spiky solutions? Are they overfitting, such that slightly different test sets would yield significantly worse results? This hinted at interesting conclusions but left them under-explored.\nWeaknesses:\n-- This paper assumes the AUC is a perfect measurement of the ultimate goal of training these models. While acknowledging that the traditional CE loss is only an approximation of the testing metric AUC, it ignores that the testing metric AUC may only be an approximation of the actual goal in our prediction.\n-- The datasets considered are too limited. Not only is the main dataset synthetic, but there is only one synthetic dataset and one real dataset. The results are not convincing as a result of the limited experimental setting. The real credit card data is given merely a couple sentences total, meaning the paper is almost exclusively analyzing a single synthetic dataset. This raises many questions, including how it would perform on data of other modalities, as well as whether the results would even replicate on additional synthetic or simple real datasets. The single synthetic dataset is also never shown, leaving the reader unclear about how these two models differ in their learned classifications in any way other than in terms of their AUC.\n-- As briefly mentioned in the related work, this optimization is still very similar to standard prediction losses. It throws extra terms in such that an incorrect prediction contributes to making the loss go up in multiple terms, but it is still moving in the same direction. The added terms make it a more complicated, less stable formula. While studying the slightly different landscape is interesting, ultimately it is not practically useful as a loss function.\n-- The loss compared to is a simple CE loss, but the data trained on is unbalanced. While the appAUC accounts for the class imbalance, the CE does not. There are very basic balancing techniques that could be used (e.g. proportional sampling) that would be used in practice for the CE model, and I wonder if those would eliminate the minor improvement in test AUC obtained from the appAUC loss. This is just one example of the much bigger problem that the types of models analyzed are far too minimal, with a single CE model compared to the proposed model.\n-- Many of the figures are unclear. Despite briefly defining disconnectivity graphs, their implications are not explained. In fact, many conclusions are just summarily stated from informally looking at these graphs (e.g. \u201cthe CE landscape is much more funnelled\u201d). This statement is vague and qualitative and its implications are unclear.\n-- The correlation between the loss and the true AUC is not especially strong (section 3.3), which is concerning, considering this is the fundamental assumption made in designing the loss.\n",
          "summary_of_the_review": "The model is not convincingly valuable, either as one for study or for practical use. Almost exclusively using a single synthetic dataset with a brief paragraph on a small real dataset is insufficient to draw any conclusions.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper144/Reviewer_Q2LC"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper144/Reviewer_Q2LC"
        ]
      },
      {
        "id": "NLzUrGrByTO",
        "original": null,
        "number": 4,
        "cdate": 1635983775728,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635983775728,
        "tmdate": 1635983967045,
        "tddate": null,
        "forum": "IY4IsjvUhZ",
        "replyto": "IY4IsjvUhZ",
        "invitation": "ICLR.cc/2022/Conference/Paper144/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper studied the AUC loss by investigating its approximates and by comparing it with the cross-entropy loss. The landscapes of the optimization based on approximate AUC and based on the cross-entropy loss are also studied. Numerical experiments are conducted.  ",
          "main_review": "The paper is well presented and is easy to follow. My main concern about this paper is its contribution. Frankly speaking, from my view, the contribution made in this paper is quite limited, especially given that the approximate AUC is not proposed by the authors. It is true that the authors did observe something from numerical experiments. However, I'm afraid that these observations cannot be articulated as real contributions. In particular, the authors stated that they \"provide a theoretical foundation to explain\" the observations they made, which, in my view, is overclaimed. ",
          "summary_of_the_review": "See above. ",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper144/Reviewer_hg7P"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper144/Reviewer_hg7P"
        ]
      },
      {
        "id": "9MucHfLbfsA",
        "original": null,
        "number": 5,
        "cdate": 1636645871511,
        "mdate": 1636645871511,
        "ddate": null,
        "tcdate": 1636645871511,
        "tmdate": 1636645871511,
        "tddate": null,
        "forum": "IY4IsjvUhZ",
        "replyto": "IY4IsjvUhZ",
        "invitation": "ICLR.cc/2022/Conference/Paper144/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies AUC maximization in classification tasks, which aims to directly optimize the AUC metric instead of minimizing traditional classification losses such as the cross-entropy loss. The main contribution of this paper is an empirical study of an existing AUC maximization method termed approximate AUC. Specifically, this paper compares the landscape of the approximate AUC surrogate loss with that of the cross-entropy loss. The empirical results gives an explanation on why AUC maximization can achieve higher test AUC compared to the conventional method that directly minimizes the cross-entropy loss. ",
          "main_review": "Writing: In general, this paper is well-written and easy to follow.\n\nNovelty: The significance and novelty of this paper is rather limited for several reasons.\n- The approximate surrogate of AUC examined in this paper has actually been proposed in previous research. Given that this paper only gives some empirical validations on an existing method without many innovations in algorithm design or theoretical analysis, I am not sure this work is significant enough to match the ICLR\u2019s standard. \n- AppAUC was really an old metric. As far as I am concerned, there are some recently proposed surrogate functions for AUC maximization (see the Relevance part). These methods are not considered in this paper, which largely affects the significance of this work.\n\nRelevance: This paper missed several literatures on AUC maximization which are closely related with this work. \n- As commented above, besides appAUC, there are actually many other literatures investigating deep AUC maximization [1,2]. Some of them have also proposed new surrogate function of AUC. These surrogates should be discussed and compared with appAUC.\n- As a very relevant research field, online AUC maximization also aims to directly optimize the AUC metric through some computationally efficient surrogate functions for AUC [3]. I would strongly recommend the author to consider this field in the Related Work section.\n\n[1] Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification.\n[2] Stochastic auc maximization with deep neural networks.\n[3] Stochastic online AUC maximization.",
          "summary_of_the_review": "Overall, the novelty of this paper is rather limited and the relevance with previous work is not clear enough, hence I would vote for rejection. I would encourage the authors to consider more recent research on deep AUC maximization, and possibly elaborate a more systematic empirical study on the new surrogate AUC methods. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper144/Reviewer_qWyh"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper144/Reviewer_qWyh"
        ]
      },
      {
        "id": "3tByD9-5-tm",
        "original": null,
        "number": 6,
        "cdate": 1637148764639,
        "mdate": 1637148764639,
        "ddate": null,
        "tcdate": 1637148764639,
        "tmdate": 1637148764639,
        "tddate": null,
        "forum": "IY4IsjvUhZ",
        "replyto": "IY4IsjvUhZ",
        "invitation": "ICLR.cc/2022/Conference/Paper144/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper compares using the cross-entropy and approximated AUC loss functions in neural network training with AUC evaluation. The authors analyze loss function landscapes via solution space and minima properties.",
          "main_review": "Some approximated AUC loss functions show better performance than standard learning via cross-entropy. However, most cases are about empirical results.\nGetting theoretical evidence about the advantages of those loss functions is a promising idea.\n\nHowever, the evaluation part of the paper is not satisfied standard requirements. First, the paper only considers a simple architecture of a neural network, using one linear layer. It could be irrelevant because the existing models have a much more difficult architecture. Second, just two datasets (synthetic and real ones) are used. Further conclusions could be non-reliable.\n\nAlso, it is mentioned that hinge loss is not AUC-consistent, but it is used in the SOTA AUC-M loss that is top-1 in the CheXpert competition.",
          "summary_of_the_review": "The authors suggest a useful way to analyze loss function selection, but experiments are not persuasive.\nI recommend expanding the evaluation part in terms of the number of datasets and real deep learning models with a sufficient number of layers.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper144/Reviewer_yYns"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper144/Reviewer_yYns"
        ]
      },
      {
        "id": "KjAhxlicF6j",
        "original": null,
        "number": 1,
        "cdate": 1642696840216,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696840216,
        "tmdate": 1642696840216,
        "tddate": null,
        "forum": "IY4IsjvUhZ",
        "replyto": "IY4IsjvUhZ",
        "invitation": "ICLR.cc/2022/Conference/Paper144/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The paper analyses the loss landscape induced by AUC loss. Reviewers found critical issues with the paper, and the Authors have not provided feedback. As such I have to recommend rejecting the paper. I thank the Authors for submitting the paper to the ICLR conference. I hope the reviews will be helpful in improving the paper."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}