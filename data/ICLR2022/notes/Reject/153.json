{
  "id": "9FfAEgUYGON",
  "original": "AAVoLbggAuc",
  "number": 153,
  "cdate": 1632875432708,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875432708,
  "tmdate": 1697934960964,
  "ddate": null,
  "content": {
    "title": "Mismatched No More: Joint Model-Policy Optimization for Model-Based RL",
    "authorids": [
      "~Benjamin_Eysenbach1",
      "~Alexander_Khazatsky1",
      "~Sergey_Levine1",
      "~Ruslan_Salakhutdinov1"
    ],
    "authors": [
      "Benjamin Eysenbach",
      "Alexander Khazatsky",
      "Sergey Levine",
      "Ruslan Salakhutdinov"
    ],
    "keywords": [
      "reinforcement learning",
      "model-based RL",
      "joint optimization"
    ],
    "abstract": "Many model-based reinforcement learning (RL) methods follow a similar template: fit a model to previously observed data, and then use data from that model for RL or planning. However, models that achieve better training performance (e.g., lower MSE) are not necessarily better for control: an RL agent may seek out the small fraction of states where an accurate model makes mistakes, or it might act in ways that do not expose the errors of an inaccurate model. As noted in prior work, there is an objective mismatch: models are useful if they yield good policies, but they are trained to maximize their accuracy, rather than the performance of the policies that result from them.  In this work we propose a single objective for jointly training the model and the policy, such that updates to either component increases a lower bound on expected return. This joint optimization mends the objective mismatch in prior work. Our objective is a global lower bound on expected return, and this bound becomes tight under certain assumptions. The resulting algorithm (MnM) is conceptually similar to a GAN: a classifier distinguishes between real and fake transitions, the model is updated to produce transitions that look realistic, and the policy is updated to avoid states where the model predictions are unrealistic.",
    "one-sentence_summary": "An algorithm for model-based RL where the model and policy optimize the same objective, which is a (global) lower bound on expected rewards.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "eysenbach|mismatched_no_more_joint_modelpolicy_optimization_for_modelbased_rl",
    "pdf": "/pdf/7ecfc3ab671633cb60ebfe2501e966bf1e2d66a0.pdf",
    "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.02758/code)",
    "_bibtex": "@misc{\neysenbach2022mismatched,\ntitle={Mismatched No More: Joint Model-Policy Optimization for Model-Based {RL}},\nauthor={Benjamin Eysenbach and Alexander Khazatsky and Sergey Levine and Ruslan Salakhutdinov},\nyear={2022},\nurl={https://openreview.net/forum?id=9FfAEgUYGON}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "9FfAEgUYGON",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 22,
    "directReplyCount": 5,
    "revisions": true,
    "replies": [
      {
        "id": "Ar24yIo1Di7",
        "original": null,
        "number": 1,
        "cdate": 1635861302877,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635861302877,
        "tmdate": 1636076760130,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "9FfAEgUYGON",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a new model-based RL method by devising a new reward function, which incorporates a term measuring the difference between the learned dynamics model and the true dynamics probability, in addition to the logarithm of the original environment reward. Maximizing this augmented reward function is proved to maximizing a lower bound of the logarithm of the original expected return. The reward function is not practically usable since the true dynamics probability is not known. To practically apply the proposed reward function, a GAN-like classifier is introduced to differentiate the transition generated from the dynamics model and the true dynamics. The experiments show that under an unified objective, the learned dynamics tends to assign higher transition probability for some (s,a,s\u2019) if this transition can assist the policy to achieve a higher return in the true environments.",
          "main_review": "The method is well motivated, and the proposed augmented reward function is novel. The experiments are also well conducted, and the visualization of the learned dynamics provides intuitive illustrations that it tends to help the policy to succeed. The overall performance of MnM is also demonstrated to perform at least as well compared with SAC and MBPO.\n\nDespite these advantages mentioned above, I have some comments below that I\u2019d like to hear from the authors in the responses.\n\n1. The augmented reward in Eq. (3) is informative, while in Theorem 3.1, the equality of the lower bound is hard to hold. In the proof of Theorem 3.1, Jensen\u2019s inequality is applied, twice, to move the logarithm inside the expectation. For Jensen\u2019s inequality, the equality holds when the function (logarithm here) is affine or the variable inside the function (the reward r here) is constant. Obviously, neither is the case here. Therefore, at least for the reward function in Eq. (3), maximizing over its cumulative expectation does not guarantee the optimality in the original expected return.\n\n2. Although, immediately, the following contexts about \u201cTightening the lower bound\u201d gives a new reward function by employing a learnable $\\gamma_{\\theta}$, I cannot agree that this is really significant. From the proof of Lemma 3.2, the equality holds when the two analytical solutions are satisfied for $\\gamma_{\\theta}$ and the dynamics model, and the two analytical solutions are independent with each other. Then, for the learnable $\\gamma_{\\theta}$, it indeed plays a role of reward shaping that shapes the reward in Eq. (3) probably nonlinearly to reach the logarithm of the original expected return (probably a way to approximate the affine function to remove the Jensen\u2019s gap?). Also, in the experiments, this new lower bound and reward function are not applied. So, I agree Lemma 3.2 demonstrates that there exists some way to modify the reward in Eq. (3) to have a tighter lower bound, while I think this is a benefit from reward shaping, instead of a thoroughly informative new reward function.\n\nMinor:\nIn the proof of Lemma 3.1, line (d) in the equation, the reduced $\\pi_{\\theta}(a_t|s_t)$ missed the logarithm.\n\nOverall, I think the proposed method is novel and the experiments give nice explanations for the learned dynamics in tending to assist the policy learning, and MnM indeed has its superiority compared to prior model-based RL methods.\n",
          "summary_of_the_review": "A good paper that introduces a novel model-based RL method, where the policy learning and dynamics modeling share a unified objective.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Reviewer_yCzC"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Reviewer_yCzC"
        ]
      },
      {
        "id": "SVNmfI1ADor",
        "original": null,
        "number": 2,
        "cdate": 1635863882777,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635863882777,
        "tmdate": 1635863917932,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "9FfAEgUYGON",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper concerns the objective mismatch problem in model-based RL that the model is optimized for prediction accuracy, not a good performance. The authors propose Mismatched no More (MnM) that maximizes a lower bound on expected reward in an adversarial training manner. They also give a global lower bound that holds for any dynamics. Experiments also validate the effectiveness of MnM.",
          "main_review": "Objective mismatch problem is important in MBRL. And the proposed MnM is motivated by the idea of GAN to solve this problem. The whole paper is easy to follow and I also appreciate the theorems in the paper. However, I would like to see more comparisons, both theoretical and experimental, that if MnM performs better than the VAML framework [1,2] and the simple weighting strategy in [3]. Both these works are addressing the same problem, so it would be important to directly compare them, especially considering that MnM is a more complicated algorithm.\n\n[1] Value-Aware Loss Function for Model-based Reinforcement Learning. Amir-massoud Farahmand et al. AISTATS 2017\\\n[2] Iterative Value-Aware Model Learning. Amir-massoud Farahmand et al. NeurIPS 2018 \\\n[3] Objective Mismatch in Model-based Reinforcement Learning. Nathan Lambert et al. L4DC 2020",
          "summary_of_the_review": "The overall paper is motivated and clear, The concern I have is the advantage over previous works that address exactly the same problem with relatively simple mechanisms.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Reviewer_uKVv"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Reviewer_uKVv"
        ]
      },
      {
        "id": "AA_eC9RLdjW",
        "original": null,
        "number": 3,
        "cdate": 1635875622548,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635875622548,
        "tmdate": 1635894965665,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "9FfAEgUYGON",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper studies the model learning aspect in model-based reinforcement learning (MBRL). Standard Dyna-like MBRL approaches that train the model using the maximum likelihood estimation (MLE) or its variations. In contrast, the authors propose an algorithm called Mismatched no More (MnM) that optimizes model parameters using a lower bound on the agent\u2019s performance. By doing so, the paper mitigates the known objective mismatch of standard MBRL: MLE-optimized model might not necessarily be useful for optimizing agent\u2019s returns, the overall objective of an RL system. Experimentally, the paper presents two studies. First, in 2 tabular environments, the authors compare MnM to Q-learning and VMBPO, an alternative MBRL algorithm that addresses the objective mismatch. Second, the authors compare MnM to an MLE-like MBRL baseline and a model-free algorithm on 12 continuous control tasks.\n",
          "main_review": "Novelty\n\nTo the best of the reviewer's knowledge, the idea is novel. The authors propose a lower bound on the logarithm of expected returns that the policy and the model can jointly optimize against an adversary that tries to distinguish between real and model rollouts. As a result, the MnM model mitigates the objective mismatch yet produces realistic samples. This contrasts the approach with other non-MLE methods (e.g. see [1]) that address the objective mismatch but produce unrealistic rollouts. The authors might want to emphasize the property more in the paper and study the effect in more detail.\n\nSignificance\n\nThe significance of the paper is limited. The main concern of the reviewer is about the empirical results: \n- First, most experiments appear to have a very limited number of runs. The reviewer did not find the details in the paper explicitly articulating the number of random seeds. The reviewer might infer from the plots that MnM and the baselines were run 2-3 times (e.g. Figure 5, `meta-world-door-open-v2`). Given the high variance of the results, it is hard to make conclusions about the comparison of MnM to the baselines. Reporting the results using 10 random seeds might improve the credibility of the results. See [2] for a discussion about the significance of the experimentation in RL.\n- Second, a few important baselines are missing. In tabular experiments, there is no comparison to an MLE MBRL agent. In continuous control, there is no comparison to any MBRL algorithm that addresses the objective mismatch. Why not use VMBPO in these experiments if you already used it in tabular experiments? Furthermore, Appendix A.4 suggests that the optimal dynamics for the lower bound are achieved by weighting the true dynamics with returns. Why not try return-weighted MLE as a baseline? Including other baselines that address the objective mismatch would further increase the significance of the results (see notes about related work).\n- Lastly, while MnM mitigates the objective mismatch, it is unclear from the results what are the benefits of this. Figure 5 suggests that out of 12 continuous control environments, only on `DClawScrewRandom-v0` we can make a conclusion that MnM significantly outperforms the presented baselines. In 3 MuJoCo environments, the performance of MnM and MBPO does not differ significantly. In the other 8 environments, it is hard to make definite conclusions due to the variance of returns.\n\nThe authors provide theoretical analyses associated with MnM. There are several concerns about this part of the paper as well:\n- The proposed method optimizes a bound $L(\\theta)$ on the log expected returns. The authors claim that \u201cthis bound becomes tight under certain assumptions\u201d. To support the claim, the paper introduces another bound $L_\\gamma(\\theta)$ and proves that the supremum of $L_\\gamma(\\theta)$ coincides with the log expected returns. While the reviewer appreciates the similarity of the $L_\\gamma(\\theta)$ with $L(\\theta)$, the new bound relies on (a) non-markovian dynamics (b) **time-varying** and **parameterized** discount factor, limiting the justification of the method by this theoretical result.\n- With the above in mind, it is possible to argue that even a standard MLE-based MBRL algorithm optimizes a lower bound on the expected returns (yet this bound will not be tight) using the Simulation Lemma [3]. See section 2 in [4] for an explanation. Since the bound that the practical algorithm optimizes is not tight, it is unclear to which extent the proposed method mitigates the objective mismatch.\n- Lastly, the analyses make several assumptions that might not be entirely justified. The most noticeable one is the assumption about the positive reward. While it is common to assume that the rewards are *non-negative*, the assumption about positive rewards might be limiting. Moreover, the rewards in certain environments (e.g. `HalfCheetah-v2`) where MnM is tested take negative values.\n\nDetailed notes and questions\n\nThere are several smaller concerns and questions that the authors might address:\n1. The literature review could be improved. In particular, relevant references include [1] that characterize the set of models that are optimal for planning; [5] that optimize the policy and the model using the same objective; [6] that learn the model to directly optimize agent\u2019s performance, not a lower bound on it; [7] that learn the model that is helpful for policy improvement using a weighting scheme.\n2. Since $\\Gamma_\\theta(t)$ is a CDF, does it imply that $\\gamma(t)$ is a distribution?\n3. \u201cOne detail of note is that we omit the reward augmentation for MnM during these experiments, as it hinders exploration leading to lower returns.\u201d Is it the entropy augmentation of SAC or the proposed augmentation with the log density ratio?\n4. Is the reward function learned as well?\n\nReferences\n\n[1] Grimm, Christopher, Andr\u00e9 Barreto, Satinder Singh, and David Silver. \"The value equivalence principle for model-based reinforcement learning.\" arXiv preprint arXiv:2011.03506 (2020).\n\n[2] Henderson, Peter, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. \"Deep reinforcement learning that matters.\" In Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1. 2018.\n\n[3] Kearns, Michael, and Satinder Singh. \"Near-optimal reinforcement learning in polynomial time.\" Machine learning 49, no. 2 (2002): 209-232.\n\n[4] Farahmand, Amir-massoud, Andre Barreto, and Daniel Nikovski. \"Value-aware loss function for model-based reinforcement learning.\" In Artificial Intelligence and Statistics, pp. 1486-1494. PMLR, 2017.\n\n[5] Schrittwieser, Julian, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez et al. \"Mastering atari, go, chess and shogi by planning with a learned model.\" Nature 588, no. 7839 (2020): 604-609.\n\n[6] Nikishin, Evgenii, Romina Abachi, Rishabh Agarwal, and Pierre-Luc Bacon. \"Control-Oriented Model-Based Reinforcement Learning with Implicit Differentiation.\" arXiv preprint arXiv:2106.03273 (2021).\n\n[7] D'Oro, Pierluca, Alberto Maria Metelli, Andrea Tirinzoni, Matteo Papini, and Marcello Restelli. \"Gradient-aware model-based policy search.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, pp. 3801-3808. 2020.\n\n",
          "summary_of_the_review": "The reviewer recommends rejecting the paper. While the idea is promising, the submission in its current state needs substantial improvements. Addressing the outlined concerns might increase the overall score.\n",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Reviewer_D2vZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Reviewer_D2vZ"
        ]
      },
      {
        "id": "AYzNlGj_7zQ",
        "original": null,
        "number": 4,
        "cdate": 1636323255985,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636323255985,
        "tmdate": 1636323255985,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "9FfAEgUYGON",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies the the problem of objective mismatch problem in model-based reinforcement learning, which states that the goal of the model is different from the goal of policy optimization. To solve this issue, this paper proposes a single objective to train both the policy and the dynamics model, by deriving a lower bound of the (log of) expected return and optimizing the lower bound jointly. Based on the objective, the paper proposes the Mismatched no More (MnM) algorithm, which use a GAN-style objective to train the model. Experiments show that the proposed algorithm can match the performance of baseline algorithm (MBPO) when exploration is not a big issue, while outperforms it significantly for sparse-reward tasks. ",
          "main_review": "Writing: The paper is written clearly, although it has a few typos. \n\n1. The link to the proof of Lemma 3.2 is wrong. \n2. Eq (8): What is $\\tilde Q_\\phi$? Is it $Q_\\psi$\u200b?\n3. Appendix A.3, (d), the first cancelled term should be $\\log \\pi_\\theta(a_t | s_t)$\u200b. \n4. Appendix A.1, \"Rather, it corresponds to maximizing a sum of the expected return **and** the *variance* of the return\"\n\nNovelty: The proposed method is similar to a prior work,  VMBPO. Both share similar components, with similar method of learning. However, there are a few difference which separates them apart. One of the most important differences is that MnM takes the logarithm of rewards, and derives a lower bound of the log of expected return, while VMBPO derives a variational lower bound of an upper bound of the log of the expected return. Given VMBPO as a prior work, The theoretical result is not so surprising. If the technique of logarithm of reward can be well justified, I think the empirical novelty is good. \n\nOther comments: \n\n> Following prior work (Janner et al., 2019), we learn an additional model to predict the true environment.\nWhat does it mean by \"additional model\"? \n\nIs the dynamics model learned well? From my personel research experience, learning the model using a GAN-like objective leads to an inaccurate model and sometimes produces unrealistic states. The signals from the discriminator are much weaker than the signals from supervised learning. What if we train $C$\u200b and $\\pi$\u200b with MnM, but train maximum likelihood model $q$\u200b\u200b? \n\nIn Figure 6: What does it mean by \"For fair comparison, we use Q values corresponding to just the task reward\"? MBPO uses $r$ while MnM uses $\\log r$, so they indeed differ a lot. \n\nAs the new reward function $\\tilde r$ takes the logarithm of old reward function $r$, it seems that (a) it's tricky to apply logarithm to non-positive numbers; (b) The new reward function $\\hat r$  is not invariant to the scale of the old reward function $r$, i.e., shifting $r$ can lead to a very different $\\tilde r$. Do all these $\\tilde r$\u200b work in practice? \n\nTheoretically, there is indeed a difference whether we transform the reward. But I'm not very convinced that this technique is essential in practice. The authors constructs a 3-state MDP in Figure 3, but some details are missing, e.g., how is $\\eta$\u200b\u200b\u200b\u200b in VMBPO chosen? I agree that VMBPO optimizes an upper bound approximately, but it can somehow control overestimation by adjusting $\\eta$\u200b, while I don't see how MnM controls underestimation. Moreover, what happens if we don't apply this technique in practice? I expected to see the ablation in a more complex environment. \n\n",
          "summary_of_the_review": "This paper is written well and the idea sounds very interesting and promising. However, as stated in the review, I'm not fully concerned by the log reward technique. Overall, I think there is more merit in this paper than the flaw so I recommend weak acceptance. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Reviewer_7QkN"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Reviewer_7QkN"
        ]
      },
      {
        "id": "GjaNBtZVjXj",
        "original": null,
        "number": 2,
        "cdate": 1637259031243,
        "mdate": 1637259031243,
        "ddate": null,
        "tcdate": 1637259031243,
        "tmdate": 1637259031243,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "AYzNlGj_7zQ",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to 7QkN",
          "comment": "**[response 1 of 2]**\nWe thank the reviewer for their detailed review. The reviewer's main concerns seem to be a number of minor experimental and writing questions. We have addressed or answered all the questions below. **Do these revisions to the paper, together with the discussion below, fully address the reviewer's concerns of the paper?**\n\n> Given VMBPO as a prior work, the theoretical result is not so surprising.\n\nWe believe that the theoretical results of our paper are substantially different from VMBPO. While both papers propose solutions to the objective mismatch problem, only ours maximizes a lower bound on the expected return. \n\nMaximizing a lower bound (as done by our method) is usually preferable to maximizing an upper bound. A lower bound gives users confidence that an agent's performance in the real world will be at least as good as the performance under the learned model. In contrast, an upper bound tells the user that the agent will perform worse in the real world than under the learned model, without indicating how much worse it might perform. Maximizing an upper bound could lead to the true return not changing at all, but its (erroneous) estimate increasing arbitrarily. VMBPO does not provide any theoretical guarantees on how much overestimation may occur (for finite $\\eta$).\n\nThe theoretical results of our paper should also be evaluated in light of prior work that propose lower bounds for model-based RL. We have added a new paragraph to highlight this comparison (paragraph 3 in Section 2). To the best of our knowledge, our paper is the first to propose an objective for model based RL that has two important theoretical properties\n1. The objective provably a global lower bound on expected return that can be computed in MDPs with continuous states and actions\n2. The objective jointly optimizes the model and policy using the same objective.\n\nPerhaps the most similar prior work [1] also proposes an objective that is a lower bound, but it doesn't satisfy either of these properties: the bound in [1] only holds locally, and it involves optimizing the model and policy using different objectives. This prior work even suggests that deriving a global lower bound (as done by our method) may be \"impossible.\"\n\n> Is the dynamics model learned well?\n\nThe MnM dynamics model does train stably and its MSE decreases throughout training. As expected, the MSE of the MnM dynamics model is larger than the MSE of a maximum likelihood model. So, while the MnM dynamics model may not be as accurate as the maximum likelihood model, it may be more useful, as it sometimes results in higher return policies. We have revised the paper to add a figure (Figure 10) showing the MSE of the MnM dynamics model throughout training.\n\n> I don't see how MnM controls underestimation\n\nYou are right that the MnM objective does not have a single parameter to control underestimation. We have added a sentence to Section 6 to emphasize this limitation. \n\nThe more complex lower bound (Eq. 4) can become tight at optimality, meaning that the underestimation can be driven to zero if we have sufficiently expressive function approximators. The simpler lower bound (Eq. 2) is not necessarily tight at optimality. While we currently don't have a bound on how large this underestimation can be, our empirical experiments suggest that optimizing even this simpler lower bound results in optimal or near-optimal policies.\n\nOne way to measure the degree of underestimation would be to compare our lower bound to an upper bound on expected return, such as the objective proposed in VMBPO. If this gap is small, then both objectives are close to the true expected return. However, if the gap is large, it is unclear whether one or both of the objectives are far from the true expected return."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "myFPS0LQ7BO",
        "original": null,
        "number": 3,
        "cdate": 1637259067193,
        "mdate": 1637259067193,
        "ddate": null,
        "tcdate": 1637259067193,
        "tmdate": 1637259067193,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "GjaNBtZVjXj",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to 7QkN ",
          "comment": "**[response 2 or 2]**\n### Reward Transformation\n\n> As the new reward function $\\tilde{r}$ takes the logarithm of old reward function $r$, it seems that (a) it\u2019s tricky to apply logarithm to non-positive numbers; (b) The new reward function $\\hat{r}$ is not invariant to the scale of the old reward function $r$, i.e., shifting $r$ can lead to a very different $\\tilde{r}$. \n\n(a) Yes, we assume that the task rewards are positive (see first paragraph of Section 3). (b) Scaling the original reward function by a constant $c$ results in adding a constant $\\log c$ to the modified reward function. \n\n> Do all these $\\tilde{r}$ work in practice?\n\nNo, they do not all work in practice. As noted in Section 5.2, our continuous control experiments omit the log transformation because we found it to hurt performance on these tasks. While our objective then becomes similar to VMBPO, we find that in practice our method works quite a bit better (see the new comparison with VMBPO in Fig. 5), likely because of differences in how the model is trained (VMBPO uses a 2-step EM procedure).\n\n\n> Moreover, what happens if we don't apply [the logarithmic transformation] in practice? I expected to see the ablation in a more complex environment.\n\nAs noted in Section 5.2 (first paragraph), the continuous control experiments already don't apply this logarithmic transformation; we found that it hurt exploration on these domains. This result raises an interesting direction for future work: how can we design model-based RL algorithms for complex domains that explore efficiently while still maximizing a provable lower bound on expected return?\n\n### Writing clarifications\n\n> \u201cFor fair comparison, we use Q values corresponding to just the task reward, omitting the augmented reward used by MnM.\u201d. MBPO uses $r$ while MnM uses $\\log\u2061 r$, so they indeed differ a lot.\n\nFor this plot, we show the Q values corresponding to the task reward, without any logarithmic transformation. We have revised the text to clarify this.\n\n> [VMBPO] can somehow control overestimation by adjusting $\\eta$\n\nWe note that VMBPO maximizes an upper bound for any finite choice of $\\eta$. \n\n> Following prior work (Janner et al., 2019), we learn an additional model to predict the true environment. What does it mean by \"additional model\"?\n\nWe mean that we train another neural network (i.e., different from the dynamics model and the classifier) that takes as input the state and action and outputs the predicted reward. We have revised this sentence to clarify this point.\n\n> how is \u03b7\u200b\u200b\u200b\u200b in VMBPO chosen?\n\nWe use $\\eta = 1$. We have added this detail to Appendix C.1.\n\n> The link to the proof of Lemma 3.2 is wrong.\n\n> Eq (8): What is $Q_\\phi$? Is it $Q_\\psi$\u200b?\n\n> Appendix A.3, (d), the first cancelled term should be $\\log \\pi_\\theta(a_t \\mid s_t$\u200b.\n\n> Appendix A.1, \"Rather, it corresponds to maximizing a sum of the expected return and the variance of the return\"\n\nWe have fixed these typos. Thanks for bringing them to our attention.\n\n[1] Luo, Yuping, et al. \"Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees.\" arXiv preprint arXiv:1807.03858 (2018)."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "wwCC3ItMgPD",
        "original": null,
        "number": 4,
        "cdate": 1637259641652,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637259641652,
        "tmdate": 1637259733005,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "AA_eC9RLdjW",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to D2vZ",
          "comment": "**[response 1 of 3]**\n\nWe thank the reviewer for their detailed review. The reviewer raised questions about both the empirical and theoretical aspects of the paper. We have added three additional baselines to our experiments and run an additional experiment increasing random seeds. We have also revised the paper to clarify our theoretical results, clarifications that we explain below. **Have the revisions to the paper addressed all the reviewer's concerns?**\n\n> This contrasts the approach with other non-MLE methods that address the objective mismatch but produce unrealistic rollouts. The authors might want to emphasize the property more in the paper and study the effect in more detail.\n\nThank you for the suggestion. To emphasize these properties, we have added a sentence to the last paragraph in the introduction. We have also run a number of additional experiments to study the effect in more detail:\n* Figure 1: We have added VMBPO as a baseline.\n* Figure 8: We have added two new baselines: value-weighted maximum likelihood [2] and VAML [3]. These comparisons allow us to compare MnM to alternative model learning approaches.\n* Figure 10: We have added a new figure showing how the MSE of the MnM model evolves throughout training. This analysis shows that the training dynamics of MnM are relatively stable, even though the method resembles a GAN. While the MnM model produces better policies than the maximum likelihood model, this plot shows that the MnM model actually has worse accuracy that the maximum likelihood. This finding suggests that our joint objective enables training to focus on overall RL performance, rather than just model accuracy.\n\n> The significance of the paper is limited...  Even a standard MLE-based MBRL algorithm optimizes a lower bound on the expected returns.\n\nThank you for pointing this out. We have added a paragraph to the revised paper (paragraph 3 in Section 2) to note that the Simulation Lemma (and other prior work) also optimize a lower bound on expected returns. Our lower bound is arguably more useful than the bound from the Simulation Lemma for two reasons. First, our bound can be readily *estimated*. It is unclear how to practically optimize the bound from the Simulation Lemma in MDPs with continuous states and actions; that would require guaranteeing that the model error on every state and action be less than some threshold. In contrast, our lower bound involves an expectation, which can be estimated using samples. A second reason our bound may be more useful is that our bound can be readily *optimized*. Whereas our method jointly optimizes the model and the policy using the same objective, the method proposed in the Simulation Lemma paper involves updating the model and the policy using different objective; indeed, it is unclear how to directly optimize the bound from the Simulation Lemma in MDPs with continuous states and actions.\n\n> random seeds\n\nAll plots in the paper were run for at least 3 random seeds; we have added this detail to Appendix C. To address the reviewer's concern about reproducibility, we reran experiments on one of the environments from Figure 5 ('DClawScrewFixed-v0'), this time using 5 random seeds. The results, shown in Figure 8, result in the same conclusions as before.\n\nAs suggested by the reviewer, we will attempt to run more seeds on more tasks. However, as each experiment takes approximately 72 hours to complete, running 10 seeds on 12 tasks with 3 baselines would involve about 26,000 hours of compute.\n\n> In tabular experiments, there is no comparison to an MLE MBRL agent. \n\nIn the tabular experiments (Figure 2a), a MLE MBRL agent that learned a perfect model would be identical to the \"Q learning\" baseline. We have revised the plot to call this method \"Correct Model.\" MnM outperforms even this oracle version of the MLE MBRL agent.\n\n> In continuous control, there is no comparison to any MBRL algorithm that addresses the objective mismatch.\n\nAs suggested by the reviewer, we have added comparisons to three additional MBRL algorithms that address objective mismatch. On the benchmark locomotion tasks, we compare to VMBPO (red curves in Figures 5), finding that it performs worse than MnM. We choose these tasks for the comparison with VMBPO because VMBPO reports numbers on these tasks.\n\nWe then compare to VAML [3] and the value-weighted maximum likelihood loss proposed in [2]. We use the more difficult DClawScrewFixed-v0 task for the comparison to the other methods, as the higher dimensionality (36 dimensional observations) and complex contact dynamics of this task are more likely to cause model exploitation. We report results in new Figure 8, which uses 5 random seeds. MnM outperforms these alternative model learning approaches."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "zb5RAf8SFKs",
        "original": null,
        "number": 5,
        "cdate": 1637259693018,
        "mdate": 1637259693018,
        "ddate": null,
        "tcdate": 1637259693018,
        "tmdate": 1637259693018,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "wwCC3ItMgPD",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to D2vZ ",
          "comment": "**[response 2 of 3]**\n\n> Lastly, while MnM mitigates the objective mismatch, it is unclear from the results what are the benefits of this.\n\nThe main contribution of this paper is an objective for model based RL with appealing theoretical guarantees. Our four didactic experiments probe and experimentally verify these theoretical guarantees. These experiments show that the benefits of MnM include faster learning (Figure 2a), better handling of inaccurate models (Figure 2b), and optimizing a lower bound (Figure 3b). Because these experiments use simple environments, we can directly study whether the proposed *objective* is useful, independent of the particular *optimization* procedure. The proposed objective can be optimized using any planning or RL algorithm.\n\nTo try to directly address the reviewer's concerns about the number of random seeds in Figure 3, we repeated the experiment on one of the environments where the reviewer had concerns about statistical significance (DClawScrewFixed-v0) using 5 random seeds. The results, shown in Figure 8, are nearly identical to the original results shown in Figure 3, supporting the reproducibility of our results.\n\n> The new bound relies on (a) non-markovian dynamics (b) time-varying and parameterized discount factor, limiting the justification of the method by this theoretical result.\n\nOur practical implementation of MnM optimizes the simpler lower bound from Eq. 2 (first sentence of Section 4). We believe that optimizing a lower bound is a strong justification for a model-based RL algorithm (most model-based RL algorithms do not do this). We do not claim that our practical implementation optimizes the more complicated lower bound from Eq. 4. To clarify this point, we have revised part of the third paragraph of the introduction and the final paragraph of \"An objective for model-based RL.\"\n\nWhile we have not implemented an algorithm to optimize the more complex lower bound, prior work has successfully learned non-Markovian dynamics [6] and (separately) learned discount factors [7]. Thus, optimizing the more complex lower bound may be feasible using architectures and tricks from prior work.\n\n> Since the bound that the practical algorithm optimizes is not tight, it is unclear to which extent the proposed method mitigates the objective mismatch.\n\nThere might be a confusion in how we are using the term \"objective mismatch.\" We use this term to mean that the model and policy are trained with the same objective, regardless of how that objective relates to the true expected return objective. This property is useful because it means that gradient updates to the model will also make the policy better, according to this objective. In contrast, gradient updates to a maximum likelihood model can make the policy worse. MnM mitigates the objective mismatch problem because the model and policy are trained using the same objective. \n\nImportantly, there is a (provable) relationship between the proposed objective and the expected return: the proposed objective is a lower bound on the expected return. This is practically useful because it guarantees users that a policy trained with the model will do well when deployed in the true environment. Prior work that proposes lower bounds that are either very difficult to compute [8] or only hold for a small region of \"similar\" models and policies [1].\n\n> Lastly, the analyses make several assumptions that might not be entirely justified. The most noticeable one is the assumption about the positive reward. While it is common to assume that the rewards are non-negative, the assumption about positive rewards might be limiting. \n\nThe assumption that reward functions are bounded is standard in the literature [4, 5]. Of course, we can add a constant to make any bounded reward function positive, which does not change the optimal policy. \n\n> Moreover, the rewards in certain environments (e.g. HalfCheetah-v2) where MnM is tested take negative values\n\nAs noted in Section 5.2, we did not apply the logarithmic transformation to the robotic control experiments. We have revised this sentence to emphasize this point. Of course, computing the logarithm of a negative reward would not make sense.\n\n> The literature review could be improved.\n\nWe have added the suggested references to the related work section."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "b0HEW-nE6nD",
        "original": null,
        "number": 6,
        "cdate": 1637259722472,
        "mdate": 1637259722472,
        "ddate": null,
        "tcdate": 1637259722472,
        "tmdate": 1637259722472,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "zb5RAf8SFKs",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to D2vZ",
          "comment": "**[response 3 of 3]**\n\n\n> Since $\\Gamma_\\theta(t)$ is a CDF, does it imply that $\\gamma(t)$ is a distribution?\n\nYes, $\\gamma_\\theta(t)$ is a distribution. We have added a sentence to clarify this in Section 3.\n\n> \u201cOne detail of note is that we omit the reward augmentation for MnM during these experiments, as it hinders exploration leading to lower returns.\u201d Is it the entropy augmentation of SAC or the proposed augmentation with the log density ratio?\n\nThis is the proposed augmentation with the log density ratio (Equation 3). We have clarified this sentence to refer to the transformation in Equation 3.\n\n> Is the reward function learned as well?\n\nYes, the reward function is learned as well. We fit a separate neural network to predict the rewards (final paragraph of Section 4.2).\n\n\n[1] Luo, Yuping, et al. \"Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees.\" arXiv preprint arXiv:1807.03858 (2018).\n\n[2] Lambert, Nathan, et al. \"Objective mismatch in model-based reinforcement learning.\" arXiv preprint arXiv:2002.04523 (2020).\n\n[3] Farahmand, Amir-massoud, Andre Barreto, and Daniel Nikovski. \"Value-aware loss function for model-based reinforcement learning.\" Artificial Intelligence and Statistics. PMLR, 2017.\n\n[4] Sun, Wen, et al. \"Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches.\" Conference on learning theory. PMLR, 2019.\n\n[5] Shah, Devavrat, et al. \"Sample efficient reinforcement learning via low-rank matrix estimation.\" arXiv preprint arXiv:2006.06135 (2020).\n\n[6] Hafner, Danijar, et al. \"Dream to control: Learning behaviors by latent imagination.\" arXiv preprint arXiv:1912.01603 (2019).\n\n[7] Rudner, Tim GJ, et al. \"Outcome-Driven Reinforcement Learning via Variational Inference.\" arXiv preprint arXiv:2104.10190 (2021).\n\n[8] Kearns, Michael, and Satinder Singh. \"Near-optimal reinforcement learning in polynomial time.\" Machine learning 49.2 (2002): 209-232."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "wRvJlH78k3r",
        "original": null,
        "number": 7,
        "cdate": 1637259775476,
        "mdate": 1637259775476,
        "ddate": null,
        "tcdate": 1637259775476,
        "tmdate": 1637259775476,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "SVNmfI1ADor",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to uKVv ",
          "comment": "We thank the reviewer for their feedback. The reviewer's sole concern seemed to be the comparison with VAML and the simple weighting strategy proposed in (Lambert 2020). We have run this additional experiment and included results in Figure 8.  We use the DClawScrewFixed-v0 task for this comparison because its high dimensionality (36-dimensional observation) and complex contact dynamics are likely to cause model exploitation. MnM outperforms these alternative model learning approaches. **Does this new experiment address the reviewer's concerns with the paper?**"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "ubmLpy9kXb",
        "original": null,
        "number": 8,
        "cdate": 1637259969843,
        "mdate": 1637259969843,
        "ddate": null,
        "tcdate": 1637259969843,
        "tmdate": 1637259969843,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "Ar24yIo1Di7",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to yCzC ",
          "comment": "We thank the reviewer for their detailed feedback! The reviewer's main concern seems to be about clarifying the explanation of the lower bound (e.g., emphasizing that the simple lower bound does not always hold with equality). We have revised the paper to address this issue. **Are there additional revisions that the reviewer would like to see?** If so, we would be happy to update the paper.\n\n> In Theorem 3.1, the equality of the lower bound is hard to hold.\n\nWe believe that the statement of Theorem 3.1 is correct: the proposed objective is either less than or equal to the expected return objective. We agree with the reviewer that this lower bound will not, in general, hold with equality. In cases where the bound is loose, we are guaranteed that the policy that optimizes the lower bound will also get high reward on the true environment, but it remains an open question whether it will get the highest reward on the true environment. Nonetheless, we find empirically that maximizing this simple lower bound does result in policies that get high return on a wide range of tasks. To clarify that we are optimizing a potentially-loose lower bound, we have revised the final paragraph in \"An objective for model-based RL.\"\n\n> \u201cTightening the lower bound\u201d gives a new reward function..., I cannot agree that this is really significant.\n\nWe believe that the objective in Eq. 4 may be of *theoretical* interest because it combines properties that current model-based RL objectives lack:\n* At optimality, the objective is equal to the expected return objective.\n* The model and policy are optimized using the same objective. This makes it easy to determine which models are better for the purpose of policy optimization (similar to [1, 2, 3]).\n* The objective is a global lower bound on expected return. While prior work [4] has proposed a *local* lower bound, a *global* lower bound is arguably much more useful, as it holds for any choice of model and policy.\nWe have added a paragraph to Section 2 to explain these comparisons with prior work.\n\nWe agree that the *practical* significance of this theoretical result is currently limited, as we have not proposed a method for optimizing this objective (last paragraph of \"Tightening the lower bound.\").\n\n> missing \"log\" in line (d) of proof of Lemma 3.1\n\nWe have fixed this typo.\n\n[1] Grimm, Christopher, Andr\u00e9 Barreto, Satinder Singh, and David Silver. \"The value equivalence principle for model-based reinforcement learning.\" arXiv preprint arXiv:2011.03506 (2020).\n\n[2] Nikishin, Evgenii, Romina Abachi, Rishabh Agarwal, and Pierre-Luc Bacon. \"Control-Oriented Model-Based Reinforcement Learning with Implicit Differentiation.\" arXiv preprint arXiv:2106.03273 (2021).\n\n[3] D'Oro, Pierluca, Alberto Maria Metelli, Andrea Tirinzoni, Matteo Papini, and Marcello Restelli. \"Gradient-aware model-based policy search.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, pp. 3801-3808. 2020.\n\n[4] Luo, Yuping, et al. \"Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees.\" arXiv preprint arXiv:1807.03858 (2018).\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "Ikcm2nVc4aE",
        "original": null,
        "number": 9,
        "cdate": 1637607061599,
        "mdate": 1637607061599,
        "ddate": null,
        "tcdate": 1637607061599,
        "tmdate": 1637607061599,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "myFPS0LQ7BO",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to 7QkN",
          "comment": "Dear Reviewer,\n\nWe hope that you've had a chance to read our response. We would really appreciate a reply as to whether our response and clarifications have addressed the issues raised in your review, or whether there is anything else we can address."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "bCIS61XiIZX",
        "original": null,
        "number": 10,
        "cdate": 1637607093473,
        "mdate": 1637607093473,
        "ddate": null,
        "tcdate": 1637607093473,
        "tmdate": 1637607093473,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "b0HEW-nE6nD",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to D2vZ ",
          "comment": "Dear Reviewer,\n\nWe hope that you've had a chance to read our response. We would really appreciate a reply as to whether our response and clarifications have addressed the issues raised in your review, or whether there is anything else we can address."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "XUjAeuCVc6L",
        "original": null,
        "number": 11,
        "cdate": 1637607154634,
        "mdate": 1637607154634,
        "ddate": null,
        "tcdate": 1637607154634,
        "tmdate": 1637607154634,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "wRvJlH78k3r",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to uKVv ",
          "comment": "Dear Reviewer,\n\nWe have run the experiments suggested by the reviewer. We would really appreciate a reply as to whether our response has addressed the issues raised in your review, or whether there is anything else we can address."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "39387VoFT8Z",
        "original": null,
        "number": 12,
        "cdate": 1637607180145,
        "mdate": 1637607180145,
        "ddate": null,
        "tcdate": 1637607180145,
        "tmdate": 1637607180145,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "ubmLpy9kXb",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to yCzC ",
          "comment": "Dear Reviewer,\n\nWe hope that you've had a chance to read our response. We would really appreciate a reply as to whether our response and clarifications have addressed the issues raised in your review, or whether there is anything else we can address.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "XgsY5q7uin-",
        "original": null,
        "number": 13,
        "cdate": 1637632577327,
        "mdate": 1637632577327,
        "ddate": null,
        "tcdate": 1637632577327,
        "tmdate": 1637632577327,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "bCIS61XiIZX",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Further discussion",
          "comment": "Dear Authors,\n\nThank you for the reply and detailed explanations.\n\n**The revisions do not properly address the major concern about the reliability and the reproducibility of the results.**\n\nI appreciate the new results in Figure 8 studying the comparison of MnM with 4 baselines on `DClawScrewFixed-v0` domain using 5 random seeds.\n\nHowever, these results are given **only for 1 single environment while Figure 5 contains 11 other domains where results are still inconclusive** both because of 3 seeds (combined with high variance of returns) and lack of benchmarks (VMBPO for CartPole, MetaWorld, and DClaw beyond ScrewFixed-v0).\n\nI appreciate the computational complexity of running the experiments using more than 3 seeds. However, given the high variance of results, it is not possible to make conclusions whether MnM outperforms the MBPO and SAC baselines.\n\nMoreover, while I appreciate the similarity of the bound used in practice and for theoretical analysis, there are significant differences between them (e.g. discount factor being a parameterized distribution instead of a scalar, non-markovian dynamics) limiting the theoretical contribution of the paper."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Reviewer_D2vZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Reviewer_D2vZ"
        ]
      },
      {
        "id": "1HiaW5g9XnL",
        "original": null,
        "number": 14,
        "cdate": 1637689567149,
        "mdate": 1637689567149,
        "ddate": null,
        "tcdate": 1637689567149,
        "tmdate": 1637689567149,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "XgsY5q7uin-",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "p-values and the significance of the simple lower bound",
          "comment": "Thank you for clarifying the concerns about the paper. This discussion is useful for helping us make the paper more rigorous.\n\n\n> However, given the high variance of results, it is not possible to make conclusions whether MnM outperforms the MBPO and SAC baselines.\n\nTo make conclusions about whether MnM outperforms the baselines, we have gone back and computed p-values associated with each environment. Specifically, we looked at the final reward for each method (i.e., the right side of each plot). This allows us to make the following claims:\n* MnM outperforms VMBPO on the three gym locomotion tasks (all p values < 0.04)\n* MnM outperforms MBPO on the four DClaw tasks (all p values < 0.01)\n\nWe will revise the paper also note the following negative results:\n* MnM does not outperform SAC on all the metaworld and DLaw tasks. The p-values are 0.3 for metaworld-drawer-v2, 0.33 for DLawTurnRandom-v0, and < 0.08 for all other tasks. \n\n> The difference between the two lower bounds limits the theoretical contributions of the paper\n\nEven the simple lower bound, which corresponds to the actual method used in the experiments, is stronger than prior work (see paragraph 3 of Section 2; compare to Luo 2019, Kearns 2002).\n\nWe believe that describing the more complex version of the bound in the text is a net positive improvement to the paper; we could have  omitted the more complex bound and provided only the description of the simple lower bound used in our experiments. We further revised Section 3 to more clearly emphasize that our experiments will optimize the simple bound, and that this paper does not propose a method to optimize the complex bound.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "pGtZtGj409V",
        "original": null,
        "number": 15,
        "cdate": 1637690806998,
        "mdate": 1637690806998,
        "ddate": null,
        "tcdate": 1637690806998,
        "tmdate": 1637690806998,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "XUjAeuCVc6L",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to authors",
          "comment": "Thank the authors for the additional experiments. It seems that in high-dimensional systems, the proposed algorithm experimentally outperforms previous straightforward ones. Could you give a more formal comparison between MnM and VAML/value-weighted maximum likelihood? For example why MnM is theoretically better and what properties MnM has are not held by VAML/value-weighted methods. I would like to increase my score if it's convincing. For the current version, I still can't see the obvious reasons that people will choose the complicated MnM as a tool to address objective mismatch problems instead of choosing the straightforward ones."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Reviewer_uKVv"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Reviewer_uKVv"
        ]
      },
      {
        "id": "V7dt-Fz7Khr",
        "original": null,
        "number": 16,
        "cdate": 1637703599469,
        "mdate": 1637703599469,
        "ddate": null,
        "tcdate": 1637703599469,
        "tmdate": 1637703599469,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "pGtZtGj409V",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Why users might prefer MnM",
          "comment": "There are two reasons why users might prefer to use MnM, compared with VAML (Farahmand 2017) and the value-weighted maximum likelihood (Lambert 2020):\n\n* **MnM avoids objective mismatch**: While VAML and value-weighted maximum likelihood take steps to address the objective mismatch problem, they do not avoid it: the models and policies trained by these methods are optimized using different objectives. In contrast, our method does avoid the objective mismatch problem: the model and policy are optimized using the same objective. Avoiding model mismatch is practically useful because it tells users that taking gradient steps to update the model will also improve the policy. Otherwise, updates to the model can make the policy worse; Fig. 4 from Lambert 2020 does a great job illustrating this point.\n* **MnM optimizes a lower bound**: Even in the presence of model function approximation, MnM optimizes a lower bound on expected return. VAML and value-weighted maximum likelihood do not maximize a lower bound on expected return, and are liable to overestimate the policy's expected return in the real environment. Optimizing a lower bound, as done by MnM, is practically useful because it gives users confidence that an agent's performance in the real world will be at least as good as the performance under the learned model.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "d2kKweL7PEQ",
        "original": null,
        "number": 18,
        "cdate": 1637720136072,
        "mdate": 1637720136072,
        "ddate": null,
        "tcdate": 1637720136072,
        "tmdate": 1637720136072,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "39387VoFT8Z",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Response to authors",
          "comment": "Dear authors,\n\nThank you for your reply. I think the changes in the revision are more accurate to position the theoretical results. I do not have further questions."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Reviewer_yCzC"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Reviewer_yCzC"
        ]
      },
      {
        "id": "mucAvu76udS",
        "original": null,
        "number": 19,
        "cdate": 1637788387775,
        "mdate": 1637788387775,
        "ddate": null,
        "tcdate": 1637788387775,
        "tmdate": 1637788387775,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "myFPS0LQ7BO",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Comment",
        "content": {
          "title": "Any additional questions/concerns?",
          "comment": "Dear Reviewer,\n\nThank you for raising a number of questions and concerns in the initial review. Revising the paper to address these concerns has made the paper more precise and rigorous. Have the revisions and responses above addressed all the reviewer's concerns? We would be happy to address any additional questions."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Authors"
        ]
      },
      {
        "id": "FC8KS6xadLrK",
        "original": null,
        "number": 1,
        "cdate": 1642696840931,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696840931,
        "tmdate": 1642696840931,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "9FfAEgUYGON",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The reviewers agree that the proposed method of joint model-policy optimization using a lower bound is novel and interesting and worthwhile pursuing. But all reviewers find a variety of issues in the paper, such that ratings are just above borderline or below. Given all the mixed feedback, it appears that the paper is still a bit premature for publication and could greatly benefit from improvements in a future submission."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "Ar24yIo1Di7",
        "original": null,
        "number": 1,
        "cdate": 1635861302877,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635861302877,
        "tmdate": 1636076760130,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "9FfAEgUYGON",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a new model-based RL method by devising a new reward function, which incorporates a term measuring the difference between the learned dynamics model and the true dynamics probability, in addition to the logarithm of the original environment reward. Maximizing this augmented reward function is proved to maximizing a lower bound of the logarithm of the original expected return. The reward function is not practically usable since the true dynamics probability is not known. To practically apply the proposed reward function, a GAN-like classifier is introduced to differentiate the transition generated from the dynamics model and the true dynamics. The experiments show that under an unified objective, the learned dynamics tends to assign higher transition probability for some (s,a,s\u2019) if this transition can assist the policy to achieve a higher return in the true environments.",
          "main_review": "The method is well motivated, and the proposed augmented reward function is novel. The experiments are also well conducted, and the visualization of the learned dynamics provides intuitive illustrations that it tends to help the policy to succeed. The overall performance of MnM is also demonstrated to perform at least as well compared with SAC and MBPO.\n\nDespite these advantages mentioned above, I have some comments below that I\u2019d like to hear from the authors in the responses.\n\n1. The augmented reward in Eq. (3) is informative, while in Theorem 3.1, the equality of the lower bound is hard to hold. In the proof of Theorem 3.1, Jensen\u2019s inequality is applied, twice, to move the logarithm inside the expectation. For Jensen\u2019s inequality, the equality holds when the function (logarithm here) is affine or the variable inside the function (the reward r here) is constant. Obviously, neither is the case here. Therefore, at least for the reward function in Eq. (3), maximizing over its cumulative expectation does not guarantee the optimality in the original expected return.\n\n2. Although, immediately, the following contexts about \u201cTightening the lower bound\u201d gives a new reward function by employing a learnable $\\gamma_{\\theta}$, I cannot agree that this is really significant. From the proof of Lemma 3.2, the equality holds when the two analytical solutions are satisfied for $\\gamma_{\\theta}$ and the dynamics model, and the two analytical solutions are independent with each other. Then, for the learnable $\\gamma_{\\theta}$, it indeed plays a role of reward shaping that shapes the reward in Eq. (3) probably nonlinearly to reach the logarithm of the original expected return (probably a way to approximate the affine function to remove the Jensen\u2019s gap?). Also, in the experiments, this new lower bound and reward function are not applied. So, I agree Lemma 3.2 demonstrates that there exists some way to modify the reward in Eq. (3) to have a tighter lower bound, while I think this is a benefit from reward shaping, instead of a thoroughly informative new reward function.\n\nMinor:\nIn the proof of Lemma 3.1, line (d) in the equation, the reduced $\\pi_{\\theta}(a_t|s_t)$ missed the logarithm.\n\nOverall, I think the proposed method is novel and the experiments give nice explanations for the learned dynamics in tending to assist the policy learning, and MnM indeed has its superiority compared to prior model-based RL methods.\n",
          "summary_of_the_review": "A good paper that introduces a novel model-based RL method, where the policy learning and dynamics modeling share a unified objective.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Reviewer_yCzC"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Reviewer_yCzC"
        ]
      },
      {
        "id": "SVNmfI1ADor",
        "original": null,
        "number": 2,
        "cdate": 1635863882777,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635863882777,
        "tmdate": 1635863917932,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "9FfAEgUYGON",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper concerns the objective mismatch problem in model-based RL that the model is optimized for prediction accuracy, not a good performance. The authors propose Mismatched no More (MnM) that maximizes a lower bound on expected reward in an adversarial training manner. They also give a global lower bound that holds for any dynamics. Experiments also validate the effectiveness of MnM.",
          "main_review": "Objective mismatch problem is important in MBRL. And the proposed MnM is motivated by the idea of GAN to solve this problem. The whole paper is easy to follow and I also appreciate the theorems in the paper. However, I would like to see more comparisons, both theoretical and experimental, that if MnM performs better than the VAML framework [1,2] and the simple weighting strategy in [3]. Both these works are addressing the same problem, so it would be important to directly compare them, especially considering that MnM is a more complicated algorithm.\n\n[1] Value-Aware Loss Function for Model-based Reinforcement Learning. Amir-massoud Farahmand et al. AISTATS 2017\\\n[2] Iterative Value-Aware Model Learning. Amir-massoud Farahmand et al. NeurIPS 2018 \\\n[3] Objective Mismatch in Model-based Reinforcement Learning. Nathan Lambert et al. L4DC 2020",
          "summary_of_the_review": "The overall paper is motivated and clear, The concern I have is the advantage over previous works that address exactly the same problem with relatively simple mechanisms.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Reviewer_uKVv"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Reviewer_uKVv"
        ]
      },
      {
        "id": "AA_eC9RLdjW",
        "original": null,
        "number": 3,
        "cdate": 1635875622548,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635875622548,
        "tmdate": 1635894965665,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "9FfAEgUYGON",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper studies the model learning aspect in model-based reinforcement learning (MBRL). Standard Dyna-like MBRL approaches that train the model using the maximum likelihood estimation (MLE) or its variations. In contrast, the authors propose an algorithm called Mismatched no More (MnM) that optimizes model parameters using a lower bound on the agent\u2019s performance. By doing so, the paper mitigates the known objective mismatch of standard MBRL: MLE-optimized model might not necessarily be useful for optimizing agent\u2019s returns, the overall objective of an RL system. Experimentally, the paper presents two studies. First, in 2 tabular environments, the authors compare MnM to Q-learning and VMBPO, an alternative MBRL algorithm that addresses the objective mismatch. Second, the authors compare MnM to an MLE-like MBRL baseline and a model-free algorithm on 12 continuous control tasks.\n",
          "main_review": "Novelty\n\nTo the best of the reviewer's knowledge, the idea is novel. The authors propose a lower bound on the logarithm of expected returns that the policy and the model can jointly optimize against an adversary that tries to distinguish between real and model rollouts. As a result, the MnM model mitigates the objective mismatch yet produces realistic samples. This contrasts the approach with other non-MLE methods (e.g. see [1]) that address the objective mismatch but produce unrealistic rollouts. The authors might want to emphasize the property more in the paper and study the effect in more detail.\n\nSignificance\n\nThe significance of the paper is limited. The main concern of the reviewer is about the empirical results: \n- First, most experiments appear to have a very limited number of runs. The reviewer did not find the details in the paper explicitly articulating the number of random seeds. The reviewer might infer from the plots that MnM and the baselines were run 2-3 times (e.g. Figure 5, `meta-world-door-open-v2`). Given the high variance of the results, it is hard to make conclusions about the comparison of MnM to the baselines. Reporting the results using 10 random seeds might improve the credibility of the results. See [2] for a discussion about the significance of the experimentation in RL.\n- Second, a few important baselines are missing. In tabular experiments, there is no comparison to an MLE MBRL agent. In continuous control, there is no comparison to any MBRL algorithm that addresses the objective mismatch. Why not use VMBPO in these experiments if you already used it in tabular experiments? Furthermore, Appendix A.4 suggests that the optimal dynamics for the lower bound are achieved by weighting the true dynamics with returns. Why not try return-weighted MLE as a baseline? Including other baselines that address the objective mismatch would further increase the significance of the results (see notes about related work).\n- Lastly, while MnM mitigates the objective mismatch, it is unclear from the results what are the benefits of this. Figure 5 suggests that out of 12 continuous control environments, only on `DClawScrewRandom-v0` we can make a conclusion that MnM significantly outperforms the presented baselines. In 3 MuJoCo environments, the performance of MnM and MBPO does not differ significantly. In the other 8 environments, it is hard to make definite conclusions due to the variance of returns.\n\nThe authors provide theoretical analyses associated with MnM. There are several concerns about this part of the paper as well:\n- The proposed method optimizes a bound $L(\\theta)$ on the log expected returns. The authors claim that \u201cthis bound becomes tight under certain assumptions\u201d. To support the claim, the paper introduces another bound $L_\\gamma(\\theta)$ and proves that the supremum of $L_\\gamma(\\theta)$ coincides with the log expected returns. While the reviewer appreciates the similarity of the $L_\\gamma(\\theta)$ with $L(\\theta)$, the new bound relies on (a) non-markovian dynamics (b) **time-varying** and **parameterized** discount factor, limiting the justification of the method by this theoretical result.\n- With the above in mind, it is possible to argue that even a standard MLE-based MBRL algorithm optimizes a lower bound on the expected returns (yet this bound will not be tight) using the Simulation Lemma [3]. See section 2 in [4] for an explanation. Since the bound that the practical algorithm optimizes is not tight, it is unclear to which extent the proposed method mitigates the objective mismatch.\n- Lastly, the analyses make several assumptions that might not be entirely justified. The most noticeable one is the assumption about the positive reward. While it is common to assume that the rewards are *non-negative*, the assumption about positive rewards might be limiting. Moreover, the rewards in certain environments (e.g. `HalfCheetah-v2`) where MnM is tested take negative values.\n\nDetailed notes and questions\n\nThere are several smaller concerns and questions that the authors might address:\n1. The literature review could be improved. In particular, relevant references include [1] that characterize the set of models that are optimal for planning; [5] that optimize the policy and the model using the same objective; [6] that learn the model to directly optimize agent\u2019s performance, not a lower bound on it; [7] that learn the model that is helpful for policy improvement using a weighting scheme.\n2. Since $\\Gamma_\\theta(t)$ is a CDF, does it imply that $\\gamma(t)$ is a distribution?\n3. \u201cOne detail of note is that we omit the reward augmentation for MnM during these experiments, as it hinders exploration leading to lower returns.\u201d Is it the entropy augmentation of SAC or the proposed augmentation with the log density ratio?\n4. Is the reward function learned as well?\n\nReferences\n\n[1] Grimm, Christopher, Andr\u00e9 Barreto, Satinder Singh, and David Silver. \"The value equivalence principle for model-based reinforcement learning.\" arXiv preprint arXiv:2011.03506 (2020).\n\n[2] Henderson, Peter, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. \"Deep reinforcement learning that matters.\" In Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1. 2018.\n\n[3] Kearns, Michael, and Satinder Singh. \"Near-optimal reinforcement learning in polynomial time.\" Machine learning 49, no. 2 (2002): 209-232.\n\n[4] Farahmand, Amir-massoud, Andre Barreto, and Daniel Nikovski. \"Value-aware loss function for model-based reinforcement learning.\" In Artificial Intelligence and Statistics, pp. 1486-1494. PMLR, 2017.\n\n[5] Schrittwieser, Julian, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez et al. \"Mastering atari, go, chess and shogi by planning with a learned model.\" Nature 588, no. 7839 (2020): 604-609.\n\n[6] Nikishin, Evgenii, Romina Abachi, Rishabh Agarwal, and Pierre-Luc Bacon. \"Control-Oriented Model-Based Reinforcement Learning with Implicit Differentiation.\" arXiv preprint arXiv:2106.03273 (2021).\n\n[7] D'Oro, Pierluca, Alberto Maria Metelli, Andrea Tirinzoni, Matteo Papini, and Marcello Restelli. \"Gradient-aware model-based policy search.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, pp. 3801-3808. 2020.\n\n",
          "summary_of_the_review": "The reviewer recommends rejecting the paper. While the idea is promising, the submission in its current state needs substantial improvements. Addressing the outlined concerns might increase the overall score.\n",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Reviewer_D2vZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Reviewer_D2vZ"
        ]
      },
      {
        "id": "AYzNlGj_7zQ",
        "original": null,
        "number": 4,
        "cdate": 1636323255985,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636323255985,
        "tmdate": 1636323255985,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "9FfAEgUYGON",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies the the problem of objective mismatch problem in model-based reinforcement learning, which states that the goal of the model is different from the goal of policy optimization. To solve this issue, this paper proposes a single objective to train both the policy and the dynamics model, by deriving a lower bound of the (log of) expected return and optimizing the lower bound jointly. Based on the objective, the paper proposes the Mismatched no More (MnM) algorithm, which use a GAN-style objective to train the model. Experiments show that the proposed algorithm can match the performance of baseline algorithm (MBPO) when exploration is not a big issue, while outperforms it significantly for sparse-reward tasks. ",
          "main_review": "Writing: The paper is written clearly, although it has a few typos. \n\n1. The link to the proof of Lemma 3.2 is wrong. \n2. Eq (8): What is $\\tilde Q_\\phi$? Is it $Q_\\psi$\u200b?\n3. Appendix A.3, (d), the first cancelled term should be $\\log \\pi_\\theta(a_t | s_t)$\u200b. \n4. Appendix A.1, \"Rather, it corresponds to maximizing a sum of the expected return **and** the *variance* of the return\"\n\nNovelty: The proposed method is similar to a prior work,  VMBPO. Both share similar components, with similar method of learning. However, there are a few difference which separates them apart. One of the most important differences is that MnM takes the logarithm of rewards, and derives a lower bound of the log of expected return, while VMBPO derives a variational lower bound of an upper bound of the log of the expected return. Given VMBPO as a prior work, The theoretical result is not so surprising. If the technique of logarithm of reward can be well justified, I think the empirical novelty is good. \n\nOther comments: \n\n> Following prior work (Janner et al., 2019), we learn an additional model to predict the true environment.\nWhat does it mean by \"additional model\"? \n\nIs the dynamics model learned well? From my personel research experience, learning the model using a GAN-like objective leads to an inaccurate model and sometimes produces unrealistic states. The signals from the discriminator are much weaker than the signals from supervised learning. What if we train $C$\u200b and $\\pi$\u200b with MnM, but train maximum likelihood model $q$\u200b\u200b? \n\nIn Figure 6: What does it mean by \"For fair comparison, we use Q values corresponding to just the task reward\"? MBPO uses $r$ while MnM uses $\\log r$, so they indeed differ a lot. \n\nAs the new reward function $\\tilde r$ takes the logarithm of old reward function $r$, it seems that (a) it's tricky to apply logarithm to non-positive numbers; (b) The new reward function $\\hat r$  is not invariant to the scale of the old reward function $r$, i.e., shifting $r$ can lead to a very different $\\tilde r$. Do all these $\\tilde r$\u200b work in practice? \n\nTheoretically, there is indeed a difference whether we transform the reward. But I'm not very convinced that this technique is essential in practice. The authors constructs a 3-state MDP in Figure 3, but some details are missing, e.g., how is $\\eta$\u200b\u200b\u200b\u200b in VMBPO chosen? I agree that VMBPO optimizes an upper bound approximately, but it can somehow control overestimation by adjusting $\\eta$\u200b, while I don't see how MnM controls underestimation. Moreover, what happens if we don't apply this technique in practice? I expected to see the ablation in a more complex environment. \n\n",
          "summary_of_the_review": "This paper is written well and the idea sounds very interesting and promising. However, as stated in the review, I'm not fully concerned by the log reward technique. Overall, I think there is more merit in this paper than the flaw so I recommend weak acceptance. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper153/Reviewer_7QkN"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper153/Reviewer_7QkN"
        ]
      },
      {
        "id": "FC8KS6xadLrK",
        "original": null,
        "number": 1,
        "cdate": 1642696840931,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696840931,
        "tmdate": 1642696840931,
        "tddate": null,
        "forum": "9FfAEgUYGON",
        "replyto": "9FfAEgUYGON",
        "invitation": "ICLR.cc/2022/Conference/Paper153/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The reviewers agree that the proposed method of joint model-policy optimization using a lower bound is novel and interesting and worthwhile pursuing. But all reviewers find a variety of issues in the paper, such that ratings are just above borderline or below. Given all the mixed feedback, it appears that the paper is still a bit premature for publication and could greatly benefit from improvements in a future submission."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}