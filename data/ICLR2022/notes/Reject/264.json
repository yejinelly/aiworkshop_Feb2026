{
  "id": "miA4AkGK00R",
  "original": "vet8T9RH8c",
  "number": 264,
  "cdate": 1632875440308,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875440308,
  "tmdate": 1676330680147,
  "ddate": null,
  "content": {
    "title": "EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback",
    "authorids": [
      "~Ilyas_Fatkhullin1",
      "~Igor_Sokolov3",
      "~Eduard_Gorbunov1",
      "~Zhize_Li1",
      "~Peter_Richt\u00e1rik1"
    ],
    "authors": [
      "Ilyas Fatkhullin",
      "Igor Sokolov",
      "Eduard Gorbunov",
      "Zhize Li",
      "Peter Richt\u00e1rik"
    ],
    "keywords": [
      "EF21",
      "error feedback",
      "bidirectional compression",
      "regularization",
      "variance reduction",
      "heavy ball momentum",
      "stochastic approximation"
    ],
    "abstract": "First proposed by Seide et al (2014) as a heuristic, error feedback (EF) is a very popular mechanism for enforcing convergence of distributed gradient-based optimization methods enhanced with communication compression strategies based on the application of contractive compression operators. However, existing theory of EF relies on very strong assumptions (e.g., bounded gradients), and provides pessimistic convergence rates (e.g., while the best known rate for EF in the smooth nonconvex regime, and when full gradients are compressed, is $O(1/T^{2/3})$, the rate of gradient descent in the same regime is $O(1/T)$). Recently, Richt\\'{a}rik et al (2021) proposed a new error feedback mechanism, EF21, based on the construction of a Markov compressor induced by a contractive compressor. EF21 removes the aforementioned theoretical deficiencies of EF and at the same time works better in practice. In this work we propose six practical extensions of EF21: partial participation, stochastic approximation, variance reduction, proximal setting, momentum and bidirectional compression. Our extensions are supported by strong convergence theory in the smooth nonconvex and also Polyak-\u0141ojasiewicz regimes. Several of these techniques were never analyzed in conjunction with EF before, and in cases where they were (e.g., bidirectional compression), our rates are vastly superior.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "fatkhullin|ef21_with_bells_whistles_practical_algorithmic_extensions_of_modern_error_feedback",
    "pdf": "/pdf/06b12f08e80e05d990231bf9d8819fb30c2d4378.pdf",
    "supplementary_material": "/attachment/b4be401a86c65329c81bb8a6c8cbdd97152b01ae.zip",
    "_bibtex": "@misc{\nfatkhullin2022ef,\ntitle={{EF}21 with Bells \\& Whistles: Practical Algorithmic Extensions of Modern Error Feedback},\nauthor={Ilyas Fatkhullin and Igor Sokolov and Eduard Gorbunov and Zhize Li and Peter Richt{\\'a}rik},\nyear={2022},\nurl={https://openreview.net/forum?id=miA4AkGK00R}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "miA4AkGK00R",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 13,
    "directReplyCount": 8,
    "revisions": true,
    "replies": [
      {
        "id": "Cfi2UxDu7hA",
        "original": null,
        "number": 1,
        "cdate": 1635700530392,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635700530392,
        "tmdate": 1635700530392,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies the extended versions of a previously proposed algorithm called EF21. The paper includes several variants of EF21 such as stochastic optimization, partial participation, variance reduction, momentum, etc. The theoretical analysis for each extension is given and experiments on real world datasets are conducted.",
          "main_review": "This paper studies extended versions of an existing algorithm called EF21, and show its convergence complexity when combined with other techniques, such as momentum, variance reduction, etc. The theoretical analysis is detailed and given for each variant. The analysis on partial participation and proximal settings are new for error-feedback type algorithms.\n\nOverall I have two concerns:\n\n- The paper provides convergence proof for each variant, which is good, however, it's still unclear to me what role EF21 plays on these techniques. More specifically, to me this paper seems to focus on the fundamental question: how would the convergence rate of any existing algorithm adapt if an EF21 gradient oracle is used? For instance, if we want to use both stochastic approximation and momentum (which is usually the case in practice), can we obtain a rate from your analysis without additional proof? \n\n- The experiments seem a little thin. The main experiments are conducted on small scale linear regression problems (where communication is not really an issue). The appendix provides additional results for Resnet18 on CIFAR10. However, only the first few epochs are shown and it's unclear what hypothesis is verified there (Resnet18 should reach 93% test accuracy on CIFAR10). \n\nSome other minor concerns:\n\n- In the abstract (and also in the paper), it's claimed that EF21-BC is better than a previous result from (Tang et al., 2020). But note that in (Tang et al., 2020), stochastic gradient is used while in  EF21-BC, full gradient is still used. So it's not clear whether EF21-BC can really improve the rate if they are compared in the same setting.\n\n- In the comparison to ChocoSGD on page 8, the authors states $G$ usually depends on dimension while $\\sigma^2$ and $\\Delta$ are dimension free, which seems a little off to me. As they are all bounded on the norm, the statements on their orders are unjustified.",
          "summary_of_the_review": "Please refer to the main review for the details.\n\nI recommend the authors to update the manuscript to make these points clearer.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Reviewer_YjdL"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Reviewer_YjdL"
        ]
      },
      {
        "id": "bDVphbUzHaM",
        "original": null,
        "number": 2,
        "cdate": 1635727753730,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635727753730,
        "tmdate": 1635727753730,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This work studies communication efficient methods by extending EF21 to six more practical settings including momentum, variance reduction, etc. The main results are the convergence rates under these settings. ",
          "main_review": "While the authors definitely did a lot of works, as this is a 70 page paper, the novelty of this work seems to largely depend on EF21. And the main results are obtained by combining the key results in of EF21 with existing analytical techniques such as PAGE and momentum. It is not clear from the main context what are the new analytical breakthrough introduced specifically in this work.\n\nMore experiments are needed to support the theoretical findings. In particular,\n\na) Comparison with other benchmarks is needed since tighter bounds in theory does not necessarily imply better empirical performance.\n\nb) More large-scale experiments should be added. Currently, many logistic regression problems have dimension less than 300.\n\nc) In the ResNet18 experiment, have the authors decreased the step sizes? Typically it is not hard for resnet18 to get a test accuracy more than 90%.\n\nd) One of the goals of communication efficient methods is to make training faster. Communication rounds does not necessarily mean fast in runtime since there are also implementation overhead for (de)compression. Hence, the runtime vs accuracy is also needed in numerical results.",
          "summary_of_the_review": "More discussions are needed to clarify the theoretical novelty, and numerical results should be extended.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Reviewer_yZfa"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Reviewer_yZfa"
        ]
      },
      {
        "id": "aiFr5S3S19v",
        "original": null,
        "number": 3,
        "cdate": 1635837594787,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635837594787,
        "tmdate": 1635837594787,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this paper, the authors propose six practical extensions of EF21, with convergence analysis and good experiment results.",
          "main_review": "The paper is well-written. The convergence analysis and experiments shows that the proposed algorithms work well in both theory and practice. \nHowever, I have the following concerns:\n1. All the proposed algorithms are extensions of EF21, the overall modification seems minor to me, which weakens the contribution of this paper.\n2. Most importantly, I hardly find the experiment results convincing due to the experiment settings. Most of the datasets and models are small and simple, which typically does not require distributed training (even being trained on CPUs), not to mention communication compression. The results on such simple and small learning problems can barely show the significance of any distributed training algorithm with communication compression. \n3. Even for the largest problems: Resnet on cifar10, is usually considered the smallest one in the distributed scenarios.  On cifar10, although the proposed algorithms shows better test accuracy than the baselines, the baselines have better training loss. Thus, it is hard to justify which algorithm is actually better. Such phenomena is also not explained in the paper.  \n4. A very important baseline is missing in the experiments: SGD without communication compression, which could be compared to the proposed algorithms in the number of epochs. Without such a baseline, it is hard to justify the performance of the proposed algorithms.",
          "summary_of_the_review": "The paper is well-written, and good in theory and practice. \nHowever, I have some concerns in the contribution and the experiment settings.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Reviewer_2qZ7"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Reviewer_2qZ7"
        ]
      },
      {
        "id": "gDsSJZnmlez",
        "original": null,
        "number": 4,
        "cdate": 1636333690838,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636333690838,
        "tmdate": 1636401999865,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Review",
        "content": {
          "summary_of_the_paper": "Error feedback (EF) is a technique for ensuring convergence of biased contractive compressor. However, it achieves suboptimal convergence rate when full gradient is used. Recently, EF21 was introduced to mitigate the theoretical deficiencies of EF. This paper studies several extensions of EF21 including EF21 with stochastic gradient, variance reduction, heavy momentum, partial participation, bidirectional compression, and proximal gradient. The paper establishes the convergence results for the six variants and conducts experiments on several small datasets.",
          "main_review": "I appreciate the authors' efforts for analyzing EF21 for various settings and provide their convergence results as well as the experimental results. However, it is not clear to me what challenge the paper addresses and novelty the paper presents. All the six variants including stochastic gradient, variance reduction, heavy momentum, partial participation, bidirectional compression, and proximal gradient have been extensively studied in the literature. Merely combining them with EF21 and providing theoretical analyses do not seems novel to me.\n\nEF has worse error bound than the full-precision counterpart when the full gradient is used. However, it has the same asymptotic bound with stochastic gradient. How does EF21-SGD compare to EF-SGD? Does EF21 still yield better bound when stochastic gradient is adopted?\n\nThe aforementioned question is from theoretical side. From empirical side, Figure 7 in the appendix shows that both EF-SGD and EF-SGD-HB converge faster than EF21-SGD and EF21-SGD-HB, respectively. Only the hybrid method EF21+-SGD-HB obtains higher testing accuracy. These seems to contradict the claim that EF21 is better than EF for the stochastic setting. How do we explain these findings?\n\nSome minor comments:\n1. It is clearer to split the updates in Table 2 to worker's and server's, which I think are more friendly to the readers.\n2. $L_j$ in Example 2 should be $L_{ij}$? ",
          "summary_of_the_review": "This paper provides solid theoretical analysis for several EF21 variants. But the novelty is limited.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Reviewer_DvwK"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Reviewer_DvwK"
        ]
      },
      {
        "id": "-1ubB-ZNfxK",
        "original": null,
        "number": 2,
        "cdate": 1637667412715,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637667412715,
        "tmdate": 1637667506065,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "gDsSJZnmlez",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Comment",
        "content": {
          "title": "Official Response to Reviewer DvwK [Part 2/2]",
          "comment": "> 1. It is clearer to split the updates in Table 2 to worker's and server's, which I think are more friendly to the readers.\n\nWe thank the reviewer for the suggestion: indeed if we had more space, we would make the table better this way. However, taking into account the space limitations, we did our best in making the table short and reader-friendly.\n\n> 2. $L_{j}$ in Example 2 should be $L_{ij}$?\n\nThank you for noticing the typo in Example 2. We fixed it in the revised version."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "readers": [
          "everyone",
          "ICLR.cc/2022/Conference/Program_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Senior_Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Reviewers",
          "ICLR.cc/2022/Conference/Paper264/Reviewers/Submitted",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ]
      },
      {
        "id": "64-C10Gu1eD",
        "original": null,
        "number": 3,
        "cdate": 1637667451222,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637667451222,
        "tmdate": 1637667485845,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "gDsSJZnmlez",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Comment",
        "content": {
          "title": "Official Response to Reviewer DvwK [Part 1/2]",
          "comment": "We thank the reviewer for their feedback. Below we respond to all the comments by the reviewer.\n\n> However, it is not clear to me what challenge the paper addresses and novelty the paper presents. All the six variants including stochastic gradient, variance reduction, heavy momentum, partial participation, bidirectional compression, and proximal gradient have been extensively studied in the literature. Merely combining them with EF21 and providing theoretical analyses do not seems novel to me.\n\nIndeed, each of the six techniques (extensions of GD) are well established in optimization and federated learning, they have been extensively analyzed due to their high importance for both theory and practice. Richtarik et. al (2021) propose and study EF21 mechanism, however, they present and analyze it in pure form. The paper leaves an important open question of whether the proposed mechanism is flexible enough to be combined with other techniques in federated learning. In particular, we addressed the following questions: How these combinations should be implemented? Can one obtain strong convergence results for these methods without additional restrictive assumptions? Our paper answers all these questions in the affirmative and presents a clean analysis and the rates. We emphasize that it is NOT always the case that the combination of different techniques provably works in optimization (for instance, it is not all apparent whether or not the original EF can be combined with bidirectional compression, partial participation, or variance reduction under the same assumptions as in the analysis of non-compressed methods). The theoretical investigation that we propose in our work is an important contribution to the field of error feedback. It shows that EF21 is not just a standalone technique but rather a natural way to design a distributed training along with other popular techniques. Moreover, it shows further theoretical advantages of EF21 over EF since it is not known whether EF can be combined with all the above extensions without restrictive assumptions. \n\n> EF has worse error bound than the full-precision counterpart when the full gradient is used. However, it has the same asymptotic bound with stochastic gradient. How does EF21-SGD compare to EF-SGD? Does EF21 still yield better bound when stochastic gradient is adopted?\n\nTo the best of our knowledge, the existing results of EF-SGD rely on a strong assumption the gradients are bounded (see the \u201cComment\u201d column in Table 1 for Choco-SGD). This is a restrictive assumption since, for example, a simple quadratic function does not satisfy this property. Richtarik et. al (2021) derive the result under a weaker assumption that the variance of the stochastic gradient is bounded. We prove the result under a much more general assumption (Assumption 2) than in any analysis of EF, therefore, it is not trivial to compare the asymptotic bound of EF-SGD and EF21-SGD. In particular, although our rate has worse dependence on the number of workers $n$ than the one derived for Choco-SGD, the quantity $G$ is often infinite meaning that the rate is not well-defined in these cases. In contrast, our rates are well-defined for much wider class of problems.\n\n> From empirical side, Figure 7 in the appendix shows that both EF-SGD and EF-SGD-HB converge faster than EF21-SGD and EF21-SGD-HB, respectively. Only the hybrid method EF21+-SGD-HB obtains higher testing accuracy. These seems to contradict the claim that EF21 is better than EF for the stochastic setting. How do we explain these findings?\n\nIn our paper, we do not claim that the methods with EF21 are always better than previously known methods with EF. What we claim is that for EF21 we prove better theoretical results than previously best-known ones. So, there is no contradiction here. Regarding the DL experiments in Figure 7, we write: *\u201cmomentum methods show a considerable improvement in the accuracy score on the test set over the existing EF21-SGD and EF-SGD.\u201d* We do not claim that EF21 is better than EF in a stochastic setting. We mean that the methods with momentum (EF21-SGD-HB, EF21+-SGD-HB, and EF-SGD-HB) show better test accuracy results than EF21-SGD, EF-SGD (non-momentum methods, that was introduced and compared for the same DL problems in the original EF21 paper). \n\nMoreover, our empirical findings imply that, as with many other methods, EF21-SGD-HB should be used properly, and, in practice, it is worth applying some heuristic, e.g., EF21+, to get better performance for a particular problem. This phenomenon does not diminish our theoretical results,  but rather gives useful information for the practical use of the method."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "readers": [
          "everyone",
          "ICLR.cc/2022/Conference/Program_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Senior_Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Reviewers",
          "ICLR.cc/2022/Conference/Paper264/Reviewers/Submitted",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ]
      },
      {
        "id": "8WDD6P6J7bK",
        "original": null,
        "number": 4,
        "cdate": 1637667578913,
        "mdate": 1637667578913,
        "ddate": null,
        "tcdate": 1637667578913,
        "tmdate": 1637667578913,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "aiFr5S3S19v",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Comment",
        "content": {
          "title": "Official Response to Reviewer 2qZ7",
          "comment": "We thank the reviewer for their feedback. Below we respond to all the comments by the reviewer.\n\n> 1. All the proposed algorithms are extensions of EF21, the overall modification seems minor to me, which weakens the contribution of this paper.\n\nWe politely disagree with the evaluation of our methods. As we explain in our response to the first comment of Reviewer DvwK, our paper addresses important open questions and provides a systematic study of error feedback improving/generalizing previously known results. Error feedback is a popular tool in distributed learning with compression. Therefore, it is important to have such a detailed study (with a clean comparison with existing work) of it as our paper contains.\n\n> 2. Most importantly, I hardly find the experiment results convincing due to the experiment settings. Most of the datasets and models are small and simple, which typically does not require distributed training (even being trained on CPUs), not to mention communication compression. The results on such simple and small learning problems can barely show the significance of any distributed training algorithm with communication compression.\n\nIn our work, we focus on improving the theory of methods with error feedback. We conducted several numerical experiments to illustrate our theoretical findings. We believe these results can be used as a good starting point for the implementation of the proposed methods for larger problems using a more practical distributed setup.\n\n> 3. Even for the largest problems: Resnet on cifar10, is usually considered the smallest one in the distributed scenarios. On cifar10, although the proposed algorithms shows better test accuracy than the baselines, the baselines have better training loss. Thus, it is hard to justify which algorithm is actually better. Such phenomena is also not explained in the paper.\n\nSuch phenomena (worse train loss but better test accuracy) are quite common for not over-parameterized problems. Indeed, good performance on the train data does not imply good performance in the test data.\n\n> 4. A very important baseline is missing in the experiments: SGD without communication compression, which could be compared to the proposed algorithms in the number of epochs. Without such a baseline, it is hard to justify the performance of the proposed algorithms.\n\nWe thank the reviewer for the suggestion. We have added a comparison with non-compressed SGD in the revised version in terms of the sent bits from clients to the server (see Figures 7 and 8). Since we focus on reducing the number of transmitted information to achieve the desired accuracy of the solution, we do not provide a comparison in terms of the epochs."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "readers": [
          "everyone",
          "ICLR.cc/2022/Conference/Program_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Senior_Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Reviewers",
          "ICLR.cc/2022/Conference/Paper264/Reviewers/Submitted",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ]
      },
      {
        "id": "W_Qlwy6Kjcz",
        "original": null,
        "number": 5,
        "cdate": 1637667669020,
        "mdate": 1637667669020,
        "ddate": null,
        "tcdate": 1637667669020,
        "tmdate": 1637667669020,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "bDVphbUzHaM",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Comment",
        "content": {
          "title": "Official Response to Reviewer yZfa",
          "comment": "We thank the reviewer for their feedback. Below we respond to all the comments by the reviewer.\n\n> \u2026 the novelty of this work seems to largely depend on EF21. And the main results are obtained by combining the key results in of EF21 with existing analytical techniques such as PAGE and momentum. It is not clear from the main context what are the new analytical breakthrough introduced specifically in this work.\n\nWe politely disagree with the evaluation of our methods. As we explain in our response to the first comment of Reviewer DvwK, our paper addresses important open questions and provides a systematic study of error feedback improving/generalizing previously known results. We emphasize that it was unclear beforehand how the known techniques should be combined in order to get the results that we derived. Error feedback is a popular tool in distributed learning with compression. Therefore, it is important to have such a detailed study (with a clean comparison with existing work) of it as our paper contains.\n\n> More experiments are needed to support the theoretical findings. In particular,\na) Comparison with other benchmarks is needed since tighter bounds in theory does not necessarily imply better empirical performance.\n\nThe goal of our numerical experiments is to illustrate our theoretical results. We do not have a goal to provide an extensive comparison with various benchmarks, since we focus on the theoretical analysis of error feedback.\n\n> b) More large-scale experiments should be added. Currently, many logistic regression problems have dimension less than 300.\n\nAlthough our work is mainly theoretical, we take seriously the criticism of our numerical experiments. In order to strengthen our paper further, we conducted additional experiments on a larger dataset (real-sim) and added the new plots in the revised version.\n\n> c) In the ResNet18 experiment, have the authors decreased the step sizes? Typically it is not hard for resnet18 to get a test accuracy more than 90%.\n\nWe did not apply stepsize decreasing strategies, but we tuned the stepsize for each method extensively. Our main goal of the experiments on the deep learning problem is to demonstrate how does the distributed training using EF21-SGD-HB and EF21+-SGD-HB compare to EF21-SGD and other existing methods. We choose not to apply stepsize scheduling or other additional techniques for the sake of clarity of comparison. We refer to the experiments on EF with top-K compressor in [Samuel Horv\u00e1th and Peter Richt\u00e1rik. A better alternative to error feedback for communication efficient distributed learning. In 9th International Conference on Learning Representations (ICLR), 2021]. \n\nOur experimental results are consistent with their findings in that they also achieve test accuracy just above 80% in the same setting. To achieve better test accuracy one needs to apply a more sophisticated stepsize schedule. \n\n> d) One of the goals of communication efficient methods is to make training faster. Communication rounds does not necessarily mean fast in runtime since there are also implementation overhead for (de)compression. Hence, the runtime vs accuracy is also needed in numerical results.\n\nWe do not have runtime vs accuracy comparison, because such a comparison is infrastructure-dependent and, therefore, is not fair. Moreover, the mentioned compression/decompression overhead appears in all methods with compression, so, there is no need to take it into account when we compare methods with the same compression as we do in our experiments."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "readers": [
          "everyone",
          "ICLR.cc/2022/Conference/Program_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Senior_Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Reviewers",
          "ICLR.cc/2022/Conference/Paper264/Reviewers/Submitted",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ]
      },
      {
        "id": "oUBodTQv4JK",
        "original": null,
        "number": 6,
        "cdate": 1637667735957,
        "mdate": 1637667735957,
        "ddate": null,
        "tcdate": 1637667735957,
        "tmdate": 1637667735957,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "Cfi2UxDu7hA",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Comment",
        "content": {
          "title": "Official Response to Reviewer YjdL",
          "comment": "We thank the reviewer for their feedback. Below we respond to all the comments by the reviewer.\n\n> The paper provides convergence proof for each variant, which is good, however, it's still unclear to me what role EF21 plays on these techniques. More specifically, to me this paper seems to focus on the fundamental question: how would the convergence rate of any existing algorithm adapt if an EF21 gradient oracle is used? For instance, if we want to use both stochastic approximation and momentum (which is usually the case in practice), can we obtain a rate from your analysis without additional proof?\n\nWe thank the reviewer for raising this question. Due to the flexibility of our analysis, several of the proposed extensions can be combined together. For instance, in order to obtain the analysis for EF21-SGD-HB one needs to use the results of Lemma 2 (for EF21-SGD) instead of Lemma 1 (for EF21) and apply the steps in the proof of Theorem 11 (for EF21-HB). Similar combinations can be obtained with other methods as well. However, for the sake of readability, we do not combine EF21 with more than one technique considered in the paper.\n\n> The experiments seem a little thin. The main experiments are conducted on small scale linear regression problems (where communication is not really an issue). The appendix provides additional results for Resnet18 on CIFAR10. However, only the first few epochs are shown and it's unclear what hypothesis is verified there (Resnet18 should reach 93% test accuracy on CIFAR10).\n\nIn our work, we focus on improving the theory of methods with error feedback. We conducted several numerical experiments to illustrate our theoretical findings. We believe these results can be used as a good starting point for the implementation of the proposed methods for larger problems using a more practical distributed setup. In order to strengthen our paper further, we conducted additional experiments on a larger dataset (real-sim) and added the new plots in the revised version. Regarding the experiments for Resnet18 on CIFAR10 we refer to our reply to the fourth comment of Reviewer yZfa. \n\n> In the abstract (and also in the paper), it's claimed that EF21-BC is better than a previous result from (Tang et al., 2020). But note that in (Tang et al., 2020), stochastic gradient is used while in EF21-BC, full gradient is still used. So it's not clear whether EF21-BC can really improve the rate if they are compared in the same setting.\n\nFirst of all, taking $\\sigma = 0$ in the rate from (Tang et al., 2020), one gets the rate with worse dependence on $\\varepsilon$ than we have for EF21-BC. Secondly, as we explained above, our analysis is quite flexible. In particular, we can combine EF21-BC with SGD or PAGE and compare this to (Tang et al., 2020). It is straightforward from the provided analysis to obtain the analysis for EF21-SGD-BC.\n\n> In the comparison to ChocoSGD on page 8, the authors states $G$ usually depends on dimension while $\\sigma^2$ and $\\Delta$ are dimension free, which seems a little off to me. As they are all bounded on the norm, the statements on their orders are unjustified.\n\nWe thank the reviewer for pointing this. We do not claim that it is always the case, but when $G$ is finite (typically it is not) it has an implicit dependence on the dimension when the problem is not sparse. However, $\\sigma^2$ can be dimension-independent, e.g., when the noise is small. Next, $\\Delta_{\\inf}$ depends on the discrepancy between lower bounds on the functional values of local functions that is in general dimension-independent.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "readers": [
          "everyone",
          "ICLR.cc/2022/Conference/Program_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Senior_Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Reviewers",
          "ICLR.cc/2022/Conference/Paper264/Reviewers/Submitted",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ]
      },
      {
        "id": "x6vHpA5gvlK",
        "original": null,
        "number": 7,
        "cdate": 1637667960462,
        "mdate": 1637667960462,
        "ddate": null,
        "tcdate": 1637667960462,
        "tmdate": 1637667960462,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Comment",
        "content": {
          "title": "General Response to Reviewers",
          "comment": "We thank the reviewers for their time and efforts to study our paper and for their feedback.\n\nWe appreciate the reviewers acknowledging that our analysis is solid (Reviewer DvwK), our algorithms *\u201cwork well in both theory and practice\u201d*, our paper is well-written (Reviewer 2qZ7), our analysis is detailed, and the combinations of error feedback with partial participation and proximal setting are novel in the field of error feedback methods (Reviewer YjdL).\n\nWe also added the replies to all the reviewers\u2019 questions, comments, and concerns. Following the reviewers\u2019 requests, we conducted additional experiments on logistic regression with a larger dataset (real-sim dataset from LIBSVM) and provided a numerical comparison of EF21-SGD and EF21-PAGE with non-compressed SGD and PAGE. New experimental results are given in Appendix A.1. All changes are highlighted in blue color.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "readers": [
          "everyone",
          "ICLR.cc/2022/Conference/Program_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Senior_Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Reviewers",
          "ICLR.cc/2022/Conference/Paper264/Reviewers/Submitted",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ]
      },
      {
        "id": "fGyS17yu6Fg",
        "original": null,
        "number": 10,
        "cdate": 1638115192067,
        "mdate": 1638115192067,
        "ddate": null,
        "tcdate": 1638115192067,
        "tmdate": 1638115192067,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Comment",
        "content": {
          "title": "Feedback request",
          "comment": "We thank the reviewers once again for their reviews. In our responses, we addressed all concerns of the reviewers. \n\nConsidering that the discussion period ends tomorrow, we kindly ask the reviewers to let us know whether our replies are convincing and whether additional clarifications are required. Furthermore, we would be happy to address new questions and criticism in case of any.\n\nWe thank the reviewers in advance."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "readers": [
          "everyone",
          "ICLR.cc/2022/Conference/Program_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Senior_Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Reviewers",
          "ICLR.cc/2022/Conference/Paper264/Reviewers/Submitted",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ]
      },
      {
        "id": "44-hlRhKA3",
        "original": null,
        "number": 11,
        "cdate": 1638384950871,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638384950871,
        "tmdate": 1638386029621,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Comment",
        "content": {
          "title": "To all reviewers: We feel our paper and results were not understood and hence not appreciated",
          "comment": "Dear reviewers, \n\nThanks for your valuable comments. **We feel our paper and our results were not understood and hence not appreciated. We propose several new and practical enhancements of the newly proposed EF21 error feedback method, obtaining state-of-the-art theoretical results for the error feedback mechanism.** \n\n**Is theoretical SOTA not appreciated by the ICLR community anymore? Please study our Table 1.** Several of the proposed variants (\"bells and whistles\") of error feedback were not considered in the literature before because this was hard or impossible to do before the new analysis approach pioneered by Richtarik et al in their EF21 paper. In some cases, similar variants existed, but our rates are the new SOTA. \nSome examples:\n\n- Prior theoretical best rate for error feedback with compression also at the master (which we call bidirectional compression, or BC) was $O(1/\\epsilon^4)$ for the double squeeze method. Their rate does not apply to quantization nor sparsification operators commonly used in practice since they do not satisfy the assumption $\\mathbb{E}||C(x)-x|| \\leq \\Delta$. Our results apply to the commonly used contractive compression operators, and we also get the improved rate $O(1/\\epsilon^2)$. **We believe that this alone is a result publishable in ICLR.**  Moreover, Tang et al do not obtain any rate in the PL setting, we do. \n\n- Prior to our work, to the best of our knowledge, there was no analysis of error feedback with a proximal operator. We obtain the first results in this setting. This alone is an answer to an open problem since 2014 when error feedback was first proposed by Seide et al, and **we believe that this alone is a result publishable in ICLR.**\n\n- Prior to our work, the best rate for error feedback with the very popular momentum mechanism was obtained by Xie et al (2020). They obtained a $O(G/\\epsilon^3)$ rate, where $G$ is a uniform bound on the gradients. However, this is a strong assumption since gradients of many functions used in practice are not uniformly bounded (e.g., quadratics). Methods such as gradient descent and Nesterov momentum are normally analyzed without the need for such an assumption. We resolve this problem: we do not need this assumption. Moreover, our rate is better by an order of magnitude: $O(1/\\epsilon^2)$. We proved better rates for PL functions. **We believe that this alone is a result publishable in ICLR.**  \n\n- Prior to our work, no EF mechanism was analyzed in the important-to-federated learning *partial participation regime*. This is because previous analysis techniques were not good enough to allow for such an analysis. Our method EF21-PP is the first error feedback mechanism provably combinable with partial participation! Since communication compression, error feedback and partial participation are of key importance to modern federated learning, we resolve an important open problem in the field. Again, we obtain an even better rate in the PL regime. **We believe that this alone is a result publishable in ICLR.**  \n\n- Our rate for EF21-PAGE is a massive improvement on the best error feedback results with minibatching (=subsampling) on the clients. In modern machine learning, each client/machine owns many training datapoints, and it is customary to rely on stochastic gradients rather than full gradients. So, it is very important to study this problem. In EF21-PAGE  we managed to combine (and this was not trivial) he EF21 method with the optimal PAGE estimator of Li et al which allows for optimal variance reduced subsampling/minbatching on the clients. Our rate $O(1/\\epsilon^2)$ is a massive improvement on the previous best rate for error feedback with stochastic gradients due to Koloskova et al (2020) and Richtarik et al (2021), which were both $O(1/\\epsilon^4)$. Again, this resolves an important problem in modern distributed and federated learning. **We believe that this alone is a result publishable in ICLR.**  \n\n**In summary, our work contains several independent contributions each of which we deem to be publishable in a top ML venue as each resolves an important theoretical problem in the area of modern communication efficient distributed training.**\n\n**We do not understand why the reviewers do not appreciate such contributions.** Some reviewers have the impression that combining techniques is easy and not novel enough. This can't be further from the truth. If this was the case, error feedback would already have been combined with all the techniques we propose here, and the improved complexity results would have been obtained. There are hundreds of examples in the field where it is not known whether certain  techniques, understood in isolation, are combinable in the sense that the individual benefits they each provide combine for an even greater benefit. For example, we do not know whether adaptive stepsizes for gradient descent (proposed by Mishchenko and Malitsky) can be combined with stochastic approximation.\n\nAuthors "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ]
      },
      {
        "id": "7JN7MZsHqEvt",
        "original": null,
        "number": 1,
        "cdate": 1642696848534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696848534,
        "tmdate": 1642696848534,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "This paper presents several variants and extensions (including stochastic and proximal) of the error-feedback method EF21 and provides convergence rates for each of them and shows that they improve upon previous state of the arts. Despite the much broadened application scenarios and SOTA  in convergence rates/complexity, the main and common concern from the reviewers is the novelty of the paper beyond the original EF21 work. There are also concerns on the empirical evaluations that do not fully support the theoretical promises. I agree with the reviewers and regrettably have to recommend rejection for ICLR."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "Cfi2UxDu7hA",
        "original": null,
        "number": 1,
        "cdate": 1635700530392,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635700530392,
        "tmdate": 1635700530392,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies the extended versions of a previously proposed algorithm called EF21. The paper includes several variants of EF21 such as stochastic optimization, partial participation, variance reduction, momentum, etc. The theoretical analysis for each extension is given and experiments on real world datasets are conducted.",
          "main_review": "This paper studies extended versions of an existing algorithm called EF21, and show its convergence complexity when combined with other techniques, such as momentum, variance reduction, etc. The theoretical analysis is detailed and given for each variant. The analysis on partial participation and proximal settings are new for error-feedback type algorithms.\n\nOverall I have two concerns:\n\n- The paper provides convergence proof for each variant, which is good, however, it's still unclear to me what role EF21 plays on these techniques. More specifically, to me this paper seems to focus on the fundamental question: how would the convergence rate of any existing algorithm adapt if an EF21 gradient oracle is used? For instance, if we want to use both stochastic approximation and momentum (which is usually the case in practice), can we obtain a rate from your analysis without additional proof? \n\n- The experiments seem a little thin. The main experiments are conducted on small scale linear regression problems (where communication is not really an issue). The appendix provides additional results for Resnet18 on CIFAR10. However, only the first few epochs are shown and it's unclear what hypothesis is verified there (Resnet18 should reach 93% test accuracy on CIFAR10). \n\nSome other minor concerns:\n\n- In the abstract (and also in the paper), it's claimed that EF21-BC is better than a previous result from (Tang et al., 2020). But note that in (Tang et al., 2020), stochastic gradient is used while in  EF21-BC, full gradient is still used. So it's not clear whether EF21-BC can really improve the rate if they are compared in the same setting.\n\n- In the comparison to ChocoSGD on page 8, the authors states $G$ usually depends on dimension while $\\sigma^2$ and $\\Delta$ are dimension free, which seems a little off to me. As they are all bounded on the norm, the statements on their orders are unjustified.",
          "summary_of_the_review": "Please refer to the main review for the details.\n\nI recommend the authors to update the manuscript to make these points clearer.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Reviewer_YjdL"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Reviewer_YjdL"
        ]
      },
      {
        "id": "bDVphbUzHaM",
        "original": null,
        "number": 2,
        "cdate": 1635727753730,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635727753730,
        "tmdate": 1635727753730,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This work studies communication efficient methods by extending EF21 to six more practical settings including momentum, variance reduction, etc. The main results are the convergence rates under these settings. ",
          "main_review": "While the authors definitely did a lot of works, as this is a 70 page paper, the novelty of this work seems to largely depend on EF21. And the main results are obtained by combining the key results in of EF21 with existing analytical techniques such as PAGE and momentum. It is not clear from the main context what are the new analytical breakthrough introduced specifically in this work.\n\nMore experiments are needed to support the theoretical findings. In particular,\n\na) Comparison with other benchmarks is needed since tighter bounds in theory does not necessarily imply better empirical performance.\n\nb) More large-scale experiments should be added. Currently, many logistic regression problems have dimension less than 300.\n\nc) In the ResNet18 experiment, have the authors decreased the step sizes? Typically it is not hard for resnet18 to get a test accuracy more than 90%.\n\nd) One of the goals of communication efficient methods is to make training faster. Communication rounds does not necessarily mean fast in runtime since there are also implementation overhead for (de)compression. Hence, the runtime vs accuracy is also needed in numerical results.",
          "summary_of_the_review": "More discussions are needed to clarify the theoretical novelty, and numerical results should be extended.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Reviewer_yZfa"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Reviewer_yZfa"
        ]
      },
      {
        "id": "aiFr5S3S19v",
        "original": null,
        "number": 3,
        "cdate": 1635837594787,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635837594787,
        "tmdate": 1635837594787,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this paper, the authors propose six practical extensions of EF21, with convergence analysis and good experiment results.",
          "main_review": "The paper is well-written. The convergence analysis and experiments shows that the proposed algorithms work well in both theory and practice. \nHowever, I have the following concerns:\n1. All the proposed algorithms are extensions of EF21, the overall modification seems minor to me, which weakens the contribution of this paper.\n2. Most importantly, I hardly find the experiment results convincing due to the experiment settings. Most of the datasets and models are small and simple, which typically does not require distributed training (even being trained on CPUs), not to mention communication compression. The results on such simple and small learning problems can barely show the significance of any distributed training algorithm with communication compression. \n3. Even for the largest problems: Resnet on cifar10, is usually considered the smallest one in the distributed scenarios.  On cifar10, although the proposed algorithms shows better test accuracy than the baselines, the baselines have better training loss. Thus, it is hard to justify which algorithm is actually better. Such phenomena is also not explained in the paper.  \n4. A very important baseline is missing in the experiments: SGD without communication compression, which could be compared to the proposed algorithms in the number of epochs. Without such a baseline, it is hard to justify the performance of the proposed algorithms.",
          "summary_of_the_review": "The paper is well-written, and good in theory and practice. \nHowever, I have some concerns in the contribution and the experiment settings.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Reviewer_2qZ7"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Reviewer_2qZ7"
        ]
      },
      {
        "id": "gDsSJZnmlez",
        "original": null,
        "number": 4,
        "cdate": 1636333690838,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636333690838,
        "tmdate": 1636401999865,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Review",
        "content": {
          "summary_of_the_paper": "Error feedback (EF) is a technique for ensuring convergence of biased contractive compressor. However, it achieves suboptimal convergence rate when full gradient is used. Recently, EF21 was introduced to mitigate the theoretical deficiencies of EF. This paper studies several extensions of EF21 including EF21 with stochastic gradient, variance reduction, heavy momentum, partial participation, bidirectional compression, and proximal gradient. The paper establishes the convergence results for the six variants and conducts experiments on several small datasets.",
          "main_review": "I appreciate the authors' efforts for analyzing EF21 for various settings and provide their convergence results as well as the experimental results. However, it is not clear to me what challenge the paper addresses and novelty the paper presents. All the six variants including stochastic gradient, variance reduction, heavy momentum, partial participation, bidirectional compression, and proximal gradient have been extensively studied in the literature. Merely combining them with EF21 and providing theoretical analyses do not seems novel to me.\n\nEF has worse error bound than the full-precision counterpart when the full gradient is used. However, it has the same asymptotic bound with stochastic gradient. How does EF21-SGD compare to EF-SGD? Does EF21 still yield better bound when stochastic gradient is adopted?\n\nThe aforementioned question is from theoretical side. From empirical side, Figure 7 in the appendix shows that both EF-SGD and EF-SGD-HB converge faster than EF21-SGD and EF21-SGD-HB, respectively. Only the hybrid method EF21+-SGD-HB obtains higher testing accuracy. These seems to contradict the claim that EF21 is better than EF for the stochastic setting. How do we explain these findings?\n\nSome minor comments:\n1. It is clearer to split the updates in Table 2 to worker's and server's, which I think are more friendly to the readers.\n2. $L_j$ in Example 2 should be $L_{ij}$? ",
          "summary_of_the_review": "This paper provides solid theoretical analysis for several EF21 variants. But the novelty is limited.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Reviewer_DvwK"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Reviewer_DvwK"
        ]
      },
      {
        "id": "x6vHpA5gvlK",
        "original": null,
        "number": 7,
        "cdate": 1637667960462,
        "mdate": 1637667960462,
        "ddate": null,
        "tcdate": 1637667960462,
        "tmdate": 1637667960462,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Comment",
        "content": {
          "title": "General Response to Reviewers",
          "comment": "We thank the reviewers for their time and efforts to study our paper and for their feedback.\n\nWe appreciate the reviewers acknowledging that our analysis is solid (Reviewer DvwK), our algorithms *\u201cwork well in both theory and practice\u201d*, our paper is well-written (Reviewer 2qZ7), our analysis is detailed, and the combinations of error feedback with partial participation and proximal setting are novel in the field of error feedback methods (Reviewer YjdL).\n\nWe also added the replies to all the reviewers\u2019 questions, comments, and concerns. Following the reviewers\u2019 requests, we conducted additional experiments on logistic regression with a larger dataset (real-sim dataset from LIBSVM) and provided a numerical comparison of EF21-SGD and EF21-PAGE with non-compressed SGD and PAGE. New experimental results are given in Appendix A.1. All changes are highlighted in blue color.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "readers": [
          "everyone",
          "ICLR.cc/2022/Conference/Program_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Senior_Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Reviewers",
          "ICLR.cc/2022/Conference/Paper264/Reviewers/Submitted",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ]
      },
      {
        "id": "fGyS17yu6Fg",
        "original": null,
        "number": 10,
        "cdate": 1638115192067,
        "mdate": 1638115192067,
        "ddate": null,
        "tcdate": 1638115192067,
        "tmdate": 1638115192067,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Comment",
        "content": {
          "title": "Feedback request",
          "comment": "We thank the reviewers once again for their reviews. In our responses, we addressed all concerns of the reviewers. \n\nConsidering that the discussion period ends tomorrow, we kindly ask the reviewers to let us know whether our replies are convincing and whether additional clarifications are required. Furthermore, we would be happy to address new questions and criticism in case of any.\n\nWe thank the reviewers in advance."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "readers": [
          "everyone",
          "ICLR.cc/2022/Conference/Program_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Senior_Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Area_Chairs",
          "ICLR.cc/2022/Conference/Paper264/Reviewers",
          "ICLR.cc/2022/Conference/Paper264/Reviewers/Submitted",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ]
      },
      {
        "id": "44-hlRhKA3",
        "original": null,
        "number": 11,
        "cdate": 1638384950871,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638384950871,
        "tmdate": 1638386029621,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Official_Comment",
        "content": {
          "title": "To all reviewers: We feel our paper and results were not understood and hence not appreciated",
          "comment": "Dear reviewers, \n\nThanks for your valuable comments. **We feel our paper and our results were not understood and hence not appreciated. We propose several new and practical enhancements of the newly proposed EF21 error feedback method, obtaining state-of-the-art theoretical results for the error feedback mechanism.** \n\n**Is theoretical SOTA not appreciated by the ICLR community anymore? Please study our Table 1.** Several of the proposed variants (\"bells and whistles\") of error feedback were not considered in the literature before because this was hard or impossible to do before the new analysis approach pioneered by Richtarik et al in their EF21 paper. In some cases, similar variants existed, but our rates are the new SOTA. \nSome examples:\n\n- Prior theoretical best rate for error feedback with compression also at the master (which we call bidirectional compression, or BC) was $O(1/\\epsilon^4)$ for the double squeeze method. Their rate does not apply to quantization nor sparsification operators commonly used in practice since they do not satisfy the assumption $\\mathbb{E}||C(x)-x|| \\leq \\Delta$. Our results apply to the commonly used contractive compression operators, and we also get the improved rate $O(1/\\epsilon^2)$. **We believe that this alone is a result publishable in ICLR.**  Moreover, Tang et al do not obtain any rate in the PL setting, we do. \n\n- Prior to our work, to the best of our knowledge, there was no analysis of error feedback with a proximal operator. We obtain the first results in this setting. This alone is an answer to an open problem since 2014 when error feedback was first proposed by Seide et al, and **we believe that this alone is a result publishable in ICLR.**\n\n- Prior to our work, the best rate for error feedback with the very popular momentum mechanism was obtained by Xie et al (2020). They obtained a $O(G/\\epsilon^3)$ rate, where $G$ is a uniform bound on the gradients. However, this is a strong assumption since gradients of many functions used in practice are not uniformly bounded (e.g., quadratics). Methods such as gradient descent and Nesterov momentum are normally analyzed without the need for such an assumption. We resolve this problem: we do not need this assumption. Moreover, our rate is better by an order of magnitude: $O(1/\\epsilon^2)$. We proved better rates for PL functions. **We believe that this alone is a result publishable in ICLR.**  \n\n- Prior to our work, no EF mechanism was analyzed in the important-to-federated learning *partial participation regime*. This is because previous analysis techniques were not good enough to allow for such an analysis. Our method EF21-PP is the first error feedback mechanism provably combinable with partial participation! Since communication compression, error feedback and partial participation are of key importance to modern federated learning, we resolve an important open problem in the field. Again, we obtain an even better rate in the PL regime. **We believe that this alone is a result publishable in ICLR.**  \n\n- Our rate for EF21-PAGE is a massive improvement on the best error feedback results with minibatching (=subsampling) on the clients. In modern machine learning, each client/machine owns many training datapoints, and it is customary to rely on stochastic gradients rather than full gradients. So, it is very important to study this problem. In EF21-PAGE  we managed to combine (and this was not trivial) he EF21 method with the optimal PAGE estimator of Li et al which allows for optimal variance reduced subsampling/minbatching on the clients. Our rate $O(1/\\epsilon^2)$ is a massive improvement on the previous best rate for error feedback with stochastic gradients due to Koloskova et al (2020) and Richtarik et al (2021), which were both $O(1/\\epsilon^4)$. Again, this resolves an important problem in modern distributed and federated learning. **We believe that this alone is a result publishable in ICLR.**  \n\n**In summary, our work contains several independent contributions each of which we deem to be publishable in a top ML venue as each resolves an important theoretical problem in the area of modern communication efficient distributed training.**\n\n**We do not understand why the reviewers do not appreciate such contributions.** Some reviewers have the impression that combining techniques is easy and not novel enough. This can't be further from the truth. If this was the case, error feedback would already have been combined with all the techniques we propose here, and the improved complexity results would have been obtained. There are hundreds of examples in the field where it is not known whether certain  techniques, understood in isolation, are combinable in the sense that the individual benefits they each provide combine for an even greater benefit. For example, we do not know whether adaptive stepsizes for gradient descent (proposed by Mishchenko and Malitsky) can be combined with stochastic approximation.\n\nAuthors "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper264/Authors"
        ]
      },
      {
        "id": "7JN7MZsHqEvt",
        "original": null,
        "number": 1,
        "cdate": 1642696848534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696848534,
        "tmdate": 1642696848534,
        "tddate": null,
        "forum": "miA4AkGK00R",
        "replyto": "miA4AkGK00R",
        "invitation": "ICLR.cc/2022/Conference/Paper264/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "This paper presents several variants and extensions (including stochastic and proximal) of the error-feedback method EF21 and provides convergence rates for each of them and shows that they improve upon previous state of the arts. Despite the much broadened application scenarios and SOTA  in convergence rates/complexity, the main and common concern from the reviewers is the novelty of the paper beyond the original EF21 work. There are also concerns on the empirical evaluations that do not fully support the theoretical promises. I agree with the reviewers and regrettably have to recommend rejection for ICLR."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}