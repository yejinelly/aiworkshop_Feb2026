{
  "id": "OgCcfc1m0TO",
  "original": "24zca9o1V9s",
  "number": 270,
  "cdate": 1632875440615,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875440615,
  "tmdate": 1697934947905,
  "ddate": null,
  "content": {
    "title": "Learning to Prompt for Vision-Language Models",
    "authorids": [
      "~Kaiyang_Zhou1",
      "~Jingkang_Yang1",
      "~Chen_Change_Loy2",
      "~Ziwei_Liu1"
    ],
    "authors": [
      "Kaiyang Zhou",
      "Jingkang Yang",
      "Chen Change Loy",
      "Ziwei Liu"
    ],
    "keywords": [
      "vision-language models",
      "prompt learning",
      "computer vision",
      "transfer learning"
    ],
    "abstract": "Vision-language pre-training has recently emerged as a promising alternative for representation learning. It shifts from the tradition of using images and discrete labels for learning a fixed set of weights, seen as visual concepts, to aligning images and raw text for two separate encoders. Such a paradigm benefits from a broader source of supervision and allows zero-shot transfer to downstream tasks since visual concepts can be diametrically generated from natural language, known as prompt. In this paper, we identify that a major challenge of deploying such models in practice is prompt engineering. This is because designing a proper prompt, especially for context words surrounding a class name, requires domain expertise and typically takes a significant amount of time for words tuning since a slight change in wording could have a huge impact on performance. Moreover, different downstream tasks require specific designs, further hampering the efficiency of deployment. To overcome this challenge, we propose a novel approach named \\emph{context optimization (CoOp)}. The main idea is to model context in prompts using continuous representations and perform end-to-end learning from data while keeping the pre-trained parameters fixed. In this way, the design of task-relevant prompts can be fully automated. Experiments on 11 datasets show that CoOp effectively turns pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin and able to gain significant improvements when using more shots (e.g., at 16 shots the average gain is around 17\\% with the highest reaching over 50\\%). CoOp also exhibits strong robustness to distribution shift.",
    "pdf": "/pdf/d5cc7431896065653d714ec294fe4c292e4aac1c.pdf",
    "one-sentence_summary": "A continuous prompt learning approach that facilitates the deployment of pre-trained vision-language models like CLIP in downstream datasets.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "zhou|learning_to_prompt_for_visionlanguage_models",
    "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2109.01134/code)",
    "_bibtex": "@misc{\nzhou2022learning,\ntitle={Learning to Prompt for Vision-Language Models},\nauthor={Kaiyang Zhou and Jingkang Yang and Chen Change Loy and Ziwei Liu},\nyear={2022},\nurl={https://openreview.net/forum?id=OgCcfc1m0TO}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "OgCcfc1m0TO",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 12,
    "directReplyCount": 6,
    "revisions": true,
    "replies": [
      {
        "id": "CwHrJmanoON",
        "original": null,
        "number": 1,
        "cdate": 1634859356975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1634859356975,
        "tmdate": 1634859356975,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "OgCcfc1m0TO",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a context optimisation (CoOp) approach that can automate prompt engineering and allow more efficient and task-specific transfer for pretrained vision-language models.",
          "main_review": "+ Extensive experiments\n+ Motivation for the automatic prompt is useful\n\no Investment in Fig 1 is super useful. It is helpful to see how fragile the prompt is affected by word twisting. However, the paper claimed prompt engineering is time-consuming, which I do not disagree with. Yes, human annotators have no way to know whether adding a \"flower\" or changing the sentence order will improve or harm the performance. But the key motivation of the prompt is to reduce the human working load and adding some general description is not super complicated. The task should be down to exploring more robust models and algorithms that can withstand these minor changes in my opinion.\n\n- The biggest concern is about the lack of theoretical contribution. The main content of the methodology is broken down into two sections. The first section is just a review of CLIP which is already well-known. The key content of context optimisation in 2.2 is just half a page. The description is rough.\n\n- It is not super clear why adding extra dimensions (claimed as the same number in that of word embedding) to the class token can help the prompt.\n\n- On page 4 above Eq.3, the concept \"unified context\" and \"class-specific context\" are not well explained.\n\n- paragraph under Eq.3, \"Training is performed ...\" is redundant and not informative.\n\n- Figure 2 is not helpful in understanding the framework and looks very low-quality.",
          "summary_of_the_review": "Despite extensive evaluation, the paper presents in very low quality and lack of description of key methods and the theory behind it. The overall quality is clearly below the threshold of the ICLR standard.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "No",
          "recommendation": "1: strong reject",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Reviewer_KMdr"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Reviewer_KMdr"
        ]
      },
      {
        "id": "mo5m2Y33ekj",
        "original": null,
        "number": 2,
        "cdate": 1635354950579,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635354950579,
        "tmdate": 1635354950579,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "OgCcfc1m0TO",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The authors demonstrate a more efficient form of few-shot learning\nusing CLIP compared to linear probing for image classification: CoOp.\nInstead of fine-tuning a small linear classifier on the output of\nCLIP, they propose fine-tuning a number of additional embeddings at\nthe input layer; this modification, in theory, allows CLIP to leverage\nmore computation when adapting to tasks, while still only optimizing a\nsmall number of parameters. Experiments across several corpora\ndemonstrate the efficacy of the approach, which generally yields a\nfew accuracy points of gain versus a linear probe trained on the same\namount of data.",
          "main_review": "The paper's motivation is clear, experiments thorough, and results\nconvincing. In addition to the standard few-shot evaluations, I\nappreciated the authors considering the distribution shift scenario:\nin that case, they show that their model can more effectively leverage\nlabelled data from a different dataset versus the linear probes (even\nthough zero-shot CLIP remains somewhat competitive in that\nregime). Additional experiments about the optimal context length,\nplacement of fine-tune-able token embeddings, and vision backbones\nwhere appreciated.\n\nWhile the CoOp method appears to work well, but I had some concerns\nabout novelty. In particular, this method is more-or-less identical to\nLi and Liang (2021) --- that paper came out on arXiv Jan 1 of this\nyear, and was published at ACL earlier this year. In addition, this\nidea has been \"rediscovered\" in the NLP context several times (which\nare also cited, but perhaps cannot be considered contemporaneous,\ngiven their arXiv dates...). While the authors cite the arXiv version,\nI think that, given the timing, the authors should perhaps not pitch\ntheir method as a \"novel\" approach (as is done in the abstract);\nrather, this seems to be applying prefix-tuning to CLIP.\n\nI had a few technical concerns:\n\n1. there are a lack of details for the linear probe --- how was the\n   regularization parameter chosen?\n   \n2. The baseline for comparison to CoOp was the linear probe, which\n   makes sense. However, I would have also liked to have seen a\n   comparison where all the parameters of CLIP are fine-tuned --- how\n   well does that work for few shot learning?\n\nI also had a few presentation concerns:\n\n1. Figure 1 compares a supervised CoOp method to a zero-shot CLIP\n   baseline. While the CoOp method is \"few shot\", in this figure,\n   there are 16 examples per class provided, which, for some datasets\n   may amount to hundreds or thousands of labelled examples. I would\n   have appreciated Figure 1 comparing to the linear probe.\n\n2. Figure 5b has a similar problem with being potentially misleading:\n   I would recommend including not only zero-shot CLIP, but also\n   linear probe CLIP.",
          "summary_of_the_review": "Overall: the work is generally clear with thorough and convincing\nexperiments. While there were a few presentation/technical concerns,\nthe main drawback of this work is novelty: the proposed idea is\nidentical to Li and Liang (2021) [arxiv in january], Zhong et\nal. (2021) [arxiv in April], and perhaps Lester et al (2021) [arxiv in\nSep, this one is more recent and I haven't read it yet]. While these\npapers consider only the NLP case and not the vision+language case,\nit's still difficult to call this method \"novel,\" as the authors do in\nthe intro.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Reviewer_AYDN"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Reviewer_AYDN"
        ]
      },
      {
        "id": "uLmfK84ybVF",
        "original": null,
        "number": 3,
        "cdate": 1635920943009,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635920943009,
        "tmdate": 1635920943009,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "OgCcfc1m0TO",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes context optimization (CoOp) which learns task-aware continuous prompts to improve CLIP in terms of few-shot image classification. By fixing the pretrained backbone, CoOp performs end-to-end learning to update the learnable context vectors for target domain datasets. The simple yet effective approach substantially beats hand-crafted prompts with a large margin. Meanwhile, CoOp also exhibits better robustness to distribution shift than CLIP.",
          "main_review": "Strengths: \n1) The paper is generally well-written and easy to follow. The author conducts extensive experiments and ablation studies.\n2) CoOp outperforms CLIP on all 11 diverse datasets, especially showing large improvements in specific domains like texture and satellite images.\n3) CoOp is data-efficient and can boost the classification performance with only a few shots of training data.\n4) CoOp demonstrates better robustness to distribution shift than zero-shot CLIP and linear probe CLIP. \n\nWeakness: \n1) Though effective, the technical novelty of this paper is quite limited. Since the soft prompt tuning approaches (e.g., Prompt Tuning [1] and P-Tuning [2]) have already been proposed in NLP domain. And CoOp adopts a very similar technique.\n[1] Lester, Brian, Rami Al-Rfou, and Noah Constant. \u201cThe power of scale for parameter-efficient prompt tuning.\u201d EMNLP 2021.\n[2] Liu, Xiao, et al. \u201cGPT Understands, Too.\u201d *arXiv preprint arXiv:2103.10385* (2021).\n\n2) According to Table 4 in the paper, the nearest neighbor words of the learned context vectors rarely have practical semantic meaning. This casts doubt on using the word \u201ccontext\u201d. From this perspective, these soft prompts are just more parameters to improve the model capacity.  Hence, can we say that CoOp is just a better version of fine-tuning? In other words, there might be a similar fine-tuning way to utilize additional parameters to get better performance.",
          "summary_of_the_review": "The paper achieves better few-shot image classification performance improvements but lacks enough technical novelty and explanations for learned prompts",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Reviewer_1WJt"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Reviewer_1WJt"
        ]
      },
      {
        "id": "iQ0p4tsDGS",
        "original": null,
        "number": 4,
        "cdate": 1635930182488,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635930182488,
        "tmdate": 1635930472726,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "OgCcfc1m0TO",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a novel approach named context optimization (CoOp) for prompt engineering of vision-language pre-training models.   The main idea is to model context in prompts using continuous representations and perform end-to-end learning from data while keeping the pre-trained parameters fixed. Experiments on 11 datasets show that CoOp effectively turns pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin and able to gain significant improvements when using more shots.",
          "main_review": "Strengths:\n1. This paper provides a novel approach named CoOp for prompt engineering based on  CLIP. \n2. CLIP-based experiments are sufficient to prove the advantages of CoOp.\n\nWeaknesses:\n1. Only the results based on CLIP are compared, and there are no more experiments of other visual-language models.\n2. It is an improvement of CLIP, and the idea is similar to many existing works such as [1].\n[1] Prefix-Tuning: Optimizing Continuous Prompts for Generation\n",
          "summary_of_the_review": "This is a somewhat novel but solid work. The comparative experiment based on clip is very sufficient, but it also lacks other important experiments, such as the effect of CoOp on other vision-language models, just as the title of the paper is learning to prompt for vision language models",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Reviewer_pGwp"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Reviewer_pGwp"
        ]
      },
      {
        "id": "10KOSclsdSw",
        "original": null,
        "number": 1,
        "cdate": 1637225400612,
        "mdate": 1637225400612,
        "ddate": null,
        "tcdate": 1637225400612,
        "tmdate": 1637225400612,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "iQ0p4tsDGS",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Comment",
        "content": {
          "title": "Response",
          "comment": "Thank you for your review. We are very encouraged by your positive comment on the solidity of our paper.\n\nWe address your concerns below.\n\n> Only the results based on CLIP are compared, and there are no more experiments of other visual-language models.\n\nFirst, we would like to clarify that we refer to vision-language models specifically as recent *CLIP-like image recognition models*, which generate classification weights directly from natural language. This concept was used in Jia et al. (2021).\n\nTo our knowledge, only CLIP (Radford et al., 2021) is open-source and has been widely used by the community. We expect our approach to also work for other similar models like ALIGN (Jia et al., 2021) and the more recent DeCLIP [a] and CLOOB [b].\n\n[a] Li, Y., Liang, F., Zhao, L., Cui, Y., Ouyang, W., Shao, J., ... & Yan, J. (2021). Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm. arXiv preprint arXiv:2110.05208.\n\n[b] F\u00fcrst, A., Rumetshofer, E., Tran, V., Ramsauer, H., Tang, F., Lehner, J., ... & Hochreiter, S. (2021). CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP. arXiv preprint arXiv:2110.11316.\n\n> It is an improvement of CLIP, and the idea is similar to many existing works such as [1]. [1] Prefix-Tuning: Optimizing Continuous Prompts for Generation.\n\nAs this concern was also raised by other reviewers, we provide a comprehensive discussion in the common area."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ]
      },
      {
        "id": "poAn9cNoHvx",
        "original": null,
        "number": 2,
        "cdate": 1637225450002,
        "mdate": 1637225450002,
        "ddate": null,
        "tcdate": 1637225450002,
        "tmdate": 1637225450002,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "uLmfK84ybVF",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Comment",
        "content": {
          "title": "Response",
          "comment": "Thank you for your review. Also thank you for endorsing the empirical results. We are very encouraged by the comments.\n\nWe address your concerns below.\n\n> Though effective, the technical novelty of this paper is quite limited. Since the soft prompt tuning approaches (e.g., Prompt Tuning [1] and P-Tuning [2]) have already been proposed in NLP domain. And CoOp adopts a very similar technique. [1] Lester, Brian, Rami Al-Rfou, and Noah Constant. \u201cThe power of scale for parameter-efficient prompt tuning.\u201d EMNLP 2021. [2] Liu, Xiao, et al. \u201cGPT Understands, Too.\u201d arXiv preprint arXiv:2103.10385 (2021).\n\nAs this concern was also raised by other reviewers, we provide a comprehensive discussion in the common area.\n\n> According to Table 4 in the paper, the nearest neighbor words of the learned context vectors rarely have practical semantic meaning. This casts doubt on using the word \u201ccontext\u201d. From this perspective, these soft prompts are just more parameters to improve the model capacity. Hence, can we say that CoOp is just a better version of fine-tuning? In other words, there might be a similar fine-tuning way to utilize additional parameters to get better performance.\n\nAs discussed in the paper, interpreting the learned tokens is difficult and using nearest words from the pre-trained embeddings might be inaccurate (so we do not recommend future work to use this interpretation method). Since the tokens are updated using knowledge (i.e., gradients) distilled from the pre-trained text encoder, it is reasonable to think that the learned tokens still reside within the pre-trained word space but contain more abstract meanings that are useful for downstream recognition.\n\nWe would also like to point out that the interpretation issue is not unique to CoOp but common to broader continuous prompt learning methods developed in NLP (which is discussed in the related work section). To our knowledge, none of the existing continuous prompt learning papers in NLP has solved the interpretation issue. We hope our openness to discuss the issue is encouraged rather than criticized as a weakness.\n\nCoOp is the first prompt learning-based adaptation method for CLIP-like foundation models [a] *in computer vision*, but certainly not the only workable solution. We think our simple design and the findings presented in the paper can be useful for the community to investigate more advanced, and effective, designs for adapting large pre-trained vision-language models\u2014which we view as an important research direction to democratize foundation models [a].\n\n[a] Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ]
      },
      {
        "id": "dBpQ5TvSpYO",
        "original": null,
        "number": 3,
        "cdate": 1637225494080,
        "mdate": 1637225494080,
        "ddate": null,
        "tcdate": 1637225494080,
        "tmdate": 1637225494080,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "mo5m2Y33ekj",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Comment",
        "content": {
          "title": "Response",
          "comment": "Thank you for your review. Also thank you for endorsing our paper\u2019s motivation and the empirical results. We are very encouraged by the comments.\n\nAs the concern over the novelty was also raised by other reviewers, we provide a comprehensive discussion in the common area.\n\nWe address your remaining concerns below.\n\n> There are a lack of details for the linear probe --- how was the regularization parameter chosen?\n\nAs mentioned in the paper, we followed the same steps as in Radford et al. (2021). Please see Appendix A.3 in their original paper at https://arxiv.org/pdf/2103.00020.pdf.\n\n> The baseline for comparison to CoOp was the linear probe, which makes sense. However, I would have also liked to have seen a comparison where all the parameters of CLIP are fine-tuned --- how well does that work for few shot learning?\n\nGood question. We did tried this experiment early on where we fine-tuned all parameters in the image and text encoders. We found that this fine-tuning method did not work at all for all CLIP\u2019s (open-source) vision backbones, i.e., RN50, RN101, ViT-B/32 and ViT-B/16. Specifically, the loss initially declined for a few iterations but then quickly went up and stayed at a \u201cdead\u201d mode with zero training accuracy.\n\n> Figure 1 compares a supervised CoOp method to a zero-shot CLIP baseline. While the CoOp method is \"few shot\", in this figure, there are 16 examples per class provided, which, for some datasets may amount to hundreds or thousands of labelled examples. I would have appreciated Figure 1 comparing to the linear probe.\n\nPlease note that Fig.1\u2019s purpose is not to show a comprehensive comparison of all methods (which is detailed in the experiments section, specifically in Sec.3.1 and 3.2) but to highlight the problems of hand-crafted prompts, explaining why we should learn the prompts rather than manually design them.\n\n> Figure 5b has a similar problem with being potentially misleading: I would recommend including not only zero-shot CLIP, but also linear probe CLIP.\n\nPlease note that Fig.5b is part of the ablation studies, which mainly aims to verify whether the learning-based prompts work with different architectures. Sec.3.1 (few-shot learning) and 3.2 (domain generalization) have provided comprehensive comparisons with linear probe CLIP where the results are sufficient to justify the learning-based prompts. So repeating the experiments for linear probe CLIP across all 11 datasets and vision backbones would not give extra new insights but will certainly consume a lot of compute resources and time."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ]
      },
      {
        "id": "kQJtX83Zj0",
        "original": null,
        "number": 4,
        "cdate": 1637225632082,
        "mdate": 1637225632082,
        "ddate": null,
        "tcdate": 1637225632082,
        "tmdate": 1637225632082,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "CwHrJmanoON",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Comment",
        "content": {
          "title": "Response",
          "comment": "Thank you for your review. We think designing efficient adaptation methods for large pre-trained vision-language models is an important step to democratize foundation models [a], which is an emerging topic in computer vision. We\u2019d like to emphasize that *our paper is the first to tackle this problem in computer vision using prompt learning*, and hope our paper can be assessed fairly based on whether the insights and findings presented in the paper are novel to the field of computer vision.\n\n[a] Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.\n\nWe address your concerns below.\n\n> However, the paper claimed prompt engineering is time-consuming ... The task should be down to exploring more robust models and algorithms that can withstand these minor changes in my opinion.\n\nAutomatic prompt learning has been an established research problem in NLP (see [b] for a comprehensive survey in this topic) where the motivation has been widely recognized: prompt engineering needs labeled data and a significant amount of efforts for words tuning but does not guarantee that the resulting prompts are familiar to what the model has been learned with; so the idea is why not design an efficient algorithm to leverage the labeled data for automatic prompt learning.\n\nWe think the adaptation of large pre-trained vision-language models is a promising research direction, which is of great importance to facilitating deployments of these models in downstream applications. Being *the first to study efficient prompt learning in computer vision*, our paper provides timely insights, and importantly, demonstrates strong empirical performance for a simple approach, which we think is worth sharing with the community.\n\n[b] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2021). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.\n\n> The biggest concern is about the lack of theoretical contribution.\n\nOur prompt learning-based approach has a very simple and intuitive design, i.e., to turn context words in a prompt into a set of learnable vectors. From the learning perspective, our approach is based on supervised learning, or more formally empirical risk minimization, which is backed by the statistical learning theory [c] well known to the machine learning community. Therefore, having a theory seems trivial for prompt learning methods. To our knowledge, none of the existing prompt learning papers in NLP has a theory (please refer to the relevant references in Section 4).\n\n[c] Vapnik, V. (1999). The nature of statistical learning theory. Springer science & business media.\n\n> The main content of the methodology is broken down into two sections. The first section is just a review of CLIP which is already well-known. The key content of context optimisation in 2.2 is just half a page. The description is rough.\n\nThe purpose of putting a review of CLIP is to introduce the model architecture\u2014on top of which our model is built\u2014and the prompting-based zero-shot inference. We think the organization is clear and useful for readers who might not know CLIP\u2019s technical details.\n\nOur approach is quite simple and easy to understand, particularly for readers who are already familiar with CLIP. Therefore, we chose to leave more space for the experiments.\n\n> It is not super clear why adding extra dimensions (claimed as the same number in that of word embedding) to the class token can help the prompt.\n\nAdding more tokens can increase the capacity of context, which is analogous to adding more parameters/layers in a neural network.\n\n> On page 4 above Eq.3, the concept \"unified context\" and \"class-specific context\" are not well explained.\n\nThe explanation for \u201cunified context\u201d appears right before it, \u201cNote that the context here is shared among all classes, which is called unified context ...\u201d\n\nClass-specific context is explicitly explained in the last paragraph of Section 2, \u201cAnother option is to design class-specific context \u2026\u201d\n\n> paragraph under Eq.3, \"Training is performed ...\" is redundant and not informative.\n\nThe paragraph is necessary because its purpose is to discuss how the model is trained as well as to explain the intuition why the design helps learn \u201cmeaningful\u201d context, which we think would give readers a clearer picture of our approach.\n\n> Figure 2 is not helpful in understanding the framework and looks very low-quality.\n\nThe figure is inspired by OpenAI\u2019s sketch of the CLIP model and designed to be as simple as possible. Could you please give more specific comments about which parts you think are unclear or can be improved? We are happy to revise the figure."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ]
      },
      {
        "id": "X5WAqRUtXIg",
        "original": null,
        "number": 5,
        "cdate": 1637225746918,
        "mdate": 1637225746918,
        "ddate": null,
        "tcdate": 1637225746918,
        "tmdate": 1637225746918,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "OgCcfc1m0TO",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Comment",
        "content": {
          "title": "Response to the concern about novelty",
          "comment": "Since most reviewers share the same concern about the novelty that the idea of prompt learning has been proposed in NLP, we provide our response in this common area and hope our answer can convince the reviewers to support our paper.\n\nFirst of all, our research deals with *computer vision* problems that are significantly different from those in NLP so we hope our paper can be assessed based on whether the insights and findings are useful to the vision community. To avoid misunderstanding, we\u2019ve changed the word \u201cnovel\u201d to \u201csimple\u201d in the abstract when describing the proposed approach.\n\nSuccessfully applying ideas developed in a different domain like NLP to computer vision is *non-trivial* and could inspire new ideas that might not be possible by only focusing on technical innovation in a single field. A recent example is Vision Transformers [a], which applies the same Transformers [b] architecture developed in NLP to image recognition and has now been extended to many other computer vision problems.\n\nWe are the first to successfully apply the concept of prompt learning to computer vision, specifically to adapting large pre-trained vision-language models, which is an important topic essential for democratizing foundation models [c] (i.e., to make these models more accessible to wider communities that do not have sufficient training data and compute resources for pre-training). Technically, we propose two variants of prompt learning: unified context optimization and class-specific context optimization, which are specifically designed for solving image recognition problems. We believe the insights and findings presented in the paper are novel to computer vision and worth sharing with the community.\n\n[a] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. https://openreview.net/forum?id=YicbFdNTTy\n\n[b] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).\n\n[c] Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ]
      },
      {
        "id": "UBvArpkLtaq",
        "original": null,
        "number": 7,
        "cdate": 1637226821360,
        "mdate": 1637226821360,
        "ddate": null,
        "tcdate": 1637226821360,
        "tmdate": 1637226821360,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "kQJtX83Zj0",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Comment",
        "content": {
          "title": "Response",
          "comment": "> \u200b\u200bDespite extensive evaluation, the paper presents in very low quality and lack of description of key methods and the theory behind it. The overall quality is clearly below the threshold of the ICLR standard.\n\nWe hope our answer to your main concern about the theory is resolved. The paper was carefully organized and polished, which is also endorsed by other reviewers who found our paper \u201cgenerally well-written and easy to follow.\u201d Any more advice that you think could improve the paper is welcome. We think the \u201cstrong reject\u201d rating isn\u2019t a fair assessment and hope you can re-evaluate our paper based on whether it has positive impacts on computer vision."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ]
      },
      {
        "id": "GClEN1b4a1M",
        "original": null,
        "number": 10,
        "cdate": 1637776507615,
        "mdate": 1637776507615,
        "ddate": null,
        "tcdate": 1637776507615,
        "tmdate": 1637776507615,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "X5WAqRUtXIg",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Comment",
        "content": {
          "title": "Thanks for the info!",
          "comment": "Hi there --- just confirming that I saw this and am considering your response in my continued capacity as a reviewer for this work; thank you for detailing your thoughts."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Reviewer_AYDN"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Reviewer_AYDN"
        ]
      },
      {
        "id": "5Kcks_JU1pY",
        "original": null,
        "number": 1,
        "cdate": 1642696849171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696849171,
        "tmdate": 1642696849171,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "OgCcfc1m0TO",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "Given the increasing scale of large models (e.g. CLIP), there's an argument that we need better automated techniques for properly utilizing (prompting) these models. Given the success of prompt learning within pure NLP models, the authors apply the same approach to the V+L domain and show that it also is applicable here.  Generally, reviewers felt that the results were clear and thorough, yet technically limited.  The approach is not novel and the result not surprising.  There is a documentary benefit to having this work out in the community for others to reference and extend."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "CwHrJmanoON",
        "original": null,
        "number": 1,
        "cdate": 1634859356975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1634859356975,
        "tmdate": 1634859356975,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "OgCcfc1m0TO",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a context optimisation (CoOp) approach that can automate prompt engineering and allow more efficient and task-specific transfer for pretrained vision-language models.",
          "main_review": "+ Extensive experiments\n+ Motivation for the automatic prompt is useful\n\no Investment in Fig 1 is super useful. It is helpful to see how fragile the prompt is affected by word twisting. However, the paper claimed prompt engineering is time-consuming, which I do not disagree with. Yes, human annotators have no way to know whether adding a \"flower\" or changing the sentence order will improve or harm the performance. But the key motivation of the prompt is to reduce the human working load and adding some general description is not super complicated. The task should be down to exploring more robust models and algorithms that can withstand these minor changes in my opinion.\n\n- The biggest concern is about the lack of theoretical contribution. The main content of the methodology is broken down into two sections. The first section is just a review of CLIP which is already well-known. The key content of context optimisation in 2.2 is just half a page. The description is rough.\n\n- It is not super clear why adding extra dimensions (claimed as the same number in that of word embedding) to the class token can help the prompt.\n\n- On page 4 above Eq.3, the concept \"unified context\" and \"class-specific context\" are not well explained.\n\n- paragraph under Eq.3, \"Training is performed ...\" is redundant and not informative.\n\n- Figure 2 is not helpful in understanding the framework and looks very low-quality.",
          "summary_of_the_review": "Despite extensive evaluation, the paper presents in very low quality and lack of description of key methods and the theory behind it. The overall quality is clearly below the threshold of the ICLR standard.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "No",
          "recommendation": "1: strong reject",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Reviewer_KMdr"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Reviewer_KMdr"
        ]
      },
      {
        "id": "mo5m2Y33ekj",
        "original": null,
        "number": 2,
        "cdate": 1635354950579,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635354950579,
        "tmdate": 1635354950579,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "OgCcfc1m0TO",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The authors demonstrate a more efficient form of few-shot learning\nusing CLIP compared to linear probing for image classification: CoOp.\nInstead of fine-tuning a small linear classifier on the output of\nCLIP, they propose fine-tuning a number of additional embeddings at\nthe input layer; this modification, in theory, allows CLIP to leverage\nmore computation when adapting to tasks, while still only optimizing a\nsmall number of parameters. Experiments across several corpora\ndemonstrate the efficacy of the approach, which generally yields a\nfew accuracy points of gain versus a linear probe trained on the same\namount of data.",
          "main_review": "The paper's motivation is clear, experiments thorough, and results\nconvincing. In addition to the standard few-shot evaluations, I\nappreciated the authors considering the distribution shift scenario:\nin that case, they show that their model can more effectively leverage\nlabelled data from a different dataset versus the linear probes (even\nthough zero-shot CLIP remains somewhat competitive in that\nregime). Additional experiments about the optimal context length,\nplacement of fine-tune-able token embeddings, and vision backbones\nwhere appreciated.\n\nWhile the CoOp method appears to work well, but I had some concerns\nabout novelty. In particular, this method is more-or-less identical to\nLi and Liang (2021) --- that paper came out on arXiv Jan 1 of this\nyear, and was published at ACL earlier this year. In addition, this\nidea has been \"rediscovered\" in the NLP context several times (which\nare also cited, but perhaps cannot be considered contemporaneous,\ngiven their arXiv dates...). While the authors cite the arXiv version,\nI think that, given the timing, the authors should perhaps not pitch\ntheir method as a \"novel\" approach (as is done in the abstract);\nrather, this seems to be applying prefix-tuning to CLIP.\n\nI had a few technical concerns:\n\n1. there are a lack of details for the linear probe --- how was the\n   regularization parameter chosen?\n   \n2. The baseline for comparison to CoOp was the linear probe, which\n   makes sense. However, I would have also liked to have seen a\n   comparison where all the parameters of CLIP are fine-tuned --- how\n   well does that work for few shot learning?\n\nI also had a few presentation concerns:\n\n1. Figure 1 compares a supervised CoOp method to a zero-shot CLIP\n   baseline. While the CoOp method is \"few shot\", in this figure,\n   there are 16 examples per class provided, which, for some datasets\n   may amount to hundreds or thousands of labelled examples. I would\n   have appreciated Figure 1 comparing to the linear probe.\n\n2. Figure 5b has a similar problem with being potentially misleading:\n   I would recommend including not only zero-shot CLIP, but also\n   linear probe CLIP.",
          "summary_of_the_review": "Overall: the work is generally clear with thorough and convincing\nexperiments. While there were a few presentation/technical concerns,\nthe main drawback of this work is novelty: the proposed idea is\nidentical to Li and Liang (2021) [arxiv in january], Zhong et\nal. (2021) [arxiv in April], and perhaps Lester et al (2021) [arxiv in\nSep, this one is more recent and I haven't read it yet]. While these\npapers consider only the NLP case and not the vision+language case,\nit's still difficult to call this method \"novel,\" as the authors do in\nthe intro.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Reviewer_AYDN"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Reviewer_AYDN"
        ]
      },
      {
        "id": "uLmfK84ybVF",
        "original": null,
        "number": 3,
        "cdate": 1635920943009,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635920943009,
        "tmdate": 1635920943009,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "OgCcfc1m0TO",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes context optimization (CoOp) which learns task-aware continuous prompts to improve CLIP in terms of few-shot image classification. By fixing the pretrained backbone, CoOp performs end-to-end learning to update the learnable context vectors for target domain datasets. The simple yet effective approach substantially beats hand-crafted prompts with a large margin. Meanwhile, CoOp also exhibits better robustness to distribution shift than CLIP.",
          "main_review": "Strengths: \n1) The paper is generally well-written and easy to follow. The author conducts extensive experiments and ablation studies.\n2) CoOp outperforms CLIP on all 11 diverse datasets, especially showing large improvements in specific domains like texture and satellite images.\n3) CoOp is data-efficient and can boost the classification performance with only a few shots of training data.\n4) CoOp demonstrates better robustness to distribution shift than zero-shot CLIP and linear probe CLIP. \n\nWeakness: \n1) Though effective, the technical novelty of this paper is quite limited. Since the soft prompt tuning approaches (e.g., Prompt Tuning [1] and P-Tuning [2]) have already been proposed in NLP domain. And CoOp adopts a very similar technique.\n[1] Lester, Brian, Rami Al-Rfou, and Noah Constant. \u201cThe power of scale for parameter-efficient prompt tuning.\u201d EMNLP 2021.\n[2] Liu, Xiao, et al. \u201cGPT Understands, Too.\u201d *arXiv preprint arXiv:2103.10385* (2021).\n\n2) According to Table 4 in the paper, the nearest neighbor words of the learned context vectors rarely have practical semantic meaning. This casts doubt on using the word \u201ccontext\u201d. From this perspective, these soft prompts are just more parameters to improve the model capacity.  Hence, can we say that CoOp is just a better version of fine-tuning? In other words, there might be a similar fine-tuning way to utilize additional parameters to get better performance.",
          "summary_of_the_review": "The paper achieves better few-shot image classification performance improvements but lacks enough technical novelty and explanations for learned prompts",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Reviewer_1WJt"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Reviewer_1WJt"
        ]
      },
      {
        "id": "iQ0p4tsDGS",
        "original": null,
        "number": 4,
        "cdate": 1635930182488,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635930182488,
        "tmdate": 1635930472726,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "OgCcfc1m0TO",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a novel approach named context optimization (CoOp) for prompt engineering of vision-language pre-training models.   The main idea is to model context in prompts using continuous representations and perform end-to-end learning from data while keeping the pre-trained parameters fixed. Experiments on 11 datasets show that CoOp effectively turns pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin and able to gain significant improvements when using more shots.",
          "main_review": "Strengths:\n1. This paper provides a novel approach named CoOp for prompt engineering based on  CLIP. \n2. CLIP-based experiments are sufficient to prove the advantages of CoOp.\n\nWeaknesses:\n1. Only the results based on CLIP are compared, and there are no more experiments of other visual-language models.\n2. It is an improvement of CLIP, and the idea is similar to many existing works such as [1].\n[1] Prefix-Tuning: Optimizing Continuous Prompts for Generation\n",
          "summary_of_the_review": "This is a somewhat novel but solid work. The comparative experiment based on clip is very sufficient, but it also lacks other important experiments, such as the effect of CoOp on other vision-language models, just as the title of the paper is learning to prompt for vision language models",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Reviewer_pGwp"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Reviewer_pGwp"
        ]
      },
      {
        "id": "X5WAqRUtXIg",
        "original": null,
        "number": 5,
        "cdate": 1637225746918,
        "mdate": 1637225746918,
        "ddate": null,
        "tcdate": 1637225746918,
        "tmdate": 1637225746918,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "OgCcfc1m0TO",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Official_Comment",
        "content": {
          "title": "Response to the concern about novelty",
          "comment": "Since most reviewers share the same concern about the novelty that the idea of prompt learning has been proposed in NLP, we provide our response in this common area and hope our answer can convince the reviewers to support our paper.\n\nFirst of all, our research deals with *computer vision* problems that are significantly different from those in NLP so we hope our paper can be assessed based on whether the insights and findings are useful to the vision community. To avoid misunderstanding, we\u2019ve changed the word \u201cnovel\u201d to \u201csimple\u201d in the abstract when describing the proposed approach.\n\nSuccessfully applying ideas developed in a different domain like NLP to computer vision is *non-trivial* and could inspire new ideas that might not be possible by only focusing on technical innovation in a single field. A recent example is Vision Transformers [a], which applies the same Transformers [b] architecture developed in NLP to image recognition and has now been extended to many other computer vision problems.\n\nWe are the first to successfully apply the concept of prompt learning to computer vision, specifically to adapting large pre-trained vision-language models, which is an important topic essential for democratizing foundation models [c] (i.e., to make these models more accessible to wider communities that do not have sufficient training data and compute resources for pre-training). Technically, we propose two variants of prompt learning: unified context optimization and class-specific context optimization, which are specifically designed for solving image recognition problems. We believe the insights and findings presented in the paper are novel to computer vision and worth sharing with the community.\n\n[a] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. https://openreview.net/forum?id=YicbFdNTTy\n\n[b] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).\n\n[c] Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper270/Authors"
        ]
      },
      {
        "id": "5Kcks_JU1pY",
        "original": null,
        "number": 1,
        "cdate": 1642696849171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696849171,
        "tmdate": 1642696849171,
        "tddate": null,
        "forum": "OgCcfc1m0TO",
        "replyto": "OgCcfc1m0TO",
        "invitation": "ICLR.cc/2022/Conference/Paper270/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "Given the increasing scale of large models (e.g. CLIP), there's an argument that we need better automated techniques for properly utilizing (prompting) these models. Given the success of prompt learning within pure NLP models, the authors apply the same approach to the V+L domain and show that it also is applicable here.  Generally, reviewers felt that the results were clear and thorough, yet technically limited.  The approach is not novel and the result not surprising.  There is a documentary benefit to having this work out in the community for others to reference and extend."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}