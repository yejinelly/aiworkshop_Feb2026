{
  "id": "Xg47v73CDaj",
  "original": "w12LXpj68i4",
  "number": 173,
  "cdate": 1632875434080,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875434080,
  "tmdate": 1697934958121,
  "ddate": null,
  "content": {
    "title": "Non-deep Networks",
    "authorids": [
      "~Ankit_Goyal1",
      "alexeyab84@gmail.com",
      "~Jia_Deng1",
      "~Vladlen_Koltun1"
    ],
    "authors": [
      "Ankit Goyal",
      "Alexey Bochkovskiy",
      "Jia Deng",
      "Vladlen Koltun"
    ],
    "keywords": [
      "non-deep networks"
    ],
    "abstract": "Depth is the hallmark of deep neural networks. But more depth means more sequential computation and higher latency. This begs the question -- is it possible to build high-performing ``non-deep\" neural networks? We show that it is. To do so, we use parallel subnetworks instead of stacking one layer after another. This helps effectively reduce depth while maintaining high performance. By utilizing parallel substructures, we show, for the first time, that a network with a depth of just 12 can achieve top-1 accuracy over 80% on ImageNet, 96% on CIFAR10, and 81% on CIFAR100. We also show that a network with a low-depth (12) backbone can achieve an AP of 48% on MS-COCO. We analyze the scaling rules for our design and show how to increase performance without changing the network's depth. Finally, we provide a proof of concept for how non-deep networks could be used to build low-latency recognition systems. We will open-source our code.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "goyal|nondeep_networks",
    "pdf": "/pdf/2a5d51f1a9ed9313b10b08f3342e0ab14c9e2b1b.pdf",
    "one-sentence_summary": "A non-deep neural network that works surprising well.",
    "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.07641/code)",
    "_bibtex": "@misc{\ngoyal2022nondeep,\ntitle={Non-deep Networks},\nauthor={Ankit Goyal and Alexey Bochkovskiy and Jia Deng and Vladlen Koltun},\nyear={2022},\nurl={https://openreview.net/forum?id=Xg47v73CDaj}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "Xg47v73CDaj",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 11,
    "directReplyCount": 6,
    "revisions": true,
    "replies": [
      {
        "id": "ZIXGSpyZN-m",
        "original": null,
        "number": 1,
        "cdate": 1635689068717,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635689068717,
        "tmdate": 1635689068717,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "Xg47v73CDaj",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes a new architecture of a convolutional neural network for image classification. The authors are motivated by inference efficiency and showing that networks with about a dozen of layers can be competitive in classification accuracy with 50 layers and more. So instead of growing depth, they propose to grow width and have multiple subnetworks of the same depth within one model. The proposed architecture is evaluated on CIFAR, ImageNet and COCO classification and detection tasks, and shows that networks with 12 layers can be competitive with deeper counterparts.",
          "main_review": "Strength:\n- empirical result that low-depth networks can be competitive with deeper models is significant. Confirming observations in prior works, the number of parameters and computational complexity of low-depth networks need to be much higher to match the accuracy of deeper networks.\n\nWeaknesses:\n- architecture complexity: the authors claim outperforming ResNet in efficiency, but do not mention that the architecture is far more complex. It is also more complex than RepVGG. This needs to be stressed in the introduction. This is separate from computational complexity.\n- weak baseline: authors use Squeeze-and-Excitation blocks in their architecture which is known to improve ResNet performance, yet compare to ResNet without SE layers (section 4, tables 1-2)\n- missing baseline: the authors introduce parallel streams in their network, which might be working as an ensemble, known to be improving classification performance on the tested datasets. An important baseline then is treating the stream subnetworks as separate models applied on different resolutions, which is simpler than the proposed architecture and would not suffer from communication overhead. A baseline with ResNet and ParNet ensembles needs to be added.\n- statistical significance: test errors on CIFAR and ImageNet have high variance, and the authors show results of a single run for each experiment. A common practice to reduce variance is to train multiple networks and report mean and standard deviation for each experiment.\n- SiLU: the authors replace ReLU with SiLU motivated by the limited representational power of ReLU. However, SiLU was introduced to improve training of deep networks, which is not the case in the proposed architecture. Moreover, in the ablation study (table 7) it does not seem to bring a significant improvement over ReLU, there is also no statistical significance analysis of this result. Using SiLU seems like a needless complication of the proposed architecture.\n\nArguable weakness:\n- definition of depth: the approach takes advantage of the vague definition of depth in neural networks. In prior works, depth is defined as a number of layers in a single stream network. By introducing multiple parallel streams the authors effectively take a deep network and move layers around, so another, perhaps more fair, definition of depth could be counting the total number of layers in all substreams.\n\nNotes to authors:\n- table 3 is misleading because it does not show the number of GPUs used in the last row. It is also not clear if it is fused or not.",
          "summary_of_the_review": "The paper shows a significant result that a network with 12 layers can be competitive with deeper networks on well established image classification benchmarks. However, the architecture is more complex than ResNet or RepVGG, and the empirical evaluation is unsatisfactory due to weak and missing baselines, and the lack of statistical significance analysis. I hesitate between reject and weak reject.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Reviewer_75Wm"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Reviewer_75Wm"
        ]
      },
      {
        "id": "YW6Ua1IVn6K",
        "original": null,
        "number": 2,
        "cdate": 1635842241963,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635842241963,
        "tmdate": 1636020483003,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "Xg47v73CDaj",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces a way of designing a network fixing the depth yet involving more branches. The authors try to support the claim which is that low-depth networks can achieve a good result by providing experiments on the ImageNet and the CIFAR datasets",
          "main_review": "Pros)\n+ This paper is easy to follow.\n+ The concept of branch-parallelism in a model looks promising.\n\nCons)\n- I am agreeing that the network depth is a crucial design element towards model efficiency but the computational costs in including the number of parameters, FLOPs, and memory footprints seem to be overlooked in this paper: the proposed model has a larger # of parameters and FLOPs, so the overall computational costs are bigger than the baseline models. \n- The architectural design with the proposed building block in Figure 2 is presented without providing any intuitions or design philosophy.\n- The comparisons with the baselines in model-parallelism is not sufficient\n\nSome comments and questions)\n- The model-parallelism can be done with a single-branch model in training or inferring with multiple images (i.e., with a larger mini-batch size), then the speed gain from the branch-parallelism scheme may vanish. Please clarify a multi-branch architecture also has an advantage over the widened networks such as WideResNet in this case.\n\n- The performance comparison should be done fairly. For example in Table 2, ParNet-L is compared with much smaller networks such as R34 and R50 in terms of the computational budgets, so this is not fair. The models with widening the width while fixing depth (e.g., Wide ResNet) can achieve much better accuracy barely increase the latency.\n\n- Memory consumption would be a matter for the proposed architecture. Please specify the memory footprints when training (or inference) with the fixed batchsize.\n\n- ResNets can also be leveraged fusing methods including skip connections and BNs. Did the authors compare the latency with these fused ResNets with the proposed ParNets in Table 2?\n\n- Why vanilla SE-block is not suitable for the proposed model? Why Rep-VGG block has been adopted?\n\n- Please specify why ParNets cannot outperform DenseNet-100 even using more parameters on the CIFAR datasets in Table 6. I am just wondering whether there is a different earning behavior of a shallow network on a particular dataset.",
          "summary_of_the_review": "- See the main review",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Reviewer_pTPq"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Reviewer_pTPq"
        ]
      },
      {
        "id": "4UU6UoEazZs",
        "original": null,
        "number": 3,
        "cdate": 1635914309693,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635914309693,
        "tmdate": 1635949151179,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "Xg47v73CDaj",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Review",
        "content": {
          "summary_of_the_paper": "- **Motivation**. The paper argues that deep networks have several limitations\n  - (a) deep nets have a higher latency;\n  - (b) deep nets are hard to parallelize;\n  - (c) deep nets are not suitable for applications.\n\n\n- **Method**.\nMotivated by these observations, the paper aims to fill the performance gap between shallow networks and deep networks. The paper proposed a 12-layer shallow model framework, which contains proposed RepVGG-SSE blocks, fusion modules for multi-scale processing, and parallel streams.  \n\n\n- **Experiments**.\nThe proposed model, ParNet, is verified on CIFAR-10, CIFAR-100, and ImageNet for classification, MS-COCO for detection. ",
          "main_review": "I have four considerations:\n- (a) Table 2 only shows the details for ParNet-L and -XL. It could be better to show the full details and comparisons of ParNet-S, -M, -L, and -XL.\n- (b) What are the details of speed testing in Table 2? Do the ResNet and ParNet use the same setting for speed benchmark? If ParNet uses multiple GPUs in parallel to benchmark the inference speed, e.g., 2 GPUs with 128 global batch size, does the ResNet uses 2 GPUs within the global batch size of 128 as well? It could be better to add more words in the third paragraph on Page 6 to show how to conduct the experiments. \n- (c) ParNet develops the RepVGG-SSE based on the work of RepVGG, CVPR 2021. But there is no comparison with this high-related work. It could be better to compare with RepVGG in Table 1 and Table 2.\n- (d) The paper argues that one of the advantages is parallel. But according to Table 2 and RepVGG's Table 4, ParNet has more parameters and larger FLOPs than the competitors. More parameters mean that the model will cost more memory during training and inference. This fact fades the significance of ParNet. It could be better to discuss this and highlight why parallelism is more important than the other two aspects for selling the work.",
          "summary_of_the_review": "Please erase the above concerns. ",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Reviewer_Z9wZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Reviewer_Z9wZ"
        ]
      },
      {
        "id": "UpGm5fyJKpK",
        "original": null,
        "number": 4,
        "cdate": 1636033315561,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636033315561,
        "tmdate": 1636033315561,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "Xg47v73CDaj",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes to manually design a new 12-depth CNN architecture ParNet based on parallel subnetworks instead of traditionally deeply stacked blocks. Experiments show that ParNet is the first CNN achieving over 80% accuracy on ImageNet with 12 depth only. ParNet also achieves a competitive AP of 48% on MS-COCO for object detection.",
          "main_review": "1) My major concern is that the authors argues contribution of parallel subnetworks, while I think the traditional RepVGG-wise blocks adopted in ParNet contributes significantly more to prediction accuracy. Even if ParNet adopts 1 branch only, ParNet's accuracy can still achieve 75% more, as \"ParNet-M-OneStream\" shown in Table 8. Comparison of Table 7 and Table 10 shows that the RepVGG blocks is significantly more important than adopted number of branches on improving accuracy. \n\n2) As shown in Table 2, ParNet-L/ParNet-XL (12-depth) indeed has fewer depth compared to ResNet50 (50-depth). However, ParNet-L (54.9M)/ParNet-XL (85.0M) has significantly more parameters compared to ResNet50 (25.6M). A potential problem is that if ParNet runs on low-power computing hardware with limited cuda cores, I am not sure whether ParNet still has the advantage on speed compared to ResNet50.\n\n3) The paper proposes a new block RepVGG-SSE by introducing SSE into traditional RepVGG, while ParNet's accuracy improvement from SSE is only 1.53%, as shown in Table 7. Therefore, I think contribution of RepVGG SSE is not sufficient enough.",
          "summary_of_the_review": "The authors argues contribution of parallel subnetworks, while the experiments show that the traditional RepVGG-wise blocks adopted in ParNet contributes significantly more to prediction accuracy. Therefor, my rating is \"5: marginally below the acceptance threshold\".\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Reviewer_wcwX"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Reviewer_wcwX"
        ]
      },
      {
        "id": "2sDbocQo8_y",
        "original": null,
        "number": 3,
        "cdate": 1637750156887,
        "mdate": 1637750156887,
        "ddate": null,
        "tcdate": 1637750156887,
        "tmdate": 1637750156887,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "Xg47v73CDaj",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Comment",
        "content": {
          "title": "Overall Response",
          "comment": "We thank the reviewers for their feedback and help in improving our work. We are excited that the reviewers found our results to be significant and our idea promising. \n\nHowever, we believe that our paper is being evaluated as  a conventional neural network architecture design paper. Hence concerns have been raised about: first, novelty of the building block and second, compute (parameter count, flops, memory). We acknowledge that these  could be valid criteria for many works,  but it is not suitable for our work as it does not take into account our primary contribution.\n\nOur work is motivated by the important scientific question \u201cis it possible to build high-performing non-deep neural networks?\u201d We show that it is possible to do so by adapting tools present in the literature. Although the tools are present in the literature, the result about high-performing non-deep networks is not. This result is our novel contribution.\n\nFor the second concern regarding compute, we acknowledge that currently non-deep networks are not a replacement for their deep counterparts in low-compute settings and further investigation is required in this direction. However, as explained in the paper, the trend in hardware accelerators (like GPUs) suggests that processor frequency is getting limited while the number of cores and memory is rising. This is favourable for non-deep networks as they depend less on processor frequency because of less depth; and more on the number of cores because of wide parallel processing. Considering this trend, non-deep networks are promising considering tomorrow\u2019s hardware, even though they consume more compute (flops and parameters). \n\nWe will update our introduction and abstract to further clarify this. Below we have addressed the individual concerns of the reviewers.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ]
      },
      {
        "id": "ujvZmfJN1YH",
        "original": null,
        "number": 4,
        "cdate": 1637750549575,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637750549575,
        "tmdate": 1637750636551,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "UpGm5fyJKpK",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer wcwX",
          "comment": "Thank you so much feedback and suggestions. Following we have address your concerns:\n\n**Concern:** \"My major concern is that the authors argues contribution of parallel subnetworks, while I think the traditional RepVGG-wise blocks adopted in ParNet contributes significantly more to prediction accuracy. Even if ParNet adopts 1 branch only, ParNet's accuracy can still achieve 75% more, as \"ParNet-M-OneStream\" shown in Table 8. Comparison of Table 7 and Table 10 shows that the RepVGG blocks is significantly more important than adopted number of branches on improving accuracy.\"\n\n**Response:** We agree that the RepVGG-wise blocks adopted in ParNet are significant in our network. However, our contribution is in showing that RepVGG blocks, along with parallel sub-structures, can be used to build high-performing non-deep networks. As explained in the overall response, our novel contribution is the result, not any particular design choice. We will update our introduction to clarify this.\n\n**Concern:**  \"As shown in Table 2, ParNet-L/ParNet-XL (12-depth) indeed has fewer depth compared to ResNet50 (50-depth). However, ParNet-L (54.9M)/ParNet-XL (85.0M) has significantly more parameters compared to ResNet50 (25.6M). A potential problem is that if ParNet runs on low-power computing hardware with limited cuda cores, I am not sure whether ParNet still has the advantage on speed compared to ResNet50\"\n\n**Response:** We agree that the higher parameter count in ParNet makes it not suitable for cases like low-power computing hardware. But as explained in the overall response, non-deep networks are suitable for future hardware with more cores and limited processor frequency. Further, there exists an exciting body of complementary techniques like pruning and distillation to reduce the parameter count and memory of a model.\n\n**Concern:** \"The paper proposes a new block RepVGG-SSE by introducing SSE into traditional RepVGG, while ParNet's accuracy improvement from SSE is only 1.53%, as shown in Table 7. Therefore, I think contribution of RepVGG SSE is not sufficient enough.\"\n\n**Response:** SSE helps in improving performance while not increasing depth. Since our objective is to build best performing non-deep networks, a 1.53% improvement on a large scale dataset like ImageNet is significant."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ]
      },
      {
        "id": "JHPd68TUEcE",
        "original": null,
        "number": 5,
        "cdate": 1637750819050,
        "mdate": 1637750819050,
        "ddate": null,
        "tcdate": 1637750819050,
        "tmdate": 1637750819050,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "4UU6UoEazZs",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer Z9wZ",
          "comment": "Thank you so much feedback and suggestions. Following we have address your concerns:\n\n**Concern:** \"Table 2 only shows the details for ParNet-L and -XL. It could be better to show the full details and comparisons of ParNet-S, -M, -L, and -XL\"\n\n**Response:** Thanks for the suggestion. We will do so in an updated version of the paper. Specifically, ParNet-S has an Top1 accuracy of 75.19% with speed of 280 samples/s , 19.2 millions parameters and 9.7 billions flops ; and ParNet-M has an Top1 accuracy of 76.60% with speed of 265 samples/s, 35.6 millions parameters and 17.2 billions flops.  \n\n**Concern:** \"What are the details of speed testing in Table 2? Do the ResNet and ParNet use the same setting for speed benchmark? If ParNet uses multiple GPUs in parallel to benchmark the inference speed, e.g., 2 GPUs with 128 global batch size, does the ResNet uses 2 GPUs within the global batch size of 128 as well? It could be better to add more words in the third paragraph on Page 6 to show how to conduct the experiments.\"\n\n**Response:** Thanks for pointing out the confusion. We evaluate the latency of the models where samples are coming one at a time like in robotics and autonomous driving. Hence, the batch size in all evaluations is 1. We will clarify it in the paper.\n\n**Concern:** \"ParNet develops the RepVGG-SSE based on the work of RepVGG, CVPR 2021. But there is no comparison with this high-related work. It could be better to compare with RepVGG in Table 1 and Table 2\"\n\n**Response:** Thanks for the great suggestion. We missed this comparison. But we have  this comparison now and find that for the same accuracy, RepVGG is 1.3 times faster than ParNet for large model (ParNet-XL vs RepVGG_b2g4) and by 1.5 times faster for medium-sized model (ParNet-L vs RepVGG_b1g4). We will add this comparison in the paper. \n\nRepVGG, although being 2.5 times deeper than ParNet, is faster as it consists of only optimized layers like 3X3 convolution and ReLU. However, as explained in the overall response, our contribution is in answering the scientific question of whether non-deep networks can achieve high performance on large-scale benchmarks. Further, non-deep networks are promising considering future hardware.\n\n**Concern:** \"The paper argues that one of the advantages is parallel. But according to Table 2 and RepVGG's Table 4, ParNet has more parameters and larger FLOPs than the competitors. More parameters mean that the model will cost more memory during training and inference. This fact fades the significance of ParNet. It could be better to discuss this and highlight why parallelism is more important than the other two aspects for selling the work.\"\n\n**Response:** We acknowledge that ParNet consumes more memory and parameters than deeper counterparts. But, as explained in the overall response, the trend in hardware accelerators suggests that tomorrow\u2019s processors will have more cores and limited processor frequency. Hence, non-deep networks with more parallelism will benefit in spite of having more parameters and larger FLOPS."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ]
      },
      {
        "id": "0uYlR-2QWLW",
        "original": null,
        "number": 6,
        "cdate": 1637751109922,
        "mdate": 1637751109922,
        "ddate": null,
        "tcdate": 1637751109922,
        "tmdate": 1637751109922,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "YW6Ua1IVn6K",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer pTPq (Part 1/2)",
          "comment": "Thank you so much feedback and suggestions. Following we have address your concerns:\n\n**Concern:** \"I am agreeing that the network depth is a crucial design element towards model efficiency but the computational costs in including the number of parameters, FLOPs, and memory footprints seem to be overlooked in this paper: the proposed model has a larger # of parameters and FLOPs, so the overall computational costs are bigger than the baseline models.\"\n\n**Response:** We acknowledge that ParNet consumes more memory and parameters than deeper counterparts.  But, as explained in the overall response, the trend in hardware accelerators suggests that tomorrow\u2019s processors will have more cores and limited processor frequency. Hence, non-deep networks with more parallelism will benefit in spite of having more parameters and larger FLOPS. Further, there exists a large body of complementary techniques like pruning and distillation to build low parameter count and memory models. \n\n**Concern:** \"The architectural design with the proposed building block in Figure 2 is presented without providing any intuitions or design philosophy.\"\n\n**Response:** The motivating philosophy of our architectural design is to build the lowest depth network. So we move the sequential processing units into parallel units. Surprisingly, even with these parallel units, the network is able to achieve high performance on large-scale benchmarks like ImageNet.\n\n**Concern:** \"The comparisons with the baselines in model-parallelism is not sufficient. The model-parallelism can be done with a single-branch model in training or inferring with multiple images (i.e., with a larger mini-batch size), then the speed gain from the branch-parallelism scheme may vanish. Please clarify a multi-branch architecture also has an advantage over the widened networks such as WideResNet in this case.\"\n\n**Response:** We evaluate the latency of the models where samples are coming one at a time, like in robotics and autonomous driving. The batch size in all evaluations is 1. It is non-trivial to parallelize single-branch models across GPUs for batch size 1. In Table 8, we find multiple branches perform better than widened networks under the same parameter budget.\n\n**Concern:** \"The performance comparison should be done fairly. For example in Table 2, ParNet-L is compared with much smaller networks such as R34 and R50 in terms of the computational budgets, so this is not fair. The models with widening the width while fixing depth (e.g., Wide ResNet) can achieve much better accuracy barely increase the latency.\"\n\n**Response:** In Table 2, we compared models with similar accuracy and speed. As suggested by the reviewer, we will also add numbers for ParNet-S and ParNet-M. Specifically, ParNet-S has an Top1 accuracy of 75.19% with speed of 280 samples/s , 19.2 millions parameters and 9.7 billions flops ; and ParNet-M has an Top1 accuracy of 76.60% with speed of 265 samples/s, 35.6 millions parameters and 17.2 billions flops.   \n\nIn Table 8, we compare with wide variants of ResNet and find that RepVGG-SSE blocks perform better. \n\n**Concern:** \"Memory consumption would be a matter for the proposed architecture. Please specify the memory footprints when training (or inference) with the fixed batchsize.\"\n\n**Response:** Thanks for the suggestion. We now measure the memory consumption by comparing the largest batchsize that could fit in memory while inference We find that ParNet-L can use 1.25x times more memory than ResNet50.\n\n**Concern:**  \"ResNets can also be leveraged fusing methods including skip connections and BNs. Did the authors compare the latency with these fused ResNets with the proposed ParNets in Table 2?\"\n\n**Response:**  It is not possible to fuse the skip connection in ResNet as there is a non-linearity between the two convolution layers. However, as suggested, the BN in ResNet can be fused with a Convolution Layer. After doing this the speed of ResNets is increased but ParNet is still competitive. Specifically, ParNet-L achieves a top-1 accuracy of 77.66% and speed of 250 samples/s while ResNet-50 achieves a top-1 accuracy of 77.5% and speed of 240 samples/s.\n\n**Concern:**  \"Why vanilla SE-block is not suitable for the proposed model? Why Rep-VGG block has been adopted?\"\n\n**Response:** Vanilla SE-block increases depth of each block by 2 as it is applied sequentially after the convolution layer. Since we wanted to build a model with lower depth, we used the Skip SE variant which does not increase the depth of the model but provides the advantage of an SE block."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ]
      },
      {
        "id": "LWVk96UK8BI",
        "original": null,
        "number": 7,
        "cdate": 1637751136037,
        "mdate": 1637751136037,
        "ddate": null,
        "tcdate": 1637751136037,
        "tmdate": 1637751136037,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "0uYlR-2QWLW",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer pTPq (Part 2/2)",
          "comment": "**Concern:** \"Please specify why ParNets cannot outperform DenseNet-100 even using more parameters on the CIFAR datasets in Table 6. I am just wondering whether there is a different earning behavior of a shallow network on a particular dataset.\"\n \n**Response:** A possible explanation for this is that deeper networks are more parameter efficient. Further, even among deep networks, it is empirically observed that DenseNet is more parameter efficient. Also, as suggested by the reviewer, parameter efficiency of shallow vs deep networks could depend on the property of the dataset as well. This is an exciting direction to explore, however, it is not the focus of our work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ]
      },
      {
        "id": "EE3_bBuhKHg",
        "original": null,
        "number": 8,
        "cdate": 1637751424668,
        "mdate": 1637751424668,
        "ddate": null,
        "tcdate": 1637751424668,
        "tmdate": 1637751424668,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "ZIXGSpyZN-m",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 75Wm",
          "comment": "Thank you so much feedback and suggestions. Following we have address your concerns:\n\n**Concern:** \"architecture complexity: the authors claim outperforming ResNet in efficiency, but do not mention that the architecture is far more complex. It is also more complex than RepVGG. This needs to be stressed in the introduction. This is separate from computational complexity.\"\n\n**Response:** Exploring simple architectures is an exciting direction for research. Further investigation is required to build simpler non-deep networks, but this is not the focus of our work. We believe our work will enable progress in this direction by demonstrating the existence of high-performing non-deep networks.\n\n**Concern:** \"weak baseline: authors use Squeeze-and-Excitation blocks in their architecture which is known to improve ResNet performance, yet compare to ResNet without SE layers (section 4, tables 1-2)\"\n\n**Response:** \"We compared with ResNets that use the Squeeze-and-Excitation layers in Table 8. For the same depth and similar parameter count, the RepVGG block performs better than the ResNet block.\"\n\n**Concern:** \"missing baseline: the authors introduce parallel streams in their network, which might be working as an ensemble, known to be improving classification performance on the tested datasets. An important baseline then is treating the stream subnetworks as separate models applied on different resolutions, which is simpler than the proposed architecture and would not suffer from communication overhead. A baseline with ResNet and ParNet ensembles needs to be added.\"\n\n**Response:** \"Thanks for the suggestion. We compared ParNet with ensembles for the same resolution in Table 9. Unfortunately, we could not complete this experiment now because of computational constraints.\"\n\n**Concern:** \"statistical significance: test errors on CIFAR and ImageNet have high variance, and the authors show results of a single run for each experiment. A common practice to reduce variance is to train multiple networks and report mean and standard deviation for each experiment.\"\n\n**Response:** \"We trained some representative models and did not see much fluctuation in accuracy. Specifically, the standard deviation in accuracy for the larger ParNet model on CIFAR10 and CIFAR100 was 0.13% and 0.19% respectively. Similarly, the standard deviation for ParNet-M on Imagenet was 0.1%\"\n\n**Concern:** \"SiLU: the authors replace ReLU with SiLU motivated by the limited representational power of ReLU. However, SiLU was introduced to improve training of deep networks, which is not the case in the proposed architecture. Moreover, in the ablation study (table 7) it does not seem to bring a significant improvement over ReLU, there is also no statistical significance analysis of this result. Using SiLU seems like a needless complication of the proposed architecture.\"\n\n**Response:** Thanks for the great suggestion. Removing SiLU can make our model faster and more useful for low end devices without losing much accuracy. We will do so in an updated version.\n\n**Concern:** \"definition of depth: the approach takes advantage of the vague definition of depth in neural networks. In prior works, depth is defined as a number of layers in a single stream network. By introducing multiple parallel streams the authors effectively take a deep network and move layers around, so another, perhaps more fair, definition of depth could be counting the total number of layers in all substreams.\"\n\n**Response:** We disagree that our definition of depth is not fair. Depth is the minimum number of layers that must be run sequentially to get the output from input.\n\n**Concern:** \"table 3 is misleading because it does not show the number of GPUs used in the last row. It is also not clear if it is fused or not.\"\n\n**Response:** Thanks for pointing out the confusion. We will update table 3 to clarify it further. Specifically, row 3 uses 3 GPUs and the fused model. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ]
      },
      {
        "id": "jbBvuJqEU3",
        "original": null,
        "number": 1,
        "cdate": 1642696842307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696842307,
        "tmdate": 1642696842307,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "Xg47v73CDaj",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "This paper shows the possibility to design a relatively shallow architecture, ParNet, based on parallel subnetworks, instead of traditionally deeply stacked blocks. During discussions, the reviewers pointed out two important concerns: (1) the current design heavily hinges on the recently proposed RepVGG block, whose comparison was even missed in the original submission (later added in rebuttal); (2) comparing ParNet with RepVGG, there seems no performance advantage. Although RepVGG is 2.5 times deeper than ParNet, it is still faster due to highly optimized layers.\n\nThe authors mainly argued that their contribution is to answer the scientific question \u201cis it possible to build high-performing non-deep neural networks?\u201d While this is indeed an interesting question, AC feels: (1) it is perhaps unfair for this paper to claim as the first work proving the feasibility. WideResNet provided similar insight much earlier, among others; (2) the presented results, with tools being not novel, are pre-mature as they display no real appeal of using ParNet, in any aspect. Probing a new question is of course valuable, but presenting an immature and novelty-lacking answer shouldn't automatically grant publication. \n\nIn sum, the reviewers were unanimously UN-convinced by this paper's value, nor was the AC. The authors are suggested to very seriously take into account reviewers' suggestions to make improvements, before submitting their work to the next venue."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "ZIXGSpyZN-m",
        "original": null,
        "number": 1,
        "cdate": 1635689068717,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635689068717,
        "tmdate": 1635689068717,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "Xg47v73CDaj",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes a new architecture of a convolutional neural network for image classification. The authors are motivated by inference efficiency and showing that networks with about a dozen of layers can be competitive in classification accuracy with 50 layers and more. So instead of growing depth, they propose to grow width and have multiple subnetworks of the same depth within one model. The proposed architecture is evaluated on CIFAR, ImageNet and COCO classification and detection tasks, and shows that networks with 12 layers can be competitive with deeper counterparts.",
          "main_review": "Strength:\n- empirical result that low-depth networks can be competitive with deeper models is significant. Confirming observations in prior works, the number of parameters and computational complexity of low-depth networks need to be much higher to match the accuracy of deeper networks.\n\nWeaknesses:\n- architecture complexity: the authors claim outperforming ResNet in efficiency, but do not mention that the architecture is far more complex. It is also more complex than RepVGG. This needs to be stressed in the introduction. This is separate from computational complexity.\n- weak baseline: authors use Squeeze-and-Excitation blocks in their architecture which is known to improve ResNet performance, yet compare to ResNet without SE layers (section 4, tables 1-2)\n- missing baseline: the authors introduce parallel streams in their network, which might be working as an ensemble, known to be improving classification performance on the tested datasets. An important baseline then is treating the stream subnetworks as separate models applied on different resolutions, which is simpler than the proposed architecture and would not suffer from communication overhead. A baseline with ResNet and ParNet ensembles needs to be added.\n- statistical significance: test errors on CIFAR and ImageNet have high variance, and the authors show results of a single run for each experiment. A common practice to reduce variance is to train multiple networks and report mean and standard deviation for each experiment.\n- SiLU: the authors replace ReLU with SiLU motivated by the limited representational power of ReLU. However, SiLU was introduced to improve training of deep networks, which is not the case in the proposed architecture. Moreover, in the ablation study (table 7) it does not seem to bring a significant improvement over ReLU, there is also no statistical significance analysis of this result. Using SiLU seems like a needless complication of the proposed architecture.\n\nArguable weakness:\n- definition of depth: the approach takes advantage of the vague definition of depth in neural networks. In prior works, depth is defined as a number of layers in a single stream network. By introducing multiple parallel streams the authors effectively take a deep network and move layers around, so another, perhaps more fair, definition of depth could be counting the total number of layers in all substreams.\n\nNotes to authors:\n- table 3 is misleading because it does not show the number of GPUs used in the last row. It is also not clear if it is fused or not.",
          "summary_of_the_review": "The paper shows a significant result that a network with 12 layers can be competitive with deeper networks on well established image classification benchmarks. However, the architecture is more complex than ResNet or RepVGG, and the empirical evaluation is unsatisfactory due to weak and missing baselines, and the lack of statistical significance analysis. I hesitate between reject and weak reject.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Reviewer_75Wm"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Reviewer_75Wm"
        ]
      },
      {
        "id": "YW6Ua1IVn6K",
        "original": null,
        "number": 2,
        "cdate": 1635842241963,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635842241963,
        "tmdate": 1636020483003,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "Xg47v73CDaj",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces a way of designing a network fixing the depth yet involving more branches. The authors try to support the claim which is that low-depth networks can achieve a good result by providing experiments on the ImageNet and the CIFAR datasets",
          "main_review": "Pros)\n+ This paper is easy to follow.\n+ The concept of branch-parallelism in a model looks promising.\n\nCons)\n- I am agreeing that the network depth is a crucial design element towards model efficiency but the computational costs in including the number of parameters, FLOPs, and memory footprints seem to be overlooked in this paper: the proposed model has a larger # of parameters and FLOPs, so the overall computational costs are bigger than the baseline models. \n- The architectural design with the proposed building block in Figure 2 is presented without providing any intuitions or design philosophy.\n- The comparisons with the baselines in model-parallelism is not sufficient\n\nSome comments and questions)\n- The model-parallelism can be done with a single-branch model in training or inferring with multiple images (i.e., with a larger mini-batch size), then the speed gain from the branch-parallelism scheme may vanish. Please clarify a multi-branch architecture also has an advantage over the widened networks such as WideResNet in this case.\n\n- The performance comparison should be done fairly. For example in Table 2, ParNet-L is compared with much smaller networks such as R34 and R50 in terms of the computational budgets, so this is not fair. The models with widening the width while fixing depth (e.g., Wide ResNet) can achieve much better accuracy barely increase the latency.\n\n- Memory consumption would be a matter for the proposed architecture. Please specify the memory footprints when training (or inference) with the fixed batchsize.\n\n- ResNets can also be leveraged fusing methods including skip connections and BNs. Did the authors compare the latency with these fused ResNets with the proposed ParNets in Table 2?\n\n- Why vanilla SE-block is not suitable for the proposed model? Why Rep-VGG block has been adopted?\n\n- Please specify why ParNets cannot outperform DenseNet-100 even using more parameters on the CIFAR datasets in Table 6. I am just wondering whether there is a different earning behavior of a shallow network on a particular dataset.",
          "summary_of_the_review": "- See the main review",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Reviewer_pTPq"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Reviewer_pTPq"
        ]
      },
      {
        "id": "4UU6UoEazZs",
        "original": null,
        "number": 3,
        "cdate": 1635914309693,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635914309693,
        "tmdate": 1635949151179,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "Xg47v73CDaj",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Review",
        "content": {
          "summary_of_the_paper": "- **Motivation**. The paper argues that deep networks have several limitations\n  - (a) deep nets have a higher latency;\n  - (b) deep nets are hard to parallelize;\n  - (c) deep nets are not suitable for applications.\n\n\n- **Method**.\nMotivated by these observations, the paper aims to fill the performance gap between shallow networks and deep networks. The paper proposed a 12-layer shallow model framework, which contains proposed RepVGG-SSE blocks, fusion modules for multi-scale processing, and parallel streams.  \n\n\n- **Experiments**.\nThe proposed model, ParNet, is verified on CIFAR-10, CIFAR-100, and ImageNet for classification, MS-COCO for detection. ",
          "main_review": "I have four considerations:\n- (a) Table 2 only shows the details for ParNet-L and -XL. It could be better to show the full details and comparisons of ParNet-S, -M, -L, and -XL.\n- (b) What are the details of speed testing in Table 2? Do the ResNet and ParNet use the same setting for speed benchmark? If ParNet uses multiple GPUs in parallel to benchmark the inference speed, e.g., 2 GPUs with 128 global batch size, does the ResNet uses 2 GPUs within the global batch size of 128 as well? It could be better to add more words in the third paragraph on Page 6 to show how to conduct the experiments. \n- (c) ParNet develops the RepVGG-SSE based on the work of RepVGG, CVPR 2021. But there is no comparison with this high-related work. It could be better to compare with RepVGG in Table 1 and Table 2.\n- (d) The paper argues that one of the advantages is parallel. But according to Table 2 and RepVGG's Table 4, ParNet has more parameters and larger FLOPs than the competitors. More parameters mean that the model will cost more memory during training and inference. This fact fades the significance of ParNet. It could be better to discuss this and highlight why parallelism is more important than the other two aspects for selling the work.",
          "summary_of_the_review": "Please erase the above concerns. ",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Reviewer_Z9wZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Reviewer_Z9wZ"
        ]
      },
      {
        "id": "UpGm5fyJKpK",
        "original": null,
        "number": 4,
        "cdate": 1636033315561,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636033315561,
        "tmdate": 1636033315561,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "Xg47v73CDaj",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes to manually design a new 12-depth CNN architecture ParNet based on parallel subnetworks instead of traditionally deeply stacked blocks. Experiments show that ParNet is the first CNN achieving over 80% accuracy on ImageNet with 12 depth only. ParNet also achieves a competitive AP of 48% on MS-COCO for object detection.",
          "main_review": "1) My major concern is that the authors argues contribution of parallel subnetworks, while I think the traditional RepVGG-wise blocks adopted in ParNet contributes significantly more to prediction accuracy. Even if ParNet adopts 1 branch only, ParNet's accuracy can still achieve 75% more, as \"ParNet-M-OneStream\" shown in Table 8. Comparison of Table 7 and Table 10 shows that the RepVGG blocks is significantly more important than adopted number of branches on improving accuracy. \n\n2) As shown in Table 2, ParNet-L/ParNet-XL (12-depth) indeed has fewer depth compared to ResNet50 (50-depth). However, ParNet-L (54.9M)/ParNet-XL (85.0M) has significantly more parameters compared to ResNet50 (25.6M). A potential problem is that if ParNet runs on low-power computing hardware with limited cuda cores, I am not sure whether ParNet still has the advantage on speed compared to ResNet50.\n\n3) The paper proposes a new block RepVGG-SSE by introducing SSE into traditional RepVGG, while ParNet's accuracy improvement from SSE is only 1.53%, as shown in Table 7. Therefore, I think contribution of RepVGG SSE is not sufficient enough.",
          "summary_of_the_review": "The authors argues contribution of parallel subnetworks, while the experiments show that the traditional RepVGG-wise blocks adopted in ParNet contributes significantly more to prediction accuracy. Therefor, my rating is \"5: marginally below the acceptance threshold\".\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Reviewer_wcwX"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Reviewer_wcwX"
        ]
      },
      {
        "id": "2sDbocQo8_y",
        "original": null,
        "number": 3,
        "cdate": 1637750156887,
        "mdate": 1637750156887,
        "ddate": null,
        "tcdate": 1637750156887,
        "tmdate": 1637750156887,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "Xg47v73CDaj",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Official_Comment",
        "content": {
          "title": "Overall Response",
          "comment": "We thank the reviewers for their feedback and help in improving our work. We are excited that the reviewers found our results to be significant and our idea promising. \n\nHowever, we believe that our paper is being evaluated as  a conventional neural network architecture design paper. Hence concerns have been raised about: first, novelty of the building block and second, compute (parameter count, flops, memory). We acknowledge that these  could be valid criteria for many works,  but it is not suitable for our work as it does not take into account our primary contribution.\n\nOur work is motivated by the important scientific question \u201cis it possible to build high-performing non-deep neural networks?\u201d We show that it is possible to do so by adapting tools present in the literature. Although the tools are present in the literature, the result about high-performing non-deep networks is not. This result is our novel contribution.\n\nFor the second concern regarding compute, we acknowledge that currently non-deep networks are not a replacement for their deep counterparts in low-compute settings and further investigation is required in this direction. However, as explained in the paper, the trend in hardware accelerators (like GPUs) suggests that processor frequency is getting limited while the number of cores and memory is rising. This is favourable for non-deep networks as they depend less on processor frequency because of less depth; and more on the number of cores because of wide parallel processing. Considering this trend, non-deep networks are promising considering tomorrow\u2019s hardware, even though they consume more compute (flops and parameters). \n\nWe will update our introduction and abstract to further clarify this. Below we have addressed the individual concerns of the reviewers.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper173/Authors"
        ]
      },
      {
        "id": "jbBvuJqEU3",
        "original": null,
        "number": 1,
        "cdate": 1642696842307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696842307,
        "tmdate": 1642696842307,
        "tddate": null,
        "forum": "Xg47v73CDaj",
        "replyto": "Xg47v73CDaj",
        "invitation": "ICLR.cc/2022/Conference/Paper173/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "This paper shows the possibility to design a relatively shallow architecture, ParNet, based on parallel subnetworks, instead of traditionally deeply stacked blocks. During discussions, the reviewers pointed out two important concerns: (1) the current design heavily hinges on the recently proposed RepVGG block, whose comparison was even missed in the original submission (later added in rebuttal); (2) comparing ParNet with RepVGG, there seems no performance advantage. Although RepVGG is 2.5 times deeper than ParNet, it is still faster due to highly optimized layers.\n\nThe authors mainly argued that their contribution is to answer the scientific question \u201cis it possible to build high-performing non-deep neural networks?\u201d While this is indeed an interesting question, AC feels: (1) it is perhaps unfair for this paper to claim as the first work proving the feasibility. WideResNet provided similar insight much earlier, among others; (2) the presented results, with tools being not novel, are pre-mature as they display no real appeal of using ParNet, in any aspect. Probing a new question is of course valuable, but presenting an immature and novelty-lacking answer shouldn't automatically grant publication. \n\nIn sum, the reviewers were unanimously UN-convinced by this paper's value, nor was the AC. The authors are suggested to very seriously take into account reviewers' suggestions to make improvements, before submitting their work to the next venue."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}