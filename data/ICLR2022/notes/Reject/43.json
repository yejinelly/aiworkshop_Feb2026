{
  "id": "ZaI7Rd11G4S",
  "original": "u7D5Mg5K3zv",
  "number": 43,
  "cdate": 1632875424534,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875424534,
  "tmdate": 1676330691151,
  "ddate": null,
  "content": {
    "title": "Embedding Compression with Hashing for Efficient Representation Learning in Graph",
    "authorids": [
      "~Chin-Chia_Michael_Yeh1",
      "~Mengting_Gu1",
      "~Yan_Zheng2",
      "~Huiyuan_Chen1",
      "~Javid_Ebrahimi1",
      "zzhuang@visa.com",
      "~Junpeng_Wang1",
      "~Liang_Wang11",
      "~Wei_Zhang52"
    ],
    "authors": [
      "Chin-Chia Michael Yeh",
      "Mengting Gu",
      "Yan Zheng",
      "Huiyuan Chen",
      "Javid Ebrahimi",
      "Zhongfang Zhuang",
      "Junpeng Wang",
      "Liang Wang",
      "Wei Zhang"
    ],
    "keywords": [
      "embedding compression",
      "hashing",
      "graph"
    ],
    "abstract": "Graph neural networks (GNNs) are deep learning models designed specifically for graph data, and they typically rely on node features as the input node representation to the first layer. When applying such type of networks on graph without node feature, one can extract simple graph-based node features (e.g., number of degrees) or learn the input node representation (i.e., embeddings) when training the network. While the latter approach, which trains node embeddings, more likely leads to better performance, the number of parameters associated with the embeddings grows linearly with the number of nodes. It is therefore impractical to train the input node embeddings together with GNNs within graphics processing unit (GPU) memory in an end-to-end fashion when dealing with industrial scale graph data. Inspired by the embedding compression methods developed for natural language processing (NLP) models, we develop a node embedding compression method where each node is compactly represented with a bit vector instead of a float-point vector. The parameters utilized in the compression method can be trained together with GNNs. We show that the proposed node embedding compression method achieves superior performance compared to the alternatives.",
    "pdf": "/pdf/089fb8e142ac9bae09579dd4dd32ee4de48fbaea.pdf",
    "one-sentence_summary": "A embedding compression method is developed to represent each node in a graph compactly for graph neural networks like GraphSage.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "yeh|embedding_compression_with_hashing_for_efficient_representation_learning_in_graph",
    "_bibtex": "@misc{\nyeh2022embedding,\ntitle={Embedding Compression with Hashing for Efficient Representation Learning in Graph},\nauthor={Chin-Chia Michael Yeh and Mengting Gu and Yan Zheng and Huiyuan Chen and Javid Ebrahimi and Zhongfang Zhuang and Junpeng Wang and Liang Wang and Wei Zhang},\nyear={2022},\nurl={https://openreview.net/forum?id=ZaI7Rd11G4S}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "ZaI7Rd11G4S",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 11,
    "directReplyCount": 5,
    "revisions": true,
    "replies": [
      {
        "id": "cpdPjBcTC1U",
        "original": null,
        "number": 1,
        "cdate": 1635702231212,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635702231212,
        "tmdate": 1635702231212,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "ZaI7Rd11G4S",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this paper, the authors present a general and scalable approach to obtain shallow embeddings of many entities. The authors achieve this by first mapping each entity into a code vector using LSH, and then learning a neural decoder on top of the code vectors to reconstruct the original embeddings/adjacency matrix. The authors then apply their approach to node classification problems, where node features are unavailable (the authors ended up artificially removing node features from existing OGB datasets). The superior performance is demonstrated over the random code vector baseline.",
          "main_review": "Strength\n- The paper is well-written and easy to follow.\n- Proposed approach can directly hash the adjacency matrix (no need to have the learned embeddings).\n\nWeakness\n- The main technical novelty seems the direct hashing of the adjacency matrix, which is graph specific. When pre-trained embeddings are given, the learning-based approach (Shu & Nakayama, 2017) seems sufficient.\n- The evaluation is mainly made on the node classification benchmark. Many node classification benchmarks for GNNs are actually associated with features. In fact, in node classification, I would say that rich features are often available (so that the prediction can be based on the given features), and people often use graph information to regularize the prediction. \n\nSuggestion:\nIn many link prediction tasks (knowledge graph completion, recommender systems), the node features are often unavailable, and the primary learning signal comes from the edge connectivity information. There, the primary approach is to train shallow embeddings, and the key bottleneck is the storage of embedding for each node (especially for an industrial-scale large graph). I'd be much more impressed if you could demonstrate that your approach works well under the link prediction tasks.",
          "summary_of_the_review": "Overall, this paper is well-written, but the technical contribution is not sufficient, and the empirical evaluation should be done on a more practically-relevant task of link prediction.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Reviewer_J5SK"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Reviewer_J5SK"
        ]
      },
      {
        "id": "4Xg3L93YBHf",
        "original": null,
        "number": 2,
        "cdate": 1635726839717,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635726839717,
        "tmdate": 1635726839717,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "ZaI7Rd11G4S",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper presents a vertex embedding method with good scalability to large graphs. \nThe proposed method is based on locally-sensitive hashing and compositional coding: a memory-efficient binary representation of each vertex is constructed from a hashing technique and then, when needed, a decoder creates real vector embeddings of the vertices via compositional coding from a pool of codebooks. The encoding is constructed from prior knowledge and requires no training. Conversely, the decoder is trainable and can be learned end-to-end. The experiments show improved performance. A negative note is that some parts are unclear to me.",
          "main_review": "## Positive aspects:\n\n- The proposed method appears simple and effective. It relies on the well-established technique of locally-sensitive hashing and compositional coding. \n- The memory efficiency of the method makes it broadly applicable to large-scale graphs.\n- The LSH encoding is constructed from prior knowledge (like vertex connectivity) or pre-trained embeddings, therefore it allows exploiting already available information.\n- The method can be trained end-to-end together with all subsequent layers.\n- The experiments show that there is only a limited loss in performance with respect to the uncompressed baseline, and substantial improvement with respect to ALONE.\n\n## Negative aspects:\n\nThe only negative aspect I see concerns unclear parts that I detail right below.\n\n1. In my opinion, the proposed \"full\" method (which appears to be the main method proposed by the authors, because it is employed in all the experiments) seems to be more similar to the \"learn\" baseline by Shu and Nakayama, than to ALONE, both in terms of trainable components and performance. While the improvements over ALONE are evident, a little comparison is provided against the \"learn\" method. For example, since both methods have learnable codebooks, it is unclear to me what is the additional training phase mentioned at the end of Section 2. Is it associated with the training of the encoder in \"learn\"? I also would like to see explicit comments on whether there are substantial methodological differences or improvement in efficiency for which one would choose the proposed method rather than \"learn\" one.\n2. I could not find specified whether or not the three methods (ALONE, \"learn\" and the proposed \"full\") are set with the same hyperparameters. \n3. I think it would be great to also see whether or not there is any computational overhead associated with the three methods, and that the practitioner should be aware of.\n4. The second part of the section Implementations about the word sampling requires more clarity. The methods are trained to embed the same subset of words or the words are sampled independently? The test set of 5k words is the same for the three methods?\n5. When the auxiliary information matrix is constructed from the adjacency matrix, it seems to me that the test set results in a set of the most connected vertices because words are selected based on their frequency. Secondly, the auxiliary information for not high-frequency words is very sparse. I wonder if it is possible that the observed good performance is biased toward this specific test set. Could you comment on this?\n",
          "summary_of_the_review": "The method displays clear advantages over ALONE, and has the potential to be applicable in a wide range of problems, especially, large-scale ones.\nAlthough the methodological contribution is little, the method appears simple and effective. \nHowever, given the mentioned unclear part, I tend to recommend a weak accept. I am confident the authors can clarify the raised doubts and I am happy to increase my score, if appropriate.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "Not applicable",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Reviewer_WEHB"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Reviewer_WEHB"
        ]
      },
      {
        "id": "AFGrsIo3Ac7",
        "original": null,
        "number": 3,
        "cdate": 1635796561915,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635796561915,
        "tmdate": 1635796561915,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "ZaI7Rd11G4S",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this work, the authors propose a hashing-based node embedding compression approach, which utilizes the random projection hashing method to generate a code vector for each node using auxiliary information such as the input graph adjacency matrix. The proposed method is memory-efficient in the training procedures of Graph Neural Networks (GNN) models. Experiments also demonstrate that the proposed method outperforms other coding schemes in both the embedding reconstruction task and node classification task.\n\n",
          "main_review": "Strength\n1. The proposed method significantly reduces the memory cost when training the GNN models by adopting the random projection hashing approach.\n2. Experiments on embedding reconstruction and node classification tasks demonstrate the proposed method outperforms the prior embedding compression method based on a random coding scheme.\n3. The compressed embedding generated from the proposed method achieves a small loss compared to the raw embedding on the node classification task, which indicates that the proposed method solves the performance degradation problem to a certain extent. \n\nWeakness\n1. The first concern is whether the proposed method still works on other graph representation learning models. In this work, the authors only used GraphSAGE with different aggregators in the evaluation. It would be great if the authors could discuss and include more graph representation learning models in the experimental section, such as GIN [1], deeper GNNs (e.g., [2], [3]), and graph representation learning models with greater expressive power than GraphSAGE (e.g., [4], [5]). \n2. Second, the experimental section only considers the node classification task, and I wonder if the compressed embedding generated from the proposed method still achieves good performance on other tasks, such as structural role prediction and link prediction?\n3. Next, the authors use the adjacency matrix of the input graph as the auxiliary information in the paper. I wonder if other matrices related to graph structure can also be used as the input auxiliary information? For example, is it possible to use a higher-order adjacency matrix (i.e., $A^{k}$) to encode more global graph structure information?\n4. Finally, it would be great if the authors could discuss about how to select the code cardinality $c$ and the code length $m$.\n\n\n[1] Xu, K., Hu, W., Leskovec, J., & Jegelka, S. (2018). How powerful are graph neural networks?. arXiv preprint arXiv:1810.00826.\n\n[2] Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K. I., & Jegelka, S. (2018, July). Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning (pp. 5453-5462). PMLR.\n\n[3] Li, G., Muller, M., Thabet, A., & Ghanem, B. (2019). Deepgcns: Can gcns go as deep as cnns?. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 9267-9276).\n\n[4] You, J., Gomes-Selman, J., Ying, R., & Leskovec, J. (2021). Identity-aware graph neural networks. arXiv preprint arXiv:2101.10320.\n\n[5] Li, P., Wang, Y., Wang, H., & Leskovec, J. (2020). Distance encoding: Design provably more powerful neural networks for graph representation learning. arXiv preprint arXiv:2009.00142.",
          "summary_of_the_review": "In this work, the authors develop a hashing-based node embedding compression method, which significantly reduces the memory cost in the embedding learning procedure. The proposed method also outperforms the prior random coding based embedding compression method in the experiments. However, the experiments in this paper only utilized GraphSAGE as the graph representation learning model and considered the node classification task. It would be great if the authors could discuss and include more graph representation learning models and more graph related tasks in the experimental section. In addition, I wonder if the authors could discuss about the selection of input auxiliary information?",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Reviewer_u7vW"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Reviewer_u7vW"
        ]
      },
      {
        "id": "Qp2DqZi27yB",
        "original": null,
        "number": 4,
        "cdate": 1636007899452,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636007899452,
        "tmdate": 1636007899452,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "ZaI7Rd11G4S",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies the embedding compression problem related to GNNs and graph representation. A two-stage method is proposed to generate the compressed embeddings: firstly, it encodes each node into its composite code with hashing; secondly, it uses a MLP module to decode the embedding for the node. Experiments are performed to evaluate the compression effect with both pretrained graph embeddings and node classification task with GraphSage. ",
          "main_review": "W1. The proposed method is straightforward, whose technical novelty is kind of limited given the recent works on embedding compression, e.g., [1], [2], [3].\n\n[1] Learning to Hash with Graph Neural Networks for Recommender Systems. Tan et. al., the Web Conf 2020\n\n[2] Differentiable Product Quantization for End-to-End Embedding Compression. Chen et. al. ICML 2020\n\n[3] GHashing: Semantic Graph Hashing for Approximate Similarity Search in Graph Databases. , Qin et. al., KDD 2020\n\nW2. The experiments studies are insufficient, many of the related works are missing from the evaluation, which is hard to support the paper's claimed contribution.\n\nW3. The paper claims that the memory consumption is a bottleneck while training GNNs or graph representation models. Unfortunately, this claim is not true. In fact, the embedding table only needs to be partially loaded to GPU RAM during the training process. We would suggest the author to make reference to [4] for memory efficient implementation of their work.\n \n[4] DGL-KE: training knowledge graph embeddings at scale, Zheng et. al., SIGIR 2020",
          "summary_of_the_review": "This paper works on the embedding compression problem for GNNs and graph representation models. However, the major limitations on technical novelty and experimental studies make it unlike to be a quality publication.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Reviewer_hQPL"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Reviewer_hQPL"
        ]
      },
      {
        "id": "spPnomVggmv",
        "original": null,
        "number": 2,
        "cdate": 1637442287847,
        "mdate": 1637442287847,
        "ddate": null,
        "tcdate": 1637442287847,
        "tmdate": 1637442287847,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "Qp2DqZi27yB",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Comment",
        "content": {
          "title": "Thanks for the review",
          "comment": "Thanks for the review. Please see our response below:\n\n### Response to Weakness\n> 1. The proposed method is straightforward, whose technical novelty is kind of limited given the recent works on embedding compression, e.g., [1], [2], [3].\n\nThe problems solved by [1] and [3] are different from the problem solved by the proposed method. The method proposed in [1] and [3] are both learning-to-hash methods; its task is mainly for graph similarity search where the binary codes are generated using the sign function [1] or the binary regularization loss [3]. The methods proposed by [1] and [3] focus on training a GNN-based encoder to compress the hidden representation/embedding, but the input features remain unchanged. In our scenario, the goal is to efficiently compress the input feature/embedding without any embedding/encoder pre-training step. Indeed, the proposed method can be seamlessly compatible with [1,3], like compressing input and hidden embeddings at the same time. We leave this extension in the future.\n\nThe method presented in [2] requires the embedding table (query matrix, Q) as an input for the training process. In other words, it also does not apply to our problem setting. We are interested in the case where the embeddings are not available before the training stage (i.e., the training stage where the decoder and the downstream model are trained together).\n\n> 2. The experiments studies are insufficient, many of the related works are missing from the evaluation, which is hard to support the paper's claimed contribution.\n\nWe have included additional experiments based on the reviewer\u2019s suggestion. Note that our work proposed the embedding compression approach which is followed by a graph neural network. To our knowledge, we have included similar efforts including a pre-training-based approach and a random coding-based approach. We added experiments to validate the effectiveness of our proposed embedding compression method with several different graph neural networks, including GCN, GIN, and SGC. We also added link prediction experiments where node features are often missing.\n\n> 3. The paper claims that the memory consumption is a bottleneck while training GNNs or graph representation models. Unfortunately, this claim is not true. In fact, the embedding table only needs to be partially loaded to GPU RAM during the training process. We would suggest the author to make reference to [4] for memory efficient implementation of their work.\n\nThe system outlined in [4] is specifically designed for knowledge graph embeddings models trained with batches of triplets (head, relation, tail). On top of it, the DGL-KE system [4] mainly focuses on the distributed training, not on the embedding compression.\n\nEven so, it is still non-trivial to set up the distributed training schedule. In practice, setting up large-scale end-to-end graph learning is still largely constrained by memory usage, especially in production. Our proposed approach improves upon the existing compression method, and achieves comparable results to non-compression models, while drastically reducing memory usage.\n\n### Reference\n[1] Learning to Hash with Graph Neural Networks for Recommender Systems. Tan et. al., the Web Conf 2020\n\n[2] Differentiable Product Quantization for End-to-End Embedding Compression. Chen et. al. ICML 2020\n\n[3] GHashing: Semantic Graph Hashing for Approximate Similarity Search in Graph Databases. Qin et. al., KDD 2020\n\n[4] DGL-KE: training knowledge graph embeddings at scale, Zheng et. al., SIGIR 2020"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Authors"
        ]
      },
      {
        "id": "HGc-0y7XXs1",
        "original": null,
        "number": 3,
        "cdate": 1637442366121,
        "mdate": 1637442366121,
        "ddate": null,
        "tcdate": 1637442366121,
        "tmdate": 1637442366121,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "AFGrsIo3Ac7",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Comment",
        "content": {
          "title": "Thanks for the review",
          "comment": "Thanks for the feedback. Please see our response below:\n\n### Response to Weakness\n> 1. The first concern is whether the proposed method still works on other graph representation learning models. In this work, the authors only used GraphSAGE with different aggregators in the evaluation. It would be great if the authors could discuss and include more graph representation learning models in the experimental section, such as GIN [1], deeper GNNs (e.g., [2], [3]), and graph representation learning models with greater expressive power than GraphSAGE (e.g., [4], [5]).\n\nIt is interesting to see how well the proposed method works with models other than GraphSage. We have performed additional experiments with other architectures like GCN [6], SGC [7], and GIN [1]. The results are presented in Table 11 of the updated paper. The proposed method also works with other graph representation learning models.\n\n> 2. Second, the experimental section only considers the node classification task, and I wonder if the compressed embedding generated from the proposed method still achieves good performance on other tasks, such as structural role prediction and link prediction?\n\nWe have also added experiments on link prediction datasets. The results are summarized in Table 11 of the updated paper. The proposed method almost always outperforms the baseline. When it loses to the baseline, the difference is relatively small.\n\n> 3. Next, the authors use the adjacency matrix of the input graph as the auxiliary information in the paper. I wonder if other matrices related to graph structure can also be used as the input auxiliary information? For example, is it possible to use a higher-order adjacency matrix (i.e., ) to encode more global graph structure information?\n\nThis is an interesting question. We think it could result in better code for datasets that require higher-order information. We have included this as a potential future direction in the conclusion section of the updated paper.\n\n> 4. Finally, it would be great if the authors could discuss about how to select the code cardinality and the code length.\n\nWe have added the following passage to the paper on page 15 of the updated paper:\n\nGenerally, settings with a lower compression ratio have better performance as the potential information loss is less. In the experiments, the bit size of the binary code is fixed to 128 bits. In other words, both $\\{c=256, m=16\\}$ and $\\{c=2, m=128\\}$ uses 128 bit binary codes. The $c$ and $m$ change the compression ratio by changing the decoder size. When using the $\\{c=256, m=16\\}$ setting, there will be 4,096 vectors total stored in 16 codebooks. When using the $\\{c=2, m=128\\}$ setting, there will be 256 vectors total stored in 2 codebooks. Because the $\\{c=256, m=16\\}$ setting has a larger model (i.e., lower compression ratio), it usually is the setting that outperformed the other in terms of embedding quality. To select a suitable setting for $\\{c, m\\}$, we suggest the user compute the potential memory usage and compression ratio for different settings of $\\{c, m\\}$, then select the one with the lowest compression ratio while still meets the memory requirement.\n\n### Reference\n[1] Xu, K., Hu, W., Leskovec, J., & Jegelka, S. (2018). How powerful are graph neural networks?. arXiv preprint arXiv:1810.00826.\n\n[2] Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K. I., & Jegelka, S. (2018, July). Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning (pp. 5453-5462). PMLR.\n\n[3] Li, G., Muller, M., Thabet, A., & Ghanem, B. (2019). Deepgcns: Can gcns go as deep as cnns?. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 9267-9276).\n\n[4] You, J., Gomes-Selman, J., Ying, R., & Leskovec, J. (2021). Identity-aware graph neural networks. arXiv preprint arXiv:2101.10320.\n\n[5] Li, P., Wang, Y., Wang, H., & Leskovec, J. (2020). Distance encoding: Design provably more powerful neural networks for graph representation learning. arXiv preprint arXiv:2009.00142.\n\n[6] Kipf, T. N., & Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907.\n\n[7] Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., & Weinberger, K. (2019, May). Simplifying graph convolutional networks. In International conference on machine learning (pp. 6861-6871). PMLR."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Authors"
        ]
      },
      {
        "id": "r6N4ArTurC0",
        "original": null,
        "number": 4,
        "cdate": 1637442485010,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637442485010,
        "tmdate": 1637442653694,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "4Xg3L93YBHf",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Comment",
        "content": {
          "title": "Thanks for the review (part 1 of 2)",
          "comment": "Thanks for reviewing our paper. Please see our response below:\n\n### Response to Negative Aspects\n> 1. In my opinion, the proposed \"full\" method (which appears to be the main method proposed by the authors, because it is employed in all the experiments) seems to be more similar to the \"learn\" baseline by Shu and Nakayama, than to ALONE, both in terms of trainable components and performance. While the improvements over ALONE are evident, a little comparison is provided against the \"learn\" method. For example, since both methods have learnable codebooks, it is unclear to me what is the additional training phase mentioned at the end of Section 2. Is it associated with the training of the encoder in \"learn\"? I also would like to see explicit comments on whether there are substantial methodological differences or improvement in efficiency for which one would choose the proposed method rather than \"learn\" one.\n\nThe overall training process for the \"learn\" baseline by Shu and Nakayama has four stages: 1) pre-training the embedding, 2) training the encoder and decoder, 3) encoding each embedding with the encoder, and 4) saving only the codes and the decoder for refining with the downstream model. For both ALONE and the proposed method, the overall training process has two stages: 1) generate the codes for each word/node, and 2) train the decoder with the downstream model. The major difference (i.e., time saving) between \"learn\" and \"ALONE\"/proposed is the pre-training step. The cost associated with the pre-training embedding step differs depending on the method used to train the embedding. Shu and Nakayama's method could generate higher quality coding for each word/node but takes more time/resources to reach the end result. In short, Shu and Nakayama's method is less practical when the pre-training step is too expensive.\n\n> 2. I could not find specified whether or not the three methods (ALONE, \"learn\" and the proposed \"full\") are set with the same hyperparameters.\n\nYes, we set up them to use the same decoder design and same hyperparameter. The difference is how the code for each node/word is generated. \"ALONE\" generate random codes (free but at the cost of accuracy), \"learn\" train an encoder to generate the code (best accuracy but not time efficient), and the proposed method tries to find a good balance between time and accuracy trade-off. Thanks to the reviewer for pointing it out. We have updated Appendix A.4 to include the following sentence in the updated paper: Note, the decoder design is the same across different coding schemes tested on the same dataset.\n\n> 3. I think it would be great to also see whether or not there is any computational overhead associated with the three methods, and that the practitioner should be aware of.\n\nThe proposed method has two stages: 1) encoding and 2) decoding stage. The time complexity for the encoding stage is analyzed in Section 3.1, and the complexity is $O(n m \\log_2 c)$ where m and c are hyper-parameters (i.e., code length and code cardinality), n is the number of nodes in the graph. In other words, the first stage is linear with respect to the number of nodes in the graph. The major overhead for the second stage comes from the decoder model (i.e., the time complexity is similar to an MLP). The overhead depends on the design of the decoder. But, since the decoder is usually much lighter than the GNN, the overhead on the runtime is not significant.\n\nThe major difference between the three methods is in the first stage as all three methods could in theory have the same decoder design.\n\nIn comparison with the method proposed by Shu and Nakayama, the proposed method skips the pre-training step. The pre-training step is usually very expensive (i.e., going through the dataset multiple times while updating the embedding). As a result, the overhead of both ALONE and the proposed method is much smaller than the method proposed by Shu and Nakayama.\n\nFor ALONE, the encoding step is also linear to the number of nodes. It generates random numbers for each node, which can be slightly faster. On the whole, ALONE and the proposed method are within the same order of magnitude (linear) while the method proposed by Shu and Nakayama requires a costly pre-training step.\n\nsee [https://openreview.net/forum?id=ZaI7Rd11G4S&noteId=6nlOpSjcua](https://openreview.net/forum?id=ZaI7Rd11G4S&noteId=6nlOpSjcua) for part 2."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Authors"
        ]
      },
      {
        "id": "6nlOpSjcua",
        "original": null,
        "number": 5,
        "cdate": 1637442512754,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637442512754,
        "tmdate": 1637442610881,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "4Xg3L93YBHf",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Comment",
        "content": {
          "title": "Thanks for the review (part 2 of 2)",
          "comment": "see [https://openreview.net/forum?id=ZaI7Rd11G4S&noteId=r6N4ArTurC0](https://openreview.net/forum?id=ZaI7Rd11G4S&noteId=r6N4ArTurC0) for part 1.\n\n> 4. The second part of the section Implementations about the word sampling requires more clarity. The methods are trained to embed the same subset of words or the words are sampled independently? The test set of 5k words is the same for the three methods?\n\nAcross different experiments, the set of tested words/nodes is always the same 5k words/nodes. The set of embedded words/nodes is also the same for different methods. The test word/node set is always a subset of the embedded word set/node.\n\n> 5. When the auxiliary information matrix is constructed from the adjacency matrix, it seems to me that the test set results in a set of the most connected vertices because words are selected based on their frequency. Secondly, the auxiliary information for not high-frequency words is very sparse. I wonder if it is possible that the observed good performance is biased toward this specific test set. Could you comment on this?\n\nYour hypothesis could be true. The more a model knows about a word/node, the more likely that a good representation is learned for the word/node. The main reason for using high frequency words is that word analogy/similarity datasets mostly contain high frequency words.\n\nNote, sampling is only performed in Section 5.1. The sampling is used as a way to control the amount of compressed information (and fix the test set). We do not sample in Section 5.2 and use all nodes in the graph. The train/validation/test split comes with the dataset."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Authors"
        ]
      },
      {
        "id": "JM1bzorSE2f",
        "original": null,
        "number": 6,
        "cdate": 1637442699569,
        "mdate": 1637442699569,
        "ddate": null,
        "tcdate": 1637442699569,
        "tmdate": 1637442699569,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "cpdPjBcTC1U",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Comment",
        "content": {
          "title": "Thanks for the review",
          "comment": "Thanks for the suggestion. Please see our response below:\n\n### Response to Weakness and Suggestion\n> 1. The main technical novelty seems the direct hashing of the adjacency matrix, which is graph specific. When pre-trained embeddings are given, the learning-based approach (Shu & Nakayama, 2017) seems sufficient.\n\nIn the optimal case (i.e., when the pre-trained embeddings are available), the method proposed by Shu & Nakayama (2017) is sufficient (although it might not be the most cost-efficient).\n\nWhen our method is compared with the method proposed by Shu & Nakayama (2017) in Section 5.1, our method's performance (in terms of acc., rho, and nmi.) is comparable to Shu & Nakayama's method. However, our method does not require pre-train embedding or the additional training step for encoder training. Our method goes to the end-to-end training step directly after the hashing step.\n\nThe section where pre-trained embeddings are used is mainly for comparing different encoding methods. As the experiment includes the method proposed by Shu & Nakayama (2017), pre-trained embeddings are required. The experiment shows how good we could get using cheap hash functions (with adjacency matrix) compared to a more costly method (training an encoder/method by Shu & Nakayama) for compression.\n\nThe proposed method is mainly developed for graphs. As suggested by Reviewer u7vW, the model could take different inputs to the hash function for graphs or other applications.\n\n> 2. The evaluation is mainly made on the node classification benchmark. Many node classification benchmarks for GNNs are actually associated with features. In fact, in node classification, I would say that rich features are often available (so that the prediction can be based on the given features), and people often use graph information to regularize the prediction.\n>\n> Suggestion: In many link prediction tasks (knowledge graph completion, recommender systems), the node features are often unavailable, and the primary learning signal comes from the edge connectivity information. There, the primary approach is to train shallow embeddings, and the key bottleneck is the storage of embedding for each node (especially for an industrial-scale large graph). I'd be much more impressed if you could demonstrate that your approach works well under the link prediction tasks.\n\nWe agree to the reviewer\u2019s point and we think this is a good suggestion. We have performed additional experiments to test our approach on link prediction tasks. The result is added to the updated paper as the new Table 11. The proposed method also shows its value for the link prediction problem.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Authors"
        ]
      },
      {
        "id": "iw7AhrdtkZj",
        "original": null,
        "number": 8,
        "cdate": 1638188015957,
        "mdate": 1638188015957,
        "ddate": null,
        "tcdate": 1638188015957,
        "tmdate": 1638188015957,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "6nlOpSjcua",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Comment",
        "content": {
          "title": "Thanks for the clarifications",
          "comment": "I thank the authors for their additional clarifications"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Reviewer_WEHB"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Reviewer_WEHB"
        ]
      },
      {
        "id": "oEMLj_RWe3Iw",
        "original": null,
        "number": 1,
        "cdate": 1642696834630,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696834630,
        "tmdate": 1642696834630,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "ZaI7Rd11G4S",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "This paper studies the embedding compression problem related to GNNs and graph representation. A two-stage method is proposed to generate the compressed embeddings: firstly, it encodes each node into its composite code with hashing; secondly, it uses a MLP module to decode the embedding for the node. Experiments are performed to evaluate the compression effect with both pretrained graph embeddings and node classification tasks with GraphSage.\n\nThe paper considers hashing/compressing the rows/columns of adjacency matrices and using the compressed rows of the adjacency matrices as node features. The adjacency matrices are  intrinsically redundant. Therefore, it is unclear whether the achieved compression rate is significant, especially when applied to settings with known node features. \n\nSome reviewers pointed out existing methods on learning-to-hash methods, which train a GNN-based encoder to compress the hidden representation/embedding, are relevant. Although the authors claim that in their scenario, the goal is to efficiently compress the input feature/embedding without any embedding/encoder pre-training step, it is unclear how the proposed method compares with the learning-to-hash methods when considering the adjacency matrices as the auxiliary information. \n\nThe dependence on the number of nodes is also a concern in terms of scalability, as we know the bottleneck of scalability in GNNs is the number of nodes. \n\nThe authors use the adjacency matrix of the input graph as the auxiliary information in the paper, which only considers local structure information. The reviewers are curious whether this approach would work for tasks in which global graph structure information is required. \n\nOn a minor note, the reviewers also think that the paper would be stronger if the authors provide more principled guidance on how to select the code cardinality c and the code length m."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "cpdPjBcTC1U",
        "original": null,
        "number": 1,
        "cdate": 1635702231212,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635702231212,
        "tmdate": 1635702231212,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "ZaI7Rd11G4S",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this paper, the authors present a general and scalable approach to obtain shallow embeddings of many entities. The authors achieve this by first mapping each entity into a code vector using LSH, and then learning a neural decoder on top of the code vectors to reconstruct the original embeddings/adjacency matrix. The authors then apply their approach to node classification problems, where node features are unavailable (the authors ended up artificially removing node features from existing OGB datasets). The superior performance is demonstrated over the random code vector baseline.",
          "main_review": "Strength\n- The paper is well-written and easy to follow.\n- Proposed approach can directly hash the adjacency matrix (no need to have the learned embeddings).\n\nWeakness\n- The main technical novelty seems the direct hashing of the adjacency matrix, which is graph specific. When pre-trained embeddings are given, the learning-based approach (Shu & Nakayama, 2017) seems sufficient.\n- The evaluation is mainly made on the node classification benchmark. Many node classification benchmarks for GNNs are actually associated with features. In fact, in node classification, I would say that rich features are often available (so that the prediction can be based on the given features), and people often use graph information to regularize the prediction. \n\nSuggestion:\nIn many link prediction tasks (knowledge graph completion, recommender systems), the node features are often unavailable, and the primary learning signal comes from the edge connectivity information. There, the primary approach is to train shallow embeddings, and the key bottleneck is the storage of embedding for each node (especially for an industrial-scale large graph). I'd be much more impressed if you could demonstrate that your approach works well under the link prediction tasks.",
          "summary_of_the_review": "Overall, this paper is well-written, but the technical contribution is not sufficient, and the empirical evaluation should be done on a more practically-relevant task of link prediction.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Reviewer_J5SK"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Reviewer_J5SK"
        ]
      },
      {
        "id": "4Xg3L93YBHf",
        "original": null,
        "number": 2,
        "cdate": 1635726839717,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635726839717,
        "tmdate": 1635726839717,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "ZaI7Rd11G4S",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper presents a vertex embedding method with good scalability to large graphs. \nThe proposed method is based on locally-sensitive hashing and compositional coding: a memory-efficient binary representation of each vertex is constructed from a hashing technique and then, when needed, a decoder creates real vector embeddings of the vertices via compositional coding from a pool of codebooks. The encoding is constructed from prior knowledge and requires no training. Conversely, the decoder is trainable and can be learned end-to-end. The experiments show improved performance. A negative note is that some parts are unclear to me.",
          "main_review": "## Positive aspects:\n\n- The proposed method appears simple and effective. It relies on the well-established technique of locally-sensitive hashing and compositional coding. \n- The memory efficiency of the method makes it broadly applicable to large-scale graphs.\n- The LSH encoding is constructed from prior knowledge (like vertex connectivity) or pre-trained embeddings, therefore it allows exploiting already available information.\n- The method can be trained end-to-end together with all subsequent layers.\n- The experiments show that there is only a limited loss in performance with respect to the uncompressed baseline, and substantial improvement with respect to ALONE.\n\n## Negative aspects:\n\nThe only negative aspect I see concerns unclear parts that I detail right below.\n\n1. In my opinion, the proposed \"full\" method (which appears to be the main method proposed by the authors, because it is employed in all the experiments) seems to be more similar to the \"learn\" baseline by Shu and Nakayama, than to ALONE, both in terms of trainable components and performance. While the improvements over ALONE are evident, a little comparison is provided against the \"learn\" method. For example, since both methods have learnable codebooks, it is unclear to me what is the additional training phase mentioned at the end of Section 2. Is it associated with the training of the encoder in \"learn\"? I also would like to see explicit comments on whether there are substantial methodological differences or improvement in efficiency for which one would choose the proposed method rather than \"learn\" one.\n2. I could not find specified whether or not the three methods (ALONE, \"learn\" and the proposed \"full\") are set with the same hyperparameters. \n3. I think it would be great to also see whether or not there is any computational overhead associated with the three methods, and that the practitioner should be aware of.\n4. The second part of the section Implementations about the word sampling requires more clarity. The methods are trained to embed the same subset of words or the words are sampled independently? The test set of 5k words is the same for the three methods?\n5. When the auxiliary information matrix is constructed from the adjacency matrix, it seems to me that the test set results in a set of the most connected vertices because words are selected based on their frequency. Secondly, the auxiliary information for not high-frequency words is very sparse. I wonder if it is possible that the observed good performance is biased toward this specific test set. Could you comment on this?\n",
          "summary_of_the_review": "The method displays clear advantages over ALONE, and has the potential to be applicable in a wide range of problems, especially, large-scale ones.\nAlthough the methodological contribution is little, the method appears simple and effective. \nHowever, given the mentioned unclear part, I tend to recommend a weak accept. I am confident the authors can clarify the raised doubts and I am happy to increase my score, if appropriate.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "Not applicable",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Reviewer_WEHB"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Reviewer_WEHB"
        ]
      },
      {
        "id": "AFGrsIo3Ac7",
        "original": null,
        "number": 3,
        "cdate": 1635796561915,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635796561915,
        "tmdate": 1635796561915,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "ZaI7Rd11G4S",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this work, the authors propose a hashing-based node embedding compression approach, which utilizes the random projection hashing method to generate a code vector for each node using auxiliary information such as the input graph adjacency matrix. The proposed method is memory-efficient in the training procedures of Graph Neural Networks (GNN) models. Experiments also demonstrate that the proposed method outperforms other coding schemes in both the embedding reconstruction task and node classification task.\n\n",
          "main_review": "Strength\n1. The proposed method significantly reduces the memory cost when training the GNN models by adopting the random projection hashing approach.\n2. Experiments on embedding reconstruction and node classification tasks demonstrate the proposed method outperforms the prior embedding compression method based on a random coding scheme.\n3. The compressed embedding generated from the proposed method achieves a small loss compared to the raw embedding on the node classification task, which indicates that the proposed method solves the performance degradation problem to a certain extent. \n\nWeakness\n1. The first concern is whether the proposed method still works on other graph representation learning models. In this work, the authors only used GraphSAGE with different aggregators in the evaluation. It would be great if the authors could discuss and include more graph representation learning models in the experimental section, such as GIN [1], deeper GNNs (e.g., [2], [3]), and graph representation learning models with greater expressive power than GraphSAGE (e.g., [4], [5]). \n2. Second, the experimental section only considers the node classification task, and I wonder if the compressed embedding generated from the proposed method still achieves good performance on other tasks, such as structural role prediction and link prediction?\n3. Next, the authors use the adjacency matrix of the input graph as the auxiliary information in the paper. I wonder if other matrices related to graph structure can also be used as the input auxiliary information? For example, is it possible to use a higher-order adjacency matrix (i.e., $A^{k}$) to encode more global graph structure information?\n4. Finally, it would be great if the authors could discuss about how to select the code cardinality $c$ and the code length $m$.\n\n\n[1] Xu, K., Hu, W., Leskovec, J., & Jegelka, S. (2018). How powerful are graph neural networks?. arXiv preprint arXiv:1810.00826.\n\n[2] Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K. I., & Jegelka, S. (2018, July). Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning (pp. 5453-5462). PMLR.\n\n[3] Li, G., Muller, M., Thabet, A., & Ghanem, B. (2019). Deepgcns: Can gcns go as deep as cnns?. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 9267-9276).\n\n[4] You, J., Gomes-Selman, J., Ying, R., & Leskovec, J. (2021). Identity-aware graph neural networks. arXiv preprint arXiv:2101.10320.\n\n[5] Li, P., Wang, Y., Wang, H., & Leskovec, J. (2020). Distance encoding: Design provably more powerful neural networks for graph representation learning. arXiv preprint arXiv:2009.00142.",
          "summary_of_the_review": "In this work, the authors develop a hashing-based node embedding compression method, which significantly reduces the memory cost in the embedding learning procedure. The proposed method also outperforms the prior random coding based embedding compression method in the experiments. However, the experiments in this paper only utilized GraphSAGE as the graph representation learning model and considered the node classification task. It would be great if the authors could discuss and include more graph representation learning models and more graph related tasks in the experimental section. In addition, I wonder if the authors could discuss about the selection of input auxiliary information?",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Reviewer_u7vW"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Reviewer_u7vW"
        ]
      },
      {
        "id": "Qp2DqZi27yB",
        "original": null,
        "number": 4,
        "cdate": 1636007899452,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636007899452,
        "tmdate": 1636007899452,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "ZaI7Rd11G4S",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies the embedding compression problem related to GNNs and graph representation. A two-stage method is proposed to generate the compressed embeddings: firstly, it encodes each node into its composite code with hashing; secondly, it uses a MLP module to decode the embedding for the node. Experiments are performed to evaluate the compression effect with both pretrained graph embeddings and node classification task with GraphSage. ",
          "main_review": "W1. The proposed method is straightforward, whose technical novelty is kind of limited given the recent works on embedding compression, e.g., [1], [2], [3].\n\n[1] Learning to Hash with Graph Neural Networks for Recommender Systems. Tan et. al., the Web Conf 2020\n\n[2] Differentiable Product Quantization for End-to-End Embedding Compression. Chen et. al. ICML 2020\n\n[3] GHashing: Semantic Graph Hashing for Approximate Similarity Search in Graph Databases. , Qin et. al., KDD 2020\n\nW2. The experiments studies are insufficient, many of the related works are missing from the evaluation, which is hard to support the paper's claimed contribution.\n\nW3. The paper claims that the memory consumption is a bottleneck while training GNNs or graph representation models. Unfortunately, this claim is not true. In fact, the embedding table only needs to be partially loaded to GPU RAM during the training process. We would suggest the author to make reference to [4] for memory efficient implementation of their work.\n \n[4] DGL-KE: training knowledge graph embeddings at scale, Zheng et. al., SIGIR 2020",
          "summary_of_the_review": "This paper works on the embedding compression problem for GNNs and graph representation models. However, the major limitations on technical novelty and experimental studies make it unlike to be a quality publication.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper43/Reviewer_hQPL"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper43/Reviewer_hQPL"
        ]
      },
      {
        "id": "oEMLj_RWe3Iw",
        "original": null,
        "number": 1,
        "cdate": 1642696834630,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696834630,
        "tmdate": 1642696834630,
        "tddate": null,
        "forum": "ZaI7Rd11G4S",
        "replyto": "ZaI7Rd11G4S",
        "invitation": "ICLR.cc/2022/Conference/Paper43/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "This paper studies the embedding compression problem related to GNNs and graph representation. A two-stage method is proposed to generate the compressed embeddings: firstly, it encodes each node into its composite code with hashing; secondly, it uses a MLP module to decode the embedding for the node. Experiments are performed to evaluate the compression effect with both pretrained graph embeddings and node classification tasks with GraphSage.\n\nThe paper considers hashing/compressing the rows/columns of adjacency matrices and using the compressed rows of the adjacency matrices as node features. The adjacency matrices are  intrinsically redundant. Therefore, it is unclear whether the achieved compression rate is significant, especially when applied to settings with known node features. \n\nSome reviewers pointed out existing methods on learning-to-hash methods, which train a GNN-based encoder to compress the hidden representation/embedding, are relevant. Although the authors claim that in their scenario, the goal is to efficiently compress the input feature/embedding without any embedding/encoder pre-training step, it is unclear how the proposed method compares with the learning-to-hash methods when considering the adjacency matrices as the auxiliary information. \n\nThe dependence on the number of nodes is also a concern in terms of scalability, as we know the bottleneck of scalability in GNNs is the number of nodes. \n\nThe authors use the adjacency matrix of the input graph as the auxiliary information in the paper, which only considers local structure information. The reviewers are curious whether this approach would work for tasks in which global graph structure information is required. \n\nOn a minor note, the reviewers also think that the paper would be stronger if the authors provide more principled guidance on how to select the code cardinality c and the code length m."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}