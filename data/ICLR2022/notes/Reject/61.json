{
  "id": "tG8QrhMwEqS",
  "original": "D2kKUwGWQD",
  "number": 61,
  "cdate": 1632875425858,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875425858,
  "tmdate": 1676330690844,
  "ddate": null,
  "content": {
    "title": "Adaptive Activation-based Structured Pruning",
    "authorids": [
      "~Kaiqi_Zhao2",
      "~Animesh_Jain1",
      "~Ming_Zhao2"
    ],
    "authors": [
      "Kaiqi Zhao",
      "Animesh Jain",
      "Ming Zhao"
    ],
    "keywords": [
      "model compression",
      "structured pruning"
    ],
    "abstract": "Pruning is a promising approach to compress complex deep learning models in order to deploy them on resource-constrained edge devices. However, many existing pruning solutions are based on unstructured pruning, which yield models that cannot efficiently run on commodity hardware, and require users to manually explore and tune the pruning process, which is time consuming and often leads to sub-optimal results. To address these limitations, this paper presents an adaptive, activation-based, structured pruning approach to automatically and efficiently generate small, accurate, and hardware-efficient models that meet user requirements. First, it proposes iterative structured pruning using activation-based attention feature maps to effectively identify and prune unimportant filters. Then, it proposes adaptive pruning policies for automatically meeting the pruning objectives of accuracy-critical, memory-constrained, and latency-sensitive tasks. A comprehensive evaluation shows that the proposed method can substantially outperform the state-of-the-art structured pruning works on CIFAR-10 and ImageNet datasets. For example, on ResNet-56 with CIFAR-10, without any accuracy drop, our method achieves the largest parameter reduction (79.11%), outperforming the related works by 22.81% to 66.07%, and the largest FLOPs reduction (70.13%), outperforming the related works by 14.13% to 26.53%.",
    "pdf": "/pdf/65730848a7219335db9dc49156d30c41682aeaab.pdf",
    "one-sentence_summary": "This paper presents an adaptive, activation-based, structured pruning approach to automatically and efficiently generate small and accurate models that meet user requirements.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "zhao|adaptive_activationbased_structured_pruning",
    "_bibtex": "@misc{\nzhao2022adaptive,\ntitle={Adaptive Activation-based Structured Pruning},\nauthor={Kaiqi Zhao and Animesh Jain and Ming Zhao},\nyear={2022},\nurl={https://openreview.net/forum?id=tG8QrhMwEqS}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "tG8QrhMwEqS",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 4,
    "directReplyCount": 4,
    "revisions": true,
    "replies": [
      {
        "id": "cLdBWCliQcN",
        "original": null,
        "number": 1,
        "cdate": 1635753449869,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635753449869,
        "tmdate": 1638190501714,
        "tddate": null,
        "forum": "tG8QrhMwEqS",
        "replyto": "tG8QrhMwEqS",
        "invitation": "ICLR.cc/2022/Conference/Paper61/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes iterative structured pruning methods using activation-based attention feature maps and an adaptive threshold selection strategy. Inspired by attention transfer, Activation-based attention feature maps are constructed as the important evaluation of filters in each layer. Adaptive threshold selection strategy decides the number of removed filters, which satisfies one of three adaptive pruning policies. Experimental results on CIFAR-10 and ImageNet with ResNet architectures show performance gains over several state-of-the-art methods.",
          "main_review": "Strength: \n+ The proposed method is simple and easy to follow.\n+ This paper proposes three modes for structured pruning, including accuracy-guaranteed adaptive pruning, memory-constrained adaptive pruning, and FLOPs-constrained adaptive pruning. \n\nWeakness:\n- The contribution of the evaluation for the importance of filter, since activation-based attention maps are similar to attention score in [Sergey Zagoruyko, ICLR 16] and the activation-based score is also proposed in many related works, such as APoZ [1] and HRank. Please clarify the difference between these works and the proposed methods.\n- The rationale of calculating the threshold T in each layer. T in layer-aware threshold adjustment is directly related to the percentage of each layer on parameters or FLOPs. Moreover, the typo in line 3 of algorithm N^i[r]->T^i[r].\n- Lack of some important experimental evaluation: 1. The models selected to compress are all based resnet architectures, how about other architectures, especially light backbones (e.g. mobilenets); 2. The effect of attention score and pruning strategy should be discussed in ablation study; 3. The actual speedup of the pruned models should be evaluated if minimizing FLOPs.\n\n[1] Network trimming: A data-driven neuron pruning approach towards efficient deep architectures, arXiv preprint arXiv:1607.03250, 2016.\n",
          "summary_of_the_review": "Although the proposed method is simple yet effective on ResNets, the contribution is limited and experimental evaluation is not comprehensive. I tend to reject this paper in this version.\n\n\n-------------------POST-REBUTTAL COMMENTS-------------------\n\nI thank the authors for the response and the efforts in the updated draft. After this rebuttal, the authors well answer my comments about the comparison to SOTA methods. However, I still believe that the proposed measurement of filter importance is a limited novelty, as it is a simple revision of attention score from [Sergey Zagoruyko, ICLR 16]. Moreover, the rationale of threshold T is based on the assumption that a layer is more likely to have redundant filters to prune if it contains more remaining parameters. I think it is not a correct assumption, as the entire filters of a layer with a smaller number of parameters may be redundant to be removed safely, especially for ResNets. Thus, I still keep my original rate to reject it.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper61/Reviewer_XNxE"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper61/Reviewer_XNxE"
        ]
      },
      {
        "id": "747N5kDrz7",
        "original": null,
        "number": 2,
        "cdate": 1635854075906,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635854075906,
        "tmdate": 1635854075906,
        "tddate": null,
        "forum": "tG8QrhMwEqS",
        "replyto": "tG8QrhMwEqS",
        "invitation": "ICLR.cc/2022/Conference/Paper61/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This work proposes a technique for iterative structured pruning, without necessarily requiring too much manual human intervention. There are two parts to this paper that are important:\n\n1. It is argued that we should prune channels based on the activation maps generated, rather than focusing on the weights of the channel.\n2. They propose an iterative procedure that automatically backtracks if it has made a poor pruning decision.",
          "main_review": "I did not like the way that this paper is written. It is extremely waffley, and is not upfront about what is going on. It is not completely clear (to me at least) what the overall strategy for pruning will be in this work until half way through page 4 (!). The paper needs to be (completely) rewritten to match the expected format in the computer science literature.\n\n**Strengths**\n- I am not a total expert on the pruning literature, but I do find the idea of pruning based on the activation maps rather than the weights intuitive and sensible.\n- The results are compelling relative to other works they compare to. NOTE: again, I will not claim to be an absolute expert on the state of the pruning literature.\n- The automated approach is relatively simple, and far more interpretable than approaches such as AMC. This matters a lot in many real-world applications.\n- ImageNet results are included, which is relatively rare for pruning papers.\n\n**Weaknesses**:\n- The writing of this paper is unacceptable, as I mentioned earlier.\n- A few really important ablation studies are missing. Firstly, there is no direct reason to couple the pruning technique, with the method for iteratively pruning; can these not be compared separately. It is misleading to conflate the two. Secondly, there is no comment on runtime for this method, which is important if this is an automated technique.\n- Another important issue is that the FLOPs reduction doesn't seem that impressive. Works like Eigendamage (Wang et al.) achieve far more impressive reductions in FLOPs, which is arguably far more important than parameters in the real world. As an aside, would you be able to compare to Eigendamage?\n- The work only focuses on ResNets. I know that they're harder to prune, but I'd also like to see some experiments on VGG models, for example.",
          "summary_of_the_review": "I think this work has some merit, but it is not ready for publication at this time. I can be swayed at rebuttal time if there is convincing new experimental evidence.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper61/Reviewer_YdeV"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper61/Reviewer_YdeV"
        ]
      },
      {
        "id": "0edlJDXef-_",
        "original": null,
        "number": 3,
        "cdate": 1635878356906,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635878356906,
        "tmdate": 1635878356906,
        "tddate": null,
        "forum": "tG8QrhMwEqS",
        "replyto": "tG8QrhMwEqS",
        "invitation": "ICLR.cc/2022/Conference/Paper61/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposed an activation-based, adaptive threshold, iterative structured pruning method, combining several existing techniques together to perform a comprehensive pruning. ",
          "main_review": "Strengths:\nThe proposed method has relatively good results on cifar10 and imagenet.  The method can automatically meet users' several requirements generally, like accuracy, latency, memory, and so on. \n\nWeakness:\n1. Each part of the technique is not that novel. It is like a combination of several existing tricks to perform comprehensive pruning, not enough analysis of the convergence or correctness. For example, how to guarantee the convergence of algorithm 3? \n2. The comparisons on Imagenet are too weak. There are many SOTA pruning methods on Imagenet. The paper only lists four of them. I suggest comparing the paper with more recent SOTA pruning papers and comparing on more benchmark models besides Res50. \nfor example, DMCP: Differentiable Markov Channel Pruning for Neural Networks\neagleeye: fast sub-net evaluation for efficient neural network pruning. \ngdp: neural network pruning via gates with differentiable polarization.\nand so on. \n\n",
          "summary_of_the_review": "Due to the concerns on the novelty and the weak comparisons on Imagenet. I temporarily think this paper is marginally below the acceptance threshold. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper61/Reviewer_EiLE"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper61/Reviewer_EiLE"
        ]
      },
      {
        "id": "9KqOYKW7Eu9",
        "original": null,
        "number": 1,
        "cdate": 1642696835346,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696835346,
        "tmdate": 1642696835346,
        "tddate": null,
        "forum": "tG8QrhMwEqS",
        "replyto": "tG8QrhMwEqS",
        "invitation": "ICLR.cc/2022/Conference/Paper61/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The paper proposes a strategy for incrementally pruning deep learning models based on activation values. The approach can satisfy different kinds of requirements, trading off between accuracy and sparsity.\n\nThe approach seems promising and seems to have competitive performance. However, the method is described by reviewers as a combination of ideas that have been proposed in the literature, and the experimental evaluation relies too much on a dataset considered too small to be reliable in such experiments --- CIFAR10. We do not expect substantial experiments within the rebuttal period: such comparisons with relevant SOTA methods should have been present in the submission. Moreover, the strategy proposed for selecting a threshold seems to rely on some doubtful assumptions, and there are no benchmarks on actual runtime.\n\nThe writing has improved based on reviewer input, and the reviewers are satisfied with this aspect. I would still add that I would prefer some clarity in the method presentation: is there a quantity being optimized? is there a value we can monitor to ensure our reimplementation is correct? etc. In addition I would like to ask authors in the next revision to be mindful to the difference between `\\citet` and `\\citep` in author-year citations -- see e.g. the first two ones in 3.1."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "cLdBWCliQcN",
        "original": null,
        "number": 1,
        "cdate": 1635753449869,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635753449869,
        "tmdate": 1638190501714,
        "tddate": null,
        "forum": "tG8QrhMwEqS",
        "replyto": "tG8QrhMwEqS",
        "invitation": "ICLR.cc/2022/Conference/Paper61/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes iterative structured pruning methods using activation-based attention feature maps and an adaptive threshold selection strategy. Inspired by attention transfer, Activation-based attention feature maps are constructed as the important evaluation of filters in each layer. Adaptive threshold selection strategy decides the number of removed filters, which satisfies one of three adaptive pruning policies. Experimental results on CIFAR-10 and ImageNet with ResNet architectures show performance gains over several state-of-the-art methods.",
          "main_review": "Strength: \n+ The proposed method is simple and easy to follow.\n+ This paper proposes three modes for structured pruning, including accuracy-guaranteed adaptive pruning, memory-constrained adaptive pruning, and FLOPs-constrained adaptive pruning. \n\nWeakness:\n- The contribution of the evaluation for the importance of filter, since activation-based attention maps are similar to attention score in [Sergey Zagoruyko, ICLR 16] and the activation-based score is also proposed in many related works, such as APoZ [1] and HRank. Please clarify the difference between these works and the proposed methods.\n- The rationale of calculating the threshold T in each layer. T in layer-aware threshold adjustment is directly related to the percentage of each layer on parameters or FLOPs. Moreover, the typo in line 3 of algorithm N^i[r]->T^i[r].\n- Lack of some important experimental evaluation: 1. The models selected to compress are all based resnet architectures, how about other architectures, especially light backbones (e.g. mobilenets); 2. The effect of attention score and pruning strategy should be discussed in ablation study; 3. The actual speedup of the pruned models should be evaluated if minimizing FLOPs.\n\n[1] Network trimming: A data-driven neuron pruning approach towards efficient deep architectures, arXiv preprint arXiv:1607.03250, 2016.\n",
          "summary_of_the_review": "Although the proposed method is simple yet effective on ResNets, the contribution is limited and experimental evaluation is not comprehensive. I tend to reject this paper in this version.\n\n\n-------------------POST-REBUTTAL COMMENTS-------------------\n\nI thank the authors for the response and the efforts in the updated draft. After this rebuttal, the authors well answer my comments about the comparison to SOTA methods. However, I still believe that the proposed measurement of filter importance is a limited novelty, as it is a simple revision of attention score from [Sergey Zagoruyko, ICLR 16]. Moreover, the rationale of threshold T is based on the assumption that a layer is more likely to have redundant filters to prune if it contains more remaining parameters. I think it is not a correct assumption, as the entire filters of a layer with a smaller number of parameters may be redundant to be removed safely, especially for ResNets. Thus, I still keep my original rate to reject it.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper61/Reviewer_XNxE"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper61/Reviewer_XNxE"
        ]
      },
      {
        "id": "747N5kDrz7",
        "original": null,
        "number": 2,
        "cdate": 1635854075906,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635854075906,
        "tmdate": 1635854075906,
        "tddate": null,
        "forum": "tG8QrhMwEqS",
        "replyto": "tG8QrhMwEqS",
        "invitation": "ICLR.cc/2022/Conference/Paper61/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This work proposes a technique for iterative structured pruning, without necessarily requiring too much manual human intervention. There are two parts to this paper that are important:\n\n1. It is argued that we should prune channels based on the activation maps generated, rather than focusing on the weights of the channel.\n2. They propose an iterative procedure that automatically backtracks if it has made a poor pruning decision.",
          "main_review": "I did not like the way that this paper is written. It is extremely waffley, and is not upfront about what is going on. It is not completely clear (to me at least) what the overall strategy for pruning will be in this work until half way through page 4 (!). The paper needs to be (completely) rewritten to match the expected format in the computer science literature.\n\n**Strengths**\n- I am not a total expert on the pruning literature, but I do find the idea of pruning based on the activation maps rather than the weights intuitive and sensible.\n- The results are compelling relative to other works they compare to. NOTE: again, I will not claim to be an absolute expert on the state of the pruning literature.\n- The automated approach is relatively simple, and far more interpretable than approaches such as AMC. This matters a lot in many real-world applications.\n- ImageNet results are included, which is relatively rare for pruning papers.\n\n**Weaknesses**:\n- The writing of this paper is unacceptable, as I mentioned earlier.\n- A few really important ablation studies are missing. Firstly, there is no direct reason to couple the pruning technique, with the method for iteratively pruning; can these not be compared separately. It is misleading to conflate the two. Secondly, there is no comment on runtime for this method, which is important if this is an automated technique.\n- Another important issue is that the FLOPs reduction doesn't seem that impressive. Works like Eigendamage (Wang et al.) achieve far more impressive reductions in FLOPs, which is arguably far more important than parameters in the real world. As an aside, would you be able to compare to Eigendamage?\n- The work only focuses on ResNets. I know that they're harder to prune, but I'd also like to see some experiments on VGG models, for example.",
          "summary_of_the_review": "I think this work has some merit, but it is not ready for publication at this time. I can be swayed at rebuttal time if there is convincing new experimental evidence.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper61/Reviewer_YdeV"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper61/Reviewer_YdeV"
        ]
      },
      {
        "id": "0edlJDXef-_",
        "original": null,
        "number": 3,
        "cdate": 1635878356906,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635878356906,
        "tmdate": 1635878356906,
        "tddate": null,
        "forum": "tG8QrhMwEqS",
        "replyto": "tG8QrhMwEqS",
        "invitation": "ICLR.cc/2022/Conference/Paper61/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposed an activation-based, adaptive threshold, iterative structured pruning method, combining several existing techniques together to perform a comprehensive pruning. ",
          "main_review": "Strengths:\nThe proposed method has relatively good results on cifar10 and imagenet.  The method can automatically meet users' several requirements generally, like accuracy, latency, memory, and so on. \n\nWeakness:\n1. Each part of the technique is not that novel. It is like a combination of several existing tricks to perform comprehensive pruning, not enough analysis of the convergence or correctness. For example, how to guarantee the convergence of algorithm 3? \n2. The comparisons on Imagenet are too weak. There are many SOTA pruning methods on Imagenet. The paper only lists four of them. I suggest comparing the paper with more recent SOTA pruning papers and comparing on more benchmark models besides Res50. \nfor example, DMCP: Differentiable Markov Channel Pruning for Neural Networks\neagleeye: fast sub-net evaluation for efficient neural network pruning. \ngdp: neural network pruning via gates with differentiable polarization.\nand so on. \n\n",
          "summary_of_the_review": "Due to the concerns on the novelty and the weak comparisons on Imagenet. I temporarily think this paper is marginally below the acceptance threshold. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper61/Reviewer_EiLE"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper61/Reviewer_EiLE"
        ]
      },
      {
        "id": "9KqOYKW7Eu9",
        "original": null,
        "number": 1,
        "cdate": 1642696835346,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696835346,
        "tmdate": 1642696835346,
        "tddate": null,
        "forum": "tG8QrhMwEqS",
        "replyto": "tG8QrhMwEqS",
        "invitation": "ICLR.cc/2022/Conference/Paper61/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The paper proposes a strategy for incrementally pruning deep learning models based on activation values. The approach can satisfy different kinds of requirements, trading off between accuracy and sparsity.\n\nThe approach seems promising and seems to have competitive performance. However, the method is described by reviewers as a combination of ideas that have been proposed in the literature, and the experimental evaluation relies too much on a dataset considered too small to be reliable in such experiments --- CIFAR10. We do not expect substantial experiments within the rebuttal period: such comparisons with relevant SOTA methods should have been present in the submission. Moreover, the strategy proposed for selecting a threshold seems to rely on some doubtful assumptions, and there are no benchmarks on actual runtime.\n\nThe writing has improved based on reviewer input, and the reviewers are satisfied with this aspect. I would still add that I would prefer some clarity in the method presentation: is there a quantity being optimized? is there a value we can monitor to ensure our reimplementation is correct? etc. In addition I would like to ask authors in the next revision to be mindful to the difference between `\\citet` and `\\citep` in author-year citations -- see e.g. the first two ones in 3.1."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}