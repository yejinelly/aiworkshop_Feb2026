{
  "id": "z2zmSDKONK",
  "original": "TP3akcxcGD",
  "number": 210,
  "cdate": 1632875436760,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875436760,
  "tmdate": 1676330683033,
  "ddate": null,
  "content": {
    "title": "Exploring the Robustness of Distributional Reinforcement Learning against Noisy State Observations",
    "authorids": [
      "~Ke_Sun6",
      "~Yi_Liu13",
      "~Yingnan_Zhao1",
      "~Hengshuai_Yao2",
      "~SHANGLING_JUI1",
      "~Linglong_Kong2"
    ],
    "authors": [
      "Ke Sun",
      "Yi Liu",
      "Yingnan Zhao",
      "Hengshuai Yao",
      "SHANGLING JUI",
      "Linglong Kong"
    ],
    "keywords": [
      "distributional reinforcement learning",
      "robustness"
    ],
    "abstract": "In real scenarios, state observations that an agent observes may contain measurement errors or adversarial noises, misleading the agent to take suboptimal actions or even collapse while training. In this paper, we study the training robustness of distributional Reinforcement Learning~(RL), a class of state-of-the-art methods that estimate the whole distribution, as opposed to only the expectation, of the total return. Firstly, we propose State-Noisy Markov Decision Process~(SN-MDP) in the tabular case to incorporate both random and adversarial state observation noises, in which the contraction of both expectation-based and distributional Bellman operators is derived. Beyond SN-MDP with the function approximation, we theoretically characterize the bounded gradient norm of histogram-based distributional loss, accounting for the better training robustness of distribution RL. We also provide stricter convergence conditions of the Temporal-Difference~(TD) learning under more flexible state noises, as well as the sensitivity analysis by the leverage of influence function. Finally, extensive experiments on the suite of games show that distributional RL enjoys better training robustness compared with its expectation-based counterpart across various state observation noises.",
    "one-sentence_summary": "We study the training robsutnesss of distributional reinforcement learning against noisy state observations.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "sun|exploring_the_robustness_of_distributional_reinforcement_learning_against_noisy_state_observations",
    "pdf": "/pdf/dc058f4eb28a9f7cf107fec20448bda705921da4.pdf",
    "supplementary_material": "/attachment/b2526dfc4951b70b3a1400cf2b198fb7f4a6d361.zip",
    "_bibtex": "@misc{\nsun2022exploring,\ntitle={Exploring the Robustness of Distributional Reinforcement Learning against Noisy State Observations},\nauthor={Ke Sun and Yi Liu and Yingnan Zhao and Hengshuai Yao and SHANGLING JUI and Linglong Kong},\nyear={2022},\nurl={https://openreview.net/forum?id=z2zmSDKONK}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "z2zmSDKONK",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 14,
    "directReplyCount": 5,
    "revisions": true,
    "replies": [
      {
        "id": "EJnWIx-xMA3",
        "original": null,
        "number": 1,
        "cdate": 1635838440155,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635838440155,
        "tmdate": 1638088007704,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "z2zmSDKONK",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper investigated the adversarial robustness of distributional RL against noisy/adversarial states. They proved the Lipschitz continuity of distributional RL , provided convergence condition for TD update under noisy states, and conducted sensitivity analysis",
          "main_review": "The idea of understanding the distributional robustness of distributional RL seems novel and has not been done before. Other than that, the technical contributions and insights are marginal, unclear, and disconnected: \n- Theorem 1 is a marginal extension of (Zhang et al., 2020): they impose a distribution $N$ on a disturbance function $v(s)$\n- Function approximation: It was actually a simple linear model with KL loss as the distributional loss for distributional RL, thus the Lipschipts smoothness there is very straightforward and is not really helpful for the real setting of distributional RL. Moreover, In RL, we often consider bounded reward, so if we consider the expected RL with squared loss function, it still enjoys all the properties of KL loss arising from distributional RL \n- Disconnected results: How were the analysis in Sections 4.2 and 4.3 related to distributional RL? After the sensitivity analysis, the paper did not give any actionable insights from their analysis except that \"the degree of sensitivity is heavily determined by the task\" . This conclusion is not helpful and does not need any such sensitivity analysis. \n- The paper says that QRDQN is more robust than DQN in MountainCar but Figure 3 tells the opposite. Plus, three experiments are of the same nature; thus I think it is not necessary to split it into three subsections to make them sound comprehensive. \n- Section 5.3: \"QRDQN eventually achieves similar performance as DQN, although QRDQN significantly reduces the sample efficiency [in Breakout]\". From my own experience with this experiment, I don't think so. \n\n\n**Minor comments**:\n- Eq (4) can be simplified \n- Section 3.1: \"the state space go to infinity\": unclear \n\n- ",
          "summary_of_the_review": "The idea of exploring adversarial robustness of distributional RL is interesting and novel, but the analysis presented in the paper are marginal, disconnected, and does not support the understanding of the robustness of distributional RL. \n\n===== AFTER REBUTTAL ===    \nIncrease the score to 3",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Reviewer_DaN6"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Reviewer_DaN6"
        ]
      },
      {
        "id": "tDdPwVjcOZr",
        "original": null,
        "number": 2,
        "cdate": 1636002361003,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636002361003,
        "tmdate": 1636009325852,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "z2zmSDKONK",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies the robustness of distributional reinforcement learning, in particular the robustness on state observations, which have been demonstrated in a few papers on adversarial attacks to deep reinforcement learning. Compared to existing works on robust reinforcement learning on state observations, the main difference in this work is that it considers the distributional RL setting and also considers noise during training time. Theoretically, the authors find that distributional RL can be more robust under this setting, via the lens of Lipschitz continuity of the loss function and the influence function. The findings are also verified empirically on 4 benchmarks.",
          "main_review": "Strengths:\n\n1. This paper studies the robustness of distributional RL agents, which were never done by previous works. The results are quite promising and show that distributional RL might be more robust than expectation-based RL agents under noises on state observations.\n\n2. The analysis includes both tabular and function approximation case, and an important theorem on the Lipschitz continuity of distributional RL is given.\n\n3. Experiments are relatively comprehensive.\n\nWeaknesses:\n\n1. The proposed SN-MDP seems to be very similar to SA-MDP (Zhang et al., 2020). I understand the analysis under the distributional RL setting is new, but it is not clear to me what is the main difference in terms of SA-MDP formulation. Can you explain?\n\n1. The paper is not well organized and writing can be improved. For example, there is no need to create 4 sub-sections for showing experimental results on 4 models. I think one subsection for random noise and one subsection for adversarial noise can be better. Theorems are not well motivated and the writing is hard to follow, especially in Section 4.2 and 4.3.\n\nQuestions:\n\n1. In Figure 3 the DQN lines (dotted red lines) look better than QRDQN, which do not match texts (\"QRDQN (solid lines) almost consistently outperforms\nDQN\"). Is it a plotting error?\n\n2. Is it possible to empirically measure the Lipschitz continuity (Theorem 2), the positive definitiveness of the matrix $X^T DPE$ (Theorem 3), and the influence function (Theorem 4) on a toy example?",
          "summary_of_the_review": "I feel this paper does present an interesting study but my main concern on is its presentation. I feel this paper can become a good paper if the weakness and questions mentioned above can be addressed. I currently rate the paper at the borderline but I am willing to reevaluate the paper based on the authors' response.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Reviewer_aTVu"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Reviewer_aTVu"
        ]
      },
      {
        "id": "Hiujprwqefj",
        "original": null,
        "number": 3,
        "cdate": 1636055633945,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636055633945,
        "tmdate": 1636055756817,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "z2zmSDKONK",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This work presented State-Noisy Markov Decision Process (SN-MDP), where there is a noise generating mechanism (either from the environment noise or from the adversary), and the theoretical properties (such as convergence and contraction) for corresponding (expected) Bellman operator and distributional Bellman operator were proved. The theoretical analysis was done for both tabular and linear funcion approximation settings. Especially in function approximation setting, authors characterized the robustness blessing of distributional RL based on histogram distributional loss and analyzed how the noise factor affects TD learning by using influence function that utilizes the perburbation method. Empirical analysis was done for DQN and QRDQN by varying noise standard deviations and the position of noise (state/successor state or both), which aims to support the authors' intuition coming from their theorems. ",
          "main_review": "Authors extended State-Adversarial MDP to SN-MDP by considering more general noise generating mechanism. The theoretical contribution of this work seems clear, but I couldn't find out clear motivation and contribution from the experiments. The given experiments seems more focused on whether SN-MDP's Bellman operators and TD learnings can be used for various control problems or not and whether those are aligned with theoretical intuitions. However, I believe authors should have given experiments on the usefulness of this framework (e.g., authors may suggest more practical but simple problems where training observations are noisy as stated in the introduction). Also, only 3 runs are used for each experiments, and all results are reported without standard errors, which means that we cannot evaluate the statistical significance just by using the reported results. Therefore, I believe the empirical results should be refined for this work to be accepted. ",
          "summary_of_the_review": "Although the theoretical contribution seems clear, I believe we need additional experiments to support the authors' claim.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Reviewer_crGD"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Reviewer_crGD"
        ]
      },
      {
        "id": "XS_RQRkl0Va",
        "original": null,
        "number": 4,
        "cdate": 1636872441417,
        "mdate": 1636872441417,
        "ddate": null,
        "tcdate": 1636872441417,
        "tmdate": 1636872441417,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "Hiujprwqefj",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Comment",
        "content": {
          "title": "Response",
          "comment": "Thank you for your valuable suggestions. Below we address all your concerns. Please kindly let us know if there are any further questions.\n\n**Q1. The motivation and contribution from experiments.**\n\nThe key we hope to demonstrate in the experiments is that the training robustness of distributional RL is advantageous to expectation-based RL over different noisy settings, including perturbations on current and next states. For instance, QRDQN(solid lines) consistently outperforms DQN (dashed lines) in the same color (the same perturbation strength) across four games and various kinds of state noises. \n\nAs suggested by you, we have re-organized the experiment structure into three parts, including the SN-MDP setting, function approximation with random noises, and with adversarial noises. In particular, in the SN-MDP setting (Section 5.1) we show both expectation-based and distributional RL can converge although into different levels, which is aligned with the convergence results in Theorem 1 of Section 3. Moreover, we demonstrate distributional RL enjoys better training robustness across both random and adversarial noises in Section 5.2 and 5.3, which are consistent with the robustness blessing of distributional RL we analyzed in Section 4. Please refer to the revised version for a more detailed explanation, and it would be much appreciated for you to take account of this revision for the final score.\n\n**Q2. Lacking standard deviation of learning curves.**\n\nThank you for this useful suggestion and we have added shading indicating the standard errors in the learning curves in the revised version, which still exhibits statistical significance. We will appreciate it if you can finally take this revision into the consideration. We can perform algorithms over more runs in the final version. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ]
      },
      {
        "id": "WPlMaNBoRQ",
        "original": null,
        "number": 5,
        "cdate": 1636873029601,
        "mdate": 1636873029601,
        "ddate": null,
        "tcdate": 1636873029601,
        "tmdate": 1636873029601,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "tDdPwVjcOZr",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Comment",
        "content": {
          "title": "Response",
          "comment": "Thank you for appreciating our work. Below we address all your concerns. Please let us know if there are any further questions.\n\n**Q1. Difference between SN-MDP and SA-MDP.**\n\nThe main difference between SN-MDP and SA-MDP is that SN-MDP contains more comprehensive state noises, including both random and adversarial noises, while SA-MDP only focuses on the adversarial setting. The difference leads us to consider two kinds of distributional Bellman operators and further prove their contraction and convergence as shown in our appendix, respectively. In addition, we also simultaneously consider both these two kinds of state noises in our experiments, and thus our analysis is more general than SA-MDP. Most importantly, as you said we are focused on the distributional RL setting.   \n\n**Q2. Paper organization.**\n\nWe agree with your advice. Thus, as suggested by you we revised our paper and split the experiment into three parts, including SN-MDP, random setting and adversarial setting. The last two parts are aligned with your suggestion and we additionally leverage the first SN-MDP part to demonstrate that both expectation-based and distributional RL can convergence, which is consistent with Theorem 1 and 3 we analyzed in Section 3. In addition, we defer the original Section 4.2 and 4.3 into the appendix in order to let our writing easier to follow. We believe under our revisions both theorems and empirical results echo better with each other, and it would be much appreciated if you could take this revision into your reevaluation for our paper.\n\n**Q3. Explanation of Figure 3.**\n\nWe would like to clarify that this conclusion is made when the agent observes the noisy state observations (blue and green lines), where we find QRDQN (blue or green solid lines) is almost superior to DQN (blue or green dashed lines), respectively. In Mountain Car, DQN (red dotted line) indeed outperforms QRDQN (red solid line) without state noises, but under the noisy states, QRDQN is still more robust than DQN, making our conclusion more compelling. \n\n**Q4. Empirical measurement on a toy example.**\n\nWe guess your question might be caused by some disturbing proof from our theorems, but we would like to clarify that in the linear function approximation case these proofs in Appendix can be rigorously justified. As suggested by you, we can empirically measure these claims in theorems on a toy example in the appendix of the final version to make our conclusion more rigorous."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ]
      },
      {
        "id": "aPntTpjuMd",
        "original": null,
        "number": 6,
        "cdate": 1636877875926,
        "mdate": 1636877875926,
        "ddate": null,
        "tcdate": 1636877875926,
        "tmdate": 1636877875926,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "EJnWIx-xMA3",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Comment",
        "content": {
          "title": "Response",
          "comment": "Thank you for your valuable comments. Below we address all your concerns. Please let us know if there are any further questions.\n\n**Q1.Explanation about Theorem 1.**\n\nWe would like to clarify that Theorem 1 is not a marginal extension of Zhang et.al, 2020, as we are focused on the convergence proof of **distributional Bellman operators** rather than the only expectation-based Bellman operators in Zhang et.al, 2020. Our proof procedure is also vastly different as we mainly leverage some properties of Wasserstein distance. Please refer to Appendix for more detailed proof.\n\n**Q2. Concern about Lipchitz continuity.**\n\nTo clarify your concern, we additionally add a new subsection Section 4.1 in the revised version based on one paragraph in the original paper and provide a more detailed explanation on this issue. Specifically, the gradient norm of least square loss in the expectation-based RL can be formulated as \n\n$$| U_t-   w_{t}^{\\top} {x}_t  |  \\cdot  || w_t ||$$\n\nwhere $U_t$ can be either an unbiased estimate via Monte Carlo method with $U_t=\\sum_{k=0}^\\infty \\gamma^k r_{t+k+1}$, or a biased estimate via TD learning with $U_t=r_{t+1}+\\gamma w_{t}^{\\top} x_{t+1}$. Even though we have the bounded reward assumption, leading to $U_t  \\in [\\frac{R_{\\text{min}}}{1-\\gamma}, \\frac{R_{\\text{max}}}{1-\\gamma}]$, we can not bound the $w_{t}^{\\top} x_{t}$  as the adversary can impose arbitrary large perturbations on ${x}_{t}$. Thus, the term $|U_t - w_t^T {x}_t|$ is unbounded. More details are provided in Section 4.1 in the revised version. By contrast, the histogram distributional loss in distribution RL can guarantee a bounded gradient norm regardless of the perturbation strength.\n\n**Q3.Explanation about Section 4.2 and 4.3.** \n\nThe TD convergence condition and influence function analysis in Sections 4.2 and 4.3 are based on expectation-based and heuristically applied in the distributional RL in experiments in order to provide us with more insights into the different sensitivity of perturbation on current and next states. This analysis can be useful especially in safe-critical scenarios as we can gain more knowledge when the adversary imposes perturbations on either current or next states in the TD learning.  However, your point also makes sense as these two sections seem not to be strongly related to the main conclusion. Thus, we defer Section 4.2 and 4.3 in the appendix in the revised version.\n\n**Q4.Results in Figure 3.**\n\nWithout noisy states, DQN (red dotted line) is superior to QRDQN (red solid line) as QRDQN does not necessarily outperform DQN across all games. However, with noisy states, QRDQN (blue and green solid lines) surpasses DQN (green and blue dotted lines), respectively, especially for current and both state settings, thus showing better training robustness. Therefore, the better robustness of QRDQN when exposing state noises regardless of whether it outperforms DQN or not without state noises, in fact makes our conclusion more compelling.\n\n**Q5.Section 5.3.** \n\nIn Figure 4 of Section 5.3, both DQN and QRDQN eventually achieved average returns around 400 in Breakout, which are similar. QRDQN (red solid line) converges much faster than DQN (red dotted line), showing more sample efficiency. Our implementation is based on the distributional RL baselines implemented by Zhang (2018) [1], with 2.6k stars. Also, our result is also similar to that in README.md file in this github repository. \n\n[1] Shangtong Zhang. Modularized implementation of deep RL algorithms in pytorch.  https://github.com/ShangtongZhang/DeepRL, 2018.\n\n**Q6.Minor comments.** \n\nWe think Eq.(4) cannot be further simplified without additional assumptions because the policy function $\\pi_t$ is general and can be nonlinear. ``The state space goes to infinity'' implies that state space can be arbitrarily large, in which similar analysis can be conducted in the continuous state case.\n\nIn summary, it would be much appreciated if you can take all the revisions and responses into your consideration. We are glad to reply to all further questions. Thank you so much indeed."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ]
      },
      {
        "id": "hgn7MHCEaXV",
        "original": null,
        "number": 7,
        "cdate": 1636878119317,
        "mdate": 1636878119317,
        "ddate": null,
        "tcdate": 1636878119317,
        "tmdate": 1636878119317,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "z2zmSDKONK",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Comment",
        "content": {
          "title": "Reponse to all reviewers and ACs",
          "comment": "Dear all reviewers and ACs,\n\nAs suggested by all reviewers, in the rebuttal period we re-organized the paper structure to let the experimental part be more aligned with the theoretical results. A new version of our paper has been submitted and the original one is also provided in supplementary materials for reference. \n\nIn particular, we split the experiments into three new parts, i.e., SN-MNP, function approximation with random noises, and with adversarial noises suggested by **Reviewer aTVu and DaN6**. Moreover, we defer the original Section 4.2 and 4.3 to the appendix to make our writing easier to follow also suggested by **Reviewer aTVu and DaN6**. A new section 4.1 is given adapted from one paragraph from the original 4.1, which provides more details about the vulnerability of least squared loss in expectation-based RL in spite of the bounded reward assumption that **Reviewer DaN6** concerns.  Also, suggested by **Reviewer crGD**, we improved the presentation by providing all learning curves with standard errors, which never hurt the original conclusions.\n\n**We would like to clarify that all the revisions are still within the scope of the original paper without adding new experiments and new conclusions.** We believe this version can be more impressive and it would be much appreciated for all reviewers and ACs to take this revision into consideration. Thank you so much indeed.\n\nYours faithfully,\n\nAuthors "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ]
      },
      {
        "id": "PX6zkjv4hT8",
        "original": null,
        "number": 8,
        "cdate": 1637622194131,
        "mdate": 1637622194131,
        "ddate": null,
        "tcdate": 1637622194131,
        "tmdate": 1637622194131,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "WPlMaNBoRQ",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Comment",
        "content": {
          "title": "Thank you for the response",
          "comment": "Thank you answering my questions. The explanations on Figure 3 is helpful, and the reorganization of the paper also improved the overall quality and the results are connected better.\n\nI still feel the SN-SDP is the same as SA-MDP, because the adversary in the SA-MDP can be a random/gaussian noise adversary (random noise can be seen as a weak adversary). But the distributional Bellman operators proposed by this work is indeed novel. I feel the paper can be more acceptable if the author do not overclaim they propose a new SN-SDP model, but claim that they conduct distributional convergence analysis under the noisy adversary setting in SA-MDP. That will definitely make reviewers more comfortable (other reviewers also have the same concern).\n\nMy overall rating of this paper stays unchanged, and overall I believe the paper is borderline and I am okay to accept it, although I feel the concerns from reviewer DaN6 are also valid and the authors should work on addressing them to get the paper accepted.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Reviewer_aTVu"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Reviewer_aTVu"
        ]
      },
      {
        "id": "vc4ujgU4I3Y",
        "original": null,
        "number": 9,
        "cdate": 1637681883528,
        "mdate": 1637681883528,
        "ddate": null,
        "tcdate": 1637681883528,
        "tmdate": 1637681883528,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "PX6zkjv4hT8",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Comment",
        "content": {
          "title": "Thank you for the suggestion",
          "comment": "Thanks for your further suggestions and for appreciating our work again."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ]
      },
      {
        "id": "ggcqWbJRFFi",
        "original": null,
        "number": 11,
        "cdate": 1638082595549,
        "mdate": 1638082595549,
        "ddate": null,
        "tcdate": 1638082595549,
        "tmdate": 1638082595549,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "Hiujprwqefj",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Comment",
        "content": {
          "title": "Please give us further opinions on our papers",
          "comment": "Dear Reviewer crGD,\n\nWe have updated our manuscript and replied to your comments. Would you please check whether our efforts are satisfactory and raise your score? There is only one day left. Many thanks!\n\nAuthors"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ]
      },
      {
        "id": "9ymgvxM-VNb",
        "original": null,
        "number": 12,
        "cdate": 1638082711313,
        "mdate": 1638082711313,
        "ddate": null,
        "tcdate": 1638082711313,
        "tmdate": 1638082711313,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "EJnWIx-xMA3",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Comment",
        "content": {
          "title": "Please give us further opinions on our paper",
          "comment": "Dear Reviewer DaN6,\n\nWe have replied to your comments. Would you please check whether our efforts are satisfactory and raise your score? There is only one day left. Many thanks!\n\nAuthors"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ]
      },
      {
        "id": "pBEa2BgwTl",
        "original": null,
        "number": 13,
        "cdate": 1638086839755,
        "mdate": 1638086839755,
        "ddate": null,
        "tcdate": 1638086839755,
        "tmdate": 1638086839755,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "aPntTpjuMd",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Comment",
        "content": {
          "title": "Thank you for the response",
          "comment": "Thank you for the response. I think the re-structure makes the paper look better. I think this direction is interesting and the experimental result in the revised paper looks promising. The authors can explore and improve along this direction to make the contributions more significant. For the present form, I will only increase to \"reject\" for the revision, for the following reason. \n\nThe main point of the present paper is to show the robustness of distributional RL to adversarially noisy states. Their main argument is that the KL (distributional) loss is Liptchiz while expected RL has unbounded gradient norm, thus distributional RL is more robust than expected RL (Theorem 2). This is done in a simple linear setting that is highly disconnected from the real setting of distributional RL. I do not think this argument is sufficient for the main claim. For example, by using the setting of their Theorem 2, I can design a loss function that both are trivial and has an arbitrarily small Lipschitz constant (e.g., $\\mathcal{L}_{\\theta} = \\min_i p_i * \\epsilon x(s) $, this loss has Lipschitz constant of $\\leq \\epsilon$). According to their argument, the RL framework with this loss should be more robust than expected RL, but it is not. I think the use of the Lipschitz constant could be the right direction to quantify robustness (as in the vast literature of adversarially robust supervised learning); it just indicates that the simple setting of Theorem 2 does not help much to describe the robustness of distributional RL. In addition, Theorem 2 uses KL but in the experiments, the paper uses quantile loss (i.e., QRDQN), which is also disconnected. The setting of Theorem 2 is too simple to say anything about the robustness of distributional RL. \n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Reviewer_DaN6"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Reviewer_DaN6"
        ]
      },
      {
        "id": "0DsrNJ9LCw",
        "original": null,
        "number": 14,
        "cdate": 1638119019495,
        "mdate": 1638119019495,
        "ddate": null,
        "tcdate": 1638119019495,
        "tmdate": 1638119019495,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "pBEa2BgwTl",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Comment",
        "content": {
          "title": "Thank you for the further response",
          "comment": "We thank you for the further valuable comment.\n\nFirstly, we would say we truly understand your concern as there is indeed still some gap between theoretical analysis and real distributional RL algorithms, including analysis in the linear case but experiments on non-linear, and KL divergence to real Wasserstein distance(quantile loss). However, it is **a rule of thumb** to conduct analysis in linear case and then heuristically extend to the non-linear case to demonstrate the analysis results as in vast RL literature. Moreover, as directly analyzing Wasssertein distance or Quantile loss is very tricky, we focus on the KL divergence distributional RL loss, thus **allowing theoretical analysis**. We argue that this strategy is OK for the theoretical analysis, and we demonstrate the correctness through rigorous experiments, although analysis on quantile loss and the non-linear case would definitely make our conclusion more trustworthy."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ]
      },
      {
        "id": "Wy-im0uf0G7D",
        "original": null,
        "number": 1,
        "cdate": 1642696845032,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696845032,
        "tmdate": 1642696845032,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "z2zmSDKONK",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "Description of paper content:\n\nA mixed theoretical and experimental paper that investigates the robustness of distributional RL to perturbations of state observations as compared to expectation-based value function learning. They provide sufficient conditions for TD\u2019s convergence and prove the Lipschitz continuity of the loss of a histogram-based KL version of distributional RL with respect to the state features, whereas this is not true for expected RL. This continuity indicates a certain robustness of the loss with respect to perturbations of the state. The theory\u2019s tie to experiment is weak in the sense that it is not predictive of the actual performance of any algorithm. The theoretical methods are based on a previously published paper SA-MDP.\n\nSummary of paper discussion:\nThe reviewers raised concerns about the statistical significance of the experimental results, the clarity and organization of the writing, the novelty of the theoretical setting, and its usefulness for describing a real problem setting. The majority of reviewers rejected the paper and did not lift the scores after the rebuttal. \n\n(I personally wonder if the community would not benefit from conducting some of these kinds of theoretical analyses and experiments on LQR systems rather than Atari (etc.) environments.)"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "EJnWIx-xMA3",
        "original": null,
        "number": 1,
        "cdate": 1635838440155,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635838440155,
        "tmdate": 1638088007704,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "z2zmSDKONK",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper investigated the adversarial robustness of distributional RL against noisy/adversarial states. They proved the Lipschitz continuity of distributional RL , provided convergence condition for TD update under noisy states, and conducted sensitivity analysis",
          "main_review": "The idea of understanding the distributional robustness of distributional RL seems novel and has not been done before. Other than that, the technical contributions and insights are marginal, unclear, and disconnected: \n- Theorem 1 is a marginal extension of (Zhang et al., 2020): they impose a distribution $N$ on a disturbance function $v(s)$\n- Function approximation: It was actually a simple linear model with KL loss as the distributional loss for distributional RL, thus the Lipschipts smoothness there is very straightforward and is not really helpful for the real setting of distributional RL. Moreover, In RL, we often consider bounded reward, so if we consider the expected RL with squared loss function, it still enjoys all the properties of KL loss arising from distributional RL \n- Disconnected results: How were the analysis in Sections 4.2 and 4.3 related to distributional RL? After the sensitivity analysis, the paper did not give any actionable insights from their analysis except that \"the degree of sensitivity is heavily determined by the task\" . This conclusion is not helpful and does not need any such sensitivity analysis. \n- The paper says that QRDQN is more robust than DQN in MountainCar but Figure 3 tells the opposite. Plus, three experiments are of the same nature; thus I think it is not necessary to split it into three subsections to make them sound comprehensive. \n- Section 5.3: \"QRDQN eventually achieves similar performance as DQN, although QRDQN significantly reduces the sample efficiency [in Breakout]\". From my own experience with this experiment, I don't think so. \n\n\n**Minor comments**:\n- Eq (4) can be simplified \n- Section 3.1: \"the state space go to infinity\": unclear \n\n- ",
          "summary_of_the_review": "The idea of exploring adversarial robustness of distributional RL is interesting and novel, but the analysis presented in the paper are marginal, disconnected, and does not support the understanding of the robustness of distributional RL. \n\n===== AFTER REBUTTAL ===    \nIncrease the score to 3",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Reviewer_DaN6"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Reviewer_DaN6"
        ]
      },
      {
        "id": "tDdPwVjcOZr",
        "original": null,
        "number": 2,
        "cdate": 1636002361003,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636002361003,
        "tmdate": 1636009325852,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "z2zmSDKONK",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies the robustness of distributional reinforcement learning, in particular the robustness on state observations, which have been demonstrated in a few papers on adversarial attacks to deep reinforcement learning. Compared to existing works on robust reinforcement learning on state observations, the main difference in this work is that it considers the distributional RL setting and also considers noise during training time. Theoretically, the authors find that distributional RL can be more robust under this setting, via the lens of Lipschitz continuity of the loss function and the influence function. The findings are also verified empirically on 4 benchmarks.",
          "main_review": "Strengths:\n\n1. This paper studies the robustness of distributional RL agents, which were never done by previous works. The results are quite promising and show that distributional RL might be more robust than expectation-based RL agents under noises on state observations.\n\n2. The analysis includes both tabular and function approximation case, and an important theorem on the Lipschitz continuity of distributional RL is given.\n\n3. Experiments are relatively comprehensive.\n\nWeaknesses:\n\n1. The proposed SN-MDP seems to be very similar to SA-MDP (Zhang et al., 2020). I understand the analysis under the distributional RL setting is new, but it is not clear to me what is the main difference in terms of SA-MDP formulation. Can you explain?\n\n1. The paper is not well organized and writing can be improved. For example, there is no need to create 4 sub-sections for showing experimental results on 4 models. I think one subsection for random noise and one subsection for adversarial noise can be better. Theorems are not well motivated and the writing is hard to follow, especially in Section 4.2 and 4.3.\n\nQuestions:\n\n1. In Figure 3 the DQN lines (dotted red lines) look better than QRDQN, which do not match texts (\"QRDQN (solid lines) almost consistently outperforms\nDQN\"). Is it a plotting error?\n\n2. Is it possible to empirically measure the Lipschitz continuity (Theorem 2), the positive definitiveness of the matrix $X^T DPE$ (Theorem 3), and the influence function (Theorem 4) on a toy example?",
          "summary_of_the_review": "I feel this paper does present an interesting study but my main concern on is its presentation. I feel this paper can become a good paper if the weakness and questions mentioned above can be addressed. I currently rate the paper at the borderline but I am willing to reevaluate the paper based on the authors' response.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Reviewer_aTVu"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Reviewer_aTVu"
        ]
      },
      {
        "id": "Hiujprwqefj",
        "original": null,
        "number": 3,
        "cdate": 1636055633945,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636055633945,
        "tmdate": 1636055756817,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "z2zmSDKONK",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This work presented State-Noisy Markov Decision Process (SN-MDP), where there is a noise generating mechanism (either from the environment noise or from the adversary), and the theoretical properties (such as convergence and contraction) for corresponding (expected) Bellman operator and distributional Bellman operator were proved. The theoretical analysis was done for both tabular and linear funcion approximation settings. Especially in function approximation setting, authors characterized the robustness blessing of distributional RL based on histogram distributional loss and analyzed how the noise factor affects TD learning by using influence function that utilizes the perburbation method. Empirical analysis was done for DQN and QRDQN by varying noise standard deviations and the position of noise (state/successor state or both), which aims to support the authors' intuition coming from their theorems. ",
          "main_review": "Authors extended State-Adversarial MDP to SN-MDP by considering more general noise generating mechanism. The theoretical contribution of this work seems clear, but I couldn't find out clear motivation and contribution from the experiments. The given experiments seems more focused on whether SN-MDP's Bellman operators and TD learnings can be used for various control problems or not and whether those are aligned with theoretical intuitions. However, I believe authors should have given experiments on the usefulness of this framework (e.g., authors may suggest more practical but simple problems where training observations are noisy as stated in the introduction). Also, only 3 runs are used for each experiments, and all results are reported without standard errors, which means that we cannot evaluate the statistical significance just by using the reported results. Therefore, I believe the empirical results should be refined for this work to be accepted. ",
          "summary_of_the_review": "Although the theoretical contribution seems clear, I believe we need additional experiments to support the authors' claim.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Reviewer_crGD"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Reviewer_crGD"
        ]
      },
      {
        "id": "hgn7MHCEaXV",
        "original": null,
        "number": 7,
        "cdate": 1636878119317,
        "mdate": 1636878119317,
        "ddate": null,
        "tcdate": 1636878119317,
        "tmdate": 1636878119317,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "z2zmSDKONK",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Official_Comment",
        "content": {
          "title": "Reponse to all reviewers and ACs",
          "comment": "Dear all reviewers and ACs,\n\nAs suggested by all reviewers, in the rebuttal period we re-organized the paper structure to let the experimental part be more aligned with the theoretical results. A new version of our paper has been submitted and the original one is also provided in supplementary materials for reference. \n\nIn particular, we split the experiments into three new parts, i.e., SN-MNP, function approximation with random noises, and with adversarial noises suggested by **Reviewer aTVu and DaN6**. Moreover, we defer the original Section 4.2 and 4.3 to the appendix to make our writing easier to follow also suggested by **Reviewer aTVu and DaN6**. A new section 4.1 is given adapted from one paragraph from the original 4.1, which provides more details about the vulnerability of least squared loss in expectation-based RL in spite of the bounded reward assumption that **Reviewer DaN6** concerns.  Also, suggested by **Reviewer crGD**, we improved the presentation by providing all learning curves with standard errors, which never hurt the original conclusions.\n\n**We would like to clarify that all the revisions are still within the scope of the original paper without adding new experiments and new conclusions.** We believe this version can be more impressive and it would be much appreciated for all reviewers and ACs to take this revision into consideration. Thank you so much indeed.\n\nYours faithfully,\n\nAuthors "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper210/Authors"
        ]
      },
      {
        "id": "Wy-im0uf0G7D",
        "original": null,
        "number": 1,
        "cdate": 1642696845032,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696845032,
        "tmdate": 1642696845032,
        "tddate": null,
        "forum": "z2zmSDKONK",
        "replyto": "z2zmSDKONK",
        "invitation": "ICLR.cc/2022/Conference/Paper210/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "Description of paper content:\n\nA mixed theoretical and experimental paper that investigates the robustness of distributional RL to perturbations of state observations as compared to expectation-based value function learning. They provide sufficient conditions for TD\u2019s convergence and prove the Lipschitz continuity of the loss of a histogram-based KL version of distributional RL with respect to the state features, whereas this is not true for expected RL. This continuity indicates a certain robustness of the loss with respect to perturbations of the state. The theory\u2019s tie to experiment is weak in the sense that it is not predictive of the actual performance of any algorithm. The theoretical methods are based on a previously published paper SA-MDP.\n\nSummary of paper discussion:\nThe reviewers raised concerns about the statistical significance of the experimental results, the clarity and organization of the writing, the novelty of the theoretical setting, and its usefulness for describing a real problem setting. The majority of reviewers rejected the paper and did not lift the scores after the rebuttal. \n\n(I personally wonder if the community would not benefit from conducting some of these kinds of theoretical analyses and experiments on LQR systems rather than Atari (etc.) environments.)"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}