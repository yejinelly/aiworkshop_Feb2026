{
  "id": "Jvoe8JCGvy",
  "original": "NuteUKFFZpP",
  "number": 205,
  "cdate": 1632875436449,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875436449,
  "tmdate": 1676330683408,
  "ddate": null,
  "content": {
    "title": "Online MAP Inference and Learning for Nonsymmetric Determinantal Point Processes",
    "authorids": [
      "~Aravind_Reddy1",
      "~Ryan_Rossi1",
      "zsong@adobe.com",
      "~Anup_Rao1",
      "~Tung_Mai1",
      "lipka@adobe.com",
      "~Gang_Wu4",
      "~Eunyee_Koh1",
      "~Nesreen_Ahmed1"
    ],
    "authors": [
      "Aravind Reddy",
      "Ryan Rossi",
      "Zhao Song",
      "Anup Rao",
      "Tung Mai",
      "Nedim Lipka",
      "Gang Wu",
      "Eunyee Koh",
      "Nesreen Ahmed"
    ],
    "keywords": [
      "online algorithms",
      "nonsymmetric determinantal point processes"
    ],
    "abstract": "In this paper, we introduce the online and streaming MAP inference and learning problems for Non-symmetric Determinantal Point Processes (NDPPs) where data points arrive in an arbitrary order and the algorithms are constrained to use a single-pass over the data as well as sub-linear memory. The online setting has an additional requirement of maintaining a valid solution at any point in time. For solving these new problems, we propose algorithms with theoretical guarantees, evaluate them on several real-world datasets, and show that they give comparable performance to state-of-the-art offline algorithms that store the entire data in memory and take multiple passes over it.",
    "one-sentence_summary": "We introduce online and streaming MAP inference and learning problems for Non-symmetric Determinantal Point Processes (NDPPs), design algorithms with both theoretical and empirical guarantees, and prove a space lower bound for our inference problem. ",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "reddy|online_map_inference_and_learning_for_nonsymmetric_determinantal_point_processes",
    "pdf": "/pdf/34f9927672350890749dc22d406734c6a5118d48.pdf",
    "_bibtex": "@misc{\nreddy2022online,\ntitle={Online {MAP} Inference and Learning for Nonsymmetric Determinantal Point Processes},\nauthor={Aravind Reddy and Ryan Rossi and Zhao Song and Anup Rao and Tung Mai and Nedim Lipka and Gang Wu and Eunyee Koh and Nesreen Ahmed},\nyear={2022},\nurl={https://openreview.net/forum?id=Jvoe8JCGvy}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "Jvoe8JCGvy",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 14,
    "directReplyCount": 7,
    "revisions": true,
    "replies": [
      {
        "id": "fevVCOB6bEr",
        "original": null,
        "number": 1,
        "cdate": 1635872990624,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635872990624,
        "tmdate": 1635872990624,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes online and streaming algorithms for MAP inference and learning for nonsymmetric determinantal point processes (NDPPs).  For the streaming setting, data points arrive in an arbitrary order, and the algorithms are constrained to using a single pass over the data, along with requiring sublinear memory consumption.  In the online setting, there is the additional requirement of maintaining a valid solution at any time step.  The authors provide some theoretical guarantees for the proposed algorithms, and perform experiments that demonstrate that their performance is comparable to (or better than) offline algorithms for these tasks.\n",
          "main_review": "Strengths:\n- The proposed online and streaming algorithms for MAP inference and learning appear to be novel, and in a number of cases empirically outperform state-of-the-art offline NDPP algorithms for these tasks.\n- The proposed streaming MAP inference algorithm (Alg. 1) has theoretical guarantees for MAP approximation quality, and time and space complexity.  The proposed online MAP inference algorithms (Algs. 2 and 4) have theoretical guarantees for time and space complexity.\n- The proposed online learning algorithm (Alg. 3) has theoretical guarantees for time and space complexity.\n- The paper is reasonably well written and easy to follow.\n\nWeaknesses:\n- Some of the claims made in Sec. 6.1 regarding the state-of-the-art NDPP learning algorithm in Gartrell et al. (2021) appear to be incorrect.  Sec. 6.1 claims that the Gartrell et al. (2021) learning algorithm: 1) must store all training data in memory; 2) must make multiple passes over the data; and 3) subsets are not processed sequentially as they arrive.  However, the implementation of the learning algorithm in Gartrell et al. (2021) uses Adam (a variant of SGD), which can run with a batch size of 1, or with minibatches, and thus can be run in a streaming setting that does not require all data to be loaded into memory.  Thus, claims 1 - 3 seem to be incorrect.\n- The normalization term, $Z(V_{S_t}, B_{S_t}, C)$, used in the approximate objective (Eq. 4) for the online learning algorithm is not equivalent to the standard normalization term for a NDPP ($Z(V, B, C)$), nor is it clear that this \u201capproximate\u201d normalization term actually provides a reasonable approximation to the true NDPP normalizer.  The authors do not address this issue in the paper, and thus the proposed online learning algorithm has no theoretical approximation guarantees when compared to standard NDPP learning algorithms.  Therefore, optimizing Eq. 4 appears to violate the requirements described in Definition 8, since the true NDPP regularized log-likelihood is not being maximized.  This seems to be a critical issue. \n",
          "summary_of_the_review": "This paper has strong contributions in the area of streaming and online MAP inference algorithms.  However, there are some notable issues with the contributions regarding the online learning algorithm, including the comparison with prior work, and the correctness of the approximate optimization objective (Eq. 4), as described above.  Thus, unless the authors can address these issues in the rebuttal, it is hard to recommend this paper for acceptance.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Reviewer_9kPN"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Reviewer_9kPN"
        ]
      },
      {
        "id": "sKfvH-miYj3",
        "original": null,
        "number": 2,
        "cdate": 1635896514877,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635896514877,
        "tmdate": 1635896804946,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces MAP-inference and learning algorithms for nonsymmetric determinantal point processes (NDPPs) in the streaming and online settings. \n\nThis paper provides the first analysis of NDPP-related algorithms within a streaming context; its contributions include the algorithms themselves, theoretical analyses (algorithm guarantees, space and time complexity), and experimental evaluation of these algorithms across several standard DPP-evaluation datasets.",
          "main_review": "##  Strengths\n- Clarity: this paper is well-exposed and the intuition behind the algorithms is clear (the background exposition on NDPPs is also very well written and accessible).\n- Novelty: as the authors mention, this work provides the first analysis of MAP-inference and learning for the streaming settings of NDPPs.\n- Empirical evaluation: the authors show that the online 2-neighbor greedy MAP algorithm improves upon _the offline algorithm_, which is a surprising and meaningful result.\n\n## Weaknesses\n- The streaming context is novel for NDPPs. However, as the authors point out, there exists previous work looking into streaming for standard DPPs [1]. Comparing the proposed algorithms to [1] to verify that the lack of symmetry provides similar benefits in the streaming setting to those in the offline setting stands out as a missing comparison.\n- The first streaming algorithm (Algorithm 1) is quite simplistic, and is more interesting as a baseline rather than as a contribution in of itself.\n- The core idea behind the better performing MAP algorithms (online LSS, online 2-neighbor) lies in the construction of a \"stash\" of discarded items. This idea was introduced in [1] for the MAP inference of (symmetric) DPPs, but the explicit connection between these two works is not made in this paper. If there are crucial differences between the stashes introduced in [1] and the stash used in this work, this should be mentioned explicitly and in detail; if the two ideas are similar, this should be discussed prominently in this work. Similarly, the results in Theorem 5 are (as far as I can tell) almost identical to those of [1, Theorem 3.1]. Again, this should be discussed explicitly in this paper. (Relatedly, I think there is a confusion between $\\epsilon$ (used in [1]) and $\\alpha$ (used here) to describe Online LSS, since I believe the authors use $\\epsilon$ to describe Online-LSS in the experimental section. \n\n## Questions / comments\n- In [1], algorithm 1's dependency on \u0394 is an issue, since \u0394 can be arbitrarily small. Does a similar issue arise for Online LSS and Online 2-neighbor? If so, can this be addressed?\n- More generally, being explicit about how the lack of symmetry in the DPP changes how the streaming setting must be approached would make for an interesting contribution of this work.\n- Could 2-neighbor be extended to arbitrary sizes of subsets (3-neighbor, etc.)? Understanding at which point the degree of interactions cease to provide benefits that are worth the increase in memory/time constraints would be a valuable contribution to the DPP community: are pairwise interactions, as in 2-neighbor, sufficient to characterize most of the necessary DPP properties?\n- The derivation of the gradient updates for the online algorithm can be removed from the main paper.\n- Can the authors provide any insight into the stunning performance of the online learning algorithm? Compared to the offline algorithm, the online learning seems to converge almost an order of magnitude faster. It would be interesting to see if it's possible to switch from the online to the offline algorithm after the initial jump in log-likelihood, to achieve the best of both worlds (fast convergence, low NLL).\n- Am I correct in understanding that Figure 1 reports the volume rather than the log volume? If so, the authors should consider log-warping the evaluation function $f$, since improvements in the range of $10^{-20}$ are difficult to gauge.\n- Can you clarify the experimental conditions used for the offline learning algorithm (batch size, etc.)?\n\n[1] Online MAP Inference of Determinantal Point Processes, Bhaskara et al., 2020",
          "summary_of_the_review": "This paper proposes the first analysis of MAP inference and learning of nonsymmetric DPPs in a streaming setting; the authors propose novel algorithms, provide guarantees and complexity analyses, and evaluate their algorithms empirically across a variety of benchmarks. Startlingly, the authors show that their online algorithms are competitive with (and often outperform) their offline equivalents.\n\nMy main concern with this paper is novelty: there is significant overlap between the MAP-inference section of this work and previous work by Bhaskara et al. (2020), both in terms of the key ideas (using a stash) and in how the algorithms are analyzed. If this overlap is only in appearance, the authors should discuss in detail where their contributions depart from this previous work. Currently, it is difficult to understand the extent of the novelty of this work.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Reviewer_NfZW"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Reviewer_NfZW"
        ]
      },
      {
        "id": "tRjOnHfVDHz",
        "original": null,
        "number": 3,
        "cdate": 1635896581534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635896581534,
        "tmdate": 1635896581534,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces the streaming and online MAP inference and learning problems for NDPPs.\n- For streaming MAP inference, an algorithm is proposed with total time linear in $n$ and memory that is constant in $n$;\n- For online MAP inference, several algorithms are proposed such that at any point in time a valid solution is maintained;\n- For online learning algorithm, a single pass algorithm is proposed with memory that is constant in $m$;\n- Experiments are conducted to show that these streaming and online algorithms achieves comparable performance to state-of-the-art offline algorithms.",
          "main_review": "The problems that this paper studies are very interesting. Several algorithms are proposed to solve these problems, and the effectiveness of the algorithms are verified through experiments.\n\nThe technical sections are a bit hard to follow, and a lot of details are omitted to save space. For example, for outline of Algorithm 2, the auxiliary set $T$ is defined but its size estimate is not given. It would be great to include some intuition in the main paper.",
          "summary_of_the_review": "Overall I think the problems that this paper studies are interesting and the proposed algorithm are effective.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Reviewer_FZX8"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Reviewer_FZX8"
        ]
      },
      {
        "id": "-EmM2xm8cfz",
        "original": null,
        "number": 4,
        "cdate": 1635914763394,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635914763394,
        "tmdate": 1635914763394,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies the online inference and learning problems for nonsymmetric determinantal point processes (NDPPs). The authors use the online greedy algorithm for MAP inference and modify the learning objective for being suitable in the online setting. Experiments with real-world datasets show that the proposed online algorithms are comparable or even better than state-of-the-art offline algorithms.",
          "main_review": "Strengths:\n- The paper studies a new problem of NDPPs under the online setting that has not been covered before. \n- The authors properly apply the prior online algorithm for greedy submodular maximization to the NDPPs\n- Experimental results are convincing that the effectiveness of the proposed online inference and learning algorithms for NDPPs\n\nWeaknesses:\n- The writing quality needs to be improved. The manuscript contains several typos, notational abusements which are provided with minor comments (below). Also, the paper simply introduces the proposed algorithms without any justification or intuition, which is hard to understand how the authors deal with problems under the online settings.\n- Moreover, some algorithms are already proposed in prior works, but there is no reference. For example, Algorithm 2 (Online-LSS) was proposed in [1].\n\n  [1] Bhaskara et al., Online MAP Inference of Determinantal Point Processes, NeurIPS, 2019\n- It is not clear how the streaming setting in section 4 is different from the online setting in section 5. Since both Algorithm 1 and 4 take sequential inputs (with random order or on-the-fly), it seems that both can be used for both settings. It would be great if more detailed descriptions of streaming and online settings are provided.\n- In section 5, the authors provide 2 different algorithms, i.e., local search and 2-neighborhood local search. Comparing Theorem 5 and 7, the latter has no gain in terms of runtime complexity. However, it shows better empirical performance. What is the reason for it? Can the optimality of these algorithms be analyzed? \n- The authors propose the approximate objective (Eq (4)) for learning NDPPs under the online setting. However, this is somewhat very different from the log-likelihood of DPP because the objective contains $\\log \\det(L_S) -\\log \\det(L_S + I_S)$. Does learning this objective guarantee convergence of the ground-truth NDPP objective? How does the approximated objective Eq (4) relate to the offline version of the MLE objective?\n\nMinor comments:\n\n  - A comma is missing in the fourth line of the second paragraph on page 1.\n  - Please edit \u201cstate-of-the-art\u201d in the second paragraph on page 2.\n  - Please edit \u201care minimize\u201d -> \u201care minimizing\u201d in the second last row on page 3.\n  - It would be good to place Algorithm 4 in the main manuscript.\n  - In Theorem 5 and 7, please edit $f(S) = \\det( V_S^\\top V_S + B_S^\\top C B_S)$\n  - What is $j_{\\mathrm{\\max}}$ in line 9 in Algorithm 1?\n",
          "summary_of_the_review": "Although this paper firstly studies new problems of online NDPPs, it has a lack of algorithm novelty, theoretical analyses as well as writing quality for addressing their methodology. Hence, the paper should be improved for acceptance.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Reviewer_J9Ht"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Reviewer_J9Ht"
        ]
      },
      {
        "id": "2nL-DUN1TxN",
        "original": null,
        "number": 1,
        "cdate": 1637640025487,
        "mdate": 1637640025487,
        "ddate": null,
        "tcdate": 1637640025487,
        "tmdate": 1637640025487,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Comment",
        "content": {
          "title": "General response to all reviewers",
          "comment": "First of all, we would like to thank all the reviewers for their valuable feedback. We will first address some comments which were common across multiple reviews and will respond to individual reviews separately. We have updated the draft taking a good chunk of the reviewer feedback into account. Please have a look at the updated version.\n\nWe want to stress that our paper gives the first problem formulations for streaming and online versions of MAP Inference of low-rank-NDPPs (this problem formulation is not-at-all obvious from any of the previous works. We spent a non-trivial amount of time coming up with this formulation). Our formulations themselves give a good structure on how to store extremely large NDPP models (models which cannot fit by themselves in RAM) i.e. store the C matrix separately and all the v_i and b_i as (v_i, b_i) pairs (the straight-forward way to store the data would be to store V,C,B separately (as in the open-source code provided by the Scalable NDPPs ICLR 2021 paper [1]).\n\nOnce we do have these formulations, yes, our Online-LSS algorithm strictly generalizes the Online-LS algorithm from the Online-DPPs paper [2]. We did cite the paper in the related works section but we have now cited it again in the Online-LSS section of the paper. We want to stress again that this generalization is (sort of straight-forward) only because our online problem formulation generalizes the online problem formulation of [2].\n\nRegarding justification and intuition for MAP Inference algorithms: We agree that having a clean explanation for why our MAP inference algorithms work would be nice to have. Unfortunately, NDPPs are significantly more general (and complex) than DPPs and unlike the case for DPPs where the objective function corresponds to a nice geometric notion i.e. volume, the objective function for MAP Inference on NDPPs doesn\u2019t have a corresponding clean notion (if any of the reviewers are aware of such notions, please feel free to share it with us). This is actually a core issue because of which it is not at all clear how the proofs of approximation factors for similar algorithms for DPPs would generalize to NDPPs (the proof techniques for DPPs from [2] for Online-LS heavily use the fact that the objective function is a volume and thus use some coresets developed for geometric problems). Actually, we believe it might be impossible to prove any bounded approximation factor guarantees for our algorithms without any additional assumptions. We have provided a hard instance in the appendix on which all of our algorithms fail. But note that this certainly doesn\u2019t mean that our algorithms aren\u2019t very useful in practice (as our experiments demonstrate). The hope is that some theorist might be inspired by our paper and prove guarantees for our online algorithms under reasonable assumptions on the data in future work.\n\nRegarding the learning objective (Eq 4), we do not make any claim in the paper that optimizing this objective leads to any convergence of the ground-truth NDPP objective. That would be wonderful to have but unfortunately, we don\u2019t know whether we can show any such statement. Our learning section\u2019s main contributions are to shed light on the online learning problem for NDPPs and our algorithm (or heuristic, if you wish to call it) is only to show that a simple online algorithm can give good empirical performance.\n\nSome insight regarding the speedup for the learning: Effectively, we are doing 2 approximations at every time step. One is that we only use the subset seen at that time step. Two is that we only update the representation of items in that particular subset, not the whole set of items. If we think of all the items as vertices in a graph, with edges between items if they appear together in subsets, then at every time step, instead of propagating the changes from one subset to the entire graph, we are restricting it to only that subset. In practice, one reason why we get fast convergence might be that the weight of interactions between items might be very small but the offline algorithm spends a lot of resources to update all the item representations in every time step.\n\nReferences:\n\n[1]: Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes: Mike Gartrell, Insu Han, Elvis Dohmatob, Jennifer Gillenwater, and Victor-Emmanuel Brunel, ICLR 2021.\n\n[2]: Online MAP Inference of Determinantal Point Processes: Aditya Bhaskara, Amin Karbasi, Silvio Lattanzi, and Morteza Zadimoghaddam, NeurIPS 2020)"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ]
      },
      {
        "id": "mJEE1mB3xUA",
        "original": null,
        "number": 2,
        "cdate": 1637640136586,
        "mdate": 1637640136586,
        "ddate": null,
        "tcdate": 1637640136586,
        "tmdate": 1637640136586,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "-EmM2xm8cfz",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer J9Ht",
          "comment": "Thank you for your review. We have corrected all the typos you have mentioned. We have also placed Algorithm 4 in the main manuscript. The streaming algorithm we have proposed doesn\u2019t work in the online setting because it only gives a valid solution at the end of the stream (and it needs to know the length of the stream before it begins it\u2019s run). For your questions regarding optimality of the MAP Inference algorithms and the approximate learning objective, please see the common response."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ]
      },
      {
        "id": "DqGeilOEveY",
        "original": null,
        "number": 3,
        "cdate": 1637640171856,
        "mdate": 1637640171856,
        "ddate": null,
        "tcdate": 1637640171856,
        "tmdate": 1637640171856,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "tRjOnHfVDHz",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer FZX8",
          "comment": "Thank you for your review. We have provided an upper bound on the size of the stash in the appendix but could not do so in the main paper due to space constraints. Please see the common response about intuition for the algorithms."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ]
      },
      {
        "id": "kM4RTgdRP1H",
        "original": null,
        "number": 4,
        "cdate": 1637640228752,
        "mdate": 1637640228752,
        "ddate": null,
        "tcdate": 1637640228752,
        "tmdate": 1637640228752,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "sKfvH-miYj3",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer NfZW",
          "comment": "Thank you for your review. We have now provided a hard instance in the appendix which separates online MAP inference on NDPPs from DPPs. This should also clear your question about dependency on Delta (yes, Delta can be arbitrarily small).\n\nWe agree that the first streaming algorithm is very simple. The main reason we have chosen to include it is because it is one for which we have a provable guarantee on the approximation quality. Hopefully, some of these ideas might be useful for future proofs on approximation quality of our more complex online algorithms.\n\nThe extension of 2-neighbor to subsets of arbitrary sizes is an interesting question. We leave this to future work.\n\nWe have moved the derivation of gradient updates to the appendix.\n\nYes, Figure 1 reports the quantity which is the generalization of volume in the case of DPPs. Thanks for the suggestion. Since the main goal of Figure 1 was only to compare the performance of different algorithms, we don\u2019t think changing the det to log det would make much of a difference to our main claims. But we will try it out.\n\nFor the offline learning algorithm, we used a batch size of 800 for both Instacart and MovieLens (we used the same default hyperparameters for Instacart as the paper which introduced the algorithm [1]). \n\nPlease see the common response for some of your questions regarding comparisons between NDPPs and symmetric DPPs."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ]
      },
      {
        "id": "jwDpbmAtvPf",
        "original": null,
        "number": 5,
        "cdate": 1637640284762,
        "mdate": 1637640284762,
        "ddate": null,
        "tcdate": 1637640284762,
        "tmdate": 1637640284762,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "fevVCOB6bEr",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 9kPN",
          "comment": "Thank you for your review. We agree that if a batch size of 1 is used in the algorithm by Gartrell et.al (without any shuffling of the data for SGD) and also using only a single iteration, then the 3 claims about the learning algorithm of Gartrell et.al are incorrect. But that is not what they do in the paper (they use a batch size of 800 and take 1000 iterations over the entire data for Instacart with which we compare). Even if a batch size of 1 is used in the algorithm by Gartrell et.al, our algorithm will run much faster as it doesn\u2019t need to update the representations of all the items in every time step (unlike the algorithm by Gartrell et.al). Please see the common response for more details."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ]
      },
      {
        "id": "LZXh-VE3Lsd",
        "original": null,
        "number": 6,
        "cdate": 1637855151846,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637855151846,
        "tmdate": 1637855276236,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "jwDpbmAtvPf",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Comment",
        "content": {
          "title": "Post-rebuttal feedback ",
          "comment": "I thank the authors for their rebuttal comments and the revision of their paper.  The rebuttal comments and changes have helped to clarify the contributions of this paper.  The proposed online and streaming MAP inference algorithms, and the online learning algorithm, clearly have strong empirical performance.  However, my concerns about the lack of theoretical guarantees and understanding for approximation quality remain, particularly for the online learning algorithm.  More work should be done to resolve these issues.  Therefore, I retain my score of a 5 (marginally below the acceptance threshold)."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Reviewer_9kPN"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Reviewer_9kPN"
        ]
      },
      {
        "id": "zmwbuOlwn-T",
        "original": null,
        "number": 7,
        "cdate": 1637857404576,
        "mdate": 1637857404576,
        "ddate": null,
        "tcdate": 1637857404576,
        "tmdate": 1637857404576,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "LZXh-VE3Lsd",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Comment",
        "content": {
          "title": "About guarantees of previous work",
          "comment": "Thanks for reading the rebuttal comments and looking at the revision. Regarding approximation quality, particularly for the learning algorithm, we wanted to point out that even in the paper which introduced the low-rank decomposition for NDPPs [1], the only learning guarantee they have [Theorem 1] is about the time required for computing the log-likelihood and the gradients. The paper doesn't say anything about the approximation quality of the objective (comparing their algorithm's solution value to the global maxima). We don't think there are any such learning guarantees in the current NDPP literature. If we missed something, please do let us know. Also, regarding the approximation quality guarantees for the greedy algorithm for MAP Inference (Theorem 2) in [1], the assumptions are quite strong and don't really hold for real-world datasets (we tried to compute some of the parameters they mention since we also use them in our guarantees for the streaming algorithm). Therefore, even though [1] has some theoretical guarantees on the approximation quality of the MAP solution (and no guarantees for the learning objective), its main \"effectiveness\" guarantees are also primarily empirical.\n\n[1] Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes, Mike Gartrell, Insu Han, Elvis Dohmatob, Jennifer Gillenwater, Victor-Emmanuel Brunel, ICLR 2021."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ]
      },
      {
        "id": "FCEukvCYRT2",
        "original": null,
        "number": 8,
        "cdate": 1637859411765,
        "mdate": 1637859411765,
        "ddate": null,
        "tcdate": 1637859411765,
        "tmdate": 1637859411765,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "zmwbuOlwn-T",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Comment",
        "content": {
          "title": "Clarification regarding approximation guarantees for online learning algorithm",
          "comment": "Thanks for the additional comments.  To clarify what I mean regarding an approximation guarantee for the online learning algorithm: here I am *not* referring to a guarantee that optimization of the non-convex NDPP objective function converges to a global maximum, which is a much more difficult problem; such guarantees are not generally available even for symmetric DPPs (except for very restricted cases).  Instead, I am referring to the fact that the approximate objective function (Eq. 4) optimized by the proposed online learning algorithm is not the same as the standard (offline or \"ground-truth\") NDPP objective function, as pointed out in my original review (and by reviewer J9Ht), and in your general response to all reviewers.  More theoretical understanding of the optimization of Eq. 4, and how it relates to the standard NDPP objective, should be developed."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Reviewer_9kPN"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Reviewer_9kPN"
        ]
      },
      {
        "id": "9DmqupB7v8E",
        "original": null,
        "number": 1,
        "cdate": 1642696844970,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696844970,
        "tmdate": 1642696844970,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "This paper studies online MAP inference and learning for nonsymmetric determinantal point processes (NDPPs). The main contribution is an online greedy algorithm. Surprisingly they show that their algorithm outperforms various offline algorithms on real-world datasets. That said, the main concern was the novelty with respect to the prior work of Bhaskara et al. who gave an online approximation algorithm for MAP inference in DPPs. To compare the two works: (1) Bhaskara et al. give an algorithm for DPPs, and NDPPs are more complex (2) Bhaskara et al. give provable guarantees on the approximation ratio, but no such guarantees are known for NDPPs (3) And finally, some of the key ingredients in the online algorithm for NDPPs, like the stash, were already in the work of Bhaskara et al. Overall the reviewers felt that this submission would be improved with a clearer discussion of the contributions over prior work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      },
      {
        "id": "gwG8pGrAFvQ",
        "original": null,
        "number": 1,
        "cdate": 1653021082609,
        "mdate": 1653021082609,
        "ddate": null,
        "tcdate": 1653021082609,
        "tmdate": 1653021082609,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Public_Comment",
        "content": {
          "title": "Thanks again for the feedback! Revised version accepted to ICML 2022 :)",
          "comment": "We are happy to let you know that a revised version of this paper titled \"One-Pass Algorithms for MAP Inference of Nonsymmetric Determinantal Point Processes\", in which we focus exclusively on the MAP Inference portion of this submission, was recently accepted to ICML 2022 as a short presentation. The reviews provided by the ICLR reviewers were very beneficial to us in revising our paper. We wanted to thank all of you once again!"
        },
        "signatures": [
          "~Aravind_Reddy1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "~Aravind_Reddy1"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "fevVCOB6bEr",
        "original": null,
        "number": 1,
        "cdate": 1635872990624,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635872990624,
        "tmdate": 1635872990624,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes online and streaming algorithms for MAP inference and learning for nonsymmetric determinantal point processes (NDPPs).  For the streaming setting, data points arrive in an arbitrary order, and the algorithms are constrained to using a single pass over the data, along with requiring sublinear memory consumption.  In the online setting, there is the additional requirement of maintaining a valid solution at any time step.  The authors provide some theoretical guarantees for the proposed algorithms, and perform experiments that demonstrate that their performance is comparable to (or better than) offline algorithms for these tasks.\n",
          "main_review": "Strengths:\n- The proposed online and streaming algorithms for MAP inference and learning appear to be novel, and in a number of cases empirically outperform state-of-the-art offline NDPP algorithms for these tasks.\n- The proposed streaming MAP inference algorithm (Alg. 1) has theoretical guarantees for MAP approximation quality, and time and space complexity.  The proposed online MAP inference algorithms (Algs. 2 and 4) have theoretical guarantees for time and space complexity.\n- The proposed online learning algorithm (Alg. 3) has theoretical guarantees for time and space complexity.\n- The paper is reasonably well written and easy to follow.\n\nWeaknesses:\n- Some of the claims made in Sec. 6.1 regarding the state-of-the-art NDPP learning algorithm in Gartrell et al. (2021) appear to be incorrect.  Sec. 6.1 claims that the Gartrell et al. (2021) learning algorithm: 1) must store all training data in memory; 2) must make multiple passes over the data; and 3) subsets are not processed sequentially as they arrive.  However, the implementation of the learning algorithm in Gartrell et al. (2021) uses Adam (a variant of SGD), which can run with a batch size of 1, or with minibatches, and thus can be run in a streaming setting that does not require all data to be loaded into memory.  Thus, claims 1 - 3 seem to be incorrect.\n- The normalization term, $Z(V_{S_t}, B_{S_t}, C)$, used in the approximate objective (Eq. 4) for the online learning algorithm is not equivalent to the standard normalization term for a NDPP ($Z(V, B, C)$), nor is it clear that this \u201capproximate\u201d normalization term actually provides a reasonable approximation to the true NDPP normalizer.  The authors do not address this issue in the paper, and thus the proposed online learning algorithm has no theoretical approximation guarantees when compared to standard NDPP learning algorithms.  Therefore, optimizing Eq. 4 appears to violate the requirements described in Definition 8, since the true NDPP regularized log-likelihood is not being maximized.  This seems to be a critical issue. \n",
          "summary_of_the_review": "This paper has strong contributions in the area of streaming and online MAP inference algorithms.  However, there are some notable issues with the contributions regarding the online learning algorithm, including the comparison with prior work, and the correctness of the approximate optimization objective (Eq. 4), as described above.  Thus, unless the authors can address these issues in the rebuttal, it is hard to recommend this paper for acceptance.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Reviewer_9kPN"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Reviewer_9kPN"
        ]
      },
      {
        "id": "sKfvH-miYj3",
        "original": null,
        "number": 2,
        "cdate": 1635896514877,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635896514877,
        "tmdate": 1635896804946,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces MAP-inference and learning algorithms for nonsymmetric determinantal point processes (NDPPs) in the streaming and online settings. \n\nThis paper provides the first analysis of NDPP-related algorithms within a streaming context; its contributions include the algorithms themselves, theoretical analyses (algorithm guarantees, space and time complexity), and experimental evaluation of these algorithms across several standard DPP-evaluation datasets.",
          "main_review": "##  Strengths\n- Clarity: this paper is well-exposed and the intuition behind the algorithms is clear (the background exposition on NDPPs is also very well written and accessible).\n- Novelty: as the authors mention, this work provides the first analysis of MAP-inference and learning for the streaming settings of NDPPs.\n- Empirical evaluation: the authors show that the online 2-neighbor greedy MAP algorithm improves upon _the offline algorithm_, which is a surprising and meaningful result.\n\n## Weaknesses\n- The streaming context is novel for NDPPs. However, as the authors point out, there exists previous work looking into streaming for standard DPPs [1]. Comparing the proposed algorithms to [1] to verify that the lack of symmetry provides similar benefits in the streaming setting to those in the offline setting stands out as a missing comparison.\n- The first streaming algorithm (Algorithm 1) is quite simplistic, and is more interesting as a baseline rather than as a contribution in of itself.\n- The core idea behind the better performing MAP algorithms (online LSS, online 2-neighbor) lies in the construction of a \"stash\" of discarded items. This idea was introduced in [1] for the MAP inference of (symmetric) DPPs, but the explicit connection between these two works is not made in this paper. If there are crucial differences between the stashes introduced in [1] and the stash used in this work, this should be mentioned explicitly and in detail; if the two ideas are similar, this should be discussed prominently in this work. Similarly, the results in Theorem 5 are (as far as I can tell) almost identical to those of [1, Theorem 3.1]. Again, this should be discussed explicitly in this paper. (Relatedly, I think there is a confusion between $\\epsilon$ (used in [1]) and $\\alpha$ (used here) to describe Online LSS, since I believe the authors use $\\epsilon$ to describe Online-LSS in the experimental section. \n\n## Questions / comments\n- In [1], algorithm 1's dependency on \u0394 is an issue, since \u0394 can be arbitrarily small. Does a similar issue arise for Online LSS and Online 2-neighbor? If so, can this be addressed?\n- More generally, being explicit about how the lack of symmetry in the DPP changes how the streaming setting must be approached would make for an interesting contribution of this work.\n- Could 2-neighbor be extended to arbitrary sizes of subsets (3-neighbor, etc.)? Understanding at which point the degree of interactions cease to provide benefits that are worth the increase in memory/time constraints would be a valuable contribution to the DPP community: are pairwise interactions, as in 2-neighbor, sufficient to characterize most of the necessary DPP properties?\n- The derivation of the gradient updates for the online algorithm can be removed from the main paper.\n- Can the authors provide any insight into the stunning performance of the online learning algorithm? Compared to the offline algorithm, the online learning seems to converge almost an order of magnitude faster. It would be interesting to see if it's possible to switch from the online to the offline algorithm after the initial jump in log-likelihood, to achieve the best of both worlds (fast convergence, low NLL).\n- Am I correct in understanding that Figure 1 reports the volume rather than the log volume? If so, the authors should consider log-warping the evaluation function $f$, since improvements in the range of $10^{-20}$ are difficult to gauge.\n- Can you clarify the experimental conditions used for the offline learning algorithm (batch size, etc.)?\n\n[1] Online MAP Inference of Determinantal Point Processes, Bhaskara et al., 2020",
          "summary_of_the_review": "This paper proposes the first analysis of MAP inference and learning of nonsymmetric DPPs in a streaming setting; the authors propose novel algorithms, provide guarantees and complexity analyses, and evaluate their algorithms empirically across a variety of benchmarks. Startlingly, the authors show that their online algorithms are competitive with (and often outperform) their offline equivalents.\n\nMy main concern with this paper is novelty: there is significant overlap between the MAP-inference section of this work and previous work by Bhaskara et al. (2020), both in terms of the key ideas (using a stash) and in how the algorithms are analyzed. If this overlap is only in appearance, the authors should discuss in detail where their contributions depart from this previous work. Currently, it is difficult to understand the extent of the novelty of this work.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Reviewer_NfZW"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Reviewer_NfZW"
        ]
      },
      {
        "id": "tRjOnHfVDHz",
        "original": null,
        "number": 3,
        "cdate": 1635896581534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635896581534,
        "tmdate": 1635896581534,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces the streaming and online MAP inference and learning problems for NDPPs.\n- For streaming MAP inference, an algorithm is proposed with total time linear in $n$ and memory that is constant in $n$;\n- For online MAP inference, several algorithms are proposed such that at any point in time a valid solution is maintained;\n- For online learning algorithm, a single pass algorithm is proposed with memory that is constant in $m$;\n- Experiments are conducted to show that these streaming and online algorithms achieves comparable performance to state-of-the-art offline algorithms.",
          "main_review": "The problems that this paper studies are very interesting. Several algorithms are proposed to solve these problems, and the effectiveness of the algorithms are verified through experiments.\n\nThe technical sections are a bit hard to follow, and a lot of details are omitted to save space. For example, for outline of Algorithm 2, the auxiliary set $T$ is defined but its size estimate is not given. It would be great to include some intuition in the main paper.",
          "summary_of_the_review": "Overall I think the problems that this paper studies are interesting and the proposed algorithm are effective.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Reviewer_FZX8"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Reviewer_FZX8"
        ]
      },
      {
        "id": "-EmM2xm8cfz",
        "original": null,
        "number": 4,
        "cdate": 1635914763394,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635914763394,
        "tmdate": 1635914763394,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies the online inference and learning problems for nonsymmetric determinantal point processes (NDPPs). The authors use the online greedy algorithm for MAP inference and modify the learning objective for being suitable in the online setting. Experiments with real-world datasets show that the proposed online algorithms are comparable or even better than state-of-the-art offline algorithms.",
          "main_review": "Strengths:\n- The paper studies a new problem of NDPPs under the online setting that has not been covered before. \n- The authors properly apply the prior online algorithm for greedy submodular maximization to the NDPPs\n- Experimental results are convincing that the effectiveness of the proposed online inference and learning algorithms for NDPPs\n\nWeaknesses:\n- The writing quality needs to be improved. The manuscript contains several typos, notational abusements which are provided with minor comments (below). Also, the paper simply introduces the proposed algorithms without any justification or intuition, which is hard to understand how the authors deal with problems under the online settings.\n- Moreover, some algorithms are already proposed in prior works, but there is no reference. For example, Algorithm 2 (Online-LSS) was proposed in [1].\n\n  [1] Bhaskara et al., Online MAP Inference of Determinantal Point Processes, NeurIPS, 2019\n- It is not clear how the streaming setting in section 4 is different from the online setting in section 5. Since both Algorithm 1 and 4 take sequential inputs (with random order or on-the-fly), it seems that both can be used for both settings. It would be great if more detailed descriptions of streaming and online settings are provided.\n- In section 5, the authors provide 2 different algorithms, i.e., local search and 2-neighborhood local search. Comparing Theorem 5 and 7, the latter has no gain in terms of runtime complexity. However, it shows better empirical performance. What is the reason for it? Can the optimality of these algorithms be analyzed? \n- The authors propose the approximate objective (Eq (4)) for learning NDPPs under the online setting. However, this is somewhat very different from the log-likelihood of DPP because the objective contains $\\log \\det(L_S) -\\log \\det(L_S + I_S)$. Does learning this objective guarantee convergence of the ground-truth NDPP objective? How does the approximated objective Eq (4) relate to the offline version of the MLE objective?\n\nMinor comments:\n\n  - A comma is missing in the fourth line of the second paragraph on page 1.\n  - Please edit \u201cstate-of-the-art\u201d in the second paragraph on page 2.\n  - Please edit \u201care minimize\u201d -> \u201care minimizing\u201d in the second last row on page 3.\n  - It would be good to place Algorithm 4 in the main manuscript.\n  - In Theorem 5 and 7, please edit $f(S) = \\det( V_S^\\top V_S + B_S^\\top C B_S)$\n  - What is $j_{\\mathrm{\\max}}$ in line 9 in Algorithm 1?\n",
          "summary_of_the_review": "Although this paper firstly studies new problems of online NDPPs, it has a lack of algorithm novelty, theoretical analyses as well as writing quality for addressing their methodology. Hence, the paper should be improved for acceptance.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Reviewer_J9Ht"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Reviewer_J9Ht"
        ]
      },
      {
        "id": "2nL-DUN1TxN",
        "original": null,
        "number": 1,
        "cdate": 1637640025487,
        "mdate": 1637640025487,
        "ddate": null,
        "tcdate": 1637640025487,
        "tmdate": 1637640025487,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Official_Comment",
        "content": {
          "title": "General response to all reviewers",
          "comment": "First of all, we would like to thank all the reviewers for their valuable feedback. We will first address some comments which were common across multiple reviews and will respond to individual reviews separately. We have updated the draft taking a good chunk of the reviewer feedback into account. Please have a look at the updated version.\n\nWe want to stress that our paper gives the first problem formulations for streaming and online versions of MAP Inference of low-rank-NDPPs (this problem formulation is not-at-all obvious from any of the previous works. We spent a non-trivial amount of time coming up with this formulation). Our formulations themselves give a good structure on how to store extremely large NDPP models (models which cannot fit by themselves in RAM) i.e. store the C matrix separately and all the v_i and b_i as (v_i, b_i) pairs (the straight-forward way to store the data would be to store V,C,B separately (as in the open-source code provided by the Scalable NDPPs ICLR 2021 paper [1]).\n\nOnce we do have these formulations, yes, our Online-LSS algorithm strictly generalizes the Online-LS algorithm from the Online-DPPs paper [2]. We did cite the paper in the related works section but we have now cited it again in the Online-LSS section of the paper. We want to stress again that this generalization is (sort of straight-forward) only because our online problem formulation generalizes the online problem formulation of [2].\n\nRegarding justification and intuition for MAP Inference algorithms: We agree that having a clean explanation for why our MAP inference algorithms work would be nice to have. Unfortunately, NDPPs are significantly more general (and complex) than DPPs and unlike the case for DPPs where the objective function corresponds to a nice geometric notion i.e. volume, the objective function for MAP Inference on NDPPs doesn\u2019t have a corresponding clean notion (if any of the reviewers are aware of such notions, please feel free to share it with us). This is actually a core issue because of which it is not at all clear how the proofs of approximation factors for similar algorithms for DPPs would generalize to NDPPs (the proof techniques for DPPs from [2] for Online-LS heavily use the fact that the objective function is a volume and thus use some coresets developed for geometric problems). Actually, we believe it might be impossible to prove any bounded approximation factor guarantees for our algorithms without any additional assumptions. We have provided a hard instance in the appendix on which all of our algorithms fail. But note that this certainly doesn\u2019t mean that our algorithms aren\u2019t very useful in practice (as our experiments demonstrate). The hope is that some theorist might be inspired by our paper and prove guarantees for our online algorithms under reasonable assumptions on the data in future work.\n\nRegarding the learning objective (Eq 4), we do not make any claim in the paper that optimizing this objective leads to any convergence of the ground-truth NDPP objective. That would be wonderful to have but unfortunately, we don\u2019t know whether we can show any such statement. Our learning section\u2019s main contributions are to shed light on the online learning problem for NDPPs and our algorithm (or heuristic, if you wish to call it) is only to show that a simple online algorithm can give good empirical performance.\n\nSome insight regarding the speedup for the learning: Effectively, we are doing 2 approximations at every time step. One is that we only use the subset seen at that time step. Two is that we only update the representation of items in that particular subset, not the whole set of items. If we think of all the items as vertices in a graph, with edges between items if they appear together in subsets, then at every time step, instead of propagating the changes from one subset to the entire graph, we are restricting it to only that subset. In practice, one reason why we get fast convergence might be that the weight of interactions between items might be very small but the offline algorithm spends a lot of resources to update all the item representations in every time step.\n\nReferences:\n\n[1]: Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes: Mike Gartrell, Insu Han, Elvis Dohmatob, Jennifer Gillenwater, and Victor-Emmanuel Brunel, ICLR 2021.\n\n[2]: Online MAP Inference of Determinantal Point Processes: Aditya Bhaskara, Amin Karbasi, Silvio Lattanzi, and Morteza Zadimoghaddam, NeurIPS 2020)"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper205/Authors"
        ]
      },
      {
        "id": "9DmqupB7v8E",
        "original": null,
        "number": 1,
        "cdate": 1642696844970,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696844970,
        "tmdate": 1642696844970,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "This paper studies online MAP inference and learning for nonsymmetric determinantal point processes (NDPPs). The main contribution is an online greedy algorithm. Surprisingly they show that their algorithm outperforms various offline algorithms on real-world datasets. That said, the main concern was the novelty with respect to the prior work of Bhaskara et al. who gave an online approximation algorithm for MAP inference in DPPs. To compare the two works: (1) Bhaskara et al. give an algorithm for DPPs, and NDPPs are more complex (2) Bhaskara et al. give provable guarantees on the approximation ratio, but no such guarantees are known for NDPPs (3) And finally, some of the key ingredients in the online algorithm for NDPPs, like the stash, were already in the work of Bhaskara et al. Overall the reviewers felt that this submission would be improved with a clearer discussion of the contributions over prior work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      },
      {
        "id": "gwG8pGrAFvQ",
        "original": null,
        "number": 1,
        "cdate": 1653021082609,
        "mdate": 1653021082609,
        "ddate": null,
        "tcdate": 1653021082609,
        "tmdate": 1653021082609,
        "tddate": null,
        "forum": "Jvoe8JCGvy",
        "replyto": "Jvoe8JCGvy",
        "invitation": "ICLR.cc/2022/Conference/Paper205/-/Public_Comment",
        "content": {
          "title": "Thanks again for the feedback! Revised version accepted to ICML 2022 :)",
          "comment": "We are happy to let you know that a revised version of this paper titled \"One-Pass Algorithms for MAP Inference of Nonsymmetric Determinantal Point Processes\", in which we focus exclusively on the MAP Inference portion of this submission, was recently accepted to ICML 2022 as a short presentation. The reviews provided by the ICLR reviewers were very beneficial to us in revising our paper. We wanted to thank all of you once again!"
        },
        "signatures": [
          "~Aravind_Reddy1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "~Aravind_Reddy1"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}