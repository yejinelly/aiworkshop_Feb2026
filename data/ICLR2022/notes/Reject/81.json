{
  "id": "7zFokR7k_86",
  "original": "yMuxn8yeLD",
  "number": 81,
  "cdate": 1632875427238,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875427238,
  "tmdate": 1697934969008,
  "ddate": null,
  "content": {
    "title": "Learning Symbolic Rules for Reasoning in Quasi-Natural Language",
    "authorids": [
      "~Kaiyu_Yang1",
      "~Jia_Deng1"
    ],
    "authors": [
      "Kaiyu Yang",
      "Jia Deng"
    ],
    "keywords": [],
    "abstract": "Symbolic reasoning, rule-based symbol manipulation, is a hallmark of human intelligence.  However, rule-based systems have had limited success competing with learning-based systems outside formalized domains such as automated theorem proving. We hypothesize that this is due to the manual construction of rules in past attempts. In this work, we ask how we can build a rule-based system that can reason with natural language input but without the manual construction of rules. We propose MetaQNL, a \"Quasi-Natural\" language that can express both formal logic and natural language sentences, and MetaInduce, a learning algorithm that induces MetaQNL rules from training data consisting of questions and answers, with or without intermediate reasoning steps. Our approach achieves state-of-the-art accuracy on multiple reasoning benchmarks; it learns compact models with much less data and produces not only answers but also checkable proofs.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "yang|learning_symbolic_rules_for_reasoning_in_quasinatural_language",
    "pdf": "/pdf/2cdb22933e3af98b31d272f1021372cfd1069be8.pdf",
    "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.12038/code)",
    "_bibtex": "@misc{\nyang2022learning,\ntitle={Learning Symbolic Rules for Reasoning in Quasi-Natural Language},\nauthor={Kaiyu Yang and Jia Deng},\nyear={2022},\nurl={https://openreview.net/forum?id=7zFokR7k_86}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "7zFokR7k_86",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 17,
    "directReplyCount": 6,
    "revisions": true,
    "replies": [
      {
        "id": "7s9vVGYza3d",
        "original": null,
        "number": 1,
        "cdate": 1635171670006,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635171670006,
        "tmdate": 1635171670006,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7zFokR7k_86",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Review",
        "content": {
          "summary_of_the_paper": "Traditional research on symbolic reasoning assumes input data are already translated into a format that complies with the formalism of the underlying system. A major struggle for symbolic reasoning is to handle the data coming in a natural form (e.g. images/text) and perform reasoning on the same. Inspired by this challenge, this paper undertakes the problem of converting text inputs into the format of a system formalism. \n\nThe formalism is also proposed by this paper and is called MetaQNL. The framework of MetaQNL is designed keeping in mind the specific need of operating directly on natural language sentences. This formalism supports the natural language sentences with variables within them. This trick alleviates the need of using a semantic parser to parse natural language sentences for the purpose of logical reasoning. Thus, MetaQNL, by design, is conducive to working with natural language inputs. \n\nNext, this paper proposes an algorithm to induce rules from natural language inputs within the MetaQNL framework. This paper doesn\u2019t worry about performing actual deductive reasoning/theorem proving and instead proposes to use existing provers and instead focus on the more challenging problem of rule induction. MetaInduce algorithm draws inspiration from existing ILP approaches. MetaInduce encodes the rule induction problem as a maximum satisfiability (MAX-SAT) problem, which can be solved efficiently by off-the-shelf solvers. The proposed method consists of 3 steps. \n\n1. Given a training example, a rule proposer proposes a set of concrete rules as candidates. This set can be overcomplete/inaccurate. \n2. It generates abstract rules from concrete rules via a symbolic procedure called anti-unification. This is essentially a process of aligning common substring segments across two or more strings.\n3. It encodes the proof paths in MAX-SAT and solves for a subset of all rules using a MAX-SAT solver. \n\nThis paper benchmarks the proposed method on 2 tasks - learning compositional instructions and logical reasoning. For learning compositional instructions, it works on two standard benchmarks: MiniSCAN and SCAN and recovers precisely the ground truth rules. For logical reasoning, the proposed method achieves SOTA on the RuleTaker dataset. \n\n",
          "main_review": "Overall, I liked the scope of this paper, the importance, and non-triviality of the problem, and the novelty of the proposed approach. In my view, it certainly adds a dimension to the literature on symbolic rule learning. The idea of MetaQNL is simple yet effective to handle the natural language inputs within a formal symbolic system. The idea behind MetaInduce is also quite natural. Although there are some weaknesses of the proposed approach, I still feel this is a novel idea and has the potential to yield something big in the future. \n\n__Strength__\n\n- A well-written paper.\n- A very nice literature survey and positioning of the work relative to the prior art.\n- An important problem in the broad space of AI.\n\n__Weakness__\n\n- Time complexity of each of the three steps in MetaInduce is not discussed. It will be good to shed some light on this. \n- As stated in the limitation section, the proposed approach is far from mature but serves as proof of concept.  It does not scale to millions of training examples.",
          "summary_of_the_review": "See my comments in the main review.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Reviewer_7F3Z"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Reviewer_7F3Z"
        ]
      },
      {
        "id": "JoRDRqUmoAq",
        "original": null,
        "number": 2,
        "cdate": 1635584874566,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635584874566,
        "tmdate": 1635584874566,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7zFokR7k_86",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The work proposes two new concepts:\n+ MetaQNL: a symbolic system in Quasi-Natural Language. Instead of representing rules in a formal symbolic format such as first order logic rules, in MetaQNL, a rule is represented in a Quasi-Natural Language format which includes words, variables and control symbols. Since the rules are represented in an informal representation. An interesting property of the MetaQNL representation is that it allows to perform backward or forward inference by substitution of variables with sentences. The authors assume that texts can be translated into the MetaQNL format and thus solving the reasoning problems with text input is possible via mining rules from text and backward/forward reasoning with the Quasi-Natural Language.\n+ To mine rule from text, the authors proposes an algorithm called MetaInduce. MetaInduce iterates through the training data several time to build a compact set of rules that trades complexity to prediction accuracy. It is a bottom up rule induction approach where it includes a Rule proposer which propose a concrete rule from a training example and an anti-unification module to abstract the concrete rule with more generalised rules. A pruning process based on MAX-SAT is used to prune the set of rules such that it optimise the regularised objective.",
          "main_review": "Overall the idea is novel and I like the paper presentation. However, I have some concerns regarding its ability to work with real natural language and the reproducibility of the proposed approach.\n\nMAJOR CONCERN 1:\nI have a concern that the rule proposer and the anti-unification require very well-formed of the sentences to be working well. \n\nAs we can see with the example on page 7 figure (a) about anti-unification, the sample shows very simple rules. I wonder how it will work when the sentence is really complicated with real natural language? Also the experiments in RuleTaker only with synthetically generated sentences in a very controlled language, could you please  demonstrate your results with complicated sentences beyond synthetic texts, I think the paraphrased datasets within the RuleTakers even not really natural language but that could be a good exercise for your methods to test on.\n\n\nMAJOR CONCERN 2:\nI worry about reproducibility as the source code is not open but the details explanation of the proposed approaches are missing. For example, from the paper I don't know how the rule proposer work and how the anti-unification is implemented with quasi-natural language. I would suggest to provide very detailed about the methods, with the current information I doubt that people can reimplement your work and reproduce what you have demonstrated.\n\nMAJOR CONERN 3:\nIn the RuleTaker example, it is known that Transformers are good at generalisation when they are trained on queries with depth 3 or greater. Yet transformers are not good at generalisation when it is trained on lower depth queries. Could you please provide comparison results with Transformers when it is trained at lower depths? \nOther minor comments:\n\n\n\"In contrast, our approach does not require a semantic parser, because rules in MetaQNL are directly applicable to natural language.\" ---> This is a strong statement, it requires a support with real natural language examples rather than synthetically generated sentences in the experiments.\n\n\nDefinition 6: what happen if there is no proof for a goal and the goal is proved via the close world assumption?\n\n",
          "summary_of_the_review": "The paper proposes a new concept called quasi-language which allows representing rules in an new informal format that still allows to perform forward or backward reasoning while it is assumed to be mined easily from texts. The experimental results with some datasets with synthetically generated texts show that the methods work very well and advance state-of-the-art results. \nHowever, I doubt the application of the work with natural language input, I explicitly request to perform more experiments with paraphrased datasets in RuleTaker (MAJOR CONCERN 1). I also have a concern about reproducibility as the presentation lacks details of the core components of the proposed algorithm (MAJOR CONCERN 2). I also requested an additional experiment regarding the training data with low depth queries to check the ability of generalization of the proposed approaches. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Reviewer_9uxa"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Reviewer_9uxa"
        ]
      },
      {
        "id": "egVauJudI_u",
        "original": null,
        "number": 3,
        "cdate": 1635908903510,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635908903510,
        "tmdate": 1638896292302,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7zFokR7k_86",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes an algorithm that learns rules from natural language data, and a symbolic system for manipulating these rules, where existing provers can be applied. The objective is to maximize the number of examples in a test set that are consistent with the proposed mode while minimizing the number of rules in the model. The algorithm consists of three steps - given a training example, it proposes concrete rules, abstracts concrete rules into rules with variables, and prunes the resulting rules. ",
          "main_review": "Pros: \n- The learning algorithm is interesting and the problem of automatically discovering prepositions from examples is important for ATP in formal or informal languages.\n- The paper clearly defines the terms used and explains the methods and experiments well.\n\nCons:\n- As the authors pointed out, the experiments are neither large-scale nor real-world. One result of this is existing methods already achieve good performance, and it's not clear that the proposed method results in better performance.\n- None of the components (theorem prover, rule abstraction, rule pruning) are novel individually.\n\nQuestions:\n1) Are TRUE FALSE and MAPS_TO the only special symbols? It would be helpful to state this. \n2) Are there any unprovable examples in SCAN?\n3) The number of rules and symbols learned by MetaInduce is hard to compare with the number of learned parameters in ProofWriter. Is there a better metric to compare the two methods on? \n4) What are the advantages to using the proposed symbol system instead of first order logic? ",
          "summary_of_the_review": "The paper proposes an interesting solution to an important problem, but as-is the experimental settings and results are not compelling. ",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Reviewer_CPsz"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Reviewer_CPsz"
        ]
      },
      {
        "id": "vsnyod7rm9k",
        "original": null,
        "number": 4,
        "cdate": 1635997434722,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635997434722,
        "tmdate": 1638255066314,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7zFokR7k_86",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a symbolic system in Quasi-Natural Language, MetaQNL, which is compatible with both logical inference and quasi natural language expressions, and where the basic building blocks are sentences and rules. The authors also propose MetaInduce, which learns to generalize a set of rules that explains the examples in MetaInduce.\n\nMetaInduce consists of three mains steps:\n1) a rule proposer proposes a set of concrete rules as candidates for each individual example. This set may not be fully correct and may not be the minimal explanations, and they are used to prove the example using forward/backward chaining.\n2) the authors apply a symbolic procedure called anti-unification to generate abstract rules from concrete rules.\n3) and finally, proof paths are encoded in MAX-SAT and a subset of all rules are solved using a MAX-SAT solver to find minimal possible explanations\n\nThis paper evaluates its methods on two synthetic datasets, and the authors claim to learn compact models with much less data, and produces answers as well as proofs",
          "main_review": "strengths:\n1) this is indeed novel research problem - using a learning system together with a maxsat solver to make logical inference and identify rules.  There is a spectrum where on one end logical inference instances fully in natural language, and on the other end they are fully symbolic, and this paper falls somewhere inbetween.\n2) The paper is generally well-written, and mathematical part of the paper seems to be correct.\n\nWeakness:\n\nI fail to see the real contributions in this paper.  The only novelty seems to me is MetaQNL and MetaInduce can solve formal systems as well as systems represented in quasi natural languages.  However, are there actual applications that can potentially benefit from this problem formulation?\nOne of the major claims of this paper is it can produces checkable proofs, but isn't Proofwriter also capable of generating proofs which seemed more impressive because it is a fully neural system without any explicit encodings of rules and has the potential of working with real languages.  I get that proofwriter has billions of parameters and MetaInduce learns a system with much fewer parameters, but then what is the difference between this and combinatorial optimization?\n\nFurthermore, I failed to see a way to scale up this method.  In the end, it depends on a maxsat solver, which will become intractable quickly when there are more rules.\n\nIn summary, if the authors can present a real-world application that can potentially benefit from their system while other learning systems fail to do so and evaluate their method on a small real-world dataset, I would be less concerned.\n\nSmall comments and questions:\n1) Would appreciate a few citations to support the claim \"At a glance, this may appear a large departure from the conventional wisdom that learning-based systems, particularly deep networks, are far superior to rule-based systems, as history has demonstrated repeatedly.\"  The authors need to be specific what on tasks.  For example, NNs are far behind SAT solvers on propositional formulas (GQSAT as an example for learning-based system).\n2) I fail to understand why rule proposers need to generate concrete rules, aren't all the rules already included in MetaQNL systems?",
          "summary_of_the_review": "In short, while I acknowledge this work is novel, its setting is not very practical.  This paper would benefit from presenting a potential real-world application, or else some theoretical generalization results on how well their systems can learn.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Reviewer_v7TB"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Reviewer_v7TB"
        ]
      },
      {
        "id": "OPKO1skpy5U",
        "original": null,
        "number": 1,
        "cdate": 1637288498423,
        "mdate": 1637288498423,
        "ddate": null,
        "tcdate": 1637288498423,
        "tmdate": 1637288498423,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7zFokR7k_86",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "Common Response (1/3)",
          "comment": "We thank all reviewers for their thoughtful comments. We are encouraged that they agree we address an important problem (CPsz, 7F3Z). Reviewers consider our approach novel (v7TB, 9uxa, 7F3Z), interesting (CPsz), natural, simple yet effective (7F3Z), and our paper well-written (v7TB, CPsz, 9uxa, 7F3Z). We are especially pleased that reviewer 7F3Z thinks our work adds a dimension to the literature and has the potential to yield to something big in the future.\n\n## Additional Experiments on Real-world Non-synthetic Data\n\nA common concern among reviewers (v7TB, CPsz, 9uxa) is that our original experiments were synthetic and could not demonstrate how well the method works on real-world natural language. This is a legitimate concern. Due to the complex nature of natural language, extending our method to natural language may require non-trivial future developments beyond a single paper (also discussed in Sec. 6).\n\nThat said, we have conducted additional experiments on a real-world, non-synthetic task\u2014analyzing the morphology of words. We hope the new experiments can mitigate the reviewers\u2019 concerns and provide evidence that our current system can work with noisy real-world data to some extent. \n\n\n\n ## Task and dataset\n\nWe use the same task and dataset in Sec. 5.2 of Aky\u00fcrek et al. [A]: Given the surface form of a word (e.g., `studied`), the model predicts its lemma (`study`) and an unknown number of morphological tags, such as`V` (verb), `SG` (singular), and `PST` (past tense).\n\nThe data is constructed from the SIGMORPHON 2018 dataset. It consists of 3 languages with varying morphological complexity\u2014Spanish, Swahili, and Turkish. For each language, they sample a training set of 1000 examples and three test sets (FUT, PST, and OTHER) of 100 examples each. FUT consists exclusively of words in the future tense; PST consists of words in the past tense; OTHER consists of other words. The training set has only 8 past-tense words and 8 future-tense words. Therefore, FUT and PST test models' few-shot learning capabilities.\n\nAlthough this task differs from general natural language understanding, it is still challenging. The morphological data is not synthetic; it contains noise and ambiguity, just like most real-world data. A standard seq2seq neural network performs far from perfect, especially on FUT and PST (E.g., an F1 score of 66% on Spanish; see Table 2 of Aky\u00fcrek et al.). \n\n* [A] Aky\u00fcrek, Ekin, Afra Feyza Aky\u00fcrek, and Jacob Andreas. \"Learning to Recombine and Resample Data For Compositional Generalization.\" ICLR 2020.\n\n\n## Method\n\n### Data preprocessing\n\nTo apply our method to the task of morphological analysis, we represent both the surface form and the lemma as characters. The surface form serves as the assumption, whereas the lemma and the tags serve as conclusions. For example, below is a training example in the dataset:\n```\nInput surface form:  zarandeamos\nOutput lemma:        zarandear\nOutput tags:         V;IND;PRS;1;PL\n```\nWe treat `z a r a n d e a m o s` as the assumption. Provable conclusions include `$LEMMA$ z a r a n d e a r`, `$TAG$ V`, `$TAG$ IND`, `$TAG$ PRS`, `$TAG$ 1`, and `$TAG$ PL`. Everything else is unprovable.\n### Rule proposal\n\nThe rule proposer simply generates concrete rules that can prove the conclusions in a single step:\n```\nz a r a n d e a m o s\n\u2014\n$LEMMA$ z a r a n d e a r\n```\n\n```\nz a r a n d e a m o s\n\u2014\n$TAG$ V\n```\n\n```\nz a r a n d e a m o s\n\u2014\n$TAG$ IND\n```\n\n```\nz a r a n d e a m o s\n\u2014\n$TAG$ PRS\n```\n\n```\nz a r a n d e a m o s\n\u2014\n$TAG$ 1\n```\n\n```\nz a r a n d e a m o s\n\u2014\n$TAG$ PL\n```\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ]
      },
      {
        "id": "X1FAZGYwgWC",
        "original": null,
        "number": 2,
        "cdate": 1637288560808,
        "mdate": 1637288560808,
        "ddate": null,
        "tcdate": 1637288560808,
        "tmdate": 1637288560808,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "OPKO1skpy5U",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "Common Response (2/3)",
          "comment": "## Results\n\nWe follow the same baseline and evaluation setup in Table 2 of Aky\u00fcrek et al. The predicted lemmas and tags are evaluated using F1 score. The results on the FUT set and the PST set are averaged. The baseline is a standard seq2seq neural network: LSTMs with attention. The table below shows the results: \n\n| Model                          | Spanish FUT+PST | Spanish OTHER | Swahili FUT+PST | Swahili OTHER | Turkish FUT+PST | Turkish OTHER |\n| ------------------------- | --------------------- | ------------------- | -------------------- | ----------------- | -------------------- |  ----------------- |\n| LSTMs + attention       | **66**                     | **88**                   |  75                       | **90**                | **69**                    | **85**                 |  \n| Ours                             | 55                          | 82                        | **81**                   | 86                     | 53                         | 71                      |\n\nNote that we\u2019re comparing with the baseline in Aky\u00fcrek et al., not their proposed method. Their method focuses on generating synthetic data for augmenting the training set. Therefore it is orthogonal to our work.\n\nAnd here are some example rules learned by our method on Spanish:\n```\n[A] e a m o s\n\u2014\n$LEMMA$ [A] e a r\n```\n\n```\n[A] \u00e1 r a m o s\n\u2014\n$TAG$ PST\n```\n\nThe results show that our method is competitive with the baseline on Swahili, but there are still gaps on Spanish and Turkish. By analyzing the model's predictions, we find different reasons for the gaps. Turkish morphology is known to be very complex. But our current way of instantiating MetaQNL only considers proofs of depth 1, which could be a restriction for learning more expressive rules. \n\nOn the other hand, Spanish morphology is relatively simple. But the F1 score of our system is not great, because it learns a set of over-specific rules that achieve high precision but low recall. Next, we explore how to mitigate this issue through soft matching.\n\n## Soft matching\n\nIn the original MetaQNL, a rule is applicable only if its premises match the assumptions precisely. For example, the rule\n```\ne [A] \u00e1 r a m o s\n\u2014\n$LEMMA$ e [A] a r\n```\nis not applicable to `m u t i l \u00e1 r a m o s` due to the mismatch between `e` and `m`. \n\nThe rigid and precise matching could be a restriction if we want to apply the learned rules to noisy testing examples. One potential solution is to perform \u201csoft matching\u201d\u2014applying rules without requiring precise matching. There are potentially many ways to implement soft matching, some of which may also leverage machine learning. Here we use a simple implementation based on anti-unification as a first step to explore whether soft matching could be useful.\n\nGiven a rule such as \n```\ne [A] \u00e1 r a m o s\n\u2014\n$LEMMA$ e [A] a r\n```\nand an assumption such as `m u t i l \u00e1 r a m o s`. The rule is not applicable, but we anti-unify `e [A] \u00e1 r a m o s` and `m u t i l \u00e1 r a m o s` to find a more general rule that is applicable:\n```\n[A] \u00e1 r a m o s\n\u2014\n$LEMMA$ [A] a r\n```\nAll such additional rules are ranked based on how different they are from the original rules. Note that we only perform soft matching in testing, and the training process remains the same.\n\nOur preliminary results show that even this simple form of soft matching can bridge the performance gap on Spanish:\n\n| Model                          | Spanish FUT+PST | Spanish OTHER | \n| ------------------------- | --------------------- | ------------------- | \n| LSTMs + attention       | **66**                     | **88**                  |  \n| Ours                             | 55                          | 82                       |\n| Ours + soft matching   | **66**                     |  84                      |\n\nHowever, it leads to no improvements on Swahili and Turkish. We did some analysis and found that the rules learned on Swahili and Turkish are more noisy, due to the increased morphological complexity. Therefore, relaxing the matching conditions naively may lead to too many spurious rules. In the future, it is interesting to explore more principled mechanisms for soft matching, which can potentially improve MetaQNL on noisy real-world domains.\n\n \n## Limitations and Future work\n\nIn summary, our experiments on non-synthetic data are a step towards applying MetaQNL to real-world problems that are complex and noisy. They show encouraging results (e.g., on Swahili) and reveal some remaining challenges (e.g., on Turkish). And they also suggest soft matching could be an important direction to explore in the future. \n\nWe hope these experiments can mitigate the reviewers\u2019 concerns and provide a more holistic picture of our method. However, we do not claim these experiments alone can already show MetaQNL to work on unconstrained natural language. Most challenges discussed in Sec. 6 still apply, but we believe resolving them requires efforts beyond a single paper.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ]
      },
      {
        "id": "ujaJ1Pec3O7",
        "original": null,
        "number": 3,
        "cdate": 1637288619889,
        "mdate": 1637288619889,
        "ddate": null,
        "tcdate": 1637288619889,
        "tmdate": 1637288619889,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "X1FAZGYwgWC",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "Common Response (3/3)",
          "comment": "## Scalability of MetaInduce\n\nReviewer v7TB and 7F3Z asked about how to potentially scale MetaInduce to more training examples. We approach this problem from two angles: theoretical and experimental.\n\n### Time complexity of MetaInduce\n\nMetaInduce consists of 4 steps: rule proposal, theorem proving, rule abstraction, and rule pruning. Let $n$ be the number of training examples, rule proposal and theorem proving are $O\n(n)$. Rule abstraction is $\\Omega(n^2)$ since we anti-unify each pair of rules. Rule pruning is NP-hard due to the use of MAX-SAT. Therefore, our system, in its current form, would indeed struggle with very large $n$. In order to scale to millions of examples, future work would have to reduce the complexity to $O(n)$.\n\nFor rule abstraction, note that our pairwise anti-unification is only a design choice at the implementation level, rather than a conceptual necessity. In principle, all we need is a procedure for generating abstract rules from concrete ones. It is possible to develop more efficient algorithms for rule abstraction with linear or even sublinear runtime. They can also be based on machine learning, e.g., Cingillioglu and Russo [G].\n\nFor rule pruning, one potential way to scale it up is through approximate MAX-SAT solvers. Similar to deep learning, MetaInduce does not necessarily require achieving the global minimum to work well. Another important direction to explore is learning in mini-batches. It avoids encoding the entire dataset as a large MAX-SAT problem, and instead solves multiple small MAX-SAT problems. \n\n\n\n\n### Experimental run time of MetaInduce\n\nOur SCAN experiments take 30 minutes to train on a laptop, which compares favorably with methods that use deep neural networks (e.g., 1 day on GPUs in Liu et al. [H]). On RuleTaker, our experiments take 2 days. It is impossible to directly compare with ProofWriter since the authors haven't released the code. \n\nOur  run time is noteworthy considering that deep neural networks are trained on software/hardware stacks (GPUs, cuDNN, PyTorch, etc.) highly optimized for them. And our method is at an early stage without nearly as much engineering effort for improving the run time efficiency. \n\n\n* [H] Liu et al. \"Compositional generalization by learning analytical expressions.\" NeurIPS, 2020.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ]
      },
      {
        "id": "mHVfo7vIYja",
        "original": null,
        "number": 4,
        "cdate": 1637288658538,
        "mdate": 1637288658538,
        "ddate": null,
        "tcdate": 1637288658538,
        "tmdate": 1637288658538,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "vsnyod7rm9k",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "Individual Response to Reviewer v7TB",
          "comment": "Thank you for your valuable feedback! Below we address your questions and concerns. Please feel free to post additional comments if you have further questions. \n\n\n## Potential real-world applications\n\nPlease see the common response above.\n\n\n## Doesn't ProofWriter generate checkable proofs?\n\nNo, as explained in the 2nd paragraph of page 4, the proofs generated by ProofWriters cannot be checked mechanically by computers. \n\nThis is not to be confused with whether the proof can be verified by humans. Take the proof in Fig. 1 of the ProofWriter paper (Tafjord et al. 2020) as an example; humans can verify it is a correct proof. But it is non-trivial to write a general computer program for checking all such proofs mechanically. In contrast, proofs in MetaQNL are mechanically checkable by checking the conditions in Definition 5 (Sec. 3).\n\n\n## Difference between MetaInduce and combinatorial optimization. \n\nThese two are not directly comparable.  MetaInduce uses combinatorial optimization (MAX-SAT specifically) to learn symbolic rules. This is analogous to deep learning using gradient descent to learn the weights of neural networks. MetaInduce and combinatorial optimization are not directly comparable for the same reason deep learning and gradient descent are not directly comparable. \n\n\n## Why do rule proposers need to generate concrete rules, aren't all the rules already included in MetaQNL?\n\nNo, rules are not included in MetaQNL. MetaQNL only defines the syntax of rules and how they can be used in theorem proving (Sec. 3). But it does not come with any built-in rules. Instead, rules are learned from data using MetaInduce. \n\n\nThe learned rules take very different forms depending on the application domain. For example, a rule learned from SCAN might look like:\n```\n[A] $MAPS_TO$ [B]\n\u2014\n[A] twice $MAPS_TO$ [B] [B]\n```\n, whereas a rule learned from RuleTaker might look like:\n```\nIf [A] then [B]\n[A]\n\u2014\n[B]\n```\nTherefore, instead of building domain-agnostic rules into MetaQNL, we use rule proposers to generate concrete rules for each domain.\n\n\n## MAX-SAT solvers are not scalable. \n\nPlease see the common response above.\n\n\n## Additional citations.\n\nThank you for the suggestion. We'll incorporate them in the next revision.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ]
      },
      {
        "id": "oq-JrRE0K1T",
        "original": null,
        "number": 5,
        "cdate": 1637288721099,
        "mdate": 1637288721099,
        "ddate": null,
        "tcdate": 1637288721099,
        "tmdate": 1637288721099,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "egVauJudI_u",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "Individual Response to Reviewer CPsz",
          "comment": "Thank you for your valuable feedback! Below we address your questions and concerns. Please feel free to post additional comments if you have further questions. \n\n\n## Experiments on real-world data\n\nPlease see the common response above.\n\n\n## MetaQNL/MetaInduce do not significantly outperform deep neural networks\n\nOur method is a novel learning paradigm that radically differs from predominant methods based on deep neural networks, e.g., it learns symbolic rules without any continuous weights. Our experiments have demonstrated its promise in learning more compact and interpretable models, but yes, it does not significantly outperform deep neural networks right away. As we have discussed in Sec. 6, substantial future developments beyond a single paper are needed.\n\n\n## MetaInduce consists of existing components for theorem proving, rule abstraction, and rule pruning.\n\nYes, MetaInduce uses existing components, but this does not diminish  the novelty of our method. First, the MetaQNL symbolic system is our novel contribution. MetaInduce is proposed as an effective framework for learning MetaQNL rules from data. And its novelty cannot be disentangled with the novelty of MetaQNL. Second, MetaInduce itself is not a simple combination of existing components. We have discussed its difference with existing ILP methods in paragraphs 4\u20136 on page 4. And we elaborate more below:\n\nMetaInduce is the most similar to meta-level inductive logic programming (ILP) [B, C, D, E, F] in that it delegates rule induction to existing solvers (SAT, MAX-SAT, ASP, etc.). However, there are a few differences: \n\n1. Encoding rules vs. encoding proofs: Most meta-level ILP approaches (e.g., ASPAL [B], ILASP [D], Apperception [E], and Popper [F]) directly encode candidate rules in answer set programming (ASP) and ask the ASP solver to find a subset of them, without a separate theorem proving stage. This is possible because they learn rules in Prolog, Datalog, or ASP, whose formal semantics can be encoded in ASP. In contrast, MetaQNL rules cannot be encoded in existing solvers directly. Therefore, we encode the proofs found by a prover.  \n\n2. Encoding proofs has been explored in ProSynth [C] for the provenance-guided synthesis of Datalog programs. However, we extend the SAT encoding of ProSynth. First, we encode the disjunction of all proof paths, whereas only one proof path is available in ProSynth due to how provenance works. Second, we have additional constraints about rule instantiation. Third, we can tolerate noise, whereas ProSynth cannot. We use soft constraints to enforce the training examples to be provable, but ProSynth uses hard constraints.\n\n* [B] Corapi et al.. \"Inductive logic programming in answer set programming.\" International conference on inductive logic programming. 2011\n* [C] Raghothaman et al. \"Provenance-guided synthesis of Datalog programs.\" POPL. 2019\n* [D] Law et al. \"Inductive learning of answer set programs.\" European Workshop on Logics in Artificial Intelligence. 2014\n* [E] Evans et al. \"Making sense of sensory input.\" Artificial Intelligence 2021\n* [F] Cropper and Morel. \"Learning programs by learning from failures.\" Machine Learning. 2021\n\n\n## Are `$TRUE$`, `$FALSE$`, and `$MAPS_TO$` the only special symbols? \n\nThese are the only special symbols in our experiments on MiniSCAN, SCAN, and RuleTaker. However, MetaQNL does not restrict what special symbols can be used. When instantiating MetaQNL on a new task/dataset, one is free to use additional special symbols as needed. For example, in our additional experiment on morphological analysis (see the common response above), we introduce special symbols `$LEMMA$` and `$TAG$`.\n\n\n## Are there unprovable examples in SCAN?\n\nNo, as explained in the 3rd paragraph of page 8.\n\n\n## How to properly compare the model size with ProofWriter?\n\nWe are not aware of any widely accepted metric for comparing the size of symbolic rules and continuous weights. Fortunately, it doesn\u2019t take a precise  comparison to see our model (~2869 symbols) is much smaller than ProofWriter (> 11 billion parameters in T5-11B). One can browse our model using a text editor on a laptop, but one cannot even load a [T5-11B](https://huggingface.co/t5-11b) model using a GPU with 40GB memory.\n\n\n## What is the advantage of MetaQNL compared to first-order logic?\n\nThe paper has discussed the advantage of MetaQNL compared to purely symbolic methods such as first-order logic (page 1 and page 3). In summary:\n* Symbolic methods are not directly applicable to domains not amenable to rigid formalizations, such as commonsense reasoning and natural language. \n* To handle natural language, one could apply semantic parsing before symbolic methods. However, it requires (1) high-quality semantic parsers covering a wide range of texts; (2) an ontology of objects and predicates in the world. None of which can be achieved easily.\n* The syntax of MetaQNL is compatible with natural language sentences without semantic parsing.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ]
      },
      {
        "id": "pkn_-ZLEsIJ",
        "original": null,
        "number": 6,
        "cdate": 1637288797591,
        "mdate": 1637288797591,
        "ddate": null,
        "tcdate": 1637288797591,
        "tmdate": 1637288797591,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "JoRDRqUmoAq",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "Individual Response to Reviewer 9uxa",
          "comment": "Thank you for your valuable feedback! Below we address your questions and concerns. Please feel free to post additional comments if you have further questions. \n\n\n## Experiments on real-world non-synthetic data\n\nPlease see the common response above.\n\n\n## Reproducibility.\n\nWe will release the code for reproducing our experiments. \n\n\n## Comparison with ProofWriter trained on lower-depth proofs.\n\nWe conducted additional experiments of our method trained on the D1 part of RuleTaker (proof depth \u2264 1) and tested on D5. Results are shown in the table below (similar to Table 2 in the paper).\n\n\n| Test proof depth | N/A | 0        | 1        | 2       | 3      | 4      | 5      | All  |\n| ----------------- | ----- |------- | ------ | ----- | ------ | ----- | ----- | ---- |\n| Accuracy          | 99.3 | 100.0 | 100.0 | 99.8 | 98.6 | 98.4 | 98.3 | 99.2 |  \n\nOur results when training on D1 are very close to when training on D3, showing that our method can generalize to longer proofs unseen in training. We are unable to compare with ProofWriter directly under this setting, because they haven\u2019t released code.\n\n\n## What if there is no proof for a goal and the goal is proved via the close world assumption?\n\nMetaQNL assumes the open-world assumption. Currently, it doesn't have a mechanism to support the negation as failure inference in the closed-world assumption. It is interesting to extend MetaQNL to the closed-world assumption. However, we believe the open-world assumption is a more realistic setting for accommodating incomplete information\u2014in most real-world scenarios, there are certainly many statements that we don\u2019t have enough information to either prove or disprove. \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ]
      },
      {
        "id": "tVlOX2FrD9n",
        "original": null,
        "number": 7,
        "cdate": 1637288832903,
        "mdate": 1637288832903,
        "ddate": null,
        "tcdate": 1637288832903,
        "tmdate": 1637288832903,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7s9vVGYza3d",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "Individual Response to Reviewer 7F3Z",
          "comment": "Thank you for your valuable feedback! In the common response above, we address your questions about the time complexity of MetaInduce and how to potentially scale it to more training examples. Please feel free to post additional comments if you have further questions. \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ]
      },
      {
        "id": "vcVd4GH8PIV",
        "original": null,
        "number": 8,
        "cdate": 1637306520506,
        "mdate": 1637306520506,
        "ddate": null,
        "tcdate": 1637306520506,
        "tmdate": 1637306520506,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "pkn_-ZLEsIJ",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "On additional experiments on natural language",
          "comment": "Thank you for the team's hard work to respond to my reviews!\n\nCould you please kindly explain why you did not perform experiments on the paraphrased RuleTaker datasets as I requested but chose another dataset to do experiments? I think the paraphrased dataset of RuleTaker is a good exercise for the proposed approach because it will show how robust your approach w.r.t. to small changes in synthetic language via paraphrasing."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Reviewer_9uxa"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Reviewer_9uxa"
        ]
      },
      {
        "id": "ZTI8pJ16ZrS",
        "original": null,
        "number": 9,
        "cdate": 1637513606995,
        "mdate": 1637513606995,
        "ddate": null,
        "tcdate": 1637513606995,
        "tmdate": 1637513606995,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "vcVd4GH8PIV",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "Response to the Additional Question from Reviewer 9uxa",
          "comment": "We agree that the paraphrased RuleTaker dataset is a great suggestion! We only reported the morphology results because we had started working on it before the reviews were released. Therefore we were able to present relatively complete results despite the tight time frame.\n\nAs for paraphrased RuleTaker, we have been actively working on it upon receiving the suggestion. Our initial results show that our system did not work well when applied directly: with linguistic variations, our system learns overly general rules. We believe the main reason is that the training set is too small (28K) for learning invariance to linguistic variations (many different ways of expressing the same thing), and we believe that training from such a small training set from scratch will be challenging for not just our method but also other learning-based methods. ProofWriter finetunes a T5-11B model pretrained on the C4 dataset with hundreds of millions of examples. The pretraining data exposes the model to abundant examples of linguistic variations, which may not be learnable from paraphrased RuleTaker alone. It would be interesting to see if ProofWriter can work without pretraining, but we are unable to perform this comparison since the code of ProofWriter is not publicly available. However, many works on large language models (e.g., [I], [J], [K]) reported performance drop when the pretraining data is downsized. The smallest datasets explored by those papers were still orders of magnitude larger than paraphrased RuleTaker. \n\nWe believe that soft matching, which is a simple extension we have shown to be useful for the morphology task, is the key to addressing linguistic variations for paraphrased RuleTaker. We could use a pretrained language model to output matching scores between rules and assumptions. Soft matching allows outputting an approximate proof when a rigorous proof is not possible, with a score indicating the degree of rigor. We are investigating this extension as future work. \n\n\n* [I] Popel, Martin, and Ond\u0159ej Bojar. \"Training tips for the transformer model.\" arXiv preprint arXiv:1804.00247 (2018).\n* [J] Kaplan, Jared, et al. \"Scaling laws for neural language models.\" arXiv preprint arXiv:2001.08361 (2020).\n* [K] Raffel, Colin, et al. \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.\" Journal of Machine Learning Research 21.140 (2020): 1-67."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ]
      },
      {
        "id": "1wbgtKL98Y",
        "original": null,
        "number": 10,
        "cdate": 1638194606589,
        "mdate": 1638194606589,
        "ddate": null,
        "tcdate": 1638194606589,
        "tmdate": 1638194606589,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "tVlOX2FrD9n",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "Thoughts after reading authors' feedback",
          "comment": "Thank you for addressing my concern regarding time complexity. I guess your answer is fair given the hardness and nature of the problem. I have no more questions. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Reviewer_7F3Z"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Reviewer_7F3Z"
        ]
      },
      {
        "id": "hxVy-Gx4lKH",
        "original": null,
        "number": 11,
        "cdate": 1638255371597,
        "mdate": 1638255371597,
        "ddate": null,
        "tcdate": 1638255371597,
        "tmdate": 1638255371597,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "mHVfo7vIYja",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "Thanks for your rebuttal",
          "comment": "Thanks for doing so much work on responding my reviews.  While the new datasets are less toy problems, I'm still concerned about the practical use of these algorithms.  I also feel that the related literature is not solid.  For these reasons, I will keep my original score, but I have lowered my confidence in case I miss anything important in this paper."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Reviewer_v7TB"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Reviewer_v7TB"
        ]
      },
      {
        "id": "jPYzzXRYu2-",
        "original": null,
        "number": 12,
        "cdate": 1638896275951,
        "mdate": 1638896275951,
        "ddate": null,
        "tcdate": 1638896275951,
        "tmdate": 1638896275951,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "oq-JrRE0K1T",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "Thank you for the update",
          "comment": "Thank you for doing your hard work on responding the reviews. I appreciate the new experiments, but am still concerned that the method would not scale well to real problems in effectiveness or time complexity, but I adjusted the score based on the novelty of your approach. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Reviewer_CPsz"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Reviewer_CPsz"
        ]
      },
      {
        "id": "m8rr6uKT4p5",
        "original": null,
        "number": 1,
        "cdate": 1642696836057,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696836057,
        "tmdate": 1642696836057,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7zFokR7k_86",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The paper describes a system for learning rules in a quasi-NL format: roughly Horn clauses where a predicate p(X1,...,Xk) is replaced by a natural language pattern interleaving ground tokens and variables.  The method is to propose ground sentences - using one of several task-specific approaches - use anti-unification of pairs to variabilize, and then find a minimal theory from these proposed pairs by reduction to maxsat.\n\nPros:\n - QNL is a neat idea, and makes symbolic rule-learning possible to some NLP tasks\n - The use of maxsat is novel in rule-learning AFAIK\n\nCons:\n - Unification is a highly simplified model of the NL task of cross-document co-reference\n - It's unclear if maxsat process will work in the presence of noise, or how well it scales\n - The datasets use clean text generated from templates or synthetic grammars\n - Experimentally, the generality of the system is not well demonstrated, because there are differences in how it is applied: eg a subset of short examples for scan, input engineering ($TRUE, $FALSE) for RuleTaker, plus the \"heuristics for filtering invalid rules generated by anti-unification\u201d\n - It's not clear if this work really speaks to the main \"point\" of the SCAN and RuleTaker datasets.  These are both the kind tasks that symbolic systems would be expected to do well, and are used as ANN benchmarks because ANNs perform in unexpected ways: worse than one would expect for SCAN, and better for RuleTaker.  They are important for understanding ANNs but I'm not certain what the research benefit is of using them for symbolic methods as a benchmark."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "7s9vVGYza3d",
        "original": null,
        "number": 1,
        "cdate": 1635171670006,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635171670006,
        "tmdate": 1635171670006,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7zFokR7k_86",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Review",
        "content": {
          "summary_of_the_paper": "Traditional research on symbolic reasoning assumes input data are already translated into a format that complies with the formalism of the underlying system. A major struggle for symbolic reasoning is to handle the data coming in a natural form (e.g. images/text) and perform reasoning on the same. Inspired by this challenge, this paper undertakes the problem of converting text inputs into the format of a system formalism. \n\nThe formalism is also proposed by this paper and is called MetaQNL. The framework of MetaQNL is designed keeping in mind the specific need of operating directly on natural language sentences. This formalism supports the natural language sentences with variables within them. This trick alleviates the need of using a semantic parser to parse natural language sentences for the purpose of logical reasoning. Thus, MetaQNL, by design, is conducive to working with natural language inputs. \n\nNext, this paper proposes an algorithm to induce rules from natural language inputs within the MetaQNL framework. This paper doesn\u2019t worry about performing actual deductive reasoning/theorem proving and instead proposes to use existing provers and instead focus on the more challenging problem of rule induction. MetaInduce algorithm draws inspiration from existing ILP approaches. MetaInduce encodes the rule induction problem as a maximum satisfiability (MAX-SAT) problem, which can be solved efficiently by off-the-shelf solvers. The proposed method consists of 3 steps. \n\n1. Given a training example, a rule proposer proposes a set of concrete rules as candidates. This set can be overcomplete/inaccurate. \n2. It generates abstract rules from concrete rules via a symbolic procedure called anti-unification. This is essentially a process of aligning common substring segments across two or more strings.\n3. It encodes the proof paths in MAX-SAT and solves for a subset of all rules using a MAX-SAT solver. \n\nThis paper benchmarks the proposed method on 2 tasks - learning compositional instructions and logical reasoning. For learning compositional instructions, it works on two standard benchmarks: MiniSCAN and SCAN and recovers precisely the ground truth rules. For logical reasoning, the proposed method achieves SOTA on the RuleTaker dataset. \n\n",
          "main_review": "Overall, I liked the scope of this paper, the importance, and non-triviality of the problem, and the novelty of the proposed approach. In my view, it certainly adds a dimension to the literature on symbolic rule learning. The idea of MetaQNL is simple yet effective to handle the natural language inputs within a formal symbolic system. The idea behind MetaInduce is also quite natural. Although there are some weaknesses of the proposed approach, I still feel this is a novel idea and has the potential to yield something big in the future. \n\n__Strength__\n\n- A well-written paper.\n- A very nice literature survey and positioning of the work relative to the prior art.\n- An important problem in the broad space of AI.\n\n__Weakness__\n\n- Time complexity of each of the three steps in MetaInduce is not discussed. It will be good to shed some light on this. \n- As stated in the limitation section, the proposed approach is far from mature but serves as proof of concept.  It does not scale to millions of training examples.",
          "summary_of_the_review": "See my comments in the main review.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Reviewer_7F3Z"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Reviewer_7F3Z"
        ]
      },
      {
        "id": "JoRDRqUmoAq",
        "original": null,
        "number": 2,
        "cdate": 1635584874566,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635584874566,
        "tmdate": 1635584874566,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7zFokR7k_86",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The work proposes two new concepts:\n+ MetaQNL: a symbolic system in Quasi-Natural Language. Instead of representing rules in a formal symbolic format such as first order logic rules, in MetaQNL, a rule is represented in a Quasi-Natural Language format which includes words, variables and control symbols. Since the rules are represented in an informal representation. An interesting property of the MetaQNL representation is that it allows to perform backward or forward inference by substitution of variables with sentences. The authors assume that texts can be translated into the MetaQNL format and thus solving the reasoning problems with text input is possible via mining rules from text and backward/forward reasoning with the Quasi-Natural Language.\n+ To mine rule from text, the authors proposes an algorithm called MetaInduce. MetaInduce iterates through the training data several time to build a compact set of rules that trades complexity to prediction accuracy. It is a bottom up rule induction approach where it includes a Rule proposer which propose a concrete rule from a training example and an anti-unification module to abstract the concrete rule with more generalised rules. A pruning process based on MAX-SAT is used to prune the set of rules such that it optimise the regularised objective.",
          "main_review": "Overall the idea is novel and I like the paper presentation. However, I have some concerns regarding its ability to work with real natural language and the reproducibility of the proposed approach.\n\nMAJOR CONCERN 1:\nI have a concern that the rule proposer and the anti-unification require very well-formed of the sentences to be working well. \n\nAs we can see with the example on page 7 figure (a) about anti-unification, the sample shows very simple rules. I wonder how it will work when the sentence is really complicated with real natural language? Also the experiments in RuleTaker only with synthetically generated sentences in a very controlled language, could you please  demonstrate your results with complicated sentences beyond synthetic texts, I think the paraphrased datasets within the RuleTakers even not really natural language but that could be a good exercise for your methods to test on.\n\n\nMAJOR CONCERN 2:\nI worry about reproducibility as the source code is not open but the details explanation of the proposed approaches are missing. For example, from the paper I don't know how the rule proposer work and how the anti-unification is implemented with quasi-natural language. I would suggest to provide very detailed about the methods, with the current information I doubt that people can reimplement your work and reproduce what you have demonstrated.\n\nMAJOR CONERN 3:\nIn the RuleTaker example, it is known that Transformers are good at generalisation when they are trained on queries with depth 3 or greater. Yet transformers are not good at generalisation when it is trained on lower depth queries. Could you please provide comparison results with Transformers when it is trained at lower depths? \nOther minor comments:\n\n\n\"In contrast, our approach does not require a semantic parser, because rules in MetaQNL are directly applicable to natural language.\" ---> This is a strong statement, it requires a support with real natural language examples rather than synthetically generated sentences in the experiments.\n\n\nDefinition 6: what happen if there is no proof for a goal and the goal is proved via the close world assumption?\n\n",
          "summary_of_the_review": "The paper proposes a new concept called quasi-language which allows representing rules in an new informal format that still allows to perform forward or backward reasoning while it is assumed to be mined easily from texts. The experimental results with some datasets with synthetically generated texts show that the methods work very well and advance state-of-the-art results. \nHowever, I doubt the application of the work with natural language input, I explicitly request to perform more experiments with paraphrased datasets in RuleTaker (MAJOR CONCERN 1). I also have a concern about reproducibility as the presentation lacks details of the core components of the proposed algorithm (MAJOR CONCERN 2). I also requested an additional experiment regarding the training data with low depth queries to check the ability of generalization of the proposed approaches. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Reviewer_9uxa"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Reviewer_9uxa"
        ]
      },
      {
        "id": "egVauJudI_u",
        "original": null,
        "number": 3,
        "cdate": 1635908903510,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635908903510,
        "tmdate": 1638896292302,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7zFokR7k_86",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes an algorithm that learns rules from natural language data, and a symbolic system for manipulating these rules, where existing provers can be applied. The objective is to maximize the number of examples in a test set that are consistent with the proposed mode while minimizing the number of rules in the model. The algorithm consists of three steps - given a training example, it proposes concrete rules, abstracts concrete rules into rules with variables, and prunes the resulting rules. ",
          "main_review": "Pros: \n- The learning algorithm is interesting and the problem of automatically discovering prepositions from examples is important for ATP in formal or informal languages.\n- The paper clearly defines the terms used and explains the methods and experiments well.\n\nCons:\n- As the authors pointed out, the experiments are neither large-scale nor real-world. One result of this is existing methods already achieve good performance, and it's not clear that the proposed method results in better performance.\n- None of the components (theorem prover, rule abstraction, rule pruning) are novel individually.\n\nQuestions:\n1) Are TRUE FALSE and MAPS_TO the only special symbols? It would be helpful to state this. \n2) Are there any unprovable examples in SCAN?\n3) The number of rules and symbols learned by MetaInduce is hard to compare with the number of learned parameters in ProofWriter. Is there a better metric to compare the two methods on? \n4) What are the advantages to using the proposed symbol system instead of first order logic? ",
          "summary_of_the_review": "The paper proposes an interesting solution to an important problem, but as-is the experimental settings and results are not compelling. ",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Reviewer_CPsz"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Reviewer_CPsz"
        ]
      },
      {
        "id": "vsnyod7rm9k",
        "original": null,
        "number": 4,
        "cdate": 1635997434722,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635997434722,
        "tmdate": 1638255066314,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7zFokR7k_86",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a symbolic system in Quasi-Natural Language, MetaQNL, which is compatible with both logical inference and quasi natural language expressions, and where the basic building blocks are sentences and rules. The authors also propose MetaInduce, which learns to generalize a set of rules that explains the examples in MetaInduce.\n\nMetaInduce consists of three mains steps:\n1) a rule proposer proposes a set of concrete rules as candidates for each individual example. This set may not be fully correct and may not be the minimal explanations, and they are used to prove the example using forward/backward chaining.\n2) the authors apply a symbolic procedure called anti-unification to generate abstract rules from concrete rules.\n3) and finally, proof paths are encoded in MAX-SAT and a subset of all rules are solved using a MAX-SAT solver to find minimal possible explanations\n\nThis paper evaluates its methods on two synthetic datasets, and the authors claim to learn compact models with much less data, and produces answers as well as proofs",
          "main_review": "strengths:\n1) this is indeed novel research problem - using a learning system together with a maxsat solver to make logical inference and identify rules.  There is a spectrum where on one end logical inference instances fully in natural language, and on the other end they are fully symbolic, and this paper falls somewhere inbetween.\n2) The paper is generally well-written, and mathematical part of the paper seems to be correct.\n\nWeakness:\n\nI fail to see the real contributions in this paper.  The only novelty seems to me is MetaQNL and MetaInduce can solve formal systems as well as systems represented in quasi natural languages.  However, are there actual applications that can potentially benefit from this problem formulation?\nOne of the major claims of this paper is it can produces checkable proofs, but isn't Proofwriter also capable of generating proofs which seemed more impressive because it is a fully neural system without any explicit encodings of rules and has the potential of working with real languages.  I get that proofwriter has billions of parameters and MetaInduce learns a system with much fewer parameters, but then what is the difference between this and combinatorial optimization?\n\nFurthermore, I failed to see a way to scale up this method.  In the end, it depends on a maxsat solver, which will become intractable quickly when there are more rules.\n\nIn summary, if the authors can present a real-world application that can potentially benefit from their system while other learning systems fail to do so and evaluate their method on a small real-world dataset, I would be less concerned.\n\nSmall comments and questions:\n1) Would appreciate a few citations to support the claim \"At a glance, this may appear a large departure from the conventional wisdom that learning-based systems, particularly deep networks, are far superior to rule-based systems, as history has demonstrated repeatedly.\"  The authors need to be specific what on tasks.  For example, NNs are far behind SAT solvers on propositional formulas (GQSAT as an example for learning-based system).\n2) I fail to understand why rule proposers need to generate concrete rules, aren't all the rules already included in MetaQNL systems?",
          "summary_of_the_review": "In short, while I acknowledge this work is novel, its setting is not very practical.  This paper would benefit from presenting a potential real-world application, or else some theoretical generalization results on how well their systems can learn.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Reviewer_v7TB"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Reviewer_v7TB"
        ]
      },
      {
        "id": "OPKO1skpy5U",
        "original": null,
        "number": 1,
        "cdate": 1637288498423,
        "mdate": 1637288498423,
        "ddate": null,
        "tcdate": 1637288498423,
        "tmdate": 1637288498423,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7zFokR7k_86",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Official_Comment",
        "content": {
          "title": "Common Response (1/3)",
          "comment": "We thank all reviewers for their thoughtful comments. We are encouraged that they agree we address an important problem (CPsz, 7F3Z). Reviewers consider our approach novel (v7TB, 9uxa, 7F3Z), interesting (CPsz), natural, simple yet effective (7F3Z), and our paper well-written (v7TB, CPsz, 9uxa, 7F3Z). We are especially pleased that reviewer 7F3Z thinks our work adds a dimension to the literature and has the potential to yield to something big in the future.\n\n## Additional Experiments on Real-world Non-synthetic Data\n\nA common concern among reviewers (v7TB, CPsz, 9uxa) is that our original experiments were synthetic and could not demonstrate how well the method works on real-world natural language. This is a legitimate concern. Due to the complex nature of natural language, extending our method to natural language may require non-trivial future developments beyond a single paper (also discussed in Sec. 6).\n\nThat said, we have conducted additional experiments on a real-world, non-synthetic task\u2014analyzing the morphology of words. We hope the new experiments can mitigate the reviewers\u2019 concerns and provide evidence that our current system can work with noisy real-world data to some extent. \n\n\n\n ## Task and dataset\n\nWe use the same task and dataset in Sec. 5.2 of Aky\u00fcrek et al. [A]: Given the surface form of a word (e.g., `studied`), the model predicts its lemma (`study`) and an unknown number of morphological tags, such as`V` (verb), `SG` (singular), and `PST` (past tense).\n\nThe data is constructed from the SIGMORPHON 2018 dataset. It consists of 3 languages with varying morphological complexity\u2014Spanish, Swahili, and Turkish. For each language, they sample a training set of 1000 examples and three test sets (FUT, PST, and OTHER) of 100 examples each. FUT consists exclusively of words in the future tense; PST consists of words in the past tense; OTHER consists of other words. The training set has only 8 past-tense words and 8 future-tense words. Therefore, FUT and PST test models' few-shot learning capabilities.\n\nAlthough this task differs from general natural language understanding, it is still challenging. The morphological data is not synthetic; it contains noise and ambiguity, just like most real-world data. A standard seq2seq neural network performs far from perfect, especially on FUT and PST (E.g., an F1 score of 66% on Spanish; see Table 2 of Aky\u00fcrek et al.). \n\n* [A] Aky\u00fcrek, Ekin, Afra Feyza Aky\u00fcrek, and Jacob Andreas. \"Learning to Recombine and Resample Data For Compositional Generalization.\" ICLR 2020.\n\n\n## Method\n\n### Data preprocessing\n\nTo apply our method to the task of morphological analysis, we represent both the surface form and the lemma as characters. The surface form serves as the assumption, whereas the lemma and the tags serve as conclusions. For example, below is a training example in the dataset:\n```\nInput surface form:  zarandeamos\nOutput lemma:        zarandear\nOutput tags:         V;IND;PRS;1;PL\n```\nWe treat `z a r a n d e a m o s` as the assumption. Provable conclusions include `$LEMMA$ z a r a n d e a r`, `$TAG$ V`, `$TAG$ IND`, `$TAG$ PRS`, `$TAG$ 1`, and `$TAG$ PL`. Everything else is unprovable.\n### Rule proposal\n\nThe rule proposer simply generates concrete rules that can prove the conclusions in a single step:\n```\nz a r a n d e a m o s\n\u2014\n$LEMMA$ z a r a n d e a r\n```\n\n```\nz a r a n d e a m o s\n\u2014\n$TAG$ V\n```\n\n```\nz a r a n d e a m o s\n\u2014\n$TAG$ IND\n```\n\n```\nz a r a n d e a m o s\n\u2014\n$TAG$ PRS\n```\n\n```\nz a r a n d e a m o s\n\u2014\n$TAG$ 1\n```\n\n```\nz a r a n d e a m o s\n\u2014\n$TAG$ PL\n```\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper81/Authors"
        ]
      },
      {
        "id": "m8rr6uKT4p5",
        "original": null,
        "number": 1,
        "cdate": 1642696836057,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696836057,
        "tmdate": 1642696836057,
        "tddate": null,
        "forum": "7zFokR7k_86",
        "replyto": "7zFokR7k_86",
        "invitation": "ICLR.cc/2022/Conference/Paper81/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The paper describes a system for learning rules in a quasi-NL format: roughly Horn clauses where a predicate p(X1,...,Xk) is replaced by a natural language pattern interleaving ground tokens and variables.  The method is to propose ground sentences - using one of several task-specific approaches - use anti-unification of pairs to variabilize, and then find a minimal theory from these proposed pairs by reduction to maxsat.\n\nPros:\n - QNL is a neat idea, and makes symbolic rule-learning possible to some NLP tasks\n - The use of maxsat is novel in rule-learning AFAIK\n\nCons:\n - Unification is a highly simplified model of the NL task of cross-document co-reference\n - It's unclear if maxsat process will work in the presence of noise, or how well it scales\n - The datasets use clean text generated from templates or synthetic grammars\n - Experimentally, the generality of the system is not well demonstrated, because there are differences in how it is applied: eg a subset of short examples for scan, input engineering ($TRUE, $FALSE) for RuleTaker, plus the \"heuristics for filtering invalid rules generated by anti-unification\u201d\n - It's not clear if this work really speaks to the main \"point\" of the SCAN and RuleTaker datasets.  These are both the kind tasks that symbolic systems would be expected to do well, and are used as ANN benchmarks because ANNs perform in unexpected ways: worse than one would expect for SCAN, and better for RuleTaker.  They are important for understanding ANNs but I'm not certain what the research benefit is of using them for symbolic methods as a benchmark."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}