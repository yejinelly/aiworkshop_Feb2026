{
  "id": "D1TYemnoRN",
  "original": "E9-h6y56oC",
  "number": 286,
  "cdate": 1632875441723,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875441723,
  "tmdate": 1676330678632,
  "ddate": null,
  "content": {
    "title": "Short optimization paths lead to good generalization",
    "authorids": [
      "~Fusheng_Liu1",
      "~Haizhao_Yang1",
      "~Qianxiao_Li1"
    ],
    "authors": [
      "Fusheng Liu",
      "Haizhao Yang",
      "Qianxiao Li"
    ],
    "keywords": [
      "optimization",
      "generalization",
      "machine learning theory"
    ],
    "abstract": "Optimization and generalization are two essential aspects of machine learning. In this paper, we propose a framework to connect optimization with generalization by analyzing the generalization error based on the length of optimization trajectory under the gradient flow algorithm after convergence. Through our approach, we show that, with a proper initialization, gradient flow converges following a short path with an explicit length estimate. Such an estimate induces a length-based generalization bound, showing that short optimization paths after convergence indicate good generalization. Our framework can be applied to broad settings. For example, we use it to obtain generalization estimates on three distinct machine learning models: underdetermined $\\ell_p$ linear regression, kernel regression, and overparameterized two-layer ReLU neural networks.",
    "pdf": "/pdf/d415033fae55fca0fdca3050197e238246526670.pdf",
    "one-sentence_summary": "We propose a framework to connect optimization and generalization and use it to obtain generalization estimates on three machine learning models.",
    "supplementary_material": "/attachment/93d9c0dec480d42f91653624e26c432b388f42e5.zip",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "liu|short_optimization_paths_lead_to_good_generalization",
    "_bibtex": "@misc{\nliu2022short,\ntitle={Short optimization paths lead to good generalization},\nauthor={Fusheng Liu and Haizhao Yang and Qianxiao Li},\nyear={2022},\nurl={https://openreview.net/forum?id=D1TYemnoRN}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "D1TYemnoRN",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 15,
    "directReplyCount": 6,
    "revisions": true,
    "replies": [
      {
        "id": "E8DxyFQ3y9",
        "original": null,
        "number": 1,
        "cdate": 1635900319741,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635900319741,
        "tmdate": 1635972415141,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "D1TYemnoRN",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper provides a novel generalization bound for the gradient flow equation related to the length of the optimization path. The bound is valid for loss function that locally satisfies \u0141ojasiewicz gradient inequality, which is applicable to different machine learning models such as underdetermined $\\ell_p$ linear regression, kernel regression, and overparameterized two-layer ReLU neural networks. Explicit derivations are provided for these three models to show that the length-based generalization bound is non-vacuous. ",
          "main_review": "The paper is well organized and easy to follow. My main concern is regarding the claim \"short optimization paths lead to good generalization\" which I feel misleading. The reason is that\n\na. The upper bound $r_n(w)$ is not path-length related. In contrast, it only depends on the function value gap $L_n(w) - L_n^*$. In other words, we first upper bound the path length by the function value gap, which is expected under \u0141ojasiewicz gradient inequality, then use the later function value gap to derive the generalization bound. Unless the path-length is used in the proof of Theorem 2, I wouldn't call it a length-based generalization bound. Please provide more details on this point. \n\nb. I am not sure whether the following case is possible: there are $w_0$ and $w_1$ that for any $L_n$, the gradient flow from $w_0$ always pass through $w_1$. In this case, they always share the same solution but the path length of $w_1$ is always shorter than the path length of $w_0$. Even though this might be impossible, what I want to say is I don't find the shorter length-path as the cause (in terms of causality) of better generalization, instead, I think the function value gap is the real cause of it, which I believe is well studied in the literature. (it might be wrong but I am happy to further discuss it)\n\nOverall, I can't see how explicitly the length of optimization path influence the generalization bound in the current analysis, instead it is based on the function value gap. I am willing to raise my score if my concern is addressed in the rebuttal. ",
          "summary_of_the_review": "My major concern is how explicitly the length of optimization path influence the generalization bound in the current analysis, instead it is based on the function value gap.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "Not applicable",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Reviewer_shcV"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Reviewer_shcV"
        ]
      },
      {
        "id": "wRVDi8zyBxA",
        "original": null,
        "number": 2,
        "cdate": 1635971747798,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635971747798,
        "tmdate": 1635971747798,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "D1TYemnoRN",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The authors study the connection between optimization and generalization for gradient flow (GF) on loss functions that satisfy a global version of the Lojasiewicz gradient inequality. Under this assumption, they prove convergence of GF to a global minimum and they find an upper bound on the optimization length \u2014 measured as the integral of the $\\ell_2-$norm of the gradient from time $t=0$ to the final time. This upper bound depends on the specific choice of the loss function and on the number of samples. With an additional assumption on the hypothesis class (encompassing, e.g., linear shallow networks, two-layer networks) the first result is used to derive an upper bound on the generalization gap. The bound depends on the choice of the loss function, its initial value, and the length of the optimisation path. This leads to the main result that shorter optimization paths induce smaller generalization gap. The authors apply this result to three models (underdetermined $\\ell_p$ linear regression, kernel regression, and overparametrized two-layer networks with ReLU activation) with a given target function. They compute non-asymptotic expressions for the generalization bounds at fixed ratio between sample size and ambient dimension, and show that in these cases the bounds are non-vacuous when the dimension increases.\n",
          "main_review": "The paper inscribes in the relevant research direction aiming at understanding the interplay between efficient optimization and good generalization performances by identifying complexity measures that are implicitly minimised during training. \n\nThe analysis is sound and presented in a clear way. Overall, I found the results interesting and worth of publication although some improvements are needed. \n\nIn my opinion, the main weakness of the work regards the numerical experiments. I find that the authors could have provided better evidence than Figure 1. It is not clear to me whether each point in Figure 1 comes from an average (at fixed initialisation variance) or is the result of a single simulation starting from random Gaussian initialization with that variance. The first option does not make much sense to me, so I am assuming the second. However, in this case, since the initialization is random I do not understand why the authors need to change also the variance instead of comparing the optimization paths starting from different initializations from the same distribution. The way the figure is presented suggests that the length of the optimization path trivially depends on initialization, and similarly the generalization gap. \n\nMoreover, all the experiments are performed for a learning rate $\\eta=0.05$. Since the theory is valid in the limit of gradient flow, I would be interested in seeing how Figure 1 changes for different learning rates.\n\nFinally, it would be interesting to test the generality of the proposed framework by checking the relation between short optimization paths and good generalization in more realistic data model. Has this relation been previously observed in applications? \u2028\u2028",
          "summary_of_the_review": "I find that this paper is worth of publication since it provides an interesting contribution to a relevant research direction in the theory of machine learning. However, I believe that the authors must improve their numerical results to provide a convincing final version of the work.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "Not applicable",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Reviewer_xkEt"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Reviewer_xkEt"
        ]
      },
      {
        "id": "2eC-f-pbHOa",
        "original": null,
        "number": 3,
        "cdate": 1635972090404,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635972090404,
        "tmdate": 1638383662076,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "D1TYemnoRN",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a framework to analyze both optimization and generalization properties under the Uniform-LGI condition (Def 1). From my understanding, the main results consist of two parts:\n\n* Optimization: define the Uniform-LGI as an extension of the PL condition, prove the corresponding convergence result with a sublinear rate, and bound the optimization path length.\n* Generalization: use the Rademacher complexity to estimate the generalization error. The Rademacher complexity scales with the diameter of the parameter set, thus can be bounded by the optimization path length, which connects with the optimization results.\n\nThen the paper apply this framework to three application models: first establish the Uniform-LGI, then calculate the optimization path length and estimate the generalization error.",
          "main_review": "This paper is well organized following the approach from optimization to generalization. The theorems and proofs are clearly stated and easy to follow. However, my main concern is that it seems to put two separate results (one for optimization and the other for generalization) together directly, and the novelty of each of the two results may not be significant enough. Also, the results may not support the claim \u201cshort optimization paths lead to good generalization\u201d exactly.\n\nFor the optimization part, the definition of the Uniform-LGI (with parameter $\\theta$) is inspired by the original Lojasiewicz inequality, and the proof of convergence is straightforward and similar to Bolte et al. (2007). Nevertheless, I think it is good to introduce the result to the community as an extension to the widely-used PL condition. The sublinear convergence rate is also interesting. It would be better to have more applications satisfying the Uniform-LGI but not the PL condition. In Section 4 only the $l_p$ linear regression has $\\theta > 1/2$, and the other two models still fall into the PL condition.\n\nFor the generalization part, the estimation of generalization error follows the Rademacher approach. The proof is nontrivial while the idea is not complicated: the Rademacher complexity scales with the diameter of the parameter set, i.e., the optimization path length. To claim \u201cshort optimization paths lead to good generalization\u201d, I may expect a path-based generalization bound; ~~I think the good generalization here comes more from the good \u201clocal landscape\u201d (Uniform-LGI) rather than the \u201cpath\u201d.~~ (Edit: my statement was not accurate\u2014Theorem 2 does not require the Uniform-LGI. I mean the generalization bound does not depend on the whole optimization path, but only the endpoint, and the generalization is good if the endpoint falls in a small region.)\n\nIn addition, I think the numerical result in Figure 1 may not illustrate the relationship between the optimization path length and generalization\u2014the larger $\\sigma$ in the initialization (Appendix A) leads to both larger optimization path length and larger generalization gap. I think it would be more fair to compare them with the same $\\sigma$ in the initialization.\n\n**Update**\n\nI appreciate the detailed response from the authors and the careful improvement of the manuscript. Here is my summary of the contributions of the paper:\n* Optimization: it is good to propose the Uniform-LGI as an extension of the PL condition and show the sublinear rate. However, the Uniform-LGI is similar to the original Lojasiewicz inequality, and the proof is straightforward (similar to Bolte et al., 2007?).\n* Generalization: Theorem 2 is equivalent to the following: given a region $S$ with radius $R$, with probability $1 - \\delta$ over the training samples, for all $w \\in S$, the generalization gap\u2028$$L_D(w) - L_n(w) \\le \\frac{A R + B \\sqrt{3(p + q) + \\log(2 / \\delta)}}{\\sqrt n}$$\u2028where $A$ depends on the Lipschitz constants in $S$ and $B$ depends on the upper bound of $l$. Similar approach is taken by some previous work (e.g., Allen-Zhu et al., 2019); nevertheless, it is good to state the result precisely. I think the result would be better summarized as \u201csmall parameter region leads to good generalization bound\u201d, and this is not surprising as the Rademacher complexity increases w.r.t. $R$.\n\nTherefore, I would like to keep my original score given my concern about the significance of the novelty.",
          "summary_of_the_review": "The paper is well organized and clearly written. It is interesting to introduce the Uniform-LGI as an extension to the PL condition. However, I think the novelty of each of the two main results is not significant enough, and putting them together may not support the claim \u201cshort optimization paths lead to good generalization\u201d exactly.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "Not applicable",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Reviewer_8Scj"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Reviewer_8Scj"
        ]
      },
      {
        "id": "54quulh2VwS",
        "original": null,
        "number": 4,
        "cdate": 1636222403693,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636222403693,
        "tmdate": 1636222403693,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "D1TYemnoRN",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper studies generalization and optimization of kernel-based and one-hidden layer neural network models. The convergence guarantee is shown for a Uniform-LGI loss function in Theorem 1. Then, in Theorem 2 the paper proves a generalization guarantee assuming a particular parametric model in Equation (3). Finally, the paper applies these results to p-norm regression, kernel regression, and one hidden layer neural network learning problems.  ",
          "main_review": "The paper develops a convergence and generalization analysis of machine learning models under Uniform-LGI loss functions. The concept of Uniform-LGI loss functions extends the well-known PL condition in the optimization literature and as the paper shows leads to a nice theoretical setting for analyzing the optimization and generalization aspects of different machine learning models. The paper shows applications of the framework to norm-p regression and kernel regression problems as well as one-hidden-layer neural network learning problems.\n\nOverall I think this is an interesting work which shows some insightful generalization results under Uniform-LGI loss functions. My main concern with the paper is that while the paper's title, abstract, and introduction claim that the analysis helps to understand the connection between the length of optimization and the generalization error of learned models, the theoretical results in Theorems 2-5 seem to only bound the generalization error of the final model and do not have anything to say about how the generalization error changes during training the model. I, therefore, suggest the authors either clearly explain how the theorems analyze the role of optimization length in the final generalization performance or change the title and introduction appropriately to reflect the main contributions of the work. \n\nIn addition to the above comment, I am wondering whether the gradient flow simplification in Equation (1) results in a realistic analysis of gradient-based optimization in non-convex deep learning problems. It seems to me that the analysis may not be suitable for stochastic gradient methods which have been shown to achieve a significantly better generalization performance than full-batch gradient descent modeled in the paper. As my final comment, the paper has no numerical results validating the theoretical generalization bounds over standard supervised learning problems. Therefore, it is unclear whether the theory results are useful to bound the generalization error of practical deep learning experiments.",
          "summary_of_the_review": "The paper shows several interesting generalization results for Uniform-LGI loss functions. While the paper proves several insightful generalization bounds, it seems Theorems 2-5 do not explain the connection between the optimization length and generalization error. Also, the paper has no numerical results validating the generalization results.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "Not applicable",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Reviewer_ZB5L"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Reviewer_ZB5L"
        ]
      },
      {
        "id": "WdZgKeJBSya",
        "original": null,
        "number": 2,
        "cdate": 1637218444725,
        "mdate": 1637218444725,
        "ddate": null,
        "tcdate": 1637218444725,
        "tmdate": 1637218444725,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "D1TYemnoRN",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Comment",
        "content": {
          "title": "To all reviewers.",
          "comment": "*  We appreciate all the reviewers for their valuable suggestions and comments on our paper. \n\n*  In the revised paper, we highlighted the main changes in blue, and in the following's response, we emphasize the changes in boldface.\n\n*  The main concerns about the paper are two-fold. \n\n   First, the connection between optimization and generalization is not clearly presented by our results. \n    We emphasize that Theorem 2 is precisely a path-based generalization bound.\n    It shows that as long as one has an explicit length estimate for the trajectory length, one can obtain a generalization estimate.\n    That is to say, the role of Theorem 1 is to provide a sufficient but not necessary condition to ensure such a length estimate exists. \n    In that sense, the statement ''short optimization paths lead to good generalization'' is supported by our results.\n    \n    \n    The second main concern is that Figure 1 may not illustrate the relationship between the optimization path length and generalization.\n    We would like to point out that the length estimate we consider in Theorem 2 is a uniform estimate for a specific initialization *method*, but not a point wise estimate that depends on a specific initialization *point*. \n    Thus, the generalization performance for different initialization methods in Figure 1 motivates us to use path length to build its connection to optimization. \n    \n    **In this revision, we have modified the statement of Theorem 2, and added explanations for the roles of Theorem 1 \\& 2 in the main text.**\n\n\n* Based on each reviewer's concerns, we summarize the main changes of our paper below:\n\n   + According to Reviewer ZB5L's comments, we have\n     + **added more explanations of the intuition in the introduction;**\n     + **provided additional theoretical results to bound the generalization error during training the model (Appendix C.1);**\n     + **added numerical results validating our generalization bounds on MNIST dataset (Appendix A.2).**\n\n  + According to Reviewer 8Scj's comments, we have\n    + **modified the statement of Theorem 2, and added explanations for the roles and novelty of Theorem 1 \\& 2 immediately after Theorem 2;**\n    + **added the analysis for kernel regression with $\\ell_p$ loss (Theorem 4). For $p>2$ , the model satisfies the Uniform-LGI but not PL condition;**\n    + **replace the original Figure 1 with a new figure that provides numerical results for Gaussian initialization with varied mean and variance.**\n\n  + According to Reviewer xkEt's comments, we have\n     + **modified the statement of Theorem 2, and add explanations for the roles of Theorem 1 \\& 2 before and after Theorem 2;**\n     + **added additional experiments for different learning rates (Appendix A.1);**\n     + **provided numerical experiments to show the relation between optimization paths and generalization on MNIST dataset (Appendix A.2).**\n\n  + According to Reviewer shcV's comments, we have\n     + **added explanations of the relation between Theorem 1 \\& 2 before and after Theorem 2,\n          for better understanding how explicitly the length of optimization path influence the generalization bound.**"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ]
      },
      {
        "id": "_2ui05hdf9m",
        "original": null,
        "number": 3,
        "cdate": 1637219465769,
        "mdate": 1637219465769,
        "ddate": null,
        "tcdate": 1637219465769,
        "tmdate": 1637219465769,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "54quulh2VwS",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Comment",
        "content": {
          "title": "Authors' response",
          "comment": "1. ''Explain how the theorems analyze the role of optimization length in the final generalization       performance.''\n\n   * The intuition for our results is as follows.\n    We show in theorem 2 that if the optimization trajectory length $\\textsf{len}(w^{(0)}, \\infty)$ is small,\n    then this implies a good generalization bound.\n    One can understand this by treating the trajectory length as a measure of complexity:\n    the initial point $w^{(0)}$ is random/independent of the training data, so it has a minimal generalization gap.\n    If the trajectory length is small, then $w^{(\\infty)}$ is necessarily close to $w^{(0)}$, thus having small\n    generalization gap as well.\n    Theorem 1 gives, based on the uniform-LGI assumption, a sufficient condition to ensure that the trajectory\n    length can be estimated.\n    Hence, one can deduce a good generalization bound from Theorem 2.\n    Together, they connect optimization with generalization.\n\n     __We have added the above explanations before the statement of Theorem 2.__\n\n2. '' Theorems 2-5 seem to only bound the generalization error of the final model and do not have anything to say about how the generalization error changes during training the model.''\n\n   * First, we emphasize that in this paper, by length we mean the trajectory length (path length) of the parameter evolution during training.\n    This is not the ``length of time'' used for training, as is usually analyzed in early-stopping type of algorithms.\n    Thus, our generalization bound concerns the weights of model trained to completion by empirical risk minimization ($w^{(\\infty)}$).\n    The generalization of this final model is more important because it will be directly used on test data. \n\n   * Nevertheless, our framework can be extended to derive generalization estimates that evolves according to the length of time (number of epochs) of training. The approach is to give a generalization bound for *early stopping* when the loss value first reaches $\\varepsilon \\geq 0$.\n    The idea is that, when there exists $T > 0$ such that $\\mathcal{L}_n(w^{T}) = \\varepsilon$, then by the inequality (11) in the proof of Lemma B.1, we can get an upper bound for the length $\\textsf{len} (w^{(0)}, T)$ in terms of $\\mathcal{L}_n(w^{(0)}), \\min_w \\mathcal{L} (w), \\varepsilon, c_n, \\theta_n$. Finally, we can also get generalization bounds byTheorem 2 with our new length estimate.\n\n     __We have added additional theoretical results based on the above tow approaches in Appendix C.1.__\n\n\n3. ''I am wondering whether the gradient flow simplification in Equation (1) results in a realistic analysis of gradient-based optimization in non-convex deep learning problems. It seems to me that the analysis may not be suitable for stochastic gradient methods.''\n\n   * First, when the step size (learning rate) of gradient is small, then the dynamics is close to the gradient flow.\n    This is guaranteed by the consistency of finite difference approximation of differential equations.\n    Thus, it is an idealized but relevant analytical setting.\n\n    * Second, continuous gradient flow allows one to employ more mathematical tools,\n    such as ODE solution methods, Gronwall inequalities, etc,\n    to obtain precise analytical results.\n\n     * Third, we emphasize that the main idea in this paper is not specific to deterministic gradient flow.\n    The key insight is our paper is to bridge a connection between optimization and generalization through path length estimates.\n    More precisely, Theorem 2 does not rely on deterministic gradient flow.\n    As long as one can obtain path length estimates (say from stochastic analysis of SGD), one can still\n    apply theorem 2 to obtain generalization estimates. Path length estimates for other types of training algorithms is an interesting future direction,\n    but not the core scope of the current paper.\n\n       __We have added the above discussion in Remark 1.__\n\n4. ''The paper has no numerical results validating the theoretical generalization bounds over standard supervised learning problems. Therefore, it is unclear whether the theory results are useful to bound the generalization error of practical deep learning experiments.''\n    \n   * __In light of your comments, we have added numerical evaluations for the generalization bounds on MNIST dataset in Appendix A.2.__ \nThe numerical results are consistent with our theoretical results that short optimization paths lead to good generalization."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ]
      },
      {
        "id": "dbFr8xgelCx",
        "original": null,
        "number": 4,
        "cdate": 1637220361461,
        "mdate": 1637220361461,
        "ddate": null,
        "tcdate": 1637220361461,
        "tmdate": 1637220361461,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "2eC-f-pbHOa",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Comment",
        "content": {
          "title": "Authors' response ",
          "comment": "1. ''It seems to put two separate results (one for optimization and the other for generalization) together directly, and the novelty of each of the two results may not be significant enough.''\n\n    * Novelty of Theorem 1:\n    Theorem 1 shows that under the uniform LGI condition,\n    the optimization path length can be bounded by the initial loss function value\n    and the uniform LGI constants.\n    Previous work [1] showed similar results under the more restrictive PL condition.\n    Here, we show that the uniform LGI condition is sufficient to guarantee length estimates,\n    and the key difference from those using the PL condition is that the convergence rate is\n    not necessarily linear.\n    As far as we are aware, this is a new result concerning optimization path length estimation\n    and convergence rate analysis.\n\n     * Novelty of Theorem 2:\n    Theorem 2 is a path length based generalization bound. \n    This differs from the work of [2] in that our bound holds for any hypothesis $f(w, \\cdot)$ that satisfies the equation (3), but not only for two-layer ReLU neural networks. \n\n    * Most importantly, the key novelty of this paper is the combined results of Theorems 1 \\& 2.\n    Previous generalization bounds that connects with optimization are based on minimum norm, or max margin solutions [3, 4].\n    These trained configurations require very special structures of the loss function/data to achieve.\n    In contrast, the generalization bound in Theorem 2 only relies on a path length estimate,\n    and Theorem 1 gives a sufficient condition to ensure that such an estimate exists.\n    Thus, the result here does not require specific setups, and can be applied to analyze a variety of model in exactly the same workflow, as we demonstrated in the practical applications.\n    We believe that this is a central novelty of this paper.\n\n      __We have added some discussion of this in Remark 1 to emphasize this point.__\n\n2. ''Also, the results may not support the claim \u201cshort optimization paths lead to good generalization\u201d exactly.''\n\n   * Theorem 2 gives a length-based generalization bound $\\mathcal{O} (R / \\sqrt{n})$, which indicates that when the length estimate is small, for example, with order $\\mathcal{O}(1)$, then we can get a non-vacuous bound $\\mathcal{O}(1 / \\sqrt{n})$, which is a good generalization bound.\n    Thus, the precise statement of Theorem 2 is that short optimization path leads to a good generalization *bound*.\n\n      __We have made this point clearer in the discussion following Theorem 2.__\n\n3. ''It would be better to have more applications satisfying the Uniform-LGI but not the PL condition. In Section 4 only the \n linear regression has $\\theta > 1 / 2$, and the other two models still fall into the PL condition.''\n\n   * In view of the reviewer's comments, __we have added the analysis for kernel regression with $\\ell_p$ loss in Theorem 4__. For $p > 2$, the model satisfies the Uniform-LGI condition but not PL condition.\n\n4. ''To claim \u201cshort optimization paths lead to good generalization\u201d, I may expect a path-based generalization bound; I think the good generalization here comes more from the good \u201clocal landscape\u201d (Uniform-LGI) rather than the \u201cpath\u201d.''\n\n    * First, Theorem 2 is precisely a path-based generalization bound.\n    It shows that if the optimization trajectory is short ($R$ is small), then this leads to small generalization error.\n    We emphasize that Theorem 2 does not assume any landscape properties.\n      The Uniform-LGI condition concerning the landscape only enters Theorem 1, which gives a *sufficient*\n    but *not necessary* condition to ensure short optimization paths.\n      In essence, while uniform-LGI condition (a landscape property) is used to derive the generalization estimates,\n    it enters only through the estimation of path lengths.\n    Thus, the statement \u201cshort optimization paths lead to good generalization\u201d is supported by our main results.\n\n5. ''I think the numerical result in Figure 1 may not illustrate the relationship between the optimization path length and generalization\u2014the larger $\\sigma$ in the initialization (Appendix A) leads to both larger optimization path length and larger generalization gap. I think it would be more fair to compare them with the same $\\sigma$ in the initialization.''\n\n    * First, we emphasize that $\\sigma$ is not the only factor that determines the trajectory length and the generalization error.\n    The mean of the initialization also matters.\n    __To clarify this statement, we have added more numerical results (the new Figure 1) by varying the mean and variance simultaneously.__\n    The experiment results suggest that short optimization paths is associated with good generalization,\n    and they are not simply affected by $\\sigma$ alone.\n    This motivate us to consider length as a complexity to characterize the generalization."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ]
      },
      {
        "id": "lzRqOjNBhIF",
        "original": null,
        "number": 5,
        "cdate": 1637221035418,
        "mdate": 1637221035418,
        "ddate": null,
        "tcdate": 1637221035418,
        "tmdate": 1637221035418,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "dbFr8xgelCx",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Comment",
        "content": {
          "title": "Authors' response (continued)",
          "comment": "* continued:\n\n   *  Second, the reason for presenting Figure 1 with different $\\sigma$ is that, our generalization bound in Theorem 2 is a high probability bound over the initialization and the training set.\n    Each set value of $\\sigma$ corresponds to one such initialization scheme.\n    Intuitively, to demonstrate our high probability result,\n    it is not meaningful to compare a small number of specific trajectory realizations.\n    Rather, we need to compare the statistics of these trajectories.\n    If we use the same $\\sigma$, then the path lengths will not vary significantly in the statistical sense.\n    Therefore, we use different $\\sigma$ (and also now, different means) to induce optimization path length variations that are statistically significant.\n    These can then support our high probability results.\n\n       **To make this point clearer, we have modified the statement of Theorem 2, where we use $R_{n, \\delta}$ to represent the uniform length estimate depending on the initialization scheme.**\n\n\nReferences:\n\n[1]  Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradientmethods under the polyak- lojasiewicz condition. In *Joint European Conference on Machine Learning and Knowledge Discovery in Databases*, pages 795\u2013811. Springer, 2016.\n\n[2] Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role of over-parametrization in generalization of neural networks. *In International Conference on Learning Representations*, 2019.\n\n[3] Peter L Bartlett, Philip M Long, G \u0301abor Lugosi, and Alexander Tsigler.  Benign overfitting in linear regression. *Proceedings of the National Academy of Sciences*, 117(48):30063\u201330070, 2020.\n\n[4] Kaifeng Lyu and Jian Li.  Gradient descent maximizes the margin of homogeneous neural networks. In *International Conference on Learning Representations*, 2020\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ]
      },
      {
        "id": "UAVhsh997n2",
        "original": null,
        "number": 6,
        "cdate": 1637221331801,
        "mdate": 1637221331801,
        "ddate": null,
        "tcdate": 1637221331801,
        "tmdate": 1637221331801,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "wRVDi8zyBxA",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Comment",
        "content": {
          "title": "Authors' response ",
          "comment": "1. ''Since the initialization is random I do not understand why the authors need to change also the variance instead of comparing the optimization paths starting from different initializations from the same distribution. The way the figure is presented suggests that the length of the optimization path trivially depends on initialization, and similarly the generalization gap.''\n\n    * First, we point out that our generalization bound in Theorem 2 is a high probability bound over the initialization and the training set.\n    Each set value of $\\sigma$ corresponds to one such initialization scheme.\n    Intuitively, to demonstrate our high probability result,\n    it is not meaningful to compare a small number of specific trajectory realizations.\n    Rather, we need to compare the statistics of these trajectories.\n    If we use the same $\\sigma$, then the path lengths will not vary significantly in the statistical sense.\n    Therefore, we use different $\\sigma$ (and also now, different means) to induce optimization path length variations that are statistically significant.\n    These can then support our high probability results.\n\n      **To make this point clearer, we have modified the statement of Theorem 2, where we use $R_{n, \\delta}$ to represent the uniform length estimate depending on the initialization scheme.**\n\n   * Second, we would like to clarify that the length of the optimization path is a sufficient but not necessary condition to guarantee the generalization. \n    Figure 1 shows that the path length can be a good indicator of generalization ability.\n \n      **To better illustrate the relation between optimization path length and generalization, we have provided additional numerical experiments for more initialization schemes (Appendix A.1).**\n\n2. ''Moreover, all the experiments are performed for a learning rate $\\eta = 0.05$. Since the theory is valid in the limit of gradient flow, I would be interested in seeing how Figure 1 changes for different learning rates.''\n\n    **In light of your comments, we have added additional experiments for different learning rates in Appendix A.1.** The results support our result that short optimization paths lead to good generalization for different learning rate.\n\n3. ''It would be interesting to test the generality of the proposed framework by checking the relation between short optimization paths and good generalization in more realistic data model. Has this relation been previously observed in applications?''\n\n    **We have provided numerical evaluations on MNIST dataset in Appendix A.2,**  the results suggest the optimization path bridges the connection between optimization and generalization in a more realistic data model as well. To the best of our knowledge, this relation has not been previously observed."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ]
      },
      {
        "id": "0wSz5U6p1N7",
        "original": null,
        "number": 7,
        "cdate": 1637221690928,
        "mdate": 1637221690928,
        "ddate": null,
        "tcdate": 1637221690928,
        "tmdate": 1637221690928,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "E8DxyFQ3y9",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Comment",
        "content": {
          "title": "Authors' response  ",
          "comment": "1. ''The upper bound $r_n(w)$ is not path-length related. In contrast, it only depends on the function value gap. \nIn other words, we first upper bound the path length by the function value gap, which is expected under \u0141ojasiewicz gradient inequality, then use the later function value gap to derive the generalization bound. Unless the path-length is used in the proof of Theorem 2, I wouldn't call it a length-based generalization bound. Please provide more details on this point.''\n\n   * We would like to point out that Theorem 2 is precisely a length-based generalization bound. \n    The intuition in Theorem 2 is that, if the initialization $w^{(0)}$ is random/independent of the training data, then it has a minimal generalization gap. \n    If the trajectory length is small, then $w^{(\\infty)}$ is necessarily close to $w^{(0)}$, thus having small generalization gap as well.\n    In fact, Theorem 2 gives a length-based generalization bound once we have an explicit length estimate.\n    The role of Theorem 1 is to provide a sufficient condition to obtain such an estimate,\n    by bounding the path length in terms of the function value gap.\n    Together, we connect optimization and generalization.\n\n     **We have improved the precision of Theorem 2 and added Remark 1 to explain the main idea above.**\n\n\n2. ''I am not sure whether the following case is possible: there are $w_0$\nand $w_1$ that for any $L_n$, the gradient flow from $w_0$\nalways pass through $w_1$. In this case, they always share the same solution but the path length of $w_1$ is always shorter than the path length of $w_0$. Even though this might be impossible, what I want to say is I don't find the shorter length-path as the cause (in terms of causality) of better generalization, instead, I think the function value gap is the real cause of it, which I believe is well studied in the literature.''\n\n   * First, we emphasize that the claim that the generalization of $w_1$ is better than the generalization of $w_0$ cannot be deduced from Theorem 2. \n    Note that the generalization bound in Theorem 2 is a high probability length-based bound over the initialization and training data, and we require the initialization to be independent of the random choice of the training data.\n    The situation where one realization of this random initialization lies in the gradient descent path of another realization is in general a 0-probability event, hence our bounds do not cover these cases.\n    For this reason, all of our numerical evaluations focus on the statistics of trajectory lengths: in each case, we vary some initialization/model configurations so that the optimization trajectories vary in a statistically significant way, from which we observed that the corresponding generalization behavior changes in the same manner.\n    See Figure 1 and our additional numerical results in Appendix A.1 \\& A.2.\n\n     **We have revised the paper to address this in the discussion of Theorem 2.**\n\n   * Second, as we mentioned before, Theorem 2 itself is a length-based generalization bound, and Theorem 1 serves as a sufficient condition to provide an explicit length estimate in terms of the function value gap. As long as one has an explicit length estimate for the trajectory length, one can obtain a generalization estimate by replacing $R_{n, \\delta}$ with the new length estimate. \n\n     **In light of your concerns, we have modified the statement of Theorem 2 and added explanations for the roles of Theorem 1 \\& 2 before and after Theorem 2.**"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ]
      },
      {
        "id": "_aA7iIZrAfC",
        "original": null,
        "number": 8,
        "cdate": 1637639732213,
        "mdate": 1637639732213,
        "ddate": null,
        "tcdate": 1637639732213,
        "tmdate": 1637639732213,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "0wSz5U6p1N7",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Comment",
        "content": {
          "title": "Re: Authors' response ",
          "comment": "I thank the provided clarification and reformulation of Theorem 2. After the revision, I agree that that Theorem 2 is a length-based generalization bound. Several questions remain:\n1. Now the question becomes in which situation we can obtain useful the length-based generalization bound. The Theorem 1 is one use case where the loss function is Uniform-LGI. However this is not that interesting as we can directly use the function value gap to bound the generalization error. A less straightforward example would be necessary to demonstrate the strength of the results \n2. Imagine we want to apply this generalization bound, then we need to bound the length bound $R_{n, \\delta}$. Except some case such as Uniform-LGI, where the length can be bounded by the function value gap, the constant $R_{n, \\delta}$ is not explicitly computable. Furthermore, it is not clear that $R_{n, \\delta}$ is independent of $n$. \nOverall, I believe further clarification is required. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Reviewer_shcV"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Reviewer_shcV"
        ]
      },
      {
        "id": "c9q05DSLUEw",
        "original": null,
        "number": 9,
        "cdate": 1637652101237,
        "mdate": 1637652101237,
        "ddate": null,
        "tcdate": 1637652101237,
        "tmdate": 1637652101237,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "_aA7iIZrAfC",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Comment",
        "content": {
          "title": "Reply to Reviewer shcV ",
          "comment": "We thank the reviewer for the comments. The main concern is on the application of Theorem 2. We attempt to clarify this below:\n\n  1. ''Now the question becomes in which situation we can obtain useful the length-based generalization bound. The Theorem 1 is one use case where the loss function is Uniform-LGI. However this is not that interesting as we can directly use the function value gap to bound the generalization error. A less straightforward example would be necessary to demonstrate the strength of the results''\n\n   * First, we would like to point out that once the loss function satisfies the Uniform-LGI, one can obtain a length estimate by Theorem 1.    We have shown the applications on three machine learning models: $\\ell_p$ linear regression (Theorem 3), $\\ell_p$ kernel regression (Theorem 4) and overparameterized two-layer neural networks (Theorem 5).   These bounds match or expand the previous results.\n\n\n  * Second, we emphasize that the function value gap is on the *training loss*, but *not* on the expected loss. Therefore, it does not in general translate to a bound on generalization.\n    Moreover,\n    under the Uniform-LGI condition, the estimation of the path length depends not only on the function value gap, but also the Uniform-LGI cosntants $c_n$ and $\\theta_n$. The asymptotic analysis of $c_n$ and $\\theta_n$ is also interesting and non-trivial. Therefore, we think our three applications demonstrate the strength of our framework.\n\n2. ''Imagine we want to apply this generalization bound, then we need to bound the length bound $R_{n, \\delta}$. Except some case such as Uniform-LGI, where the length can be bounded by the function value gap, the constant $R_{n, \\delta}$ is not explicitly computable. Furthermore, it is not clear that $R_{n, \\delta}$ is independent of $n$. Overall, I believe further clarification is required.''\n\n  * First, we emphasize that the main contribution of this paper is to propose a framework that connects optimization and generalization.\n    The key component is the Uniform-LGI property.\n    By combining Theorem 1 and Theorem 2, we have shown three applications to obtain generalization estimates on $\\ell_p$ linear/kernel regression and overparameterized two-layer neural networks.\n    These results match or expand the type of scenarios where we can rigorously establish the phenomenon of benign overfitting.\n    Path length estimates for other types of loss function property (other than Uniform-LGI) is not the core scope of the current paper.\n\n   * Second, as we mentioned, $R_{n, \\delta}$ depends not only on the training loss value gap, but also the Uniform-LGI constants $c_n$ and $\\theta_n$.\n    The asymptotic analysis of $c_n$ and $\\theta_n$ is problem-dependent, and we have given explicit computations for $R_{n, \\delta}$ with respect to different applications. See equation (21) for $\\ell_p$ linear regression, equation (24) for RBF kernel regression, equation (29) for inner product kernel regression, and equation (32) for overparemtereized two-layer neural networks."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ]
      },
      {
        "id": "8H3FC5P3pWh",
        "original": null,
        "number": 14,
        "cdate": 1638236788413,
        "mdate": 1638236788413,
        "ddate": null,
        "tcdate": 1638236788413,
        "tmdate": 1638236788413,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "_2ui05hdf9m",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Comment",
        "content": {
          "title": "Thank you for the responses",
          "comment": "I thank the authors for their thoughtful responses to my comments. While I still think the connection between generalization and optimization length can be analyzed more thoroughly, I think the overall contribution is sufficient for the paper's publication. I, therefore, keep my original weak accept rating.  "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Reviewer_ZB5L"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Reviewer_ZB5L"
        ]
      },
      {
        "id": "3f8gDS-tNEa",
        "original": null,
        "number": 15,
        "cdate": 1638312103547,
        "mdate": 1638312103547,
        "ddate": null,
        "tcdate": 1638312103547,
        "tmdate": 1638312103547,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "UAVhsh997n2",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Comment",
        "content": {
          "title": "Response to authors",
          "comment": "I thank the authors for addressing my concerns providing useful clarifications. I have appreciated the effort in improving the manuscript, that I recommend for publication. Therefore, I am keeping my score to 6."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Reviewer_xkEt"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Reviewer_xkEt"
        ]
      },
      {
        "id": "EAZudFvPnQqQ",
        "original": null,
        "number": 1,
        "cdate": 1642696850574,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696850574,
        "tmdate": 1642696850574,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "D1TYemnoRN",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The paper presents several interesting generalization results for Uniform-LGI loss functions (a generalization of PL functions). Some of these bounds seem useful, but the overall connection with the optimization length remains unclear. This concern and other points of criticism remain present after the rebuttal phase. Other minor concerns seem fixable, but in a larger timeline compared to the camera ready one. The paper should be revised for a future venue."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "E8DxyFQ3y9",
        "original": null,
        "number": 1,
        "cdate": 1635900319741,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635900319741,
        "tmdate": 1635972415141,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "D1TYemnoRN",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper provides a novel generalization bound for the gradient flow equation related to the length of the optimization path. The bound is valid for loss function that locally satisfies \u0141ojasiewicz gradient inequality, which is applicable to different machine learning models such as underdetermined $\\ell_p$ linear regression, kernel regression, and overparameterized two-layer ReLU neural networks. Explicit derivations are provided for these three models to show that the length-based generalization bound is non-vacuous. ",
          "main_review": "The paper is well organized and easy to follow. My main concern is regarding the claim \"short optimization paths lead to good generalization\" which I feel misleading. The reason is that\n\na. The upper bound $r_n(w)$ is not path-length related. In contrast, it only depends on the function value gap $L_n(w) - L_n^*$. In other words, we first upper bound the path length by the function value gap, which is expected under \u0141ojasiewicz gradient inequality, then use the later function value gap to derive the generalization bound. Unless the path-length is used in the proof of Theorem 2, I wouldn't call it a length-based generalization bound. Please provide more details on this point. \n\nb. I am not sure whether the following case is possible: there are $w_0$ and $w_1$ that for any $L_n$, the gradient flow from $w_0$ always pass through $w_1$. In this case, they always share the same solution but the path length of $w_1$ is always shorter than the path length of $w_0$. Even though this might be impossible, what I want to say is I don't find the shorter length-path as the cause (in terms of causality) of better generalization, instead, I think the function value gap is the real cause of it, which I believe is well studied in the literature. (it might be wrong but I am happy to further discuss it)\n\nOverall, I can't see how explicitly the length of optimization path influence the generalization bound in the current analysis, instead it is based on the function value gap. I am willing to raise my score if my concern is addressed in the rebuttal. ",
          "summary_of_the_review": "My major concern is how explicitly the length of optimization path influence the generalization bound in the current analysis, instead it is based on the function value gap.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "Not applicable",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Reviewer_shcV"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Reviewer_shcV"
        ]
      },
      {
        "id": "wRVDi8zyBxA",
        "original": null,
        "number": 2,
        "cdate": 1635971747798,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635971747798,
        "tmdate": 1635971747798,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "D1TYemnoRN",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The authors study the connection between optimization and generalization for gradient flow (GF) on loss functions that satisfy a global version of the Lojasiewicz gradient inequality. Under this assumption, they prove convergence of GF to a global minimum and they find an upper bound on the optimization length \u2014 measured as the integral of the $\\ell_2-$norm of the gradient from time $t=0$ to the final time. This upper bound depends on the specific choice of the loss function and on the number of samples. With an additional assumption on the hypothesis class (encompassing, e.g., linear shallow networks, two-layer networks) the first result is used to derive an upper bound on the generalization gap. The bound depends on the choice of the loss function, its initial value, and the length of the optimisation path. This leads to the main result that shorter optimization paths induce smaller generalization gap. The authors apply this result to three models (underdetermined $\\ell_p$ linear regression, kernel regression, and overparametrized two-layer networks with ReLU activation) with a given target function. They compute non-asymptotic expressions for the generalization bounds at fixed ratio between sample size and ambient dimension, and show that in these cases the bounds are non-vacuous when the dimension increases.\n",
          "main_review": "The paper inscribes in the relevant research direction aiming at understanding the interplay between efficient optimization and good generalization performances by identifying complexity measures that are implicitly minimised during training. \n\nThe analysis is sound and presented in a clear way. Overall, I found the results interesting and worth of publication although some improvements are needed. \n\nIn my opinion, the main weakness of the work regards the numerical experiments. I find that the authors could have provided better evidence than Figure 1. It is not clear to me whether each point in Figure 1 comes from an average (at fixed initialisation variance) or is the result of a single simulation starting from random Gaussian initialization with that variance. The first option does not make much sense to me, so I am assuming the second. However, in this case, since the initialization is random I do not understand why the authors need to change also the variance instead of comparing the optimization paths starting from different initializations from the same distribution. The way the figure is presented suggests that the length of the optimization path trivially depends on initialization, and similarly the generalization gap. \n\nMoreover, all the experiments are performed for a learning rate $\\eta=0.05$. Since the theory is valid in the limit of gradient flow, I would be interested in seeing how Figure 1 changes for different learning rates.\n\nFinally, it would be interesting to test the generality of the proposed framework by checking the relation between short optimization paths and good generalization in more realistic data model. Has this relation been previously observed in applications? \u2028\u2028",
          "summary_of_the_review": "I find that this paper is worth of publication since it provides an interesting contribution to a relevant research direction in the theory of machine learning. However, I believe that the authors must improve their numerical results to provide a convincing final version of the work.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "Not applicable",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Reviewer_xkEt"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Reviewer_xkEt"
        ]
      },
      {
        "id": "2eC-f-pbHOa",
        "original": null,
        "number": 3,
        "cdate": 1635972090404,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635972090404,
        "tmdate": 1638383662076,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "D1TYemnoRN",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a framework to analyze both optimization and generalization properties under the Uniform-LGI condition (Def 1). From my understanding, the main results consist of two parts:\n\n* Optimization: define the Uniform-LGI as an extension of the PL condition, prove the corresponding convergence result with a sublinear rate, and bound the optimization path length.\n* Generalization: use the Rademacher complexity to estimate the generalization error. The Rademacher complexity scales with the diameter of the parameter set, thus can be bounded by the optimization path length, which connects with the optimization results.\n\nThen the paper apply this framework to three application models: first establish the Uniform-LGI, then calculate the optimization path length and estimate the generalization error.",
          "main_review": "This paper is well organized following the approach from optimization to generalization. The theorems and proofs are clearly stated and easy to follow. However, my main concern is that it seems to put two separate results (one for optimization and the other for generalization) together directly, and the novelty of each of the two results may not be significant enough. Also, the results may not support the claim \u201cshort optimization paths lead to good generalization\u201d exactly.\n\nFor the optimization part, the definition of the Uniform-LGI (with parameter $\\theta$) is inspired by the original Lojasiewicz inequality, and the proof of convergence is straightforward and similar to Bolte et al. (2007). Nevertheless, I think it is good to introduce the result to the community as an extension to the widely-used PL condition. The sublinear convergence rate is also interesting. It would be better to have more applications satisfying the Uniform-LGI but not the PL condition. In Section 4 only the $l_p$ linear regression has $\\theta > 1/2$, and the other two models still fall into the PL condition.\n\nFor the generalization part, the estimation of generalization error follows the Rademacher approach. The proof is nontrivial while the idea is not complicated: the Rademacher complexity scales with the diameter of the parameter set, i.e., the optimization path length. To claim \u201cshort optimization paths lead to good generalization\u201d, I may expect a path-based generalization bound; ~~I think the good generalization here comes more from the good \u201clocal landscape\u201d (Uniform-LGI) rather than the \u201cpath\u201d.~~ (Edit: my statement was not accurate\u2014Theorem 2 does not require the Uniform-LGI. I mean the generalization bound does not depend on the whole optimization path, but only the endpoint, and the generalization is good if the endpoint falls in a small region.)\n\nIn addition, I think the numerical result in Figure 1 may not illustrate the relationship between the optimization path length and generalization\u2014the larger $\\sigma$ in the initialization (Appendix A) leads to both larger optimization path length and larger generalization gap. I think it would be more fair to compare them with the same $\\sigma$ in the initialization.\n\n**Update**\n\nI appreciate the detailed response from the authors and the careful improvement of the manuscript. Here is my summary of the contributions of the paper:\n* Optimization: it is good to propose the Uniform-LGI as an extension of the PL condition and show the sublinear rate. However, the Uniform-LGI is similar to the original Lojasiewicz inequality, and the proof is straightforward (similar to Bolte et al., 2007?).\n* Generalization: Theorem 2 is equivalent to the following: given a region $S$ with radius $R$, with probability $1 - \\delta$ over the training samples, for all $w \\in S$, the generalization gap\u2028$$L_D(w) - L_n(w) \\le \\frac{A R + B \\sqrt{3(p + q) + \\log(2 / \\delta)}}{\\sqrt n}$$\u2028where $A$ depends on the Lipschitz constants in $S$ and $B$ depends on the upper bound of $l$. Similar approach is taken by some previous work (e.g., Allen-Zhu et al., 2019); nevertheless, it is good to state the result precisely. I think the result would be better summarized as \u201csmall parameter region leads to good generalization bound\u201d, and this is not surprising as the Rademacher complexity increases w.r.t. $R$.\n\nTherefore, I would like to keep my original score given my concern about the significance of the novelty.",
          "summary_of_the_review": "The paper is well organized and clearly written. It is interesting to introduce the Uniform-LGI as an extension to the PL condition. However, I think the novelty of each of the two main results is not significant enough, and putting them together may not support the claim \u201cshort optimization paths lead to good generalization\u201d exactly.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "Not applicable",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Reviewer_8Scj"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Reviewer_8Scj"
        ]
      },
      {
        "id": "54quulh2VwS",
        "original": null,
        "number": 4,
        "cdate": 1636222403693,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636222403693,
        "tmdate": 1636222403693,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "D1TYemnoRN",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper studies generalization and optimization of kernel-based and one-hidden layer neural network models. The convergence guarantee is shown for a Uniform-LGI loss function in Theorem 1. Then, in Theorem 2 the paper proves a generalization guarantee assuming a particular parametric model in Equation (3). Finally, the paper applies these results to p-norm regression, kernel regression, and one hidden layer neural network learning problems.  ",
          "main_review": "The paper develops a convergence and generalization analysis of machine learning models under Uniform-LGI loss functions. The concept of Uniform-LGI loss functions extends the well-known PL condition in the optimization literature and as the paper shows leads to a nice theoretical setting for analyzing the optimization and generalization aspects of different machine learning models. The paper shows applications of the framework to norm-p regression and kernel regression problems as well as one-hidden-layer neural network learning problems.\n\nOverall I think this is an interesting work which shows some insightful generalization results under Uniform-LGI loss functions. My main concern with the paper is that while the paper's title, abstract, and introduction claim that the analysis helps to understand the connection between the length of optimization and the generalization error of learned models, the theoretical results in Theorems 2-5 seem to only bound the generalization error of the final model and do not have anything to say about how the generalization error changes during training the model. I, therefore, suggest the authors either clearly explain how the theorems analyze the role of optimization length in the final generalization performance or change the title and introduction appropriately to reflect the main contributions of the work. \n\nIn addition to the above comment, I am wondering whether the gradient flow simplification in Equation (1) results in a realistic analysis of gradient-based optimization in non-convex deep learning problems. It seems to me that the analysis may not be suitable for stochastic gradient methods which have been shown to achieve a significantly better generalization performance than full-batch gradient descent modeled in the paper. As my final comment, the paper has no numerical results validating the theoretical generalization bounds over standard supervised learning problems. Therefore, it is unclear whether the theory results are useful to bound the generalization error of practical deep learning experiments.",
          "summary_of_the_review": "The paper shows several interesting generalization results for Uniform-LGI loss functions. While the paper proves several insightful generalization bounds, it seems Theorems 2-5 do not explain the connection between the optimization length and generalization error. Also, the paper has no numerical results validating the generalization results.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "Not applicable",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Reviewer_ZB5L"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Reviewer_ZB5L"
        ]
      },
      {
        "id": "WdZgKeJBSya",
        "original": null,
        "number": 2,
        "cdate": 1637218444725,
        "mdate": 1637218444725,
        "ddate": null,
        "tcdate": 1637218444725,
        "tmdate": 1637218444725,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "D1TYemnoRN",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Official_Comment",
        "content": {
          "title": "To all reviewers.",
          "comment": "*  We appreciate all the reviewers for their valuable suggestions and comments on our paper. \n\n*  In the revised paper, we highlighted the main changes in blue, and in the following's response, we emphasize the changes in boldface.\n\n*  The main concerns about the paper are two-fold. \n\n   First, the connection between optimization and generalization is not clearly presented by our results. \n    We emphasize that Theorem 2 is precisely a path-based generalization bound.\n    It shows that as long as one has an explicit length estimate for the trajectory length, one can obtain a generalization estimate.\n    That is to say, the role of Theorem 1 is to provide a sufficient but not necessary condition to ensure such a length estimate exists. \n    In that sense, the statement ''short optimization paths lead to good generalization'' is supported by our results.\n    \n    \n    The second main concern is that Figure 1 may not illustrate the relationship between the optimization path length and generalization.\n    We would like to point out that the length estimate we consider in Theorem 2 is a uniform estimate for a specific initialization *method*, but not a point wise estimate that depends on a specific initialization *point*. \n    Thus, the generalization performance for different initialization methods in Figure 1 motivates us to use path length to build its connection to optimization. \n    \n    **In this revision, we have modified the statement of Theorem 2, and added explanations for the roles of Theorem 1 \\& 2 in the main text.**\n\n\n* Based on each reviewer's concerns, we summarize the main changes of our paper below:\n\n   + According to Reviewer ZB5L's comments, we have\n     + **added more explanations of the intuition in the introduction;**\n     + **provided additional theoretical results to bound the generalization error during training the model (Appendix C.1);**\n     + **added numerical results validating our generalization bounds on MNIST dataset (Appendix A.2).**\n\n  + According to Reviewer 8Scj's comments, we have\n    + **modified the statement of Theorem 2, and added explanations for the roles and novelty of Theorem 1 \\& 2 immediately after Theorem 2;**\n    + **added the analysis for kernel regression with $\\ell_p$ loss (Theorem 4). For $p>2$ , the model satisfies the Uniform-LGI but not PL condition;**\n    + **replace the original Figure 1 with a new figure that provides numerical results for Gaussian initialization with varied mean and variance.**\n\n  + According to Reviewer xkEt's comments, we have\n     + **modified the statement of Theorem 2, and add explanations for the roles of Theorem 1 \\& 2 before and after Theorem 2;**\n     + **added additional experiments for different learning rates (Appendix A.1);**\n     + **provided numerical experiments to show the relation between optimization paths and generalization on MNIST dataset (Appendix A.2).**\n\n  + According to Reviewer shcV's comments, we have\n     + **added explanations of the relation between Theorem 1 \\& 2 before and after Theorem 2,\n          for better understanding how explicitly the length of optimization path influence the generalization bound.**"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper286/Authors"
        ]
      },
      {
        "id": "EAZudFvPnQqQ",
        "original": null,
        "number": 1,
        "cdate": 1642696850574,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696850574,
        "tmdate": 1642696850574,
        "tddate": null,
        "forum": "D1TYemnoRN",
        "replyto": "D1TYemnoRN",
        "invitation": "ICLR.cc/2022/Conference/Paper286/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The paper presents several interesting generalization results for Uniform-LGI loss functions (a generalization of PL functions). Some of these bounds seem useful, but the overall connection with the optimization length remains unclear. This concern and other points of criticism remain present after the rebuttal phase. Other minor concerns seem fixable, but in a larger timeline compared to the camera ready one. The paper should be revised for a future venue."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}