{
  "id": "ljCoTzUsdS",
  "original": "DUBLGmKr--l",
  "number": 152,
  "cdate": 1632875432631,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875432631,
  "tmdate": 1676330686443,
  "ddate": null,
  "content": {
    "title": "Distinguishing rule- and exemplar-based generalization in learning systems",
    "authorids": [
      "~Ishita_Dasgupta1",
      "~Erin_Grant1",
      "~Thomas_L._Griffiths1"
    ],
    "authors": [
      "Ishita Dasgupta",
      "Erin Grant",
      "Thomas L. Griffiths"
    ],
    "keywords": [
      "inductive bias",
      "combinatorial generalization",
      "cognitive psychology",
      "robustness to spurious correlation"
    ],
    "abstract": "Despite the increasing scale of datasets in machine learning, generalization to unseen regions of the data distribution remains crucial. Such extrapolation is by definition underdetermined and is dictated by a learner\u2019s inductive biases. Machine learning systems often do not share the same inductive biases as humans and, as a result, extrapolate in ways that are inconsistent with our expectations. We investigate two distinct such inductive biases: feature-level bias (differences in which features are more readily learned) and exemplar-vs-rule bias (differences in how these learned features are used for generalization). Exemplar- vs. rule-based generalization has been studied extensively in cognitive psychology, and in this work we present a protocol inspired by these experimental approaches for directly probing this trade-off in learning systems. The measures we propose characterize changes in extrapolation behavior when feature coverage is manipulated in a combinatorial setting. We present empirical results across a range of models and across both expository and real-world image and language domains. We demonstrate that measuring the exemplar-rule trade-off while controlling for feature-level bias provides a more complete picture of extrapolation behavior than existing formalisms. We find that most standard neural network models have a propensity towards exemplar-based extrapolation and discuss the implications of these findings for research on data augmentation, fairness, and systematic generalization.",
    "one-sentence_summary": "We distinguish exemplar- from rule-based reasoning in learning systems, controlling for biases at the level of specific features, with implications for systematicity in combinatorial domains and sensitivity to spurious correlations.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "dasgupta|distinguishing_rule_and_exemplarbased_generalization_in_learning_systems",
    "pdf": "/pdf/170bd2298b7c5632393414dc6606b59e974f208f.pdf",
    "supplementary_material": "/attachment/c3eb3fe0c37c0ac250e286e5f751ac9290386f4c.zip",
    "_bibtex": "@misc{\ndasgupta2022distinguishing,\ntitle={Distinguishing rule- and exemplar-based generalization in learning systems},\nauthor={Ishita Dasgupta and Erin Grant and Thomas L. Griffiths},\nyear={2022},\nurl={https://openreview.net/forum?id=ljCoTzUsdS}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "ljCoTzUsdS",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 19,
    "directReplyCount": 5,
    "revisions": true,
    "replies": [
      {
        "id": "gNz0BNNARxZ",
        "original": null,
        "number": 1,
        "cdate": 1635313991263,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635313991263,
        "tmdate": 1635318118890,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "ljCoTzUsdS",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies the extrapolation of machine learning models to unseen regions, and specifically studies two types of biases: feature-level bias (differences in which features are more readily learned) and exemplar-vs-rule bias (differences in how these learned features are used for generalization). Motivated by the studies of exemplar vs. rule-based generalization in cognitive psychology, the authors present a protocol directly probing this trade-off in machine learning systems. The authors present empirical results across a range of models in both expository and real-world image and language domains and demonstrate that using the trad off provides a more complete picture of extrapolation behaviour than existing methods. \n",
          "main_review": "I found that this paper was quite difficult to read since the authors present discussions without clear definitions. Using many acronyms makes the reading difficult too. \n\nFor example, \u201cfeature-level bias is measured as deviation from chance performance in the CC condition.\u201d When reading CC condition, it says \u201cthe data presented in this condition confound color and shape\u201d. What does confound mean here? CC condition is unclear and I do not know feature-level bias.\n\nSimilarly, \u201cExemplar-vs-rule bias is measured by the difference between performance in the PE and ZS conditions\u2014\u2026\u201d When reading PE and ZS, they are not defined either. \n\nReading on, I do not see formal definitions of feature-level bias exemplar-vs-rule bias. There are measures obtained from models trained by some specific methods. So the proposed approach deals specifically with three types of methods. Why do the authors focus on three types of methods? Why are they representative? \n\nAlso, I do not know what spurious correlation means in this paper. \n\nThe authors present quite some examples to illustrate their points, but I am confused by their examples. For example, in Figure 1, only two training examples are given. Any prediction is incorrect since there is not enough evidence to learn a model. I cannot see the differences between rule-based and example-based. What rules can we generate based on the two examples?\n",
          "summary_of_the_review": "The paper is unclear based on the current presentation.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Reviewer_TPBn"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Reviewer_TPBn"
        ]
      },
      {
        "id": "26h4hnV19sn",
        "original": null,
        "number": 2,
        "cdate": 1635791197406,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635791197406,
        "tmdate": 1638238106978,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "ljCoTzUsdS",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes two measures for evaluating the feature-level bias and the exemplar-vs-rule bias in learning systems. Specifically, the authors designed three independent training conditions:\n    i. cue conflict: {x1 = 0, x2 = 1, y = 0}, {x1 = 1, x2 = 0, y = 1}\n    ii. zero shot: {x1 = 0, x2 = 0, y = 0}, {x1 = 1, x2 = 0, y = 1}\n    iii. partial exposure: {x1 = 0, x2 = 0, y = 0}, {x1 = 1, x2 = 0, y = 1}, {x1 = 0, x2 = 1, y = 0}\nand one testing condition:\n    iv. extrapolation: {x1 = 1, x2 = 1, y = 1}.\nThe inductive bias of a given learning system is measured by its extrapolation performance difference when trained on different training conditions. \n\nEmpirically, the authors first verified their framework on a synthetic dataset with 2D inputs. The results confirm that generalized linear model favors rule-based generalization while Gaussian process favors exemplar-based generalization. On IMDB, the authors show that LSTM models exhibit high feature-level bias (overfitting to the spurious token features) favors exemplar-rule bias. On CelebA, the authors show that ResNet exhibits a wide range of feature-level bias for different features (\u2018male\u2019 is easier to learn than \u2018high cheekbones\u2019). However, across all feature pairs, the model prefers exemplar-based generalization to rule-based generalization.",
          "main_review": "Strength:\n+ Probing the behavior of the learning system by comparing its generalization performance when trained on different datasets is interesting and novel.\n+ The synthetic experiment on 2D inputs demonstrate the effectiveness of the measure. \n+ The paper is very well-written.\n\nWeakness:\n+ Unclear utility.\nThe main contributions of the paper are the two proposed measures. However, I found it hard to utilize these measures in practical applications. \n  + In order to compute the measures, the values for the discriminant features and the distractor features are required. In this situation, there are many existing methods for learning models that are robust against the distractor features [1]. What will be the point of evaluating the bias of a non-robust classifier?\n  + For the exemplar-vs-rule propensity (EVR), my understanding is that it captures the extent to which the distractor features are utilized in the learning system. What will be the benefit of EVR compared to evaluating the worst-group accuracy as in [1]? Let\u2019s use CelebA as an example. The training data contains mostly {male, dark_hair}, {female, blond_hair}, {female, dark_hair} images. At test time, [1] evaluates the worst-group accuracy, which is likely to be the performance on {male, blond_hair} (since it is underrepresented in the training data).\n\nQuestions and suggestions:\n+ How do you interpret the values of the FLB and EVR measures? For instance, in IMDB, the FLB score is -0.3 while the EVR score is 0.54. What do these values mean?\n+ I think both measures can be defined in a more generalized setting (where you don\u2019t need to assume the label marginal is uniform).\n+ You argued in the paper that the measure is capturing something more than the spurious correlation. I think it will be more precise to state it as **linear** correlation in Figure 3.\n+ It will be very interesting to see a contour plot of EVR over $\\pi_0$ and $\\pi_1$ even in the synthetic experiments.\n\nI am happy to adjust my ratings after the rebuttal period.\n\n[1] Sagawa, Shiori, et al. \"Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.\" arXiv preprint arXiv:1911.08731 (2019).",
          "summary_of_the_review": "The authors proposed an interesting approach for measuring the rule- and exemplar-based  generalization for a given learning system. The perspective is novel and the writing is clear. My only concern is the practical utility of the proposed measures.\n\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Reviewer_yoH5"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Reviewer_yoH5"
        ]
      },
      {
        "id": "_WCl8DFjdrQ",
        "original": null,
        "number": 3,
        "cdate": 1636015222826,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636015222826,
        "tmdate": 1636015222826,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "ljCoTzUsdS",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes measures of how two types of explanation methods -- rule based and exemplar based -- generalize and extrapolate to unseen data regions.",
          "main_review": "Some of the ideas introduced by the paper to think about certain inductive biases is interesting. For example, feature-level bias being which features are easier or harder to learn. Here it could be interesting to go into continuous vs. categorical features.\n\nI found some of the exposition unnecessarily confusing. For example, since in Section 2, the color of objects determines their label (i.e. green objects are \u201cdax\u201d and purple objects are \u201cfep\u201d), why use the labels \"dax\" and \"fep\" at all rather than just green and purple?\n\nThere are many claims that could be substantiated or explained further. For example, why do GPs help with formalizing exemplar-based generalization (Section 4.1)? It would also be clearer to specify which statements have been substantiated through experiments and which have not. For example, Section 4.2 starts off by saying that one NN, one GLM, and one GP has been trained and then make a jump to Figure 4a which shows decision boundaries that are not surprising; I am not sure how it validates the protocol proposed to measure EVR without confounds. Section 4.4 makes a substantial claim (\"We also find that different ways to reduce \u03c1 (e.g. by reducing \u03c00 or by increasing \u03c01), give different extrapolation behavior (see Appendix)\") based on results pushed to the appendix. \n\nSome sentences could be better worded:\n- (Page 6) \"EVR increases with exemplar-basedness\" \n- (Page 7) What is the \"held-out quadrant\"?\n",
          "summary_of_the_review": "The paper is ambitious but falls short in clearly explaining the framework proposed, and substantiating the idea with strong experiments.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Reviewer_RJtk"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Reviewer_RJtk"
        ]
      },
      {
        "id": "Tdmh4Vu7vn",
        "original": null,
        "number": 4,
        "cdate": 1636052004109,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636052004109,
        "tmdate": 1636052004109,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "ljCoTzUsdS",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The generalization of data distributions to unseen regions, i.e., extrapolation, remains one of the critical challenges in machine learning. The inductive biases of the learner determine such extrapolation. Unfortunately, machine learning systems often do not share the same inductive biases as humans and, as a result, may extrapolate in ways that do not match the analyst's expectations. The authors investigated two different types of such inductive bias: feature-level bias (differences in which features are more easily learned) and exemplar-based and rule-based bias (differences in how learned features are used for generalization). Inspired by these experimental approaches, we have proposed a protocol to investigate this trade-off in learning systems directly. We present empirical results for a range of models and the domains of explanatory images and language. We demonstrate that controlling for feature-level bias while measuring the trade-off between exemplars and rules provides a complete picture of extrapolative behavior than existing formalisms.",
          "main_review": "Generalization in the domain of extrapolation, which the authors address in this paper, is one of the critical issues in machine learning. Therefore, they propose a protocol to investigate the problem of inductive bias in extrapolation. Specifically, inspired by psychological research, they propose a protocol to investigate the inductive bias of the learning system towards different features (FLB) and the inductive bias of the learning system towards different ways of using features, either by rule-based or exemplar-based generalization (EVR).\n\nThe approach, inspired by psychological research, is interesting. However, we are not convinced that it is a practically valid method, as we have only shown experiments on two datasets.",
          "summary_of_the_review": "The authors' treatment of inductive bias in extrapolation is interesting and will interest many researchers. The proposed protocol, which takes its ideas from psychology, is also interesting. However, we are not convinced that the proposed protocol is practical.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Reviewer_UbRf"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Reviewer_UbRf"
        ]
      },
      {
        "id": "3-0vrSPiplw",
        "original": null,
        "number": 1,
        "cdate": 1636995631540,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636995631540,
        "tmdate": 1637009263204,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "_WCl8DFjdrQ",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "Author Response to \"Official Review of Paper152 by Reviewer RJtk\" (1/2)",
          "comment": "We thank the reviewer for opportunities to improve the clarity of our paper.\n\n**1. The paper is ambitious but falls short in clearly explaining the framework proposed, and substantiating the idea with strong experiments.**\n\nWe believe we have addressed the clarity concerns brought up by the reviewer (see below). If any clarity concerns remain, please bring them to our attention during discussion and we will address them in a revision. Otherwise, we sincerely hope that the improvements more clearly convey the contributions of the paper to the reviewer.\n\nWe also note that the reviewer has not yet commented on the demonstrations of the exemplar-based behavior of neural network models beyond the synthetic dataset: We have probed this behavior on 2 separate domains (text and vision), and have studied this behavior in depth on the vision domain (with 6 attribute pairs, 18 ResNet depth-width combinations, and 30 random seeds). We believe this is a strong experimental demonstration of the phenomenon introduced in the paper.\n\n**2. Some of the ideas introduced by the paper to think about certain inductive biases is interesting. For example, feature-level bias being which features are easier or harder to learn. Here it could be interesting to go into continuous vs. categorical features.**\n\nWe thank the reviewer for their positive assessment of the value of formalizing these inductive biases. We agree that examining continuous features is a natural next step. However, in this paper (which is the first to simultaneously study feature-level bias and exemplar-rule bias), we focused on categorical features to maximize the connection to existing work \"combinatorial generalization\" (Andreas et al. (CVPR 2017); Johnson et al. (CVPR 2017)) and \"subgroup fairness\" (Sagawa et al. (ICLR 2020); Sagawa et al. (ICML 2020)). Relatedly, the use of binary multi-label datasets (including specifically CelebA) to explore compositional features is standard in other subfields (Higgins et al. (ICLR 2017); Azadi et al. (IJCV 2020). We will explicitly discuss these papers in the revision.\n\n**3. I found some of the exposition unnecessarily confusing. For example, since in Section 2, the color of objects determines their label (i.e., green objects are \"dax\" and purple objects are \"fep\"), why use the labels \"dax\" and \"fep\" at all rather than just green and purple?**\n\nThe goal of the illustrative example in Fig. 1 was in part to help readers examine their own biases in how they learn novel categories, as we state: \"...we encourage the reader to try the experiment themselves to examine their intuitions.\" We used these arbitrary names to discourage people from using the *a priori* knowledge of what feature is relevant to the underlying category boundary, which is the problem setting used for the ML systems. We will explicitly state this in the revision.\n\n**4. why do GPs help with formalizing exemplar-based generalization (Section 4.1)**\n\nAs we state in the manuscript, \"[n]on-parametric kernel-based models allow us to formalize exemplar-based generalization.\" In particular, in the synthetic setting, the GP model as a kernel method operates directly on **similarity in feature space** to training data when extrapolating to new data. The particular kernel we chose (radial basis function) computes similarity over all features, and is thus \"feature-dense\" and implements exemplar-based reasoning (also see \"Exemplar-vs-rule bias\" in Sec. 1).\n\nNote that a direct implementation of exemplar-based reasoning is only possible in the synthetic setting, where features over which to compute similarity are known. In contrast, this is not possible in the representation learning contexts (Sec. 5 & 6), where we do not assume knowledge of the discriminant & distractor features *a priori* and thus cannot directly implement a similarity-based kernel classifier. We will clarify this correspondence between kernel-based approaches and exemplar-based generalization in the revised manuscript.\n\n**5. Section 4.2 starts off by saying that one NN, one GLM, and one GP has been trained and then make a jump to Figure 4a which shows decision boundaries that are not surprising**\n\nWe emphasize that the synthetic experiment exists to quantitatively confirm intuitions. We chose one kind of model architecture from each broad model family (NN, GLM, GP) and plot decision boundaries over several random initializations in Fig. 4a. We agree that the boundaries reflect expected differences between the models\u2014our key point is that these qualitative differences can be described by the quantitative EvR effect (the GP has high EvR; GLM has low EvR; NN is intermediate). Showing that a model we know to be exemplar-based (the GP) has high EvR and that a model we know to be rule-based (the GLM) has low EvR validates that the EvR behavioral measure indeed captures exemplar-vs-rule generalization. This is discussed in the main text in Sec. 4.2, and we will clarify this motivation in the revised manuscript."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ]
      },
      {
        "id": "NykuRS2f7J",
        "original": null,
        "number": 2,
        "cdate": 1636995727634,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636995727634,
        "tmdate": 1637009336982,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "_WCl8DFjdrQ",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "Author Response to \"Official Review of Paper152 by Reviewer RJtk\" (2/2)",
          "comment": "**6. I am not sure how it validates the protocol proposed to measure EvR without confounds.**\n\nWhat we mean here by \u201cwithout confounds\u201d is that we control for FLB so we can examine EvR in isolation. We will clarify this statement in the revised manuscript.\n\n**7. Section 4.4 makes a substantial claim (\"We also find that different ways to reduce \u03c1 (e.g. by reducing \u03c00 or by increasing \u03c01), give different extrapolation behavior (see Appendix)\") based on results pushed to the appendix.**\n\nIn the main text, we do qualitatively describe results showing that different \u03c00 and \u03c01 values give different extrapolations (first paragraph of Section 4.4), and include quantitative results for another setting of \u03c00 and \u03c01 for the vision domain (Figure 6c), which is discussed in Section 6 under \"Controlling spurious correlation.\" Based on the reviewer\u2019s suggestion, we will briefly discuss the results in the linear case in the main text as well; these results already exist in Figure 10 in the Appendix.\n\n**8. (Page 6) \"EVR increases with exemplar-basedness\"**\n\nWe have rephrased this as follows: \u201cEvR increases with quantities known to increase exemplar-based reasoning.\u201d\n\n**9. (Page 7) What is the \"held-out quadrant\"?**\n\nAs we state under \"Training conditions\" in Section 3, \u201c[t]he upper-right quadrant in all subfigures of Fig. (2), for which $p(\\mathbf{z}_\\text{disc} = 1, \\mathbf{z}_\\text{dist} = 1) = 1$, acts as a *hold-out set* on which we can evaluate generalization to an unseen combination of attribute values \u2026 All the analyses in this paper compare model extrapolation to the held-out test quadrant across various training conditions.\u201d\n\n\n### References\n\n[Azadi, Samaneh, Deepak Pathak, Sayna Ebrahimi, and Trevor Darrell. \"Compositional GAN: Learning image-conditional binary composition.\" In IJCV, 2020.](https://arxiv.org/abs/1807.07560)\n\n[Higgins, Irina, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. \"Beta-VAE: Learning basic visual concepts with a constrained variational framework.\" In ICLR, 2017.](https://openreview.net/forum?id=Sy2fzU9gl)\n\n[Johnson, Justin, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. \"CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning.\" In CVPR, 2017.](https://arxiv.org/abs/1612.06890)\n\n[Sagawa, Shiori, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. \"Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.\" In ICLR, 2020.](https://arxiv.org/abs/1911.08731)\n\n[Sagawa, Shiori, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. \"An investigation of why overparameterization exacerbates spurious correlations.\" In ICML, 2020.](https://arxiv.org/abs/2005.04345)"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ]
      },
      {
        "id": "ZelP9eiBMBz",
        "original": null,
        "number": 3,
        "cdate": 1637003536045,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637003536045,
        "tmdate": 1637009744008,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "26h4hnV19sn",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "Author Response to \"Official Review of Paper152 by Reviewer yoH5\" (1/3)",
          "comment": "We thank the reviewer for their positive comments and valuable feedback. We respond to specific comments below, and hope that these responses more clearly convey the significance of the work. Please let us know if there is more we can answer during the discussion period.\n\n**1. Unclear utility. The main contributions of the paper are the two proposed measures. However, I found it hard to utilize these measures in practical applications.**\n\nWe note that the characterizing inductive biases in machine learning models, independent of a downstream application, is of interest to the community. For example: Ritter et al. (ICML 2018) study shape bias; Geirhos et al. (NeurIPS 2018) study inherent robustness to image perturbations; Geirhos et al. (ICLR 2018) study shape vs. texture bias; Rahaman et al. (ICML 2019) and Wang et al. (CVPR 2020) study spectral bias. Based on the existence of numerous concurrent works published at venues like ICLR, we argue that it is sufficient to clearly define, isolate, and study surprising phenomena of models that are in use in current practice, and that not every paper that does so needs to demonstrate improvements in a \"practical\" application.\n\nNevertheless, we do have some hypotheses about implications for downstream applications. In the submission, we wrote about implications of our results for systematic generalization (Sec. 7, \u201cModel design for systematic generalization\u201d), data augmentation (Sec. 7, \u201cData augmentation\u201d), and fairness (Sec. 6, \u201cPractical implications of the EVR\u201d as well as Sec. 8 (the conclusion)). We are happy to add further discussions to the revision if the reviewer feels that specific downstream implications are insufficiently discussed.\n \n**2. In order to compute the measures, the values for the discriminant features and the distractor features are required. In this situation, there are many existing methods for learning models that are robust against the distractor features [1]. What will be the point of evaluating the bias of a non-robust classifier?**\n\nWe agree that there exist methods for learning distributionally robust classifiers. However, these methods do not account for the unique contributions of feature-level bias (FLB) and exemplar-vs-rule bias (EvR). DRO in Sagawa et al. (ICLR 2020) focus on reducing scalar spurious correlation, whereas we demonstrate that different ways of manipulating this correlation (by altering $\\pi_0$ and $\\pi_1$) can have different effects on extrapolation performance, as captured by different mechanisms: feature-level bias (differences in which features are more readily learned) and exemplar-vs-rule bias (differences in how these learned features are used for generalization). Our more granular analysis informs what kinds of data augmentations or model alterations will be most effective in each setting.\n\nMore broadly, why do we care about this analysis of non-robust classifiers if we can just make classifiers robust (with DRO or some other method)? We should care about the non-robustness properties of general (non-robust) classifiers because that is what is in use in general, and methods to make classifiers robust commonly make stronger assumptions about the training setting. For example, Sagawa et al. (ICLR 2020) study the application of distributionally robust optimization (DRO) to the group DRO setting, which is related to our 2x2 distractor-discriminant setup; however, their method requires access to subgroup identities to form the training objective (in order to group data in the objective). We consider the more standard training objective with non-subgroup-identified data, and ask the question: Is a particular, commonly-employed model family sensitive to features that are not necessary for classification? We use the subgroup identities in the analysis to answer this question, but the fact that the EvR effect is persistent (across many model families and many feature pairs) suggests that this behavior is general and would persist in settings in which the subgroup identities (spurious features) are not known.  \n\n**3. For the exemplar-vs-rule propensity (EVR), my understanding is that it captures the extent to which the distractor features are utilized in the learning system.**\n\nYes, and to elaborate: EvR measures the extent to which a learning system utilizes an additional feature (ie. the distractor feature) **even when its usage does not aid in optimizing the training objective** (ie. whether it chooses feature-dense (\"exemplar-based\") over feature-sparse (\"rule-based\") solutions given no particular data evidence for either).\n\nIn contrast, FLB measures how much the distractor feature is used when both distractor and discriminator are equally informative of the category boundary (ie. when they both help equally toward reducing the training objective)."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ]
      },
      {
        "id": "QMZCRxoLu_N",
        "original": null,
        "number": 4,
        "cdate": 1637003642049,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637003642049,
        "tmdate": 1637010091189,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "26h4hnV19sn",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "Author Response to \"Official Review of Paper152 by Reviewer yoH5\" (2/3)",
          "comment": "**4. What will be the benefit of EVR compared to evaluating the worst-group accuracy as in [1]? Let\u2019s use CelebA as an example. The training data contains mostly {male, dark_hair}, {female, blond_hair}, {female, dark_hair} images. At test time, [1] evaluates the worst-group accuracy, which is likely to be the performance on {male, blond_hair} (since it is underrepresented in the training data).**\n\n\u200bEvR is indeed a function of the *un*represented group\u2019s accuracy (which we always measure as the worst-group test accuracy, as you hypothesize); we call this \u201caccuracy on the held-out/test quadrant.\u201d EvR additionally incorporates a measure of deviation from the performance of the classifier on the extrapolation task **in the absence of linear spurious correlation**: Zero-shot accuracy, which varies significantly on the CelebA task (\u00b110%) as a function of model family and the specific features employed. We additionally measure FLB alongside EvR to demonstrate that they are dependent but not reducible. This allows us to disentangle feature-level bias (FLB) from bias toward feature-dense over feature-sparse solutions (EvR), which are confounded when only measuring worst-group accuracy. These two components (zero-shot baseline and FLB) are not studied in Sagawa et al. (ICLR 2020) and related works.\n\nWe also note that our training distribution withholds a quadrant, whereas Sagawa et al. (ICLR 2020) underrepresent a quadrant, as the respective aims of our works are different: We consider the \u201ccombinatorial generalization\u201d case of extrapolation to strictly unseen feature combinations, whereas Sagawa et al. (ICLR 2020) consider how reweighting the data distribution to manipulate linear spurious correlation between features (but maintaining the same training support) affects performance on underrepresented data regions. Mixing data from the held-out quadrant into the training data as in Sagawa et al. (ICLR 2020) would complicate measuring (via our EvR measure) the bias for feature-dense vs. feature-sparse solutions because we need to do so **in the absence of any data evidence favoring either more strongly** (which would be a confound).\n\nSee for example the effects of manipulating $\\pi_0$ and $\\pi_1$ in Fig. 6, which demonstrates that fully holding out the \u201ctest\u201d quadrant (Fig. 6a) leads to qualitatively different extrapolation behavior compared to settings with the same degree of spurious linear correlation, but where the \u201ctest\u201d quadrant is simply underrepresented (Fig. 6b). This is because populating the under-represented quadrant with labeled examples in training provides data evidence for feature-dense vs. feature-sparse solutions. \n\n**5. How do you interpret the values of the FLB and EVR measures? For instance, in IMDB, the FLB score is -0.3 while the EVR score is 0.54. What do these values mean?**\n\nThe FLB and EvR measures are meant to compare differences between models in terms of the magnitude of the relevant underlying inductive biases (ie. non-endpoint values are meant to be interpreted as relative measures). The FLB can take values between -1 and 1 (indicating bias toward distractor vs. discriminant features respectively) with 0 representing no feature bias; EvR takes values between 0 and 1 (indicating fully feature-sparse, rule-based extrapolation, and fully feature-dense, exemplar-based extrapolation, respectively). We will clarify this in the revised manuscript.\n\n**6. I think both measures can be defined in a more generalized setting (where you don\u2019t need to assume the label marginal is uniform).**\n\nWe agree; however, in this work, we aim to treat the uniform marginal as a control condition, not as an independent variable that is manipulated. This is so we can focus on the independent variables of a) the training conditions (due to how they evidence the inductive biases we study, EvR and FLB), and b) the model family (to see how these inductive biases vary across them). We intend to leave the role of non-uniform label marginals to future work.\n\n**7. You argued in the paper that the measure is capturing something more than the spurious correlation. I think it will be more precise to state it as linear correlation in Figure 3.**\n\nThis is correct; we now write: \u201cThe linear correlation coefficient $\\rho$ between the discriminant and distractor features\u2014henceforth \u201cspurious correlation\u201d\u2014...\u201d We note that Sagawa et al. (ICLR 2020) also use the linear correlation coefficient.\n\n**8. It will be very interesting to see a contour plot of EVR over \u03c00 and \u03c01 even in the synthetic experiments.**\n\nNice suggestion! We will try to get this done by the 22nd, but it would require many more runs than we performed in the paper for the synthetic setting."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ]
      },
      {
        "id": "KWROTIezwIx",
        "original": null,
        "number": 5,
        "cdate": 1637003670573,
        "mdate": 1637003670573,
        "ddate": null,
        "tcdate": 1637003670573,
        "tmdate": 1637003670573,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "26h4hnV19sn",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "Author Response to \"Official Review of Paper152 by Reviewer yoH5\" (3/3)",
          "comment": "### References\n\n[Geirhos, Robert, Carlos R. Medina Temme, Jonas Rauber, Heiko H. Sch\u00fctt, Matthias Bethge, and Felix A. Wichmann. \"Generalisation in humans and deep neural networks.\" In NeurIPS, 2018.](https://arxiv.org/abs/1808.08750)\n\n[Geirhos, Robert, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.\" In ICLR, 2018.](https://arxiv.org/abs/1811.12231)\n\n[Rahaman, Nasim, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred A. Hamprecht, Yoshua Bengio, and Aaron C. Courville. \"On the Spectral Bias of Deep Neural Networks.\" In ICML, 2019.](https://arxiv.org/abs/1806.08734)\n\n[Ritter, Samuel, David GT Barrett, Adam Santoro, and Matt M. Botvinick. \"Cognitive psychology for deep neural networks: A shape bias case study.\" In ICML, 2017.](https://arxiv.org/abs/1706.08606)\n\n[Sagawa, Shiori, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. \"Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.\" In ICLR, 2020.](https://arxiv.org/abs/1911.08731)\n\n[Wang, Haohan, Xindi Wu, Zeyi Huang, and Eric P. Xing. \"High-frequency component helps explain the generalization of convolutional neural networks.\" In CVPR, 2020.](https://arxiv.org/abs/1905.13545)"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ]
      },
      {
        "id": "HV5SImoQMh0",
        "original": null,
        "number": 7,
        "cdate": 1637365786520,
        "mdate": 1637365786520,
        "ddate": null,
        "tcdate": 1637365786520,
        "tmdate": 1637365786520,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "Tdmh4Vu7vn",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "This review does not meet the standards of the review process. (Response to Reviewer UbRf)",
          "comment": "Dear Reviewer UbRf,\n\nWe ask that you request the AC to assign another reviewer, as this review does not meet the standards of the review process described in [the ICLR 2022 reviewer guide](https://iclr.cc/Conferences/2022/ReviewerGuide):\n- The review has been marked by the reviewer as extremely low-confidence (\u201cConfidence: 1\u201d).\n- The abstract of the submission is almost exactly paraphrased (twice: once in \u201cSummary Of The Paper\u201d and once in \u201cMain Review\u201d).\n- The review is not substantive: The only critical feedback in the review is that the paper is not \u201cpractical\u201d since we have \u201conly shown experiments on two datasets.\u201d We demonstrate results on 1 toy dataset and 2 larger datasets spanning vision and language; this is standard for a conference paper. The reviewer does not indicate what additional datasets/experiments would add value.\n\nThank you,\n\nThe Authors\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ]
      },
      {
        "id": "9d5bzuiyAm",
        "original": null,
        "number": 8,
        "cdate": 1637366963755,
        "mdate": 1637366963755,
        "ddate": null,
        "tcdate": 1637366963755,
        "tmdate": 1637366963755,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "gNz0BNNARxZ",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "This review does not meet the standards of the review process. (Response to Reviewer TPBn)",
          "comment": "Dear Reviewer TPBn,\n\nWe ask that you request the AC to assign another reviewer, as this review does not meet the standards of the review process described in [the ICLR 2022 reviewer guide](https://iclr.cc/Conferences/2022/ReviewerGuide):\n- The review misses definitions of terms and concepts explicitly defined, explained, and visualized in the submission.\n- The review provides no actionable feedback.\n- The abstract of the submission is closely paraphrased in \u201cSummary Of The Paper.\u201d\n\nWe respond to specific points of the review below.\n\nThank you,\n\nThe Authors\n\n\n**1. I found that this paper was quite difficult to read since the authors present discussions without clear definitions.**\n\nWe note that Reviewer yoH5 states that \u201cthe paper is very well-written.\u201d We respond below with clarifications: Much of what the reviewer asks for is already in the paper.\n\n**2. Using many acronyms makes the reading difficult too.**\n\nWe introduce 5 acronyms that are clearly defined in the paper: \n- CC is defined on pg. 2 (\u201cCue conflict (CC, top row, Fig. (1))\u201d) and visualized in Fig. 1.\n- ZS is defined on pg. 3 (\u201cZero shot (ZS, middle row, Fig. (1))\u201d) and visualized in Fig. 1.\n- PE is defined on pg. 3 (\u201cPartial exposure (PE, bottom row, Fig. (1))\u201d) and visualized in Fig. 1.\n- FLB and EvR are defined on pg. 4 (\u201cFeature-level bias (FLB) and exemplar-vs-rule propensity (EVR) are measured as\u2026\u201d).\n\nThe remaining acronyms (GLM, NN, GP) are standard in the field.\n\nThe use of acronyms creates visual correspondences between plot markings and the main text. Nevertheless, we are happy to take the feedback and increase clarity surrounding the acronyms. This is a small change that should not take a complete round of conference review.\n\n**3. For example, \u201cfeature-level bias is measured as deviation from chance performance in the CC condition.\u201d When reading CC condition, it says \u201cthe data presented in this condition confound color and shape\u201d. What does confound mean here? CC condition is unclear and I do not know feature-level bias.**\n\n\u201c[C]onfound color and shape\u201d refers to the fact that color and shape are equally predictive of category boundary, as in the CC condition. CC condition is explained as well as illustrated on pg. 2 and in Fig. 1.\n\nFeature-level bias is defined on pg. 1: \u201cFeature-level bias measures which features a system finds easier or harder to learn. This informs which feature a system will generalize on the basis of when both features are correlated or confounded.\u201d\n\n**4. Similarly, \u201cExemplar-vs-rule bias is measured by the difference between performance in the PE and ZS conditions\u2014\u2026\u201d When reading PE and ZS, they are not defined either.**\n\nAs stated above, PE and ZS are explicitly defined on pgs. 2 & 3.\n\n**5. Reading on, I do not see formal definitions of feature-level bias exemplar-vs-rule bias.**\n\n\u201cFeature-level bias\u201d and \u201cexemplar-vs-rule bias\u201d are described in pg. 1-2 and defined mathematically under \u201cMeasuring inductive bias\u201d on pg 4.\n\n**6. There are measures obtained from models trained by some specific methods. So the proposed approach deals specifically with three types of methods. Why do the authors focus on three types of methods? Why are they representative?**\n\nComparing behavior of a model family across conditions is useful in understanding differences between systems and in revealing the principles underlying systems. This is the basic premise of our work (and of many others); for example, see pg. 2, where we explain \u201c...how differences in classification behavior on this extrapolation isolate feature-level bias as well as exemplar-vs-rule bias.\u201d A similar approach has been used previously, for example, in the literature on the \u201cshape bias\u201d (Ritter 2017a; Hermann 2019; Geirhos 2018), also discussed on pg. 2.\n\n**7. Also, I do not know what spurious correlation means in this paper.**\n\nWe refer to the correlation coefficient between distractor and discriminant as the \"spurious correlation,\" as we state on pg. 4: \u201c[t]he correlation coefficient $\\rho$ between $\\mathbf{z}_\\text{disc}$ and $\\mathbf{z}_\\text{dist}$.\u201d We have not defined the correlation coefficient between two variables mathematically since it is a commonly-known and accepted definition in the field.\n\n**8. The authors present quite some examples to illustrate their points, but I am confused by their examples. For example, in Figure 1, only two training examples are given. Any prediction is incorrect since there is not enough evidence to learn a model \u2026 What rules can we generate based on the two examples?**\n\nNone of our results are based on training a system in this setting of two examples; this figure is purely for illustration; we realize the visualization as a statistical learning setting, as we state in Sec. 4.\n\n**9. I cannot see the differences between rule-based and example-based.**\n\nThe difference is clear in Fig. 1, bottom row, where we demonstrate that the PE condition distinguishes these two modes (columns \u201crule-based\u201d and \u201cexemplar-based\u201d)."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ]
      },
      {
        "id": "4zOORkiR8Pq",
        "original": null,
        "number": 11,
        "cdate": 1637537981420,
        "mdate": 1637537981420,
        "ddate": null,
        "tcdate": 1637537981420,
        "tmdate": 1637537981420,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "9d5bzuiyAm",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "Reply to authors",
          "comment": "The issues I raise are fundamental in scientific writing. The paper with the issues is not ready for publishing. \n\nI use the following example to show my point.\n\nMy review \"For example, \u201cfeature-level bias is measured as deviation from chance performance in the CC condition.\u201d When reading CC condition, it says \u201cthe data presented in this condition confound color and shape\u201d. What does confound mean here? CC condition is unclear and I do not know feature-level bias.\".\n\nThe authors' reply: \u201c[C]onfound color and shape\u201d refers to the fact that color and shape are equally predictive of category boundary, as in the CC condition. CC condition is explained as well as illustrated on pg. 2 and in Fig. 1.\n\nMy Re-reply:\n\nFeature-level bias is defined by the CC condition. However, the cue conflict (CC) condition has not been defined. Therefore, Feature-level bias is undefined.\n\nLet us read the paper again \u201cCue conflict (CC, top row, Fig. (1)). The data presented in this condition confound color and shape.\u201d\n\nFirstly, this is not a definition but an example. A definition should be a statement, such as CC condition is (or means) XXXX.\n\nSecondly, this explanation by the example is unclear either since the concept of \"confounding\" has not been explained. In the authors' reply \"\"[C]onfound color and shape\u201d refers to the fact that color and shape are equally predictive of category boundary\". This sentence is not in the paper. Furthermore, this concept of \"confounding\" is different from the \"confounding\" in statistics. A reader with an understanding of statistical confounding will be confused by reading \"The data presented in this condition confound color and shape\". This further reinforces the point that I made: the authors should define their concepts clearly before using the concepts ."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Reviewer_TPBn"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Reviewer_TPBn"
        ]
      },
      {
        "id": "Vj136N1yAZz",
        "original": null,
        "number": 12,
        "cdate": 1637551382076,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637551382076,
        "tmdate": 1637553911740,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "4zOORkiR8Pq",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "Second response to follow-up from Reviewer TPBn",
          "comment": "Our intended meaning of the word \"confound\" is its dictionary definition. We take the criticism that it might have a more specific meaning in statistics, and have included the explicit sentence \"[in the CC condition], color and shape are confounded (i.e., equally predictive of category boundary)\" in our revised version to increase clarity. No issue remains here.\n\nWe responded in detail to 9 separate points in the original review to demonstrate that \u201cmuch of what the reviewer asks for is already in the paper.\u201d This follow-up refers to a single one of these points (which is addressed by the addition of a single phrase, as detailed above) to argue that the paper has issues \u201cfundamental [to] scientific writing.\u201d We see insufficient evidence for this claim in the original review and the follow-up response.\n\nWe are very happy to take and act upon any actionable feedback and suggestions the reviewer might have to improve our submission. At this stage, we feel that this review and the follow-up do not indicate that the submission and our response were \"carefully read\" and \"comprehensively evaluated\" as is set out in the [ICLR 2022 Reviewer Guidelines](https://iclr.cc/Conferences/2022/ReviewerGuide), as we stated in the original response."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ]
      },
      {
        "id": "b8I_BOdUhJT",
        "original": null,
        "number": 13,
        "cdate": 1637981558943,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637981558943,
        "tmdate": 1637981569269,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "3-0vrSPiplw",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "Follow-up to Reviewer RJtk",
          "comment": "Dear Reviewer RJtk,\n\nSince there isn't much time left in the discussion period (ending Nov. 29th), we wanted to check whether you had any remaining concerns that we could address. We believe we addressed the concerns raised in the initial review in our response on Nov 15th, summarized below for convenience. Thank you very much again for taking the time to review our paper and for the feedback!\n\nSummary of our response so far: We responded directly to the \u201cclarity\u201d and \u201cexperimental strength\u201d concerns (Q1). We clarified that we are \u201cthe first to simultaneously study feature-level bias and exemplar-rule bias\u201d, building on the reviewer\u2019s positive comment on the protocol (Q2). We clarified that a central result (insufficiency of spurious linear correlation to predict extrapolation performance) is replicated across multiple domains in the main text, and not simply appearing in the appendix (Q7). We also directly addressed other clarity concerns in a revision (Q3, Q4, Q5, Q6, Q8, Q9).\n\nThank you,\n\nThe Authors"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ]
      },
      {
        "id": "JMaKLY9B9Ch",
        "original": null,
        "number": 14,
        "cdate": 1637981876091,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637981876091,
        "tmdate": 1637981884557,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "ZelP9eiBMBz",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "Follow-up to Reviewer yoH5",
          "comment": "Dear Reviewer yoH5,\n\nSince there isn't much time left in the discussion period (ending Nov. 29th), we wanted to check whether you had any remaining concerns that we could address. We believe we addressed the concerns raised in the initial review in our response on Nov 15th, summarized below for convenience. Thank you very much again for taking the time to review our paper and for the feedback!\n\nSummary of our response so far: We responded directly to the \u201cutility\u201d concern (Q1). We discussed in more detail the distinctions between our work and prior work (Q2, Q4). We added clarifications in the revision (Q3, Q5, Q6, Q7)\u2014we fixed one detail in Q5 just now (\u201cFLB takes values between -1 and 1\u201d -> \u201c... -0.5 and 0.5\u201d). Regarding Q8, we did not have time to carry out these additional analyses during the brief period that revisions were allowed; since they do not address any particular hypothesis raised in the submission or in the discussion, we are planning it for future work.\n\nThank you,\n\nThe Authors"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ]
      },
      {
        "id": "gq8LgNw7wB1",
        "original": null,
        "number": 15,
        "cdate": 1638238088598,
        "mdate": 1638238088598,
        "ddate": null,
        "tcdate": 1638238088598,
        "tmdate": 1638238088598,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "JMaKLY9B9Ch",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "Thank you for the reply",
          "comment": "Thank you for the detailed reply. While the practical applicability still seems vague to me, I agree with the authors that it is interesting and important to understand the rule- and exemplar- biases (I have boosted my score).\n\n*\u201cWe consider the more standard training objective with non-subgroup-identified data, and ask the question: Is a particular, commonly-employed model family sensitive to features that are not necessary for classification?\u201d*\n+ Is it reasonable to assume that a particular model family always exhibit the same level of rule- and exemplar- biases? I guess that will change according to the training data?"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Reviewer_yoH5"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Reviewer_yoH5"
        ]
      },
      {
        "id": "0L1RBdP2mG",
        "original": null,
        "number": 16,
        "cdate": 1638490092183,
        "mdate": 1638490092183,
        "ddate": null,
        "tcdate": 1638490092183,
        "tmdate": 1638490092183,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "gq8LgNw7wB1",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Comment",
        "content": {
          "title": "Thank you for updating your score!",
          "comment": "We sincerely thank the reviewer for reading our response and increasing their score!\n\n**> Is it reasonable to assume that a particular model family always exhibits the same level of rule- and exemplar- biases? I guess that will change according to the training data?**\n\nWe agree that it is important to distinguish something that is strictly a property of a model family from something that is domain-dependent. This is why we evaluate the EvR bias across various settings, demonstrating that it persists in NNs across feature difficulties (i.e., different values of CC), feature values (e.g., various feature combinations in an image classification domain), and domains (e.g., points-in-a-plane; text classification (IMDb); image classification (CelebA)). The evidence that we collected in our empirical studies suggests that NNs are exemplar-based regardless of the setting (which is consistent with prior work demonstrating the ubiquity of sensitivity to spurious correlation). But, as is the nature of empirical work, we cannot guarantee that this holds for *all* domains.\n\n**> the practical applicability still seems vague**\n\nThe problem of understanding how NNs extrapolate is important,  timely, and has practical consequences (D'Amour et al., JMLR 2021). Related works published in ICLR and similar conferences that study this question (Johnson et al., CVPR 2017; Sagawa et al., ICLR 2020; Sagawa et al., ICML 2020) use experimental settings similar to ours to manipulate the underlying example/feature distribution of datasets like CelebA. The nature of these careful studies is to understand a phenomenon that has practical importance (NN extrapolation) in a controlled setting. We make analogous (but independent) contributions to understanding NN extrapolation that we believe merit sharing with the ICLR community.\n\n### References\n\nD'Amour, Alexander, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen et al. \"Underspecification presents challenges for credibility in modern machine learning.\" In JMLR, 2021.\n\nJohnson, Justin, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. \u201cCLEVR: A diagnostic dataset for compositional language and elementary visual reasoning.\u201d In CVPR, 2017.\n\nSagawa, Shiori, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. \u201cDistributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.\u201d In ICLR, 2020.\n\nSagawa, Shiori, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. \u201cAn investigation of why overparameterization exacerbates spurious correlations.\u201d In ICML, 2020.\n\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Authors"
        ]
      },
      {
        "id": "r14C_T1y4zn",
        "original": null,
        "number": 1,
        "cdate": 1642696840871,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696840871,
        "tmdate": 1642696840871,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "ljCoTzUsdS",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The paper proposes a novel protocol for examining the inductive biases in learning systems, by quantifying the exemplar-rule trade-off (as measured by the exemplar-vs-rule propensity (EVR) defined in Eq. (2)) while controlling for feature-level bias. \n\nReviewers mostly agree that the problem studied in this paper is practically relevant and that the two bias measures are potentially interesting and (jointly) more informative than existing measures such as spurious correlation. However, a shared concern among the reviewers (with confidences scores >=3) is the clarity of the exposition (e.g., many key concepts such as the data conditions are informally specified [Section 2 (Reviewer TPBn)], some key messages not clearly conveyed in the main paper [Section 3 (Reviewer RJtk)], and results inconclusive or not sufficiently supported by the experimental results [for both the synthetic setting (Reviewer RJtk) and the real-world setting (Reviewer yoH5)]. Based on the above concerns, the reviewers were not convinced that this work is well supported in its current state to merit acceptance for publication."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      },
      {
        "id": "HVegaaqVxy9",
        "original": null,
        "number": 1,
        "cdate": 1644346052642,
        "mdate": 1644346052642,
        "ddate": null,
        "tcdate": 1644346052642,
        "tmdate": 1644346052642,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "r14C_T1y4zn",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Public_Comment",
        "content": {
          "title": "The meta-review does not reflect the author responses",
          "comment": "Since the meta-review and reviews are persistent and public, we would like to publicly comment on this meta-review. The 3 points raised as grounds for rejection have already been addressed in the author responses (most without meaningful follow-up by reviewers). We are therefore at a loss for how to incorporate feedback from this review process and improve our paper.\n\n> **a shared concern among the reviewers (with confidences scores >=3) is the clarity of the exposition (e.g., many key concepts such as the data conditions are informally specified [Section 2 (Reviewer TPBn)]**\n\nFirst, the specific claim about data conditions being informally specified is incorrect: Fig. 2 (which is suggestively titled \"Formalizing the illustrative experiment\") explicitly provides a mathematical specification of each data condition.\n\nSecond, we have [responded to concerns about clarity from Reviewer TPBn](https://openreview.net/forum?id=ljCoTzUsdS&noteId=9d5bzuiyAm) with several (9) points which show that \"much of what Reviewer TPBn asks for is already in the paper.\" Neither Reviewer TPBn nor the meta-reviewer has acknowledged all of these points.\n\nFinally, reviewer yoH5 (confidence 4) states that \"the paper is very well-written.\" Therefore, \"clarity of the exposition\" is simply not \"a shared concern among the reviewers (with confidences scores >=3)\".\n\n> **some key messages not clearly conveyed in the main paper [Section 3 (Reviewer RJtk)]**\n\nThe [specific claim from Reviewer RJtk](https://openreview.net/forum?id=ljCoTzUsdS&noteId=_WCl8DFjdrQ) is that \"Section 4.4 makes a substantial claim (\"We also find that different ways to reduce \u03c1 (e.g. by reducing \u03c00 or by increasing \u03c01), give different extrapolation behavior (see Appendix)\") based on results pushed to the appendix.\"\n\n[We clarified in the author response](https://openreview.net/forum?id=ljCoTzUsdS&noteId=NykuRS2f7J) that evidence for this claim was displayed in Figure 6c and explicitly discussed in Section 6 under \"Controlling spurious correlation\u201d **for the vision domain**. Reviewer RJtk excerpted analogous results for the illustrative **points-in-a-plane** domain, of which detailed figures are indeed in the Appendix due to the page limit. \n\nAs such, the key message targeted by Reviewer RJtk was *very clearly conveyed in the main paper at the time of the first submission.* We clarified this in the aforementioned response, but neither Reviewer RJtk nor the meta-reviewer has acknowledged this.\n\n> **results inconclusive or not sufficiently supported by the experimental results [for both the synthetic setting (Reviewer RJtk) and the real-world setting (Reviewer yoH5)]**\n\nThis point as written is unclear (\"results \u2026 not sufficiently supported by \u2026 results\"), so we assume that the claim is instead that the \"conclusions are not supported by the results.\" (?)\nWe directly addressed Reviewer RJtk\u2019s concerns and received zero engagement. Reviewer yoH5 raised their score to above the acceptance threshold after our responses. This leaves us with little actionable feedback on how to actually address the meta-reviewer's comment here in order to revise our paper.\n\nPaper152 Authors"
        },
        "signatures": [
          "~Erin_Grant1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "~Erin_Grant1"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "gNz0BNNARxZ",
        "original": null,
        "number": 1,
        "cdate": 1635313991263,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635313991263,
        "tmdate": 1635318118890,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "ljCoTzUsdS",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper studies the extrapolation of machine learning models to unseen regions, and specifically studies two types of biases: feature-level bias (differences in which features are more readily learned) and exemplar-vs-rule bias (differences in how these learned features are used for generalization). Motivated by the studies of exemplar vs. rule-based generalization in cognitive psychology, the authors present a protocol directly probing this trade-off in machine learning systems. The authors present empirical results across a range of models in both expository and real-world image and language domains and demonstrate that using the trad off provides a more complete picture of extrapolation behaviour than existing methods. \n",
          "main_review": "I found that this paper was quite difficult to read since the authors present discussions without clear definitions. Using many acronyms makes the reading difficult too. \n\nFor example, \u201cfeature-level bias is measured as deviation from chance performance in the CC condition.\u201d When reading CC condition, it says \u201cthe data presented in this condition confound color and shape\u201d. What does confound mean here? CC condition is unclear and I do not know feature-level bias.\n\nSimilarly, \u201cExemplar-vs-rule bias is measured by the difference between performance in the PE and ZS conditions\u2014\u2026\u201d When reading PE and ZS, they are not defined either. \n\nReading on, I do not see formal definitions of feature-level bias exemplar-vs-rule bias. There are measures obtained from models trained by some specific methods. So the proposed approach deals specifically with three types of methods. Why do the authors focus on three types of methods? Why are they representative? \n\nAlso, I do not know what spurious correlation means in this paper. \n\nThe authors present quite some examples to illustrate their points, but I am confused by their examples. For example, in Figure 1, only two training examples are given. Any prediction is incorrect since there is not enough evidence to learn a model. I cannot see the differences between rule-based and example-based. What rules can we generate based on the two examples?\n",
          "summary_of_the_review": "The paper is unclear based on the current presentation.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Reviewer_TPBn"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Reviewer_TPBn"
        ]
      },
      {
        "id": "26h4hnV19sn",
        "original": null,
        "number": 2,
        "cdate": 1635791197406,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635791197406,
        "tmdate": 1638238106978,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "ljCoTzUsdS",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes two measures for evaluating the feature-level bias and the exemplar-vs-rule bias in learning systems. Specifically, the authors designed three independent training conditions:\n    i. cue conflict: {x1 = 0, x2 = 1, y = 0}, {x1 = 1, x2 = 0, y = 1}\n    ii. zero shot: {x1 = 0, x2 = 0, y = 0}, {x1 = 1, x2 = 0, y = 1}\n    iii. partial exposure: {x1 = 0, x2 = 0, y = 0}, {x1 = 1, x2 = 0, y = 1}, {x1 = 0, x2 = 1, y = 0}\nand one testing condition:\n    iv. extrapolation: {x1 = 1, x2 = 1, y = 1}.\nThe inductive bias of a given learning system is measured by its extrapolation performance difference when trained on different training conditions. \n\nEmpirically, the authors first verified their framework on a synthetic dataset with 2D inputs. The results confirm that generalized linear model favors rule-based generalization while Gaussian process favors exemplar-based generalization. On IMDB, the authors show that LSTM models exhibit high feature-level bias (overfitting to the spurious token features) favors exemplar-rule bias. On CelebA, the authors show that ResNet exhibits a wide range of feature-level bias for different features (\u2018male\u2019 is easier to learn than \u2018high cheekbones\u2019). However, across all feature pairs, the model prefers exemplar-based generalization to rule-based generalization.",
          "main_review": "Strength:\n+ Probing the behavior of the learning system by comparing its generalization performance when trained on different datasets is interesting and novel.\n+ The synthetic experiment on 2D inputs demonstrate the effectiveness of the measure. \n+ The paper is very well-written.\n\nWeakness:\n+ Unclear utility.\nThe main contributions of the paper are the two proposed measures. However, I found it hard to utilize these measures in practical applications. \n  + In order to compute the measures, the values for the discriminant features and the distractor features are required. In this situation, there are many existing methods for learning models that are robust against the distractor features [1]. What will be the point of evaluating the bias of a non-robust classifier?\n  + For the exemplar-vs-rule propensity (EVR), my understanding is that it captures the extent to which the distractor features are utilized in the learning system. What will be the benefit of EVR compared to evaluating the worst-group accuracy as in [1]? Let\u2019s use CelebA as an example. The training data contains mostly {male, dark_hair}, {female, blond_hair}, {female, dark_hair} images. At test time, [1] evaluates the worst-group accuracy, which is likely to be the performance on {male, blond_hair} (since it is underrepresented in the training data).\n\nQuestions and suggestions:\n+ How do you interpret the values of the FLB and EVR measures? For instance, in IMDB, the FLB score is -0.3 while the EVR score is 0.54. What do these values mean?\n+ I think both measures can be defined in a more generalized setting (where you don\u2019t need to assume the label marginal is uniform).\n+ You argued in the paper that the measure is capturing something more than the spurious correlation. I think it will be more precise to state it as **linear** correlation in Figure 3.\n+ It will be very interesting to see a contour plot of EVR over $\\pi_0$ and $\\pi_1$ even in the synthetic experiments.\n\nI am happy to adjust my ratings after the rebuttal period.\n\n[1] Sagawa, Shiori, et al. \"Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.\" arXiv preprint arXiv:1911.08731 (2019).",
          "summary_of_the_review": "The authors proposed an interesting approach for measuring the rule- and exemplar-based  generalization for a given learning system. The perspective is novel and the writing is clear. My only concern is the practical utility of the proposed measures.\n\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Reviewer_yoH5"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Reviewer_yoH5"
        ]
      },
      {
        "id": "_WCl8DFjdrQ",
        "original": null,
        "number": 3,
        "cdate": 1636015222826,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636015222826,
        "tmdate": 1636015222826,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "ljCoTzUsdS",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes measures of how two types of explanation methods -- rule based and exemplar based -- generalize and extrapolate to unseen data regions.",
          "main_review": "Some of the ideas introduced by the paper to think about certain inductive biases is interesting. For example, feature-level bias being which features are easier or harder to learn. Here it could be interesting to go into continuous vs. categorical features.\n\nI found some of the exposition unnecessarily confusing. For example, since in Section 2, the color of objects determines their label (i.e. green objects are \u201cdax\u201d and purple objects are \u201cfep\u201d), why use the labels \"dax\" and \"fep\" at all rather than just green and purple?\n\nThere are many claims that could be substantiated or explained further. For example, why do GPs help with formalizing exemplar-based generalization (Section 4.1)? It would also be clearer to specify which statements have been substantiated through experiments and which have not. For example, Section 4.2 starts off by saying that one NN, one GLM, and one GP has been trained and then make a jump to Figure 4a which shows decision boundaries that are not surprising; I am not sure how it validates the protocol proposed to measure EVR without confounds. Section 4.4 makes a substantial claim (\"We also find that different ways to reduce \u03c1 (e.g. by reducing \u03c00 or by increasing \u03c01), give different extrapolation behavior (see Appendix)\") based on results pushed to the appendix. \n\nSome sentences could be better worded:\n- (Page 6) \"EVR increases with exemplar-basedness\" \n- (Page 7) What is the \"held-out quadrant\"?\n",
          "summary_of_the_review": "The paper is ambitious but falls short in clearly explaining the framework proposed, and substantiating the idea with strong experiments.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Reviewer_RJtk"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Reviewer_RJtk"
        ]
      },
      {
        "id": "Tdmh4Vu7vn",
        "original": null,
        "number": 4,
        "cdate": 1636052004109,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636052004109,
        "tmdate": 1636052004109,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "ljCoTzUsdS",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The generalization of data distributions to unseen regions, i.e., extrapolation, remains one of the critical challenges in machine learning. The inductive biases of the learner determine such extrapolation. Unfortunately, machine learning systems often do not share the same inductive biases as humans and, as a result, may extrapolate in ways that do not match the analyst's expectations. The authors investigated two different types of such inductive bias: feature-level bias (differences in which features are more easily learned) and exemplar-based and rule-based bias (differences in how learned features are used for generalization). Inspired by these experimental approaches, we have proposed a protocol to investigate this trade-off in learning systems directly. We present empirical results for a range of models and the domains of explanatory images and language. We demonstrate that controlling for feature-level bias while measuring the trade-off between exemplars and rules provides a complete picture of extrapolative behavior than existing formalisms.",
          "main_review": "Generalization in the domain of extrapolation, which the authors address in this paper, is one of the critical issues in machine learning. Therefore, they propose a protocol to investigate the problem of inductive bias in extrapolation. Specifically, inspired by psychological research, they propose a protocol to investigate the inductive bias of the learning system towards different features (FLB) and the inductive bias of the learning system towards different ways of using features, either by rule-based or exemplar-based generalization (EVR).\n\nThe approach, inspired by psychological research, is interesting. However, we are not convinced that it is a practically valid method, as we have only shown experiments on two datasets.",
          "summary_of_the_review": "The authors' treatment of inductive bias in extrapolation is interesting and will interest many researchers. The proposed protocol, which takes its ideas from psychology, is also interesting. However, we are not convinced that the proposed protocol is practical.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper152/Reviewer_UbRf"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper152/Reviewer_UbRf"
        ]
      },
      {
        "id": "r14C_T1y4zn",
        "original": null,
        "number": 1,
        "cdate": 1642696840871,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696840871,
        "tmdate": 1642696840871,
        "tddate": null,
        "forum": "ljCoTzUsdS",
        "replyto": "ljCoTzUsdS",
        "invitation": "ICLR.cc/2022/Conference/Paper152/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The paper proposes a novel protocol for examining the inductive biases in learning systems, by quantifying the exemplar-rule trade-off (as measured by the exemplar-vs-rule propensity (EVR) defined in Eq. (2)) while controlling for feature-level bias. \n\nReviewers mostly agree that the problem studied in this paper is practically relevant and that the two bias measures are potentially interesting and (jointly) more informative than existing measures such as spurious correlation. However, a shared concern among the reviewers (with confidences scores >=3) is the clarity of the exposition (e.g., many key concepts such as the data conditions are informally specified [Section 2 (Reviewer TPBn)], some key messages not clearly conveyed in the main paper [Section 3 (Reviewer RJtk)], and results inconclusive or not sufficiently supported by the experimental results [for both the synthetic setting (Reviewer RJtk) and the real-world setting (Reviewer yoH5)]. Based on the above concerns, the reviewers were not convinced that this work is well supported in its current state to merit acceptance for publication."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}