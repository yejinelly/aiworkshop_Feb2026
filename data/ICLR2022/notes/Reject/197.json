{
  "id": "wronZ3Mx_d",
  "original": "Ee3VPsRqssK",
  "number": 197,
  "cdate": 1632875435840,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875435840,
  "tmdate": 1676330683916,
  "ddate": null,
  "content": {
    "title": "Transfer Learning for Bayesian HPO with End-to-End Meta-Features",
    "authorids": [
      "~Hadi_Samer_Jomaa1",
      "~Sebastian_Pineda_Arango1",
      "~Lars_Schmidt-Thieme1",
      "~Josif_Grabocka1"
    ],
    "authors": [
      "Hadi Samer Jomaa",
      "Sebastian Pineda Arango",
      "Lars Schmidt-Thieme",
      "Josif Grabocka"
    ],
    "keywords": [
      "meta-learning",
      "hyperparameter optimization",
      "meta-features",
      "deep kernel learning",
      "Bayesian optimization",
      "transfer learning"
    ],
    "abstract": "Hyperparameter optimization (HPO) is a crucial component of deploying machine learning models, however, it remains an open problem due to the resource-constrained number of possible hyperparameter evaluations. As a result, prior work focus on exploring the direction of transfer learning for tackling the sample inefficiency of HPO. In contrast to existing approaches, we propose a novel Deep Kernel Gaussian Process surrogate with Landmark Meta-features (DKLM) that can be jointly meta-trained on a set of source tasks and then transferred efficiently on a new (unseen) target task. We design DKLM to capture the similarity between hyperparameter configurations with an end-to-end meta-feature network that embeds the set of evaluated configurations and their respective performance. As a result, our novel DKLM can learn contextualized dataset-specific similarity representations for hyperparameter configurations. We experimentally validate the performance of DKLM in a wide range of HPO meta-datasets from OpenML and demonstrate the empirical superiority of our method against a series of state-of-the-art baselines.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "jomaa|transfer_learning_for_bayesian_hpo_with_endtoend_metafeatures",
    "pdf": "/pdf/5a1099c1740b0ae28bd0bc286026924ccba9e09b.pdf",
    "supplementary_material": "/attachment/5d85e6066643af04ab119a7a0b830d6791bbf0d1.zip",
    "_bibtex": "@misc{\njomaa2022transfer,\ntitle={Transfer Learning for Bayesian {HPO} with End-to-End Meta-Features},\nauthor={Hadi Samer Jomaa and Sebastian Pineda Arango and Lars Schmidt-Thieme and Josif Grabocka},\nyear={2022},\nurl={https://openreview.net/forum?id=wronZ3Mx_d}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "wronZ3Mx_d",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 23,
    "directReplyCount": 7,
    "revisions": true,
    "replies": [
      {
        "id": "eHYFVqRznP_",
        "original": null,
        "number": 1,
        "cdate": 1635849304161,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635849304161,
        "tmdate": 1635849304161,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces DKLM, a novel approach to hyperparameter optimisation based on transfer learning.\nDKLM uses a Deep Kernel GP (Wilson et al., 2016) surrogate to predict test losses / response functions for given hyperparameter configurations.\nThe embeddings of the Deep Kernel GP are obtained by aggregating over hyperparameter-response pairs of previously observed datasets.\nThis is the main mechanism through which the HPO is conditioned on previously observed experiments.\nThe embedding mechanism relies on a Deepset (Zaheer et al., 2017) style neural network encoding, that sums over all hyperparameter-response pairs.\nExperiments demonstrate DKLM improves performance over prior work.",
          "main_review": "## Strengths\n\nI believe that there are merits to the proposed DKLM approach.\nIt makes intuitive sense to me that the Deepset style inclusion of prior observations is preferable to using prior observations as initialisation for the GP as done by prior work.\n\nThe hpo-b-v3 experiments look somewhat convincing.\nDKLM performs decently in the \"non-transfer setting\" and impressively well in the transfer setting.\nI'm inclined to believe the validity of these results, although I wish the authors would have included code / more details of the setup.\n\n## Weaknesses\n\nUnfortunately, I feel that many parts of this paper are severely lacking in clarity.\nThe writing often feels unnecessarily convoluted and I am sometimes unsure of what exactly the authors are trying to tell me with a particular paragraph.\nFurther, the lack of clarity unfortunately leads to open questions about the proposed approach itself.\nBelow I give specific details of this.\nI hope the authors understand my criticisms as constructive and take them as an opportunity to improve the current draft of the paper.\n\n### Introduction\n\nBy the end of the introduction, I neither understand what open challenges in HPO are, nor what DKLM does differently to established works.\nConcretely, I find the following sentences in the introduction more confusing than helpful:\n* \"we propose a novel architecture for deep GP kernels (Wilson et al., 2016a) that are enriched with novel end-to-end neural network components that generate meta-features only from the tuples of past hyperparameter configurations and their evaluated performances\" --> How do I enrich a deep GP kernel with neural network components? Don't Deep Kernel GPs already have neural networks do produce the encoding?\n* \"Introduce the first paper that tackles the negative transfer phenomenon in Bayesian HPO\" \u2192 What is the negative transfer phenomenon? This is not explained here.\n* \"Propose an end-to-end deep GP which implicitly learns networks that generate metafeatures,\" \u2192 What does it mean when a GP \"implicitly learns networks\"? Are you referring to the neural network of the deep kernel? Why does this learn this only \"implicitly\"?\nIn contrast,  Jomaa et al. (2021a) and Wistuba and Grabocka (2021), which might be the most related work to this submission, are much much more approachable.\n\n### Negative Transfer\n\nThis paper often uses terms without introducing them. \nFor example, one of the contributions listed in the introduction is that this paper is \"the first paper that tackles negative transfer phenomenon in Bayesian HPO\".  They do not explain anywhere what the 'negative transfer phenomenon is'. The authors do give a citation but I do think it would make sense to briefly explain this phenomenon to make the paper more accessible to readers.\n(The negative transfer paper (Wang et al., 2019) itself gives a great summary in the first two sentences of the papers abstract.)\nHowever, it is not clear to me how exactly DKLM tackles negative transfer in ways that prior transfer learning HPO works have not?\n(The following sentence of the paper does *not* answer my questions: \"By adding the task-specific information of the meta-features, the GP surrogate can infer a more accurate response surface on a new task based on similar source tasks that share similar meta-features. Therefore, our method is the first to tackle the negative transfer phenomenon for Bayesian HPO.\")\nThe negative transfer problem is also not once mentioned in the experimental evaluation.\n\n### Landmark  Features / Meta-Features\n\nThe authors repeatedly claim that DKLM (**D**eep **K**ernel Gaussian Process surrogate with **L**andmark **M**eta-features) makes use of \"landmark meta-features\" (Pfahringer et al., 2000; Feurer et al., 2014).\n\nIn prior work, landmark features refer to features that characterise a dataset via the predictive performance obtained by a variety of simple models on that dataset.\nThe idea here is that the performance of different machine learning models can be used to characterise the dataset itself: dataset A works well with tree-based models, Dataset B works well with simple parametric models, and so on.\nThis information can then be used as features in HPO (Feurer et al., 2014).\nNote that, here, these cheap models are not the model for which we want to perform HPO, but instead are additional models that are only used to characterise datasets.\n\nThis is, to my understanding, the definition of landmark features.\nIn this paper, no definition is given, except that landmark features are \" typically estimated by measuring the response of given datasets to machine learning algorithms\".\nIt is entirely unclear to me, how anything that DKLM does can be called \"landmark features\" given that no simple auxiliary models are trained whose performance is used as a dataset-specific feature.\nAgain, maybe this misunderstanding could have been resolved if the authors had discussed these terms in more detail.\n\nAdditionally, the paper claims that DKLM learns to generates *meta-features* which it defines as \"capturing the similarity between datasets\".\nHowever, DKLM learns embeddings that depend only on hyperparameters and loss values.\nNo dataset-specific information is available for constructing these embeddings.\nThe argument of the paper here seems to be that the embedding network learns a representation that learns to 'cluster' different dataset types.\nHowever, given that no information of the dataset is available to the embedding, this seems unnecessarily complex/indirect.\nThe experiments of Figure 5 (weakly) support the idea that this clustering does occur.\n(Also why not apply the approach of Jomaa et al. to actually include learned dataset-specific features? If DKLM is confronted with a novel dataset, unlike the approach of Jomaa et al., it cannot make any judgement of it before observing loss-response pairs.)\nIn any case, I feel that this would need to be exposed much more clearly. \n\nIn contrast, the approach of Jomaa et al. (2021a) also claims to learn meta-features.\nThey construct a neural network architecture that take as input a literal dataset (with all rows/datapoitns and columns/features) and outputs a learned representation.\nThis per-dataset representation is then useful for HPO.\nThe definition of Jomaa et al. (2021a) for learning meta-features makes sense to me.\n\nWhat are the definitions of landmark features/landmark meta-features/meta-features used in this paper and how are they consistent with those of earlier works?\n\n### Missing DKLM Details\n\nThe exposition of the approach is lacking both clarity and detail.\nIn that regard, equations (4-7) are extremely helpful in understanding what's going on.\nWithout them, I'm not sure if I would have any idea of what happens in DKLM.\n\nHowever, the exposition of DKLM is, in my eyes, not complete and lacking many details.\nFor example, this is a list of questions that I still have about how exactly DKLM works:\n* Algorithm 1 gives a recipe for meta-learning with DKLM. But what does applying DKLM look like? Do you continue to train $w$ or $\\theta$ when applying DKLM to a HPO task? (E.g. is anything retrained for the acquisition steps of Figure 3?) Maybe an Algorithm 2 illustrating the application of DKLM would be helpful here.\n* Also, when applying DKLM to a dataset, you update the history $\\mathcal{H}_t$ with observations from the current dataset, right? If so, what happens if you don't do this? You would still add new observations to the GP (just not use them in $\\phi_2$). \n* However if you don't do update $\\mathcal{H}_t$ as you apply DKLM to a dataset,  what does the \"DKLM *without* transfer learning\" setting of Figure 2 look like? This is not explained in the main body of the paper.\n* What acquisition strategy do you use for HPO? Expected improvement? As far as I can tell this is not stated in the paper.\n* From Algorithm 1, I can see that you are using REPTILE to train DKLM. Why are you using REPTILE? This is not discussed in the main text at all.\n* The information in 6.2 is not enough to replicate the experiments. How do you set the deep kernel GP hyperparameters? What are the training schedules? Do you optimise the GP hypers? It would be great if an appendix with this information and/or code of DKLM were provided by the authors.\n\n### Experimental Results\n\nFor the toy experiments, it seems to me that DKLM should be able to predict close to perfect after having found a single peak value of the sinusoid in the transfer learning setting.\nAfter all, a single peak value completely defines the function. (Technically speaking, obtaining a single derivative of the function, e.g. approximated by two close observations, should also be sufficient.)\nHowever, as is evident from Fig. 1 (top left), the predicted function is far from perfect despite observations near the peak. \nBased on my understanding, DKLM should be able to learn to perform similarly to a maximum likelihood fit of  $a * \\sin(x)$ to the observations.\nHowever, the results displayed here are looking far worse.\nWeirdly, going from DKLM + 2 trials to DKLM + 3 trials, the uncertainty of the predictions *increases* despite this not making much intuitive sense.\n\n\n## Smaller Comments\n\n* \"Nevertheless, extracting meta-features requires direct access to the datasets, which might be difficult in real settings where only the meta-dataset is available. \" Why is direct access to datasets not available in real settings? I thought a meta-dataset directly *contained* datasets (and gives direct access to them)? Unfortunately the term meta-dataset is not further defined in the context of this paper.\n* S.3.2: Why is the noise variable $\\sigma_n$ indexed with $n$? \n* \"We also notice how the performance improves gradually with the increasing number of trials, indicating the impact of the posterior variance modeling of our method (Section 5) as more observations are present on the black-box responses.\"\n    * It's unclear to me why \"posterior variance modeling\" is needed for gradually increasing performance? Also, other HPO approaches also rely on GPs with posterior variances. What is unique about DKLM here?\n\n## Language & Style\n\nI believe the writing style of this paper needs improvement. Below, I present some specific examples and suggestions of where I think the writing could be improved.\n\n* \"however, it [HPO] remains an open problem\" --> Why is HPO an open problem? When would it no longer be an open problem? I feel like saying that HPO is an open problem is either trivial or not true, and so best avoided. \n* \"In contrast to existing approaches, we propose a novel Deep Kernel Gaussian Process surrogate with Landmark Meta-features (DKLM) that can be jointly meta-trained on a set of source tasks and then transferred efficiently on a new (unseen) target task.\" --> Why is your work *in contrast* to existing approaches? It seems that you, *like* prior work, apply transfer learning to HPO.  It seems to me that \"jointly meta-training on a set of source tasks\" is also not novel.\n* \"prior work focus on\" \u2192 focuses\n* \"Transfer learning for HPO has been observed by modeling tasks jointly\" \u2192 maybe \"has been applied\". \"Observed\" makes it sound a bit like an event in nature.\n* \"Among *existing* ~~the variety of~~ acquisition functions, ~~the~~ expected improvement is widely adopted (Mockus, 1974).\"\n* \"The standard approach of fitting GPs is to optimize the weights of the kernel function, e.g. squared exponential kernel, \u03b8.\"  I think it is misleading to call the optimisation of the kernel *hyperparameters*  'fitting GPs'. Gaussian Processes perform Bayesian *inference* and are not \"fit\" by \"optimising\" any parameters. However, the hyperparameters in the GP (but not the GP itself) are often inferred by maximising the evidence.\n* \"Consequently, the solution of the joint model resides on a local minimum so that given limited information about the new (unseen) target task [______] \" This sentence is missing an ending? What happens given limited information about the task?\n",
          "summary_of_the_review": "The proposed method, DKLM, for transfer-learned HPO seems interesting and is, as far as I'm aware, novel.\nThe experimental evaluation seems to demonstrate benefits of DKLM over prior work.\nUnfortunately, the paper is (sometimes severly) lacking in clarity, and the draft requires significant further work before being, in my opinion, acceptable for publication.\nThe exposition leaves me with too many open questions as to how DKLM actually, as well as some of the experiments, actually work.\nFurther, one of the main contributions of the paper \u2013\u00a0avoiding 'negative transfer' \u2013 is not discussed sufficiently and not evaluated at all, and I have trouble following the authors in why it is appropriate to say that DKLM learns \"landmark meta-features\".\nIf the authors can address these main points of my review, I am happy to reconsider my score.\n\nThank you for including reproducibility and ethics statement. Please make code for reproducing experiments available as soon as possible.\n\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_1rkp"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_1rkp"
        ]
      },
      {
        "id": "dmpwcedmskr",
        "original": null,
        "number": 2,
        "cdate": 1635886248570,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635886248570,
        "tmdate": 1637777360925,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper focuses on doing transfer learning from related tasks for the optimization of ML model hyperparameters.\n\nPrevious work (e.g., Vanschoren, 2018) has considered meta-features as representation of related datasets or tasks; these features are incorporated into the model when transferring to the task at hand. This work proposes to _learn_ these meta features using a deep-set representation (Zaheer et al., 2017). These meta features are then passed to a kernel function that, finally, parameterizes the Gaussian process used during Bayesian optimization.\n\nExperimentally, the authors compare their method to several baselines, both in the transfer and non-transfer learning settings; aggregated metrics show that the proposed method (DKLM) outperforms other baselines on average.\n\n",
          "main_review": "Strengths\n-------------\n- This paper combines two intuitive ideas, and the method is fairly easy to understand\n- Experimentally, aggregated metrics showcase the strength of the proposed DKLM method compared to other baselines\n- The ablation test which removes the _learned_ meta features convincingly shows that investing additional effort into obtaining trained meta-features improves performance.\n\nWeaknesses \n-----------------\n- The major weakness of this paper is that it seems to simply combine to well-known ideas (deep sets + deep kernel GPs), and as such has limited novelty. \n- The clarify of the paper could be improved. In particular, the experimental setup was not entirely obvious to me. Is the optimization done as Bayesian optimization (with a DKLM in the Gaussian process)? If so, which acquisition function was used?\n\nComments/questions\n----------------------------\n- You mention the \"negative transfer phenomenon\" several times as a major motivation for this work; I would recommend describing it in more detail in the paper.\n- I'm curious about the performance variability of DKLM: do you have any insight into which types of datasets or experimental settings DKLM tends to perform better or worse on, compared to other baselines?\n- Did you compare DKLM to a GP parameterized with the learned metafeatures from Dataset2Vec?\n",
          "summary_of_the_review": "This is an interesting paper, but its novelty is limited. A more detailed analysis of the proposed method (in particular, ablations that use a deep kernel GP with different learned meta-features) would increase the impact of this work.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_5B4g"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_5B4g"
        ]
      },
      {
        "id": "BgxV24OoTU8",
        "original": null,
        "number": 3,
        "cdate": 1635890237595,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635890237595,
        "tmdate": 1635890237595,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper tackles the issue of speeding up black-box hyper-parameter optimization (HPO) by leveraging results obtained by trials on different datasets. It casts the problem into a few-shot regression one, where each task is a dataset and the goal is to predict the performance of a model trained on this dataset from the values of hyper-parameters. Meta-learning (REPTILE) is then used to initialize the (meta-)parameters of a deep kernel Gaussian process. Experiments on the HPO-B-v3 benchmark show this procedure equals or beats 10 baselines when using 20 to 100 trials.",
          "main_review": "\nStrengths\n========\nThe paper very clearly explains the background, existing methods, and issues. It shows how various existing techniques and ideas (deep kernel Gaussian processes, landmark meta-features, deepset) are combined with meta-learning to combat the \"negative transfer\" phenomenon.\n\nExperiments on the large HPO-B benchmark, as well as ablations (on synthetic few-shot regression problems, and in a non-transfer setting), are well-designed and support the paper's conclusions. The research question is clearly stated.\n\nWeaknesses\n==========\nOne would expect that the transfer/few-shot learning setting may be most useful with a small number of trials, however we see that the improvements (wrt. baselines) seem to only appear after about 10-20 trials, and are sustained afterwards. This may be investigated, or maybe addressed as a limitation in the context of the stated goal of limiting the number of HP configuration trials.\n\nMinor points\n==========\n- Some confusion in notations in section 3.2\n  * In Eq (1), is $K_x$ supposed to be $K_*$?\n  * A mean function $m$ is introduced, but not used, and Eq (1) shows the mean as $0$.\n  * In Eq (2), $y$ vs. ${\\mathbf y}$\n- It would be nice to keep the colors consistent between Figures 2 and 3 for the common curves",
          "summary_of_the_review": "Overall good, well-written, self-contained article that presents a novel combination of existing techniques, and addresses the issue of negative transfer when using transfer learning for Bayesian HPO. The experiments conducted are well designed and support the conclusion.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_kJb9"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_kJb9"
        ]
      },
      {
        "id": "jbKkZNd12bJ",
        "original": null,
        "number": 4,
        "cdate": 1635911808617,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635911808617,
        "tmdate": 1635911808617,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The authors focus on the important problem of more efficient hyperparamter optimization through (meta) transfer learning. More specifically, the author propose a new method (Deep Kernel Gaussian Process surrogate with Landmark Meta-features (DKLM)) that embeds a meta-feature extractor model within the kernel function of a deep GP. This meta-feature extractor is able to learn to extract characteristics of a dataset such that the similarity of a dataset to other ones can be (implicitly) captured and used by the rest of the model. To demonstrate the effectiveness of their proposed model, they evaluate their model on the large-scale HPO-B-v3 benchmark against 10 other baselines plus a variant of their method with a randomly-initialized meta extractor.",
          "main_review": "Strengths:\n- Adequately builds upon previous work such that the proposed change (the meta-extractor within the kernel of a deep GP) is easy to assess and compare (as opposed to larger set of changes that are not adequate ablated).\n- Large-scale benchmark (935 tasks, 16 search spaces) compared to 10 baselines.\n- Ablations with and within a randomly-initialized meta-extractor.\n- Results included error bars (Table 1).\n\nWeaknesses:\n- Experimental setup could be made clearer as to which tasks are used for training and eval in the transfer setting (i.e., which tasks are used to originally train the meta-extractor, and on which tasks is it transferred).\n\nMinor:\n- p. 3: In the fourth line from the bottom, the reference \"2020\" needs to be fixed.",
          "summary_of_the_review": "Overall, the authors propose a new method that is well-scoped within existing literature, and provide positive empirical results on a large-scale benchmark against an adequate set of baseline tasks. It is a reasonable paper for acceptance.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_bzFz"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_bzFz"
        ]
      },
      {
        "id": "GX0IeSgi_7P",
        "original": null,
        "number": 5,
        "cdate": 1635988011798,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635988011798,
        "tmdate": 1635988011798,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a new Kernel for Bayesian Optimization to incorporate 'Landmark meta-features' of groups of tasks, so that the Kernel can be used to improve the efficience of the Bayesian Optimization on a new related task. The Kernel is based on 2 nested neural networks that are jointly trained via first-order meta-learning method REPTILE. The authors show that the method is overall better performing on the benchmark HPO-B-v3.",
          "main_review": "Strengths:\n- The topic is important. Hyperparameter is expensive and transfer learning is a promising solution to reduce the cost of HPO. \n- The solution is relatively complex, involving deep Kernels using neural networks and trained with meta-learning, and yet it is presented clearly and is easy to understand.\n- The benchmark is large, including 10 different relevant algorithms on 935 tasks with 16 different search spaces. \n- The experiments cover different aspects of the contributed algorithm; T-SNE projections of the meta-features and the impact of the sample size.\n\nWeaknesses:\n- A lot of emphasize in introduction is on the fact that DKLM would address negative transfer. I do not see this discussed explicitly in any of the experiments. If it is important, it should be supported by the experiments.\n- The proposed solution is most certainly more computationally expensive than random search, or the fast Bayesian Optimization algorithm ABLR. The experiments should also show the improvement in terms of running time so that we can see how it performs compared to these faster algorithm. \n- The standard deviation on Table 1 is way too large to be able to conclude anything. I could run again the same experiments with different seeds and observe an upward trend instead. \n- The functions f and g introduces many hyperparameters to DKLM. The paper does not discuss how to chose these hyperparameters and whether tuning them is important for the performance of the algorithm. \n\nMinor comments:\nWhat is a meta-dataset? It is not defined in section 2.\n\nI don\u2019t understand this sentence:\n> Consequently, the solution of the joint model resides on a local minimum so that given limited information about the new (unseen) target task.\n\nTypos:\nnew a target task -> a new target task\nfast adaption -> fast adaptation\nwhere we notice -> while we notice",
          "summary_of_the_review": "The proposed algorithm is an interesting way of learning end-to-end meta-features across tasks to improve the efficiency of Bayesian Optimization on new similar tasks. The benchmarking experiments are extensive and convincing for the efficiency of the algorithm, but does not support clearly the claims about negative transfer. The latter point should be discussed more clearly. The additional hyperparameters of the algorithms (network size and REPTILE hyperparameters) should also analyzed empirically to see how importantly they affect the performance of the algorithm.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "I do not see any issues for ethics. ",
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_ZdAZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_ZdAZ"
        ]
      },
      {
        "id": "kX5r4fcxIW",
        "original": null,
        "number": 4,
        "cdate": 1636521957527,
        "mdate": 1636521957527,
        "ddate": null,
        "tcdate": 1636521957527,
        "tmdate": 1636521957527,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "dmpwcedmskr",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "elaborate a bit more?",
          "comment": "Reviewer,\nCould you remark a bit on your opinion of the evidence in the submission supporting the claim in the abstract that the work \"demonstrate[s] the empirical superiority of our method against a series of state-of-the-art baselines\"? In particular, suppose for the sake of argument that we assume the work presents a \"straightforward combination of well-known ideas\", we might still imagine accepting such a manuscript if the work made a strong claim to perfecting the method, reduced it to practice uniquely effectively, or got particularly impressive empirical results. For a submission to be accepted using this line of reasoning, it would be crucial for the empirical results to be of some practical interest and impressive in some way, or for the experimental data *by itself* to be valuable enough in terms of insights, etc.\n\nSpecifically, it would help if you could share your thoughts on the following questions:\n1. How \"realistic\" or practically interesting are the tasks the paper presents results on?\n2. Qualitatively, how strong are the results?\n3. What is your assessment of the strength of the baselines?\n4. How valuable are the experimental details?\n\nTo be clear, at this point I as AC am not taking a position on the answer to these questions."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Area_Chair_5eyF"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Area_Chair_5eyF"
        ]
      },
      {
        "id": "pCCikUxC0E",
        "original": null,
        "number": 5,
        "cdate": 1636522371054,
        "mdate": 1636522371054,
        "ddate": null,
        "tcdate": 1636522371054,
        "tmdate": 1636522371054,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "BgxV24OoTU8",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "elaborate on the potential impact of this submission?",
          "comment": "Reviewer,\nCould you elaborate a bit on the potential impact of this submission? Whose life would change because of this paper existing/not existing? How would it change? "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Area_Chair_5eyF"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Area_Chair_5eyF"
        ]
      },
      {
        "id": "Tp8HW85E8o9",
        "original": null,
        "number": 6,
        "cdate": 1637162905535,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637162905535,
        "tmdate": 1637163137016,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "GX0IeSgi_7P",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Rebuttal ",
          "comment": "We thank the reviewer for his feedback. We address your concerns below. \n\n- We have added a description of negative transfer phenomena and how it might occur in the context of HPO in the introduction. We designed a new negative transfer experiment to test the performance of our method vs. baselines (Section 6.5) whereby the methods are meta-trained on uncorrelated tasks. \n\n- We agree that the implementation of DKLM is more computationally expensive than random search, since the latter is a simple model free approach. However, more recent transfer learning algorithms, such as ABLR, CTS, FSBO and ours, are composed of two stages: i) meta-learning the initialization of the surrogate on a collection of source tasks, ii) adapting the surrogate to the target task. For black-box optimization, it is common to evaluate the performance of the algorithms with respect to the number of trials and not w.r.t wall clock time. However, fitting our surrogate is fast. It takes on average (mean of all datasets in all search spaces) 9.7 milliseconds/epoch to fine-tune DKLM to new observations, and only 93.34 seconds on average to run our method for 100 trials across the 16 spaces. \n\n- We have pushed Section 6.5 (old) to the Appendix C and replaced it with a new experiment designed to address negative transfer. We have updated the table, however, to better illustrate the trend. We fit a linear model to the 10000 data points that have been sampled and report the slope. The negative slope indicates that the joint response surface is improved with more data points.\n\n-  DKLM is decomposed of two modules, $\\phi_2$ with two multi-layer perceptrons f and g, and $\\phi_1$ as another multi-layer perceptron which introduces the following hyper-hyperparameters: the number of layers, hidden units, activation function, dropout rate and learning rate.  We tune these hyper-hyperparameters through a random search on the total negative likelihood loss of the joint set of validation tasks.\n\n- We adopted a commonly-accepted terminology like prior work from HPO-B where a meta-dataset is a collection of hyperparameters and their responses that has been evaluated offline and collected in a tabular form to expedite transfer learning During training/inference, the response is simply queried for the suggested hyperparameters. \n\n- We have reformulated this sentence for better understanding. \n\nPlease feel free to let us know if any questions or concerns still persist, that would prevent you from raising your score toward acceptance.\n\n\nReferences:\n\nHPO-B: Sebastian Pineda-Arango, Hadi S. Jomaa, Martin Wistuba, and Josif Grabocka. HPO-B: A largescale reproducible benchmark for black-box HPO based on openml. Accepted at NeurIPS Datasets and Benchmark Track, 2021."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ]
      },
      {
        "id": "0o_i4eyh5vA",
        "original": null,
        "number": 7,
        "cdate": 1637163125094,
        "mdate": 1637163125094,
        "ddate": null,
        "tcdate": 1637163125094,
        "tmdate": 1637163125094,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "jbKkZNd12bJ",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Rebuttal",
          "comment": "Thank you for your feedback.\n\n- We follow the same experimental protocol suggested by \u201cHPO-B\u201d. The benchmark paper introduces a predefined meta-train/meta-validation/meta-test split to facilitate transfer learning. The benchmark paper also evaluates a library of baselines, which allows us to compare directly to, considering that we adhere to the same protocol. We would like to note the meta-feature extractor is trained end-to-end and not separately. \n\n- We have added to Appendix B the detailed experimental protocol of HPO-B. \n\n- The mentioned reference is also fixed. \n\nWe hope that this improves the chances of receiving a higher score as pointed out in your summary of the review. \n\nReferences:\n\nHPO-B: Sebastian Pineda-Arango, Hadi S. Jomaa, Martin Wistuba, and Josif Grabocka. HPO-B: A largescale reproducible benchmark for black-box HPO based on openml. Accepted at NeurIPS Datasets and Benchmark Track, 2021."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ]
      },
      {
        "id": "tpL7d7GHNFZ",
        "original": null,
        "number": 8,
        "cdate": 1637163323625,
        "mdate": 1637163323625,
        "ddate": null,
        "tcdate": 1637163323625,
        "tmdate": 1637163323625,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "BgxV24OoTU8",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Rebuttal",
          "comment": "We would like to thank the reviewer for his interest in our paper and for his recommendation.\n\nWe have fixed the minor confusions that you pointed out.\n\nWe are happy to address any further remarks.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ]
      },
      {
        "id": "sxobcUFGaR",
        "original": null,
        "number": 9,
        "cdate": 1637163592886,
        "mdate": 1637163592886,
        "ddate": null,
        "tcdate": 1637163592886,
        "tmdate": 1637163592886,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "dmpwcedmskr",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Rebuttal",
          "comment": "\n- We kindly have a different view on the novelty aspect. To the best of our knowledge, ours is a novel method in two directions: it is both the first paper that conditions HPO surrogates on meta-features, as well as the first paper that actually learns meta-features in an end-to-end manner. We invite the author to point out published prior works that also conditions surrogates on meta-features in the context of Bayesian Optimization.\n\n- We follow the same experimental protocol proposed by HPO-B to allow for a fair comparison between the published results and our own. We have added the details to the Appendix B. \n\n- We propose DKLM as a new surrogate to replace the standard Gaussian process conventionally used in Bayesian optimization. DKLM is first learned to jointly approximate the response surface of the source tasks, Algorithm 1, and then fine-tuned to the target observations through a sequential model-based optimization, Algorithm 2 in the Appendix A. This is consistent with all the baselines that are iteratively fine-tuned to the observations of the target task. \n\n- You are right, the acquisition function is missing. We use expected improvement as the acquisition function and point that out in Appendix B.\n\n- Negative transfer implies a poor generalization performance on test tasks that are dissimilar to the training tasks, according to a predefined dissimilarity measure, e.g. similarity of response curves. We added this description to the introduction. \n\n- We additionally added an experiment in Section 6.5 that compares FSBO and DKLM when trained on **uncorrelated** source tasks in order to test our method\u2019s resilience to negative transfer. \n\n- **Regarding Dataset2Vec** This is a very nice observation. Unfortunately, incorporating meta-features extracted by dataset2vec is not possible for this benchmark. Namely, because the primary datasets, i.e the dataset for which the algorithms in the meta-datasets were tuned are not available in HPO-B. We only have access to the response and the configurations. HPO-B does not provide the raw dataset, attributes and classes, by default.\n\nPlease feel free to let us know if any more concerns should still arise that would prevent you from raising your score toward acceptance.\n\n\nReferences:\n\nHPO-B: Sebastian Pineda-Arango, Hadi S. Jomaa, Martin Wistuba, and Josif Grabocka. HPO-B: A largescale reproducible benchmark for black-box HPO based on openml. Accepted at NeurIPS Datasets and Benchmark Track, 2021."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ]
      },
      {
        "id": "ugBZjesfHbQ",
        "original": null,
        "number": 10,
        "cdate": 1637164401166,
        "mdate": 1637164401166,
        "ddate": null,
        "tcdate": 1637164401166,
        "tmdate": 1637164401166,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "eHYFVqRznP_",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Rebuttal",
          "comment": "Thank you for your insightful comments. \n# Introduction\n- The deep GP uses a deep kernel for capturing the similarity of a pair of hyper-parameter configuration vectors. However, it does not capture the similarity between datasets. Because the characteristics/meta-features of a dataset do not depend only on a pair of configurations but on the full set of configurations and their evaluated response. Such information is not directly inherent to the standard deep GP.\n\n- Negative transfer implies a poor generalization performance on test tasks that are dissimilar to the training tasks, according to a predefined dissimilarity measure, e.g. similarity of response curves. We added this description to the introduction. \n\n- We additionally added an additional experiment that compares FSBO and DKLM when trained on uncorrelated source tasks, Section 6.5.\n\n- The network implicitly learns to generate task-specific representations that we refer to as land-mark meta-features. It learns them implicitly because the overall objective is maximizing the marginal loglikelihood of the surrogate in an end-to-end manner, unlike how Jomaa 2021a explicitly did via a batch identification loss. \n\n# Landmark Meta-features\n\n- You are entirely correct with regards to the definition of landmark meta-features. We provide a generalized way of learning landmark-meta features. \n- Considering the classical formulation: we have K classifiers f_1,..,f_K, each achieving a performance P on a dataset as P_k(D). In essence, concatenate the classification accuracies of the classifiers and we get a K-dimensional landmark meta-feature vector [P_1(D), ..., P_K(D)]. \n- Notice that what we have achieved is given D compute a \"function with a K-dimensional output\" (a.k.a. performance of K classifiers given D). As a result, we can easily consider the end-to-end approximation [P_1(D), ..., P_K(D)] = f(D), where f: D -> R^K with f being a neural network defined on a dataset D. \n- In our case, we do not approximate the performance of classical classifiers with our function f (although that is feasible given the universal approximation nature of neural networks). Instead, we compute f more optimally by a meta-feature network for helping the actual HPO loss function we are interested in in an end-to-end manner.\n\n# Missing DKLM Details\n\n- DKLM is meta-initialized on the target task after meta-learning on the source tasks. We follow the same protocol as any Bayesian Optimization solution, Algorithm 2 in the Appendix. We iteratively select new configurations by maximizing an acquisition function, in this case Expected Improvement, as described by the experimental protocol, Appendix B. \n\n-  DKLM without transfer learning is the variation of the model that is randomly initialized, and not meta-initialized. This is explained in Section 6.3.\n\n- We implemented\tREPTILE because it has empirically shown that it performs well in several settings, FSBO, Jomaa et al 2021b and because it is faster than MAML due to not needing a double-derivative update rule.\n\n- In addition to 6.2, we added the more details in Appendix B and uploaded the source code. \n\n# Language\nWe have taken some of your comments into consideration in the updated draft. \n\nWe hope that these answers are sufficient to mitigate any concerns. Please feel free to raise any other issue that might prevent you from improving your score. \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ]
      },
      {
        "id": "iePuAayYlht",
        "original": null,
        "number": 11,
        "cdate": 1637164524007,
        "mdate": 1637164524007,
        "ddate": null,
        "tcdate": 1637164524007,
        "tmdate": 1637164524007,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Summary of Changes",
          "comment": "# Handling the major criticism on Negative Transfer\n\nIn the first round of reviews, almost all the reviewers raised criticism that a major claim of the paper in terms of handling negative transfer was not thoroughly supported empirically.\nAlthough we demonstrated that our method DKLM outperforms a large set of state-of-the-art transfer HPO baselines, we agree that our experimental protocol was not specifically designed to test negative transfer per se.\n\nTo remedy this deficiency, we designed a negative transfer experiment as follows:\n\n- For each search space in the HPO-B benchmark we reduce the set of meta-training datasets to only 10% of the most uncorrelated datasets to the meta-test datasets. As a result, competing methods would learn to transfer from training datasets that are uncorrelated with the testing datasets.\n\n- Here, the correlation between a pair of datasets was measured as the rank correlation (Kendall tau) of the vector of validation accuracies achieved by evaluating the same set of random hyperparameter configurations on both datasets.\n\nFor more details, we added a new section 6.5 as \"Resilience to Negative Transfer\". The outcome of the experiment in Section 6.5 validates that our method DKLM with meta-features provides resilience to the negative transfer, by having a larger gap in performance to the variant without meta-features (FSBO) in the negative transfer experimental setup.\n\n# Experimental Details\nWe also uploaded the source code, added two new algorithms that describe DKLM in pseudo-code, and added the experimental protocol in Appendix B. \n\nWe thank all the reviewers for their feedback, and are available to answer any additional concerns. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ]
      },
      {
        "id": "s5YXDh6JTIC",
        "original": null,
        "number": 13,
        "cdate": 1637437513717,
        "mdate": 1637437513717,
        "ddate": null,
        "tcdate": 1637437513717,
        "tmdate": 1637437513717,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "ugBZjesfHbQ",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Reviewer Response",
          "comment": "Thank you for the detailed reply to my rebuttal!\n\nIn particular, thank you for including an experiment on the negative transfer phenomenon, for including Algorithms 2 and 3, for clarifying the acquisition function used, and thanks for taking some of my comments on language/missing details into account.\n\n## Further Comments\n\nI'm not sure I share your interpretation of the new negative transfer results. It seems to me that both DKLM and FSBO take a decided performance hit when being meta-trained on uncorrelated datasets. \nYou write that the \"performance gap of the ranks between DKLM (NT) and FSBO (NT) is larger than that between DKLM and FSBO\" but wouldn't be how much of a relative performance decrease DKLM and FSBO have? Or how far their rank decreases for this test? (This would require more than two methods though.) As far as I can tell both methods take a decided hit in performance (if anything the difference between DKLM (NT) and DKLM is larger than the difference between FSBO (NT) and FSBO).\n\n> we can easily consider the end-to-end approximation [P_1(D), ..., P_K(D)] = f(D), where f: D -> R^K with f being a neural network defined on a dataset D\n\nIf I'm not mistaken, you never take a dataset as input though. Further, it is unclear if your network actually learns to \"approximate the performance of classical classifiers with our function f\".  I am not yet convinced it makes sense to 'lump together' different datasets in the encoding, without giving the model the 'explicit' option to learn a dataset representation.\n\n\n## Bottom Line\n\nMany of my concerns regarding the clarity of this submission have not been addressed.\nI am not convinced of the interpretation that the authors have of the negative transfer experiment.\n\nThe confusing use of the term 'landmark meta-features' has not been addressed.\nCompared to prior work, I still find this publication hard to read.\n\nI feel that some detail is still missing form the current draft:\n* My concerns on the toy experiments have not been addressed.\n* It is still unclear to me why DKLM should be suited to addressing negative transfer.\n* A discussion of why you use REPTILE should be included in the paper.\n* What is the shading in Figure 6?\n* Algorithm 3 suggests you use full-batch gradient descent to fit the model. However, the text mentions ADAM being used on mini-batches.\n* How is DKLM applied in a non-transfer setting?\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_1rkp"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_1rkp"
        ]
      },
      {
        "id": "ACh8-8_v4g7",
        "original": null,
        "number": 15,
        "cdate": 1637639101529,
        "mdate": 1637639101529,
        "ddate": null,
        "tcdate": 1637639101529,
        "tmdate": 1637639101529,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "pCCikUxC0E",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Impact",
          "comment": "Black-box hyper-parameter optimization is an important topic, as exploration of HPs still relies widely on personal experience and heuristics from practitioners, leading to 2 major issues:\n- computation (and energy, CO2) wasted on inefficient exploration, and\n- sub-optimal results, due to regions of interests in the HP space being left un-explored (in general, or within a resource or time budget).\n\nAnother reviewer also mensions that:\n> The topic is important. Hyperparameter is expensive and transfer learning is a promising solution to reduce the cost of HPO.\n\nI think this paper has the potential of making it easier:\n- for ML practitioners to get good performance when using existing algorithms on data not dissimilar to the datasets used for HPO-B-v3\n- for ML researchers to properly optimize baselines and explore novel algorithms on existing benchmarks (i.e., HPO-B-v3 or other HPO meta-datasets they would have access to).\n\nEven if there is not a net environmental impact (as people could decide to keep using the same amount of resources), it seems wasteful to not leverage data from past HPO optimizations, and this paper seems to be one of the best current ways of doing so."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_kJb9"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_kJb9"
        ]
      },
      {
        "id": "sddS4zFwVhu",
        "original": null,
        "number": 17,
        "cdate": 1637665697597,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637665697597,
        "tmdate": 1637667456566,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "s5YXDh6JTIC",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Reply",
          "comment": "## Regarding Negative Transfer\nWe added to Section 6.5 the critical difference diagram between FSBO and DKLM in negative transfer and standard setting. In a negative transfer setting, the performance of DKLM (NT) is statistically significant compared to FSBO (NT). We would like to point out that the hit in performance is due two-fold, learning on uncorrelated tasks, as well as having less source tasks for meta-training. We hope that this is sufficient to prove that DKLM is robust against negative transfer compared to state-of-the-art.\n\n## Regarding Concerns\n\n- DKLM is suited for negative transfer because it forces the surrogate Deep Kernel GP to be conditioned on learnable task-specific meta-features, generated from the hyperparameter configurations and their responses.\n- Shading in Figure 6 is the standard deviation across search spaces\n- Toy experiment: The function is **not defined by the peak alone** but as we point out, we meta-train on a collection of sinusoid functions of the form $f^{(k)}(x) = a^{(k)}\\sin(b^{(k)}x)$. Only one measurement is not sufficient to perfectly approximate a sine wave, because, there could be multiple sine waves passing through that point. How do you know that an observed point is \"the peak\" of the sine wave?\n\n- We use the full batch of data points during fitting of the target surrogate during inference, whereas we use Adam for meta-training, Algorithm 1. For simplicity in Algorithm 1, lines 11 and 12 show a gradient descent update\n- In a non-transfer setting, DKLM is initialized randomly, instead of via meta-learning the initialization, and is referred to as DKLM (RI)\n- REPTILE: We did not perform an ablation to highlight why REPTILE was chosen, however, based on the literature and state-of-the-art, it has been shown that REPTILE outperforms MAML, [1,2] and is faster as we do not need to compute a second-order derivative. \n\nReferences:\n\n- [1] Wistuba, Martin, and Josif Grabocka. \"Few-shot bayesian optimization with deep kernel surrogates.\" arXiv preprint arXiv:2101.07667 (2021).\n- [2]  Jomaa, Hadi S., Lars Schmidt-Thieme, and Josif Grabocka. \"Hyperparameter Optimization with Differentiable Metafeatures.\" arXiv preprint arXiv:2102.03776 (2021)."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ]
      },
      {
        "id": "dL0VM7CmlnI",
        "original": null,
        "number": 18,
        "cdate": 1637747119982,
        "mdate": 1637747119982,
        "ddate": null,
        "tcdate": 1637747119982,
        "tmdate": 1637747119982,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "sddS4zFwVhu",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Reply",
          "comment": "Thanks a lot for your reply and thanks for the clarifications! I will take these into account.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_1rkp"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_1rkp"
        ]
      },
      {
        "id": "quh0AATDW7c",
        "original": null,
        "number": 19,
        "cdate": 1637777338526,
        "mdate": 1637777338526,
        "ddate": null,
        "tcdate": 1637777338526,
        "tmdate": 1637777338526,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "sxobcUFGaR",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Re: novelty & additional experiments",
          "comment": "I thank the authors for their reply to my review. \n\nThe additional experiments on the negative transfer phenomenon are very welcome and, from my perspective, increase the impact of this work. Regarding the novelty: my perspective is that of someone more familiar with the deepset literature than that of transfer learning, and with that in mind I recognize I may have missed some context in that regard.\n\nI understand why the ablation experiment I was thinking of wrt. Dataset2Vec is not possible. Nonetheless, I believe an investigation of the impact of learned vs ad-hoc meta features would strengthen the experimental section of this work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_5B4g"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_5B4g"
        ]
      },
      {
        "id": "I1inmjLFAgG",
        "original": null,
        "number": 20,
        "cdate": 1637788594746,
        "mdate": 1637788594746,
        "ddate": null,
        "tcdate": 1637788594746,
        "tmdate": 1637788594746,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "kX5r4fcxIW",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Empirical evaluation",
          "comment": "Thanks for the questions! \n\nTo clarify, as mentioned above to the authors, my opinion on the novelty was based mostly on the perspective of combining deep sets with a deep kernel GP, rather than, for example, using on surrogate features to mitigate the negative transfer phenomenon.\n\n1. How \"realistic\" or practically interesting are the tasks the paper presents results on?\nThe dataset HBO-B used [1] is a super-set of tasks that are typically used to evaluate transfer learning methods, with unified preprocessing. Although I would prefer if this dataset were less recent and had been used by more previous work to further justify it as the evaluation benchmark, it appears to be both comprehensive and appropriate for this paper. It is worth noting, though, that HBO-B _lacks_ evaluations of SOTA deep learning methods, which are one of the highest consumers of HPO.\n\n2. Qualitatively, how strong are the results?\n  - The results are strong: the proposed method (DKLM) performs on average better than the considered baselines (Figure 3). When DKLM is not the best method, it is remains within the top methods (Figure 4). The ablation test on the value of using learned (rather than randomly initialized) meta features further justifies the method design.\n\n  - If this paper is considered as a purely empirical paper (which I am not sure it should be, as mentioned above), I think the empirical evaluation is not sufficient for acceptance. In such a case, I would have liked to see a more in-depth analysis of the non-aggregated performance of DKLM. I asked in my original review \"do you have any insight into which types of datasets or experimental settings DKLM tends to perform better or worse on, compared to other baselines?\", and I think this question should be answered for a purely experimental paper. Unless DKLM is systematically within the top-$n$ methods (let's say $n=3$ for the purpose of this argument), this paper would be made stronger by a detailed discussion of _when_ and _why_ DKLM can fail. If DKLM is always within the top-3 methods, this should be mentioned explicitly (this does not appear to be the case from Figure 7).\n\n- As pointed out in my initial review, I also think that other ablation tests would strengthen the paper, for example comparing the learned meta features to expert-created meta-features.\n\n- A discussion of the sensitivity of the method to the hyperparameter choices (in particular of functions $f$ and $g$) would also be welcome.\n\n3. What is your assessment of the strength of the baselines?\nThe paper includes 10 baselines and covers both transfer and non-transfer learning methods. Some transfer learning methods closer to RL (e.g., Witsuba et al., 2018; Volpp et al., 2020) were not included and would have been nice to have, but given that those methods focus on the acquisition function rather than the surrogate model, it seems reasonable to not have included them. As such, I find that the baselines are quite strong.\n\n4. How valuable are the experimental details?\nThe experimental details should be enough to reimplement and analyze DKLM.\n\nTo summarize: I do not believe this paper should necessarily be viewed entirely as an experimental contribution, as was pointed out by the authors as a reply to my review. If it were to be evaluated in such a way, however, I think the analysis would need to be more in-depth: more ablation tests, further analysis of the failure modes instead of aggregated or punctual examples (for example: provide the datasets where DKLM did the worst, the best, and explain qualitatively what caused this gap).\n\n[1] HPO-B: A Large-Scale Reproducible Benchmark for Black-Box HPO based on OpenML"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_5B4g"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_5B4g"
        ]
      },
      {
        "id": "YQu-CcKJEyy",
        "original": null,
        "number": 21,
        "cdate": 1638105382350,
        "mdate": 1638105382350,
        "ddate": null,
        "tcdate": 1638105382350,
        "tmdate": 1638105382350,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "sddS4zFwVhu",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Response",
          "comment": "Thanks again for your response.\n\nAs I've mentioned in my original review, the experimental results do seem convincing, and I appreciate the detail added during the rebuttal.\n\nHowever, I have to say that I still feel that this submission is lacking in clarity of presentation and does not give sufficient intuition into why DKLM is a good/better thing to do than prior work. For example, see below for some points I still don't think have been addressed sufficiently.\n\n> Only one measurement is not sufficient to perfectly approximate a sine wave, because, there could be multiple sine waves passing through that point.\n\nIn the paper, you show predictions after up to 8 observations. It's unclear to me why DKLM should have uncertainty about the predictions left at this point, given sufficient training data. (A naive fit of the observations to $a \\sin(x + b)$ should be pretty good by then.)\n\n> DKLM is suited for negative transfer because it forces the surrogate Deep Kernel GP to be conditioned on learnable task-specific meta-features, generated from the hyperparameter configurations and their responses.\n\nI am not happy with this answer.\nUnlike approaches that explicitly include dataset specific features, DKLM has no way of knowing that any test-time dataset is going to be dissimilar to one of the training datasets. All dataset-similarity has to be implicitly encoded in the learned deep kernel embeddings and then iteratively discovered over multiple trials at test time. How is this an architecture that's going to perform well for negative transfer? Why would this perform better than FSBO? If negative transfer is one of the motivations for DKLM, I would hope to see further discussion on it.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_1rkp"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_1rkp"
        ]
      },
      {
        "id": "bQ3cu8_CB-1",
        "original": null,
        "number": 22,
        "cdate": 1638560771432,
        "mdate": 1638560771432,
        "ddate": null,
        "tcdate": 1638560771432,
        "tmdate": 1638560771432,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "Tp8HW85E8o9",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Re: rebuttal",
          "comment": "I thank the author for their detailed response. \n\n- I believe the experiments of section 6.5 is a reasonable way of measuring the impact of negative transfer. I disagree however that the results support the claim of the paper. On the mean normalized regret FSBO and DKLM using the decorrelated source tasks are about equivalent. \n- It is true that hyperparameter optimization algorithms are rarely evaluated with respect to wall-clock time performance. One of the reason being that trial execution is typically much longer that the execution of the algorithm to suggest a new trial. It may not be true however when leveraging large amount of data for transfer learning in HPO. In particular, the paper presenting ABLR does evaluate wall-clock time performance because ABLR is a fast alternatives to other GP approaches for BO. Among other things, they show that BOHAMIAN performs better than ABLR for a given number of trials on a target task, but it is significantly slower on large set of trials for source tasks making it hardly usable for transfer learning in practice. This is why I am concerned with wall-clock time efficiency here.\n- The slope measurement from one experiment to another is noisy. If we compute the confidence interval for the 16 reported experiments we get [-0.23, 0.1]. That is not significant.\n```\na = numpy.array([-0.075, -0.15, -0.077, -1.473, 0.215, -0.039, 0.414, -0.003, -0.015, 0.021, 0.14, -0.118, -0.032, 0.019, -0.08, 0.229])\nconfidence_interval = a.mean() + numpy.array([-1, 1]) * scipy.stats.t.ppf(0.95, df=len(a)) * a.std() / numpy.sqrt(len(a))\n```\n- These experiments to select the hyperparameters of DKLM should be described. Sensitivity to hyperparameters of the HPO algorithm is important."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_ZdAZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_ZdAZ"
        ]
      },
      {
        "id": "2hiJAJbK80",
        "original": null,
        "number": 24,
        "cdate": 1638778634104,
        "mdate": 1638778634104,
        "ddate": null,
        "tcdate": 1638778634104,
        "tmdate": 1638778634104,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "bQ3cu8_CB-1",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Rebuttal",
          "comment": "Thank you again for engaging the in discussion:\n\n- In addition to the mean normalized regret, we present the critical difference diagram which explicitly shows that DKLM (NT) is statistically significant compared to FSBO (NT)\n- Thank you for point out the evaluation of ABLR w.r.t. to wall-clock time. We have reported the time to fine-tune DKLM in the previous rebuttal, and would like to note that even if ABLR might be faster compared to other BO approaches, the results are much worse than state-of-the-art. Since we are mainly concerned with optimizing the black-box objective, as do most of the baselines, we do not emphasize time. \n- We have moved 6.5 (old) to the appendix to avoid any confusion. The experiment was intended to highlight the improved performance of the surrogate with more observations after meta-initialization."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ]
      },
      {
        "id": "Toj121hzE5",
        "original": null,
        "number": 1,
        "cdate": 1642696844307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696844307,
        "tmdate": 1642696844307,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The submission describes a method for tuning machine learning pipeline hyperparameters using transfer learning from related tuning tasks. In particular, the method uses learned meta features to construct a covariance function for a GP.\n\nThis was an extremely difficult case and could have gone either way. It was the closest case for any paper I serve as the AC for. Two of the reviewers recommended rejecting the paper and three recommended accepting, although during discussion one of the reviewers recommending accepting the paper seemed to actually be more on the reject side.\n\nUltimately, I have decided to recommend rejecting this submission. However, if either the clarity (especially concerning the neural network setup) or the experiments were somewhat improved I would have recommended accepting it. I view clarity as an extremely important factor when weighing whether a submission should be accepted. I concur with the reviewers on the following weaknesses of the experiments: (1) the lack of an ablation test when considering ad-hoc meta features and (2) the experimental evaluation is based on mostly aggregated metrics.\n\nI know this recommendation must be disappointing, but I encourage the authors to polish the work a bit more and resubmit it somewhere."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "eHYFVqRznP_",
        "original": null,
        "number": 1,
        "cdate": 1635849304161,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635849304161,
        "tmdate": 1635849304161,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces DKLM, a novel approach to hyperparameter optimisation based on transfer learning.\nDKLM uses a Deep Kernel GP (Wilson et al., 2016) surrogate to predict test losses / response functions for given hyperparameter configurations.\nThe embeddings of the Deep Kernel GP are obtained by aggregating over hyperparameter-response pairs of previously observed datasets.\nThis is the main mechanism through which the HPO is conditioned on previously observed experiments.\nThe embedding mechanism relies on a Deepset (Zaheer et al., 2017) style neural network encoding, that sums over all hyperparameter-response pairs.\nExperiments demonstrate DKLM improves performance over prior work.",
          "main_review": "## Strengths\n\nI believe that there are merits to the proposed DKLM approach.\nIt makes intuitive sense to me that the Deepset style inclusion of prior observations is preferable to using prior observations as initialisation for the GP as done by prior work.\n\nThe hpo-b-v3 experiments look somewhat convincing.\nDKLM performs decently in the \"non-transfer setting\" and impressively well in the transfer setting.\nI'm inclined to believe the validity of these results, although I wish the authors would have included code / more details of the setup.\n\n## Weaknesses\n\nUnfortunately, I feel that many parts of this paper are severely lacking in clarity.\nThe writing often feels unnecessarily convoluted and I am sometimes unsure of what exactly the authors are trying to tell me with a particular paragraph.\nFurther, the lack of clarity unfortunately leads to open questions about the proposed approach itself.\nBelow I give specific details of this.\nI hope the authors understand my criticisms as constructive and take them as an opportunity to improve the current draft of the paper.\n\n### Introduction\n\nBy the end of the introduction, I neither understand what open challenges in HPO are, nor what DKLM does differently to established works.\nConcretely, I find the following sentences in the introduction more confusing than helpful:\n* \"we propose a novel architecture for deep GP kernels (Wilson et al., 2016a) that are enriched with novel end-to-end neural network components that generate meta-features only from the tuples of past hyperparameter configurations and their evaluated performances\" --> How do I enrich a deep GP kernel with neural network components? Don't Deep Kernel GPs already have neural networks do produce the encoding?\n* \"Introduce the first paper that tackles the negative transfer phenomenon in Bayesian HPO\" \u2192 What is the negative transfer phenomenon? This is not explained here.\n* \"Propose an end-to-end deep GP which implicitly learns networks that generate metafeatures,\" \u2192 What does it mean when a GP \"implicitly learns networks\"? Are you referring to the neural network of the deep kernel? Why does this learn this only \"implicitly\"?\nIn contrast,  Jomaa et al. (2021a) and Wistuba and Grabocka (2021), which might be the most related work to this submission, are much much more approachable.\n\n### Negative Transfer\n\nThis paper often uses terms without introducing them. \nFor example, one of the contributions listed in the introduction is that this paper is \"the first paper that tackles negative transfer phenomenon in Bayesian HPO\".  They do not explain anywhere what the 'negative transfer phenomenon is'. The authors do give a citation but I do think it would make sense to briefly explain this phenomenon to make the paper more accessible to readers.\n(The negative transfer paper (Wang et al., 2019) itself gives a great summary in the first two sentences of the papers abstract.)\nHowever, it is not clear to me how exactly DKLM tackles negative transfer in ways that prior transfer learning HPO works have not?\n(The following sentence of the paper does *not* answer my questions: \"By adding the task-specific information of the meta-features, the GP surrogate can infer a more accurate response surface on a new task based on similar source tasks that share similar meta-features. Therefore, our method is the first to tackle the negative transfer phenomenon for Bayesian HPO.\")\nThe negative transfer problem is also not once mentioned in the experimental evaluation.\n\n### Landmark  Features / Meta-Features\n\nThe authors repeatedly claim that DKLM (**D**eep **K**ernel Gaussian Process surrogate with **L**andmark **M**eta-features) makes use of \"landmark meta-features\" (Pfahringer et al., 2000; Feurer et al., 2014).\n\nIn prior work, landmark features refer to features that characterise a dataset via the predictive performance obtained by a variety of simple models on that dataset.\nThe idea here is that the performance of different machine learning models can be used to characterise the dataset itself: dataset A works well with tree-based models, Dataset B works well with simple parametric models, and so on.\nThis information can then be used as features in HPO (Feurer et al., 2014).\nNote that, here, these cheap models are not the model for which we want to perform HPO, but instead are additional models that are only used to characterise datasets.\n\nThis is, to my understanding, the definition of landmark features.\nIn this paper, no definition is given, except that landmark features are \" typically estimated by measuring the response of given datasets to machine learning algorithms\".\nIt is entirely unclear to me, how anything that DKLM does can be called \"landmark features\" given that no simple auxiliary models are trained whose performance is used as a dataset-specific feature.\nAgain, maybe this misunderstanding could have been resolved if the authors had discussed these terms in more detail.\n\nAdditionally, the paper claims that DKLM learns to generates *meta-features* which it defines as \"capturing the similarity between datasets\".\nHowever, DKLM learns embeddings that depend only on hyperparameters and loss values.\nNo dataset-specific information is available for constructing these embeddings.\nThe argument of the paper here seems to be that the embedding network learns a representation that learns to 'cluster' different dataset types.\nHowever, given that no information of the dataset is available to the embedding, this seems unnecessarily complex/indirect.\nThe experiments of Figure 5 (weakly) support the idea that this clustering does occur.\n(Also why not apply the approach of Jomaa et al. to actually include learned dataset-specific features? If DKLM is confronted with a novel dataset, unlike the approach of Jomaa et al., it cannot make any judgement of it before observing loss-response pairs.)\nIn any case, I feel that this would need to be exposed much more clearly. \n\nIn contrast, the approach of Jomaa et al. (2021a) also claims to learn meta-features.\nThey construct a neural network architecture that take as input a literal dataset (with all rows/datapoitns and columns/features) and outputs a learned representation.\nThis per-dataset representation is then useful for HPO.\nThe definition of Jomaa et al. (2021a) for learning meta-features makes sense to me.\n\nWhat are the definitions of landmark features/landmark meta-features/meta-features used in this paper and how are they consistent with those of earlier works?\n\n### Missing DKLM Details\n\nThe exposition of the approach is lacking both clarity and detail.\nIn that regard, equations (4-7) are extremely helpful in understanding what's going on.\nWithout them, I'm not sure if I would have any idea of what happens in DKLM.\n\nHowever, the exposition of DKLM is, in my eyes, not complete and lacking many details.\nFor example, this is a list of questions that I still have about how exactly DKLM works:\n* Algorithm 1 gives a recipe for meta-learning with DKLM. But what does applying DKLM look like? Do you continue to train $w$ or $\\theta$ when applying DKLM to a HPO task? (E.g. is anything retrained for the acquisition steps of Figure 3?) Maybe an Algorithm 2 illustrating the application of DKLM would be helpful here.\n* Also, when applying DKLM to a dataset, you update the history $\\mathcal{H}_t$ with observations from the current dataset, right? If so, what happens if you don't do this? You would still add new observations to the GP (just not use them in $\\phi_2$). \n* However if you don't do update $\\mathcal{H}_t$ as you apply DKLM to a dataset,  what does the \"DKLM *without* transfer learning\" setting of Figure 2 look like? This is not explained in the main body of the paper.\n* What acquisition strategy do you use for HPO? Expected improvement? As far as I can tell this is not stated in the paper.\n* From Algorithm 1, I can see that you are using REPTILE to train DKLM. Why are you using REPTILE? This is not discussed in the main text at all.\n* The information in 6.2 is not enough to replicate the experiments. How do you set the deep kernel GP hyperparameters? What are the training schedules? Do you optimise the GP hypers? It would be great if an appendix with this information and/or code of DKLM were provided by the authors.\n\n### Experimental Results\n\nFor the toy experiments, it seems to me that DKLM should be able to predict close to perfect after having found a single peak value of the sinusoid in the transfer learning setting.\nAfter all, a single peak value completely defines the function. (Technically speaking, obtaining a single derivative of the function, e.g. approximated by two close observations, should also be sufficient.)\nHowever, as is evident from Fig. 1 (top left), the predicted function is far from perfect despite observations near the peak. \nBased on my understanding, DKLM should be able to learn to perform similarly to a maximum likelihood fit of  $a * \\sin(x)$ to the observations.\nHowever, the results displayed here are looking far worse.\nWeirdly, going from DKLM + 2 trials to DKLM + 3 trials, the uncertainty of the predictions *increases* despite this not making much intuitive sense.\n\n\n## Smaller Comments\n\n* \"Nevertheless, extracting meta-features requires direct access to the datasets, which might be difficult in real settings where only the meta-dataset is available. \" Why is direct access to datasets not available in real settings? I thought a meta-dataset directly *contained* datasets (and gives direct access to them)? Unfortunately the term meta-dataset is not further defined in the context of this paper.\n* S.3.2: Why is the noise variable $\\sigma_n$ indexed with $n$? \n* \"We also notice how the performance improves gradually with the increasing number of trials, indicating the impact of the posterior variance modeling of our method (Section 5) as more observations are present on the black-box responses.\"\n    * It's unclear to me why \"posterior variance modeling\" is needed for gradually increasing performance? Also, other HPO approaches also rely on GPs with posterior variances. What is unique about DKLM here?\n\n## Language & Style\n\nI believe the writing style of this paper needs improvement. Below, I present some specific examples and suggestions of where I think the writing could be improved.\n\n* \"however, it [HPO] remains an open problem\" --> Why is HPO an open problem? When would it no longer be an open problem? I feel like saying that HPO is an open problem is either trivial or not true, and so best avoided. \n* \"In contrast to existing approaches, we propose a novel Deep Kernel Gaussian Process surrogate with Landmark Meta-features (DKLM) that can be jointly meta-trained on a set of source tasks and then transferred efficiently on a new (unseen) target task.\" --> Why is your work *in contrast* to existing approaches? It seems that you, *like* prior work, apply transfer learning to HPO.  It seems to me that \"jointly meta-training on a set of source tasks\" is also not novel.\n* \"prior work focus on\" \u2192 focuses\n* \"Transfer learning for HPO has been observed by modeling tasks jointly\" \u2192 maybe \"has been applied\". \"Observed\" makes it sound a bit like an event in nature.\n* \"Among *existing* ~~the variety of~~ acquisition functions, ~~the~~ expected improvement is widely adopted (Mockus, 1974).\"\n* \"The standard approach of fitting GPs is to optimize the weights of the kernel function, e.g. squared exponential kernel, \u03b8.\"  I think it is misleading to call the optimisation of the kernel *hyperparameters*  'fitting GPs'. Gaussian Processes perform Bayesian *inference* and are not \"fit\" by \"optimising\" any parameters. However, the hyperparameters in the GP (but not the GP itself) are often inferred by maximising the evidence.\n* \"Consequently, the solution of the joint model resides on a local minimum so that given limited information about the new (unseen) target task [______] \" This sentence is missing an ending? What happens given limited information about the task?\n",
          "summary_of_the_review": "The proposed method, DKLM, for transfer-learned HPO seems interesting and is, as far as I'm aware, novel.\nThe experimental evaluation seems to demonstrate benefits of DKLM over prior work.\nUnfortunately, the paper is (sometimes severly) lacking in clarity, and the draft requires significant further work before being, in my opinion, acceptable for publication.\nThe exposition leaves me with too many open questions as to how DKLM actually, as well as some of the experiments, actually work.\nFurther, one of the main contributions of the paper \u2013\u00a0avoiding 'negative transfer' \u2013 is not discussed sufficiently and not evaluated at all, and I have trouble following the authors in why it is appropriate to say that DKLM learns \"landmark meta-features\".\nIf the authors can address these main points of my review, I am happy to reconsider my score.\n\nThank you for including reproducibility and ethics statement. Please make code for reproducing experiments available as soon as possible.\n\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_1rkp"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_1rkp"
        ]
      },
      {
        "id": "dmpwcedmskr",
        "original": null,
        "number": 2,
        "cdate": 1635886248570,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635886248570,
        "tmdate": 1637777360925,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper focuses on doing transfer learning from related tasks for the optimization of ML model hyperparameters.\n\nPrevious work (e.g., Vanschoren, 2018) has considered meta-features as representation of related datasets or tasks; these features are incorporated into the model when transferring to the task at hand. This work proposes to _learn_ these meta features using a deep-set representation (Zaheer et al., 2017). These meta features are then passed to a kernel function that, finally, parameterizes the Gaussian process used during Bayesian optimization.\n\nExperimentally, the authors compare their method to several baselines, both in the transfer and non-transfer learning settings; aggregated metrics show that the proposed method (DKLM) outperforms other baselines on average.\n\n",
          "main_review": "Strengths\n-------------\n- This paper combines two intuitive ideas, and the method is fairly easy to understand\n- Experimentally, aggregated metrics showcase the strength of the proposed DKLM method compared to other baselines\n- The ablation test which removes the _learned_ meta features convincingly shows that investing additional effort into obtaining trained meta-features improves performance.\n\nWeaknesses \n-----------------\n- The major weakness of this paper is that it seems to simply combine to well-known ideas (deep sets + deep kernel GPs), and as such has limited novelty. \n- The clarify of the paper could be improved. In particular, the experimental setup was not entirely obvious to me. Is the optimization done as Bayesian optimization (with a DKLM in the Gaussian process)? If so, which acquisition function was used?\n\nComments/questions\n----------------------------\n- You mention the \"negative transfer phenomenon\" several times as a major motivation for this work; I would recommend describing it in more detail in the paper.\n- I'm curious about the performance variability of DKLM: do you have any insight into which types of datasets or experimental settings DKLM tends to perform better or worse on, compared to other baselines?\n- Did you compare DKLM to a GP parameterized with the learned metafeatures from Dataset2Vec?\n",
          "summary_of_the_review": "This is an interesting paper, but its novelty is limited. A more detailed analysis of the proposed method (in particular, ablations that use a deep kernel GP with different learned meta-features) would increase the impact of this work.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_5B4g"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_5B4g"
        ]
      },
      {
        "id": "BgxV24OoTU8",
        "original": null,
        "number": 3,
        "cdate": 1635890237595,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635890237595,
        "tmdate": 1635890237595,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper tackles the issue of speeding up black-box hyper-parameter optimization (HPO) by leveraging results obtained by trials on different datasets. It casts the problem into a few-shot regression one, where each task is a dataset and the goal is to predict the performance of a model trained on this dataset from the values of hyper-parameters. Meta-learning (REPTILE) is then used to initialize the (meta-)parameters of a deep kernel Gaussian process. Experiments on the HPO-B-v3 benchmark show this procedure equals or beats 10 baselines when using 20 to 100 trials.",
          "main_review": "\nStrengths\n========\nThe paper very clearly explains the background, existing methods, and issues. It shows how various existing techniques and ideas (deep kernel Gaussian processes, landmark meta-features, deepset) are combined with meta-learning to combat the \"negative transfer\" phenomenon.\n\nExperiments on the large HPO-B benchmark, as well as ablations (on synthetic few-shot regression problems, and in a non-transfer setting), are well-designed and support the paper's conclusions. The research question is clearly stated.\n\nWeaknesses\n==========\nOne would expect that the transfer/few-shot learning setting may be most useful with a small number of trials, however we see that the improvements (wrt. baselines) seem to only appear after about 10-20 trials, and are sustained afterwards. This may be investigated, or maybe addressed as a limitation in the context of the stated goal of limiting the number of HP configuration trials.\n\nMinor points\n==========\n- Some confusion in notations in section 3.2\n  * In Eq (1), is $K_x$ supposed to be $K_*$?\n  * A mean function $m$ is introduced, but not used, and Eq (1) shows the mean as $0$.\n  * In Eq (2), $y$ vs. ${\\mathbf y}$\n- It would be nice to keep the colors consistent between Figures 2 and 3 for the common curves",
          "summary_of_the_review": "Overall good, well-written, self-contained article that presents a novel combination of existing techniques, and addresses the issue of negative transfer when using transfer learning for Bayesian HPO. The experiments conducted are well designed and support the conclusion.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_kJb9"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_kJb9"
        ]
      },
      {
        "id": "jbKkZNd12bJ",
        "original": null,
        "number": 4,
        "cdate": 1635911808617,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635911808617,
        "tmdate": 1635911808617,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The authors focus on the important problem of more efficient hyperparamter optimization through (meta) transfer learning. More specifically, the author propose a new method (Deep Kernel Gaussian Process surrogate with Landmark Meta-features (DKLM)) that embeds a meta-feature extractor model within the kernel function of a deep GP. This meta-feature extractor is able to learn to extract characteristics of a dataset such that the similarity of a dataset to other ones can be (implicitly) captured and used by the rest of the model. To demonstrate the effectiveness of their proposed model, they evaluate their model on the large-scale HPO-B-v3 benchmark against 10 other baselines plus a variant of their method with a randomly-initialized meta extractor.",
          "main_review": "Strengths:\n- Adequately builds upon previous work such that the proposed change (the meta-extractor within the kernel of a deep GP) is easy to assess and compare (as opposed to larger set of changes that are not adequate ablated).\n- Large-scale benchmark (935 tasks, 16 search spaces) compared to 10 baselines.\n- Ablations with and within a randomly-initialized meta-extractor.\n- Results included error bars (Table 1).\n\nWeaknesses:\n- Experimental setup could be made clearer as to which tasks are used for training and eval in the transfer setting (i.e., which tasks are used to originally train the meta-extractor, and on which tasks is it transferred).\n\nMinor:\n- p. 3: In the fourth line from the bottom, the reference \"2020\" needs to be fixed.",
          "summary_of_the_review": "Overall, the authors propose a new method that is well-scoped within existing literature, and provide positive empirical results on a large-scale benchmark against an adequate set of baseline tasks. It is a reasonable paper for acceptance.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_bzFz"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_bzFz"
        ]
      },
      {
        "id": "GX0IeSgi_7P",
        "original": null,
        "number": 5,
        "cdate": 1635988011798,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635988011798,
        "tmdate": 1635988011798,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a new Kernel for Bayesian Optimization to incorporate 'Landmark meta-features' of groups of tasks, so that the Kernel can be used to improve the efficience of the Bayesian Optimization on a new related task. The Kernel is based on 2 nested neural networks that are jointly trained via first-order meta-learning method REPTILE. The authors show that the method is overall better performing on the benchmark HPO-B-v3.",
          "main_review": "Strengths:\n- The topic is important. Hyperparameter is expensive and transfer learning is a promising solution to reduce the cost of HPO. \n- The solution is relatively complex, involving deep Kernels using neural networks and trained with meta-learning, and yet it is presented clearly and is easy to understand.\n- The benchmark is large, including 10 different relevant algorithms on 935 tasks with 16 different search spaces. \n- The experiments cover different aspects of the contributed algorithm; T-SNE projections of the meta-features and the impact of the sample size.\n\nWeaknesses:\n- A lot of emphasize in introduction is on the fact that DKLM would address negative transfer. I do not see this discussed explicitly in any of the experiments. If it is important, it should be supported by the experiments.\n- The proposed solution is most certainly more computationally expensive than random search, or the fast Bayesian Optimization algorithm ABLR. The experiments should also show the improvement in terms of running time so that we can see how it performs compared to these faster algorithm. \n- The standard deviation on Table 1 is way too large to be able to conclude anything. I could run again the same experiments with different seeds and observe an upward trend instead. \n- The functions f and g introduces many hyperparameters to DKLM. The paper does not discuss how to chose these hyperparameters and whether tuning them is important for the performance of the algorithm. \n\nMinor comments:\nWhat is a meta-dataset? It is not defined in section 2.\n\nI don\u2019t understand this sentence:\n> Consequently, the solution of the joint model resides on a local minimum so that given limited information about the new (unseen) target task.\n\nTypos:\nnew a target task -> a new target task\nfast adaption -> fast adaptation\nwhere we notice -> while we notice",
          "summary_of_the_review": "The proposed algorithm is an interesting way of learning end-to-end meta-features across tasks to improve the efficiency of Bayesian Optimization on new similar tasks. The benchmarking experiments are extensive and convincing for the efficiency of the algorithm, but does not support clearly the claims about negative transfer. The latter point should be discussed more clearly. The additional hyperparameters of the algorithms (network size and REPTILE hyperparameters) should also analyzed empirically to see how importantly they affect the performance of the algorithm.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "I do not see any issues for ethics. ",
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Reviewer_ZdAZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Reviewer_ZdAZ"
        ]
      },
      {
        "id": "iePuAayYlht",
        "original": null,
        "number": 11,
        "cdate": 1637164524007,
        "mdate": 1637164524007,
        "ddate": null,
        "tcdate": 1637164524007,
        "tmdate": 1637164524007,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Official_Comment",
        "content": {
          "title": "Summary of Changes",
          "comment": "# Handling the major criticism on Negative Transfer\n\nIn the first round of reviews, almost all the reviewers raised criticism that a major claim of the paper in terms of handling negative transfer was not thoroughly supported empirically.\nAlthough we demonstrated that our method DKLM outperforms a large set of state-of-the-art transfer HPO baselines, we agree that our experimental protocol was not specifically designed to test negative transfer per se.\n\nTo remedy this deficiency, we designed a negative transfer experiment as follows:\n\n- For each search space in the HPO-B benchmark we reduce the set of meta-training datasets to only 10% of the most uncorrelated datasets to the meta-test datasets. As a result, competing methods would learn to transfer from training datasets that are uncorrelated with the testing datasets.\n\n- Here, the correlation between a pair of datasets was measured as the rank correlation (Kendall tau) of the vector of validation accuracies achieved by evaluating the same set of random hyperparameter configurations on both datasets.\n\nFor more details, we added a new section 6.5 as \"Resilience to Negative Transfer\". The outcome of the experiment in Section 6.5 validates that our method DKLM with meta-features provides resilience to the negative transfer, by having a larger gap in performance to the variant without meta-features (FSBO) in the negative transfer experimental setup.\n\n# Experimental Details\nWe also uploaded the source code, added two new algorithms that describe DKLM in pseudo-code, and added the experimental protocol in Appendix B. \n\nWe thank all the reviewers for their feedback, and are available to answer any additional concerns. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper197/Authors"
        ]
      },
      {
        "id": "Toj121hzE5",
        "original": null,
        "number": 1,
        "cdate": 1642696844307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696844307,
        "tmdate": 1642696844307,
        "tddate": null,
        "forum": "wronZ3Mx_d",
        "replyto": "wronZ3Mx_d",
        "invitation": "ICLR.cc/2022/Conference/Paper197/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The submission describes a method for tuning machine learning pipeline hyperparameters using transfer learning from related tuning tasks. In particular, the method uses learned meta features to construct a covariance function for a GP.\n\nThis was an extremely difficult case and could have gone either way. It was the closest case for any paper I serve as the AC for. Two of the reviewers recommended rejecting the paper and three recommended accepting, although during discussion one of the reviewers recommending accepting the paper seemed to actually be more on the reject side.\n\nUltimately, I have decided to recommend rejecting this submission. However, if either the clarity (especially concerning the neural network setup) or the experiments were somewhat improved I would have recommended accepting it. I view clarity as an extremely important factor when weighing whether a submission should be accepted. I concur with the reviewers on the following weaknesses of the experiments: (1) the lack of an ablation test when considering ad-hoc meta features and (2) the experimental evaluation is based on mostly aggregated metrics.\n\nI know this recommendation must be disappointing, but I encourage the authors to polish the work a bit more and resubmit it somewhere."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}