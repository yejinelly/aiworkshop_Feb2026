{
  "id": "p4H9QlbJvx",
  "original": "enhObKMUKdb",
  "number": 174,
  "cdate": 1632875434155,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875434155,
  "tmdate": 1676330685040,
  "ddate": null,
  "content": {
    "title": "Rethinking Again the Value of Network Pruning -- A Dynamical Isometry Perspective",
    "authorids": [
      "~Huan_Wang3",
      "~Can_Qin1",
      "~Yue_Bai1",
      "~Yun_Fu1"
    ],
    "authors": [
      "Huan Wang",
      "Can Qin",
      "Yue Bai",
      "Yun Fu"
    ],
    "keywords": [
      "neural network pruning",
      "dynamical isometry",
      "model compression",
      "filter pruning"
    ],
    "abstract": "Several recent works questioned the value of inheriting weight in structured neural network pruning because they empirically found training from scratch can match or even outperform finetuning a pruned model. In this paper, we present evidences that this argument is actually \\emph{inaccurate} because of using improperly small finetuning learning rates. With larger learning rates, our results consistently suggest pruning outperforms training from scratch on multiple networks (ResNets, VGG11) and datasets (MNIST, CIFAR10, ImageNet) over most pruning ratios. To deeply understand why finetuning learning rate holds such a critical role, we examine the theoretical reason behind through the lens of \\emph{dynamical isometry}, a nice property of networks that can make the gradient signals preserve norm during propagation. Our results suggest that weight removal in pruning breaks dynamical isometry, \\emph{which fundamentally answers for the performance gap between a large finetuning LR and~a small one}. Therefore, it is necessary to recover the dynamical isometry before finetuning. In this regard, we also present a regularization-based technique to do so, which is rather simple-to-implement yet effective in dynamical isometry recovery on modern residual convolutional neural networks.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "wang|rethinking_again_the_value_of_network_pruning_a_dynamical_isometry_perspective",
    "pdf": "/pdf/8834d63720220593485043ddff5979960c5247c9.pdf",
    "one-sentence_summary": "We show inheriting weights in filter pruning is valuable and examine the impact of finetuning LR on the final performance via dynamical isometry.",
    "_bibtex": "@misc{\nwang2022rethinking,\ntitle={Rethinking Again the Value of Network Pruning -- A Dynamical Isometry Perspective},\nauthor={Huan Wang and Can Qin and Yue Bai and Yun Fu},\nyear={2022},\nurl={https://openreview.net/forum?id=p4H9QlbJvx}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "p4H9QlbJvx",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 27,
    "directReplyCount": 7,
    "revisions": true,
    "replies": [
      {
        "id": "YMX00Wvatt",
        "original": null,
        "number": 1,
        "cdate": 1635755973818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635755973818,
        "tmdate": 1638253670038,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This work takes a second look at the set of papers (Crowley et al 2018, Liu et al 2019) which claim that there is no generalization benefit offered by pruning, i.e., training a large network first and then pruning performs identically or worse than training the same pruned network architecture from scratch. This work claims that these previous works do not set learning rates during fine-tuning correctly while performing their experiments and hence these claims are invalid. The paper also offers an explanation for this phenomenon via dynamical isometery theory.",
          "main_review": "1) **Nice analysis of previous papers**: It is commendable that this paper points out issues with two previous papers (Crowley et al 2018, Liu et al 2019) who make methodological errors by not tuning the learning rate for fine-tuning, and use the results of these erroneous experiments to make general statements about the efficacy of pruning (regardless of whether these statements are true or not). However from the perspective of these previous papers, it seems they had a fixed small fine-tuning budget of 20 epochs, and did not want to exceed this as we require number of fine-tuning epochs to be much smaller than the number of training epochs. While I do think that this requirement is arbitrary, it helps understand the claims of the previous papers.\n\n2) **Unfair experimental comparison**: In Table 1, it seems that the small Resnet models (A,B) trained from scratch are trained for 120 epochs, while the best prune-finetuned models are trained for 90 epochs. However also accounting for the training of the original large Resnet-34 model, the total number of training epochs are 90+90=180 epochs. Hence it seems that the prune+finetuned models are trained overall for a larger number of epochs, which may be a confounding factor which explains their better results. I would *strongly* suggest that the experiments be run such that the number of training epochs for \"training small model from scratch\" and \"large model training with pruning and finetuning\" be equalized for a fair comparison.      \n\n3) **Unclear why hypothesis must be true**: This paper rest on the hypothesis that training a large model followed by pruning and fine-tuning offers improved generalization benefits over training a small model from scratch. In principle it should be clear that both two methods can produce models with similar generalization given sufficiently strong optimizers, as the only difference between them is the initialization of the small model. Even in practice, we often are able to train models such that the train loss is close to zero (or sufficiently small), regardless of whether we train small models from scratch or fine-tune pruned models.  \nHence given that both methods solve the same overall optimization problem (i.e, minimizing loss of the small model), I am confused regarding why one would expect pruning + fine-tuning to generalize better than training from scratch, even in principle. For example, it is clear that given infinite epochs and a suitable (well-tuned) optimizer both methods should work equally well. Is the paper's hypothesis a claim about the speed of optimization, that pruning+fine-tuning is a fast way to obtain a small model? If so, how does this change after having accounted for the training of the dense model itself (which is usually slower to train)?\n\n4) **Incorrect metric for measuring Dynamical Isometry**: In equation 1, the paper uses mean singular values as a metric for assessing dynamical isometry. However if we require that all singular values are equal to one, then the metric to use would be the sum of (squared) deviations from unity of each singular value (\\sum_i (\\Sigma_{ii} - 1)^2). In such a metric, zero corresponds to dynamical isometry being achieved. However the metric of mean singular values has no such meaning as the standard deviation can be large. I am hence confused regarding why the paper chose to use the mean JSV metric, and if there is no strong reason for doing so, I would *strongly* recommend re-running experiments with the deviations-from-unity metric.\n\n5) **Relationship between mean JSVs and convergence unclear**: Hypotheses 1-4 and the boxed explanation talk about the \"recovery\" of mean JSVs and their relationship to network convergence. I am confused regarding why the behaviour of mean JSVs during or after training is deemed meaningful. I understand that too small or too large mean JSVs must be avoided at initialization as they can cause vanishing or exploding gradients, but can increasing mean JSVs from ~1 to ~5 (as in Fig 2) be deemed meaningful? It is thus not clear why mean JSV (or any measure of dynamical isometry) can be used to measure model convergence (ref. to boxed explanation in page 7). For example, if gradient-norm regularization is used, then the model would converge and yet have small JSVs. The only measure of model convergence is the gradient norm of the loss w.r.t. weights, and other measures are meaningless unless explicitly shown otherwise. ",
          "summary_of_the_review": "This paper does not make a solid case for the hypothesis that \"pruning+finetuning produces models with better generalization than training small models from scratch\", as there are issues with both the experimental and the analytical studies. In the experimental setup, the comparisons are not performed fairly, while in the analytical parts use flawed metric to measure model convergence and dynamical isometry. Even fundamentally, it is unclear why the proposed hypothesis must be true in the first place. For all these reasons, I would recommend rejection.\n\nPost rebuttal update: The discussions did not change my opinion of the paper. However I appreciate the efforts taken by the authors to update the paper in light of reviewer suggestions, particularly to make the core argument more rigorous. Although I do not think the paper still succeeds at this, I am increasing my score slightly to reflect this change.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Reviewer_YYie"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Reviewer_YYie"
        ]
      },
      {
        "id": "-kEDu9CDHa2",
        "original": null,
        "number": 2,
        "cdate": 1635801830055,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635801830055,
        "tmdate": 1635801830055,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper challenges an existing argument that \u201cinheriting the weights of the pruned network is not necessary for fine-tuning\u201d. This paper provides empirical evidence that the previous experiments are not carried out with proper learning rates and training epochs to ensure the network convergence. \n\nThe authors conjecture that fine-tuning pruned network requires larger learning rates and longer training epochs because its dynamical isometry is broken. As a result, large learning rates (if training for shorter epochs) or long training epochs are required for the recovery of dynamic isometry. \n\nThen the authors propose OrthP to re-initialize the pruned weights to completely recover the dynamic isometry in simple MLP. With OrthP, the fine-tuning process is less sensitive to different learning rate and training epochs. \n",
          "main_review": "Strength: \n\n1) This paper brings new insights for the role of the \u201cweights\u201d in the network pruning.\n2) This paper re-examines the established arguments with careful designed experiments over wide range of pruning ratios.\n3) The proposed technique is clearly motivated by empirical observations and is very effective on MLP networks under large pruning ratios.\n4) This paper is easy to follow.\n\nWeakness:\n\n1) Table 1,2 shows that fine-tuning a pruned network requires much more training time and tuning efforts to achieve a satisfying result. Although the author claims that OrthP can \u201ccomplete\u201d resolve the dynamical isometry issue, the pruned network converges still only after prolonged training epochs (90 and 900 epochs). Does that means the pruned networks weights are more difficult to optimize than random initialized ones, which contradicts the \"the value of weights\" claim?\n2) The application area of the proposed OrthP is limited to MLP (Table 6). Although the author proposed a Strong L2 Regularization for more complicated networks, this section seems to be \u201cunfinished\u201d \u2014 the connection with dynamical isometry seems a little bit weak to me, can the authors provides more elaborations? This seems to be more of practical interests.\n3) I have some confusions regarding the experiment designs and presented results (see the Questions section below). \n\nQuestions:\n\n1)  For Table 1: In \u201cOur rerun\u201d,  are \u201cScratch\u201d results trained under 120 epochs with a decayed LR schedule? For Table 2, what are the hyper-parameters for the \u201cScratch\u201d results, are they carefully tuned?\n2) Have the authors investigated the JSV of randomly initialized sparse networks? I am not sure if applying OrthP initialization or other orthogonal regularization methods on \u201cScratch\u201d can also boost its performance. I believe this would be a fairer comparison to support \"the value of weights\" claim.\n3) Table 5: Can the authors also provide the \u201cScratch\u201d results and their hyper-parameters? I am asking because the reasons stated in Weakness 1) \u2014 if the \u201cScratch\u201d result is comparable to OrthP without using prolonged epochs, then this means even when the dynamical isometry is recovered, the pruning selected weights need more training time to optimize than randomly initialized ones. Also, in Table 2, it looks like \u201cScratch\u201d results work well under large PR, which is exactly the case in Table 5.\n4) Related to 3), from Figure 2 and Table 5, it looks like OrthP takes the role of a large training epochs to recover dynamical isometry. It would be nice to see if OrthP works for non-prolonged epochs, e.g., < 90.",
          "summary_of_the_review": "I like this paper in that it thoroughly investigates existing arguments on the value of pruning weights. The perspective of OrthP is not that novel, but is simple and effective on MLP under large pruning ratios. \n\nMy major concern is in the experiments design and results part. I would consider raising my score if my questions can be addressed.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Reviewer_VvbL"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Reviewer_VvbL"
        ]
      },
      {
        "id": "LdBwgvSpwzf",
        "original": null,
        "number": 3,
        "cdate": 1635918954587,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635918954587,
        "tmdate": 1635918954587,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper argues pruning a pretrained network and finetuning is better than training a sparse network from scratch and tries to connect the fine-tuning phase properties to dynamical isometry (DI) literature. The empirical observation makes a case for inheriting the pretrained weights.",
          "main_review": "# Strengths\n\n1. The empirical observation that inheriting pretrained weights is better than training a sparse network from scratch if the fine-tuning phase is carefully tuned.\n2. The empirical analysis on fine-tuning hyperparameters (LR, #epochs) might be useful to practitioners.\n\n# Weaknesses\n\n1. lack of novelty: connection to DI and pruning breaks DI is known [Lee et al 2020, analysed at initialization] and analysing for a pretrained network is straightforward and claiming it as a contribution is weak.\n\n2. The hypothesis on page 7 connecting larger LR in fine-tuning and DI is dubious and not backed by any theory or experiments. In fig.2 plots mean JSV increases and that does not mean improved DI. As DI theory suggests to have mean JSV around 1.\n\n3. The comparison against scratch does not seem fair. Usually, fine-tuning phase is much shorter compared to training from scratch but in this case fine-tuning is done for almost the same no of epochs as the scratch version. Is it possible that the scratch version could be tuned to improve the performance?\n\n4. In section 4, the L2 regularization of pruned weights is regarded as a method for DI recovery which in my opinion is unsubstantiated.",
          "summary_of_the_review": "There are many unsubstantiated claims (or weak statements) in the paper. ",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Reviewer_nWUS"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Reviewer_nWUS"
        ]
      },
      {
        "id": "6sA-fgy87ZM",
        "original": null,
        "number": 4,
        "cdate": 1636303312964,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636303312964,
        "tmdate": 1636303312964,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Review",
        "content": {
          "summary_of_the_paper": "There are recent works questioning the value of inheriting weights in structured neural network pruning as it is empirically observed training from scratch can match or outperform finetuning a pruned model. This paper mainly includes three components: 1) the authors reinvestigated the problem and demonstrates that the conclusion is inaccurate because of improperly small finetuning learning rates. They show finetuning with pruned weights actually outperforms training from scratch, when larger learning rates and longer training epochs are adopted. 2)  the authors explored dynamical isometry (DI) to understand how finetuning LR affects the final performance. They show that weight pruning breaks dynamical isometry and finetuning can recover it and a larger LR can recover faster. 3) They proposed to fully recover dynamical isometry in fitler pruning before finetuning.\n",
          "main_review": "The paper is well written and organized. The effect of learning rate on fine-tuning stage of pruning is not thoroughly investigated and this paper provides an in-time and thorough study. My major concerns are as follows:\n\n- Pruning as poor initialization: the concept of initialization needs to be cleared. Does the pruning in Fig 1 refer to weights pruning for both networks? It might be true if it is weights pruning as different pruning ratios correspond to the same weights dimensions with different values. However, filter pruning results in different architectures and different pruning results are not different initializations with respect to the same architecture. \n\n- The dynamic isometry might explain the easiness of optimization, however, the relationship with the generalization is not quite clear. There might be other metrics such as flatness might show a similar trend as JSV. A comparison with other metrics for generalization could be more persuasive.\n\n- The analysis with MLP-7 and MNIST might be too simple for practical usage, and this method does not generalize to more practical non-linear convolutional neural networks. The authors propose strong regularization helps, however, no explanation on why this method helps and how it connects with previous analysis. Does this method work for MLP-7 and how is it connected to DI?",
          "summary_of_the_review": "The paper carefully studies recent thoughts on the meaning of pruning and made some rigorous investigations. Though some issues exists and DI may not be the ultimate solution to the question, the paper provides valuable thoughts for this filed, including fair experimental comparison and new explanations. Therefore I recommend to accept.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Reviewer_bhQQ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Reviewer_bhQQ"
        ]
      },
      {
        "id": "c7mCiIDjRJv",
        "original": null,
        "number": 5,
        "cdate": 1637310066269,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637310066269,
        "tmdate": 1637519386902,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "-kEDu9CDHa2",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Response to Review VvbL (R3) -- Part 3",
          "comment": "The above results seem to put us in an awkward position since they are *against* what we claimed in the paper (\"pruning has value\"). But hold on, allow us to present another set of results -- We use **MLP-7-ReLU** (unpruned accuracy 98.16%) as evaluation network instead of **MLP-7-Linear** above. Clearly, here we want to see if the *non-linearity* can make any difference. Results are shown below:\n\n**Tab. R3-#3: Comparing L1 pruning to scratch training with two initialization schemes on MLP-7-ReLU**\n\n|  \t|  PR 0.8 \t| PR 0.9 \t|\n|---\t|---\t|---\t|\n| L1 pruning, 90 epochs, LR 0.01 | 96.75 (0.09)  |  94.76 (0.15) |\n| L1 pruning, *StrongReg*, 90 epochs, LR 0.01 | **96.98 (0.04)**  |  **95.10 (0.13)** |\n| Scratch (Kaiming_uniform) \t| 96.60 (0.16) \t| 94.64 (0.24)\t|\n| Scratch (Orthogonal init) \t| 96.52 (0.17) \t| 92.56 (1.90) \t|\n\nwhere, for the L1 pruning, we replace the DI recovery method from OrthP to StrongReg since OrthP only works for *linear* MLP while here we are dealing with non-linear network.\n\nAs seen, now, either using kaiming_uniform or orthogonal initialization, L1 pruning is *consistently better* than scratch training. The advantage is amplified when using StrongReg. \n\nA worthy question is: Why on the MLP-7-Linear, pruning does not beat orthogonal initialization but on MLP-7-ReLU, they beat? This is actually straightforward to see if we see pruning as *a* kind of initialization -- as mentioned above, on linear MLP  networks, orthogonal initialization is *proved to be the optimal*. That is, no other initialization can be better. Pruning is essentially also *a kind of initialization* for the subsequent fine-tuning, so it cannot beat the (optimal) orthogonal initialization, thus no value. But when it comes to the MLP-7-ReLU, orthogonal initialization is no longer optimal (as we mentioned, there is *no* method up to date that can achieve exact isometry for non-linear networks). Then, it is *likely* that pruning provides better initialization weights than orthogonal initialization or kaiming_uniform. It just turns out pruning really achieves this, and archives more with the help of StrongReg.\n\nThere are at least 3 takeaways here:\n* First, The comparison above shows that whether pruning has value seriously hinges on the *network type, initialization scheme, and pruning ratios* (and maybe more). **Looking at pruning as a kind of initialization** as we do in this paper (and inspecting it with dynamical isometry) is actually a pretty good perspective, from which many results that appear inconsistent or random start to be explainable.\n* Second, as seen, pruning shows the value in the non-linear case but no value in the linear case. On the whole, we believe it is fair to say pruning *has* value considering the non-linear case is more practical.\n* Third, more profoundly, the above results actually suggest, **if dynamical isometry is fully recovered, pruning will (probably) have no value indeed because it cannot beat the initialization scheme that can achieve exact isometry.** Practically, up to date, for non-linear networks (not to mention BN, residuals, convolution), there has been no such method that can achieve exact isometry. Thus, it is still *likely* for pruning to be valuable at present -- In this regard, we recognize that our previous claim in the paper \"pruning has value\" is not rigorous enough, we update it to \"**pruning has the potential to be valuable if the random initialization of scratch training cannot achieve exact isometry**\".\n\n`R3-Q4`: Related to 3), from Figure 2 and Table 5, it looks like OrthP takes the role of a large training epochs to recover dynamical isometry. It would be nice to see if OrthP works for non-prolonged epochs, e.g., < 90.\n\n`R3-Response6`: This question is addressed in `R3-Response1` above, where we show the 30-epoch results using OrthP. It still works (with very marginal accuracy drop).\n\nHope our (a little bit long) response can help clear your concerns. **If you have further questions regarding our feedback, just let us know!**"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "dw_b-uhNpNt",
        "original": null,
        "number": 6,
        "cdate": 1637310118916,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637310118916,
        "tmdate": 1637517737064,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "-kEDu9CDHa2",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Response to Review VvbL (R3) -- Part 2",
          "comment": "`R3-Q1`: For Table 1: In \u201cOur rerun\u201d, are \u201cScratch\u201d results trained under 120 epochs with a decayed LR schedule? For Table 2, what are the hyper-parameters for the \u201cScratch\u201d results, are they carefully tuned?\n\n`R3-Response3`: Yes, in Table 1 and 2, all the scratch models run by ourselves use the 120-epoch training strategy: SGD optimizer, weight decay 1e-4, LR schedule: 0:1e-1, 30:1e-2, 60:1e-3, 90: 1e-4, 105: 1e-5. This is adopted from the standard 90-epoch [PyTorch ImageNet example](https://github.com/pytorch/examples/blob/master/imagenet/main.py), just we add another 30-epoch training to *make sure the model is fully converged* (also mentioned in Appendix D). Since we use an even better strategy than the standard pytorch training strategy, we believe they *are* carefully tuned. But if you ask, are they tuned *really hard*? Then no, e.g., we definitely can use more tricks (like cosine LR schedule) to get even better results, but that would make the comparison non-standard and unfair to (Liu et al., 2019) -- note the training strategy used by the rethinking paper (Liu et al., 2019)) for ImageNet is also the 90-epoch [PyTorch ImageNet example](https://github.com/pytorch/examples/blob/master/imagenet/main.py), see [their github](https://github.com/Eric-mingjie/rethinking-network-pruning) (\"For ImageNet, we use the official Pytorch ImageNet training code\").\n\n`R3-Q2`: Have the authors investigated the JSV of randomly initialized sparse networks? I am not sure if applying OrthP initialization or other orthogonal regularization methods on \u201cScratch\u201d can also boost its performance. I believe this would be a fairer comparison to support \"the value of weights\" claim.\n\n`R3-Response4`: Applying OrthP to randomly initialized networks is *the same* as orthogonal initialization proposed by (Saxe et al., 2014). Then this question actually reduces to \"what about the training from scratch results using orthogonal initialization?\". They are presented in our response `R3-Response5` below.\n\n`R3-Q3`: Table 5: Can the authors also provide the \u201cScratch\u201d results and their hyper-parameters? I am asking because the reasons stated in Weakness 1) \u2014 if the \u201cScratch\u201d result is comparable to OrthP without using prolonged epochs, then this means even when the dynamical isometry is recovered, the pruning selected weights need more training time to optimize than randomly initialized ones. Also, in Table 2, it looks like \u201cScratch\u201d results work well under large PR, which is exactly the case in Table 5.\n\n`R3-Response5`: To answer this question, we first need to be very careful with what initialization scheme is used for the \"scratch\" training because we'll show they can make a *significant* difference. Specifically, we have two initialization schemes here: **(1) kaiming_uniform** (which is the [default initialization scheme in PyTorch](https://github.com/pytorch/pytorch/blob/68d8ab0cc60536db5a9af4c08ff39e43b252802f/torch/nn/modules/linear.py#L96) for conv and linear layers), **(2) orthogonal initialization**, proposed by (Saxe et al., 2014). Note that, orthogonal initialization is proved to be **optimal** in the sense of exact isometry *for linear MLP networks*. \n\nThe hyper-parameters for training from scratch are: SGD optimizer, weight decay 1e-4, training for 90 epochs in total, initial LR 0.01, decayed by 0.1 at epoch 30, 60, batch size 100.\n\nResults are presented below: \n\n**Tab. R3-#2: Comparing L1 pruning to scratch training with two initialization schemes on MLP-7-Linear**\n\n|  \t|  PR 0.8 \t| PR 0.9 \t|\n|---\t|---\t|---\t|\n| L1 pruning, OrthP, 90 epochs, LR 0.01 \t| 92.79 (0.03) \t| 92.77 (0.04) \t|\n| Scratch (Kaiming_uniform) \t| 92.60 (0.14) \t| 91.48 (0.23) \t|\n| Scratch (Orthogonal init) \t| 92.76 (0.03) \t| 92.76 (0.04) \t|\n\nAs seen, L1 pruning beats the default PyTorch initialization (kaiming_uniform), while being on par with orthogonal initialization on PR (pruning ratio) 0.9. On PR 0.8, pruning does not show a statistically significant advantage. Again, the conclusion appears inconsistent here: if we compare L1 pruning with kaiming_uniform on PR 0.9, pruning has value; but if we compare with orthogonal initialization, pruning has no value. On PR 0.8, pruning cannot beat either of the two scratch results.  On the whole, fairly speaking, pruning has *no* value here since we find at least one kind of random initialization that can perform just as well."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "VcP-kbGnFal",
        "original": null,
        "number": 7,
        "cdate": 1637310178414,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637310178414,
        "tmdate": 1637529865492,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "-kEDu9CDHa2",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Response to Review VvbL (R3) -- Part 1",
          "comment": "Thank Reviewer VvbL (R3) for your very helpful comments! We address your concerns as follows. This response is a little bit long, so please be patient. Also, *if you have more concerns, we are more than happy to hear them! Just let us know!* \n\n`R3-Weakness1`: Table 1,2 shows that fine-tuning a pruned network requires much more training time and tuning efforts to achieve a satisfying result. Although the author claims that OrthP can \u201ccomplete\u201d resolve the dynamical isometry issue, the pruned network converges still only after prolonged training epochs (90 and 900 epochs). Does that means the pruned networks weights are more difficult to optimize than random initialized ones, which contradicts the \"the value of weights\" claim?\n\n`R3-Response1`: For the prolonged training epoch setting (90, 900 epochs), we intentionally use this setting (because we want to make sure the models are fully converged, as explained in the paper -- Page 7, \"LR schedule setup\"), which does *not* mean the model can *only* converge at this setting. With the normal number of training epochs on MNIST (30 epochs), it also converges -- we rerun the results of Tab. 5 with 30 epochs, results shown below. **All the results on MNIST in this rebuttal are averaged by 5 random runs (std shown in the parentheses)**.\n\n| OrthP+30 epochs \t|  PR 0.8 \t| PR 0.9 \t|\n|---\t|---\t|---\t|\n| LR 0.01 \t| 92.69 (0.05) \t| 92.66 (0.04) \t|\n| LR 0.001 \t| 92.71 (0.07) \t| 92.72 (0.02) \t|\n\nAs seen, they are pretty close (slightly worse) to the results of OrthP+90 and OrthP+900 epochs in Tab. 5 of the paper, but still *much better than not using OrthP*. Thus, our conclusion still holds when using this 30-epoch training strategy. \n\nWe will still use the 90 epoch setting when reporting new results for this rebuttal.\n\n`R3-Weakness2`: The application area of the proposed OrthP is limited to MLP (Table 6). Although the author proposed a Strong L2 Regularization for more complicated networks, this section seems to be \u201cunfinished\u201d \u2014 the connection with dynamical isometry seems a little bit weak to me, can the authors provides more elaborations? This seems to be more of practical interests.\n\n`R3-Response2`: First, we present more elaborations as suggested -- StrongReg does not *explicitly* target isometry recovery, which may make R3 feel the connection is weak. But implicitly, it indeed recovers dynamical isometry -- Note dynamical isometry describes a nice state of the network that signals can propagate through it without serious magnitude explosion or attenuation. Since the weights in a network are *dependent* of each other, removing some of them will definitely hurt the isometry because it is based upon *all* the weights. When we use a strong regularization to push these unimportant parameters to zero, it explicitly makes the other parameters learn to *not rely on them*, that is, encouraging the gradients not to pass through these weights/neurons because they are going to be cleaned out. This is what we believe makes StrongReg effective in maintaining dynamical isometry. Its effectiveness is shown in Tab. 6 of the paper (also the results below). Besides, R1 has a similar question regarding why StrongReg is a DI recovery method. You may see our response above `R1-Response3` to see if it can help resolve your concern.\n\nStrongReg is very simple so we discuss it in just one page. This short length does not mean \"unfinished\".  We will add more explanation and results (e.g. those in `R1-Response3` above)  to the Appendix to explain more about StrongReg. \n\nMeanwhile, note that the main goal of this paper is *not* to propose a method to recover DI in pruning, but to explain how DI is playing a role in pruning and *make a formal response to the questioning of the value of pruning (Liu et al., 2019) (Crowley et al., 2018)*. Albeit being simple, StrongReg is proposed to be a good *starting point* that may inspire more advanced algorithms in the future."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "F4vSYSfb6N6",
        "original": null,
        "number": 8,
        "cdate": 1637369881198,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637369881198,
        "tmdate": 1637679729495,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "[Response to All Reviewers] How to properly look at the mean JSV metric for dynamical isometry (1/2)",
          "comment": "We use these abbreviations to mention different reviewers: `R1`: bhQQ, `R2`: nWUS, `R3`: VvbL, `R4`: YYie.\n\n**Thanks to all the reviewers for your constructive comments!** We find several reviewers are curious about how the mean JSV is related to dynamical isometry. Thanks for letting us know this critical concept is not clearly treated in our paper. Here we elaborate more. All the reviewers are strongly encouraged to look at this section.\n\nDI (dynamical isometry) is defined by mean JSV close to 1 in (Saxe et al., 2014) (we are aware that, rigorously in (Saxe et al., 2014), DI describes the distribution of *all* JSVs. Mean JSV is only an average sketch of the distribution. But this approximation is accurate enough for analysis, also used by (Lee et al., 2020) (in fact, we are directly inspired by (Lee et al., 2020) for using this metric). In other words, **if a network has mean JSV close to 1, we can say this network has dynamical isometry**.\n\nThen a non-trivial technical question is: **When we deal with practical DNNs in the real world, how close is the so-called \u201cclose to 1\u201d**? To our best knowledge, there is no outstanding theory to *quantify* this (if any reviewer disagrees, you may suggest specific references), so we resort to empirical analysis, specifically on the MLP-7-Linear network used in our paper.\n\n In Tab. #1 below, we present the mean JSV of MLP-7-Linear network on MNIST under different pruning ratios, along with their accuracies before and after fine-tuning.\n\n**Tab. #1. Mean JSV and accuracies of MLP-7-Linear on MNIST under different pruning ratios. Each result is randomly run for 3 times. We report the mean accuracy and (std).**\n\n| pruning ratio (PR) \t| 0 \t| 0.1 \t| 0.2 \t| 0.3 \t| 0.4 \t| 0.5 \t| 0.6 \t| 0.7 \t| 0.8 \t| 0.9 \t|\n|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|\n| mean JSV \t| 2.4987 \t| 1.7132 \t| 0.9993 \t| 0.5325 \t| 0.2711 \t| 0.1180 \t| 0.0452 \t| 0.0151 \t| 0.0040 \t| 0.0004 \t|\n| Accuracy before finetuning (%) \t| 92.77 \t| 91.35 \t| 78.88 \t| 62.21 \t| 32.14 \t| 11.47 \t| 9.74 \t| 9.74 \t| 9.74 \t| 9.74 \t|\n| Accuracy after finetuning (%) \t| / \t| 92.82 (0.05) \t| 92.80 (0.04) \t| 92.80 (0.01) \t| 92.77 (0.01) \t| 92.77 (0.02) \t| 92.77 (0.00) \t| 92.78 (0.02) \t| 91.37 (0.03) \t| 87.82 (0.03) \t|\n\nAs seen, there is a clear trend: larger pruning ratio, smaller mean JSV, lower accuracy (before or after finetuning). Particularly note *the mean JSV range where the pruned network can be finetuned back to the original accuracy (92.77%)*, which is **>=0.0151**. This means, for networks with mean JSV >= 0.0151, in spite that their immediate accuracies (without finetuning) can be distinct (e.g., 91.35% at PR 0.1 vs. 9.74% at PR 0.7), intrinsically, they are equivalently potential after finetuning. By this \u201c*trainable with equivalent potential*\u201d rule, **if a mean JSV lies in the range of >=0.0151, we can regard it as \u201cclose to 1\u201d because they can do just as well as 1** (we do not need to pay as much attention to what is the upper limit of mean JSV in defining \"close to 1\", because in practice, a normally trained network *rarely* present a *very large* mean JSV by our observation, but is *quite likely* to have a *very small* mean JSV (0.0004 at PR 0.9 in Tab. #1 is an example).\n\nThen, for Fig. 2 in our paper, the most important point we want to convey is actually in the *early stage (epoch < 10)*. We have realized that Fig. 2 is not well-plotted in a proper scale, so here we use a table instead, which should be more straightforward to see:\n\n\n\n\n\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "VY05N90k-9U",
        "original": null,
        "number": 9,
        "cdate": 1637370525316,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637370525316,
        "tmdate": 1637680166679,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "F4vSYSfb6N6",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "[Response to All Reviewers] How to properly look at the mean JSV metric for dynamical isometry (2/2)",
          "comment": "\n **Tab. #2. Mean JSV of the first 10 epochs with different settings. Epoch 0 refers to the model just pruned, before any finetuning**\n\n| Epoch \t| 0 \t| 1 \t| 2 \t| 3 \t| 4 \t| 5 \t| 6 \t| 7 \t| 8 \t| 9 \t| 10 \t|\n|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|\n| LR=0.01, w/ OrthP \t| 1.0000 \t| 1.2318 \t| 1.4144 \t| 1.4277 \t| 1.4017 \t| 1.4709 \t| 1.5171 \t| 1.5551 \t| 1.6082 \t| 1.6538 \t| 1.6648 \t|\n| LR=0.001,  w/ OrthP \t| 1.0000 \t| 1.5135 \t| 1.6630 \t| 1.7449 \t| 1.8250 \t| 1.8720 \t| 1.9193 \t| 1.9556 \t| 1.9943 \t| 2.0084 \t| 2.0409 \t|\n| LR=0.01, w/o OrthP \t| 0.0004 \t| 0.6557 \t| 0.8946 \t| 1.0191 \t| 0.9826 \t| 1.0965 \t| 1.1253 \t| 1.2595 \t| 1.3298 \t| 1.2940 \t| 1.4238 \t|\n| LR=0.001, w/o OrthP \t| 0.0004 \t| 0.0004 \t| 0.0006 \t| 0.0014 \t| 0.1103 \t| 0.2765 \t| 0.3501 \t| 0.4320 \t| 0.5167 \t| 0.7478 \t| 0.8501 \t|\n\n**Tab. #3. Test accuracy of the first 10 epochs with different settings. Epoch 0 refers to the model just pruned, before any finetuning**\n\n| Epoch \t| 0 \t| 1 \t| 2 \t| 3 \t| 4 \t| 5 \t| 6 \t| 7 \t| 8 \t| 9 \t| 10 \t|\n|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|\n| LR=0.01, w/ OrthP \t| 9.74 \t| 91.05 \t| 91.39 \t|91.33 \t|91.37 \t|91.74 \t| 91.69 \t| 90.74 \t| 91.39 \t|91.58 \t|91.44 \t|\n| LR=0.001,  w/ OrthP \t| 9.74 \t|90.81 \t|91.59 \t|91.77 \t|91.85 \t|92.04 \t|92.12 \t|92.22 \t|92.12 \t|92.33 \t|92.25 \t|\n| LR=0.01, w/o OrthP \t| 9.74 \t| 63.86 \t| 79.96 \t| 79.74 \t| 80.06 \t|85.79 \t|85.82 \t|86.11 \t|86.45 \t|86.53 \t| 85.95 \t|\n| LR=0.001, w/o OrthP \t| 9.74 \t| 9.74 \t| 9.74 \t| 12.09 \t|21.74 \t|27.95 \t|33.55 \t|35.92 \t|49.19 \t|65.50 \t|69.90 \t|\n\nAs seen in Tab. #2, without OrthP, the mean JSV of LR 0.001 arises very slowly compared to LR 0.01. With OrthP, the mean JSV of LR 0.001 is not much different from LR 0.01 (note their mean JSV starting point is already 1 because OrthP can recover the pruned model to exact isometry). *Particularly note how the mean JSV trend in Tab. #2 correlates with test accuracy trend in Tab. #3*. This apparent correlation is another reason we adopt mean JSV for investigation.\n\n---\nThe mean JSV scale can vary case by case (up to the specific network, dataset, training optimizer, etc.) and there is no outstanding theory to quantify it. Therefore, looking at it properly really *needs some rule-of-thumb here*, e.g., for mean JSV = 2 vs. 0.001, we are pretty sure the former is probably better; but for mean JSV = 5 vs. 2, we cannot really have any reliable conclusion about which is better (in this sense, `R4` asked \"*can increasing mean JSVs from ~1 to ~5 (as in Fig 2) be deemed meaningful?*\", our answer is also *no*. Yet, in the early phase of Fig. 2, mean JSV arises from 0.0004 to around 1. This can be fairly deemed meaningful). \n\nHope our empirical study above can help reviewers understand how the mean JSV can be an (unperfect but still useful) proxy for dynamical isometry and especially how to look at the mean JSV improvement at a proper scale. Since this is an empirical study, not a proven theory, if any reviewer has further questions concerning it, we are more than happy to take them and see if we can have a more faithful metric to measure dynamic isometry and further improve our work.\n\n "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "IXfEyI0vgDI",
        "original": null,
        "number": 10,
        "cdate": 1637371620914,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637371620914,
        "tmdate": 1637518770211,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "LdBwgvSpwzf",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Responses to Reviewer nWUS (R2)",
          "comment": "Thank R2 for your very helpful comments! We address your concerns as follows.\n\n`R2-Weakness1`: lack of novelty: connection to DI and pruning breaks DI is known [Lee et al 2020, analysed at initialization] and analysing for a pretrained network is straightforward and claiming it as a contribution is weak.\n\n`R2-Response1`: Admittedly, we are using the *same tool* as  (Lee et al., 2020) -- dynamical isometry. But this does not mean \"the contribution is weak\". What matters more may not be what tool we use *but what we use it for*. In this regard, (Lee et al., 2020) used it for a *completely different* goal from ours: They used DI for proposing a better *pruning criterion* for pruning at initialization (trying to resolve that problem of over-pruning a layer in the global pruning ratio setting) while we use it to analyze how fine-tuning LR makes a difference in the pruning performance and further attempt to respond to the \"no value of pruning\" argument (Liu et al., 2019) (Crowley et al., 2018). Clearly, **we are towards completely different goals**. What you suggest is that the 1st difference (\"Basic setting\") we list in the paper is weak, yet we still have *3 more differences* (see the Related Work section on Page 3 \"Pruning at initialization (PaI)\"), which should sufficiently differentiate this paper from (Lee et al., 2020). Thus, we believe it is still fair for us to claim a contribution.\n\n`R2-Weakness2`: The hypothesis on page 7 connecting larger LR in fine-tuning and DI is dubious and not backed by any theory or experiments. In fig.2 plots mean JSV increases and that does not mean improved DI. As DI theory suggests to have mean JSV around 1.\n\n`R2-Response2`: Please see our response `How to properly look at the mean JSV metric for dynamical isometry` above, where we define how large of the mean JSV can be regarded as \"around 1\" for the MLP-7-Linear network. By the definition, the mean JSV range in Fig. 2 (2~5) can still be regarded as \"around 1\".\n\n`R2-Weakness3`: The comparison against scratch does not seem fair. Usually, fine-tuning phase is much shorter compared to training from scratch but in this case fine-tuning is done for almost the same no of epochs as the scratch version. Is it possible that the scratch version could be tuned to improve the performance?\n\n`R2-Response3`: \"Usually, fine-tuning phase is much shorter compared to training from scratch\" -- with all due respect, we think this statement may not be true either theoretically or practically:\n* Theoretically, there is no outstanding theory that demands the fine-tuning process *must* be short (`R2` may suggest specific references if you disagree). On the contrary, as a paper attempting to understand pruning more theoretically, this work actually suggests the *opposite* -- we should do a *long* finetuning because the broken dynamical isometry needs more training iterations to recover.\n* Practically, a short fine-tuning phase is not \"usual\" anymore. This was indeed the case for early pruning papers (such as the original L1-norm pruning paper (Li et al., 2017), which only used 20-epoch finetuning). But **more and more recent pruning methods actually prefer a *long* fine-tuning**: [*1]: 60 epochs, [*2]: 100 epochs, [*3]: 90 epochs, [*4]: 150 epochs, [*5]: 180 epochs.\nIn fact, few of these papers explained why they need such a long fine-tuning. The finding in this paper can be a good principle to explain why they chose to do so. \n\nAs for \"*Is it possible that the scratch version could be tuned to improve the performance?*\", Technically, it is always possible to get a better performance for the scratch version by adding more tricks (like cosine LR schedule). The point here is, we have to keep *fair* comparison. In this regard, the training strategy we adopt follows the standard [Pytorch ImageNet example](https://github.com/pytorch/examples/blob/master/imagenet/main.py), which is the *most standard* ImageNet training scheme. Also, this strategy was adopted by (Liu et al., 2019).\n\n`R2-Weakness4`: In section 4, the L2 regularization of pruned weights is regarded as a method for DI recovery which in my opinion is unsubstantiated.\n\n`R2-Response4`: Thanks for pointing out that it is not straightforward to understand StrongReg as a DI recovery method. Meanwhile, we also hope R2 can specifically point out *what makes you think so*. Besides, R1 also has a similar question. You may see our response `R1-Response3` above and see if it can lift your concern.\n\n--\n* [*1] Discrimination-aware channel pruning for deep neural networks, 2018, NeurIPS \n* [*2] Collaborative Channel Pruning for Deep Networks, 2019, ICML \n* [*3] Neural pruning via growing regularization, 2021, ICLR \n* [*4] Towards Compact CNNs via Collaborative Compression, 2021, CVPR \n* [*5] ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting, 2021, ICCV \n\nHope our response can help clear your concerns. **If you have further questions regarding our feedback, just let us know!**\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "fihKSr7ik51",
        "original": null,
        "number": 11,
        "cdate": 1637468819943,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637468819943,
        "tmdate": 1637654680055,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "6sA-fgy87ZM",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Responses to Reviewer bhQQ (R1) -- Part 2",
          "comment": "* **Intuition behind StrongReg**: Dynamical isometry describes a nice state of the network that signals can propagate through it without serious magnitude explosion or attenuation. Since the weights in a network are *dependent* on each other, removing some of them will definitely hurt the isometry because it is based upon *all* the weights. When we use a strong regularization to push these unimportant parameters to zero, it explicitly makes the other parameters learn to *not rely on them*, that is, encouraging the gradients not to pass through these weights/neurons because they are going to be cleaned out. This way, when the unimportant parameters are physically removed, it will incur much less damage to the left parameters, thus maintaining the network dynamical isometry well. \n* **Effectiveness on MLP-7-Linear**: We apply StrongReg to MLP-7-Linear at both PR 0.8 and PR 0.9. Results are presented below.\n\n|  \t|  PR 0.8 \t| PR 0.9 \t|\n|---\t|---\t|---\t|\n| L1 pruning, 90 epochs, LR 0.01\t    | 91.36 (0.02)\t| 87.81 (0.03) |\n| L1 pruning, 90 epochs, LR 0.001\t    | 90.54 (0.02)\t| 87.59 (0.01) |\n| L1 pruning, OrthP, 90 epochs, LR 0.01 | 92.79 (0.03)  | 92.77 (0.04) |\n| L1 pruning, OrthP, 90 epochs, LR 0.001 | 92.77 (0.03)  | 92.72 (0.03) |\n| L1 pruning, *Strong Reg*, 90 epochs, LR 0.01  | 92.80 (0.02) \t| 92.52 (0.04) \t|\n| L1 pruning, *Strong Reg*, 90 epochs, LR 0.001  | 92.80 (0.04)\t| 92.48 (0.02) \t|\n\nAs seen, similar to OrthP, StrongReg can rectify the test accuracy from the underrated ones to 92.48~92.80, close to the best possible performance (around 92.77). The left marginal gap at PR 0.9 may be because OrthP is the proven optimal isometry recovery method, while StrongReg is not mathematically optimal. But on the whole, StrongReg delivers very close performance to OrthP, demonstrating its effectiveness.\n\nTo further show StrongReg really works by *maintaining dynamical isometry*, like Tab. #2, #3 above, we list the first 10-epoch mean JSV and test accuracies (at PR 0.9) and get the following summarized results:\n\n**Tab. R1-#2: Mean JSV of the first 10 epochs with different settings. Epoch 0 refers to the model just pruned, before any finetuning**\n\n| Epoch \t| 0 \t| 1 \t| 2 \t| 3 \t| 4 \t| 5 \t| 6 \t| 7 \t| 8 \t| 9 \t| 10 \t|\n|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|\n| LR=0.01, w/ OrthP \t| 1.0000 \t| 1.2318 \t| 1.4144 \t| 1.4277 \t| 1.4017 \t| 1.4709 \t| 1.5171 \t| 1.5551 \t| 1.6082 \t| 1.6538 \t| 1.6648 \t|\n| LR=0.001,  w/ OrthP \t| 1.0000 \t| 1.5135 \t| 1.6630 \t| 1.7449 \t| 1.8250 \t| 1.8720 \t| 1.9193 \t| 1.9556 \t| 1.9943 \t| 2.0084 \t| 2.0409 \t|\n| LR=0.01, w/o OrthP \t| 0.0004 \t| 0.6557 \t| 0.8946 \t| 1.0191 \t| 0.9826 \t| 1.0965 \t| 1.1253 \t| 1.2595 \t| 1.3298 \t| 1.2940 \t| 1.4238  |\n| LR=0.001, w/o OrthP \t| 0.0004 \t| 0.0004 \t| 0.0006 \t| 0.0014 \t| 0.1103 \t| 0.2765 \t| 0.3501 \t| 0.4320 \t| 0.5167 \t| 0.7478 \t| 0.8501 \t|\n| LR=0.01, *w/ StrongReg* \t| 3.0275 \t| 2.1538 \t| 2.0390 \t| 1.0191 \t| 1.9696 \t| 2.1011 \t| 1.9734 \t| 2.0810 \t| 2.0541 | 2.0563 | 2.0581 |\n| LR=0.001, *w/ StrongReg* \t| 3.0275\t| 2.9691    | 2.9514    | 2.9921    | 2.9869    | 3.0101    | 3.0448    | 3.0516    | 3.0555 | 3.0441 | 3.0244 |  \n\n\n**Tab. R1-#3: Test accuracy of the first 10 epochs with different settings. Epoch 0 refers to the model just pruned, before any finetuning**\n\n| Epoch \t| 0 \t| 1 \t| 2 \t| 3 \t| 4 \t| 5 \t| 6 \t| 7 \t| 8 \t| 9 \t| 10 \t|\n|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|\n| LR=0.01, w/ OrthP \t| 9.74 \t| 91.05 \t| 91.39 \t|91.33 \t|91.37 \t|91.74 \t| 91.69 \t| 90.74 \t| 91.39 \t|91.58 \t|91.44 \t|\n| LR=0.001,  w/ OrthP \t| 9.74 \t|90.81 \t|91.59 \t|91.77 \t|91.85 \t|92.04 \t|92.12 \t|92.22 \t|92.12 \t|92.33 \t|92.25 \t|\n| LR=0.01, w/o OrthP \t| 9.74 \t| 63.86 \t| 79.96 \t| 79.74 \t| 80.06 \t|85.79 \t|85.82 \t|86.11 \t|86.45 \t|86.53 \t| 85.95 \t|\n| LR=0.001, w/o OrthP \t| 9.74 \t| 9.74 \t| 9.74 \t| 12.09 \t|21.74 \t|27.95 \t|33.55 \t|35.92 \t|49.19 \t|65.50 \t|69.90 \t|\n| LR=0.01, *w/ StrongReg* \t| 89.13 | 91.68 | 91.11 | 91.37 | 91.84 | 91.47 | 90.11 | 91.32 | 90.78 | 91.55 | 91.29 |\n| LR=0.001, *w/ StrongReg* \t| 89.13 | 92.03 | 91.84 | 92.13 | 92.05 | 91.85 | 92.00 | 92.02 | 92.09 | 92.08 | 92.08 |\n\nAs seen, (1) First, at Epoch 0, the mean JSV is around 3 (which can be considered close to the exact isometry 1, by our analysis in `How to properly look at the mean JSV metric for dynamical isometry`) vs. 0.0004 when the dynamical isometry is not recovered. (2) Second, the test accuracy also has a high starting point: at Epoch 1, using StrongReg achieves 91.68 (LR 0.01) and 92.03 (LR 0.001) vs. 63.86 (LR 0.01) and 9.74 (LR 0.001) when the dynamical isometry is not recovered. In short, StrongReg plays a similar role to OrthP in maintaining dynamical isometry as well as test accuracy.\n\nHope these new results can help you understand why StrongReg is a method to maintain dynamical isometry. **If you have further questions regarding our feedback, just let us know!**\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "kQ7s0yHmIVF",
        "original": null,
        "number": 12,
        "cdate": 1637468873659,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637468873659,
        "tmdate": 1637468890262,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "6sA-fgy87ZM",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Responses to Reviewer bhQQ (R1) -- Part 1",
          "comment": "Thank R1 for your very helpful comments! We address your concerns as follows.\n\n`R1-Weakness1`: Pruning as poor initialization: the concept of initialization needs to be cleared. Does the pruning in Fig 1 refer to weights pruning for both networks? It might be true if it is weights pruning as different pruning ratios correspond to the same weights dimensions with different values. However, filter pruning results in different architectures and different pruning results are not different initializations with respect to the same architecture.\n\n`R1-Response1`: We only investigate filter pruning in this paper, so in Fig. 1, the pruning scheme refers to filter pruning. To prevent misunderstanding, we try to reiterate your concern here: *The weights after filter pruning at different pruning ratios are not different initializations with respect to the same architecture because filter pruning results in different architectures (specifically, layer widths)*. Filter pruning indeed leads to different widths, yet this is *not* contradicted with our \"pruning as initialization\" perspective: If we see filter pruning as zeroing out the full network instead of physically taking away the filters, then different pruning ratios correspond to different initialized weights (with different portions of zeros) *for the same architecture*. In this sense, pruning as initialization still holds even when we are dealing with filter pruning rather than weight element-wise pruning in this paper.\n\n`R1-Weakness2`: The dynamic isometry might explain the easiness of optimization, however, the relationship with the generalization is not quite clear. There might be other metrics such as flatness might show a similar trend as JSV. A comparison with other metrics for generalization could be more persuasive.\n\n`R1-Response2`: Great thanks for pointing out flatness as a potential metric for our investigation! Currently, in our mind, we only have the following way to utilize flatness: plot the loss contours of original unpruned model, pruned model (without finetuning), pruned model (after OrthP, without finetuning), pruned model with finetuning. Specifically, we are referring to the flatness visualization tool from the paper \"Visualizing the Loss Landscape of Neural Nets\" [2018, NeurIPS]. Yet, we do not find clear patterns from the plots. We'll keep exploring this direction in the future. Meanwhile, if the above way to utilize flatness is not what you actually meant, you may let us know what you think could be a better way to use the flatness metric.\n\nBesides, to further resolve your concern, note that, in today's neural networks, with abundant regularization techniques, generalization is generally well-aligned with optimization. One concrete example is, in Tab. #2 and #3 above (`[Response to All Reviewers] How to properly look at the mean JSV metric for dynamical isometry -- Part 2`), when mean JSV is low (implying a bad optimization), the *test* accuracy usually is low too (implying a bad generalization). Therefore, our current analyses in the paper should still be valid.\n\n`R1-Weakness3`: The analysis with MLP-7 and MNIST might be too simple for practical usage, and this method does not generalize to more practical non-linear convolutional neural networks. The authors propose strong regularization helps, however, no explanation on why this method helps and how it connects with previous analysis. Does this method work for MLP-7 and how is it connected to DI?\n\n`R1-Response3`: Thank R1 for letting us know it is not straightforward to see the connection between StrongReg and DI. Here we first explain its intuition behind and further show its effectiveness on MLP-7-Linear:"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "Ikn4F6SvA3",
        "original": null,
        "number": 13,
        "cdate": 1637516625411,
        "mdate": 1637516625411,
        "ddate": null,
        "tcdate": 1637516625411,
        "tmdate": 1637516625411,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "YMX00Wvatt",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Responses to Reviewer YYie (R4) -- Part 3",
          "comment": "`R4-Weakness4`: Relationship between mean JSVs and convergence unclear.\n\n`R4-Response4`: \nWe use mean JSV because of the 3 reasons we explain in `R4-Response3`. We also think *increasing mean JSVs from ~1 to ~5 (as in Fig 2)* cannot be deemed meaningful. The point of Fig. 2 is in the early training stages. This figure is not well-plotted, so we have shown it in Tab. #2, #3 (see `[Response to All Reviewers] How to properly look at the mean JSV metric for dynamical isometry`), which should be more straightforward.\n\nIn principle, mean JSV does not imply convergence itself, just implying the *easiness of convergence*. We actually can intentionally set it to any positive number we want (just like above in `R4-Response3`, we can set it to 3 manually). But for a *normally* trained network, we do find mean JSV correlates with convergence or generalization especially for the early training stages as we show in `[Response to All Reviewers] How to properly look at the mean JSV metric for dynamical isometry`. \n\nA good metric indeed needs more deliberation. Right now, we do not have a better alternative, and considering it is still fairly useful and used by previous works, we hope R4 can see our efforts here. We are definitely more than happy to include better metrics (if we find them) to re-evaluate our hypotheses.\n\nHope our response can help clear your concerns. **If you have further questions regarding our feedback, just let us know!**"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "bJqmK6d-YLs",
        "original": null,
        "number": 14,
        "cdate": 1637517161318,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637517161318,
        "tmdate": 1637519696541,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "YMX00Wvatt",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Responses to Reviewer YYie (R4) -- Part 2",
          "comment": "`R4-Weakness3`: Incorrect metric for measuring Dynamical Isometry.\n\n`R4-Response3`: \"why the paper chose to use the mean JSV metric?\" -- We have the following 3 specific reasons for using the mean JSV as metric: \n* (1) It was used by a previous related work (Lee et al., 2020) (the differences between our paper and theirs are discussed in our related work section) and presented as an informative metric tool. \n* (2) We actually do not have a better metric in our mind either. \n* (3) In Tab. #2, #3 above (see `[Response to All Reviewers] How to properly look at the mean JSV metric for dynamical isometry`), the mean JSV correlates very well with the test accuracy in the early training stages. \n\nAs for the new metric (mean deviation-from-unity) suggested by R4, first, we exlain why this new metric may not be better than mean JSV. Second, as suggested, we rerun the experiments using this new metric.\n\n* First, consider this case: we have two initialization schemes for a neural network, *A: mean JSV = 0* (this can be done by removing one internal layer; since the network is disconnected, its mean JSV is definitely 0), *B: mean JSV = 3* (this can be done by re-scaling the weights in each layer by $3^{1/7}$ after using the orthogonal initialization, for the MLP-7-Linear). Then the former has mean deviation-from-unity of 1, the latter has mean deviation-from-unity of 4. By the metric, the former should be better, but clearly it is *not* since the network is not trainable at all while the latter is trainable -- we empirically confirmed this with pytorch on the MLP-7-Linear network. This is why we think the metric may not be better than mean JSV. \n* Second, we rerun the experiments using this new metric. Results on the MLP-7-Linear at PR 0.9 are reported below:\n\n\n**Tab. R4-#1. *Mean deviation-from-unity* of the first 10 epochs with different settings. Epoch 0 refers to the model just pruned, before any finetuning**\n\n| Epoch \t| 0 \t| 1 \t| 2 \t| 3 \t| 4 \t| 5 \t| 6 \t| 7 \t| 8 \t| 9 \t| 10 \t|\n|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|\n| LR=0.01, w/ OrthP \t| 0 \t| 0.3917 \t| 0.4850 \t| 0.6278 \t| 0.7665 \t| 0.7573 \t| 1.1150 \t| 1.3016 \t| 1.0021 \t| 1.0641 \t| 1.0292 \t|\n| LR=0.001,  w/ OrthP \t| 0 \t| 0.6068 \t| 0.8588 \t| 1.0114 \t| 1.1519 \t| 1.3061 \t| 1.4082 \t| 1.4665 \t| 1.5470 \t| 1.6939 \t| 1.7711 \t|\n| LR=0.01, w/o OrthP \t| 0.9992 \t| 1.6228 \t| 2.0774 \t| 2.5022 \t| 2.8385 \t| 2.4686 \t| 2.7849 \t| 2.8000 \t| 2.7692 \t| 2.8321 \t| 3.2931 \t|\n| LR=0.001, w/o OrthP \t| 0.9992 \t| 0.9992 \t| 0.9988 \t| 0.9972 \t| 0.9000 \t| 1.1856 \t| 1.4871 \t| 1.9261 \t| 2.4667 \t| 2.8044 \t| 3.1260 \t|\n\n**Tab. R4-#2. Test accuracy of the first 10 epochs with different settings. Epoch 0 refers to the model just pruned, before any finetuning**\n\n| Epoch \t| 0 \t| 1 \t| 2 \t| 3 \t| 4 \t| 5 \t| 6 \t| 7 \t| 8 \t| 9 \t| 10 \t|\n|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|\n| LR=0.01, w/ OrthP \t| 9.74 \t| 91.05 \t| 91.39 \t|91.33 \t|91.37 \t|91.74 \t| 91.69 \t| 90.74 \t| 91.39 \t|91.58 \t|91.44 \t|\n| LR=0.001,  w/ OrthP \t| 9.74 \t| 90.81 \t| 91.59 \t|91.77 \t|91.85 \t|92.04 \t|92.12 \t|92.22 \t|92.12 \t|92.33 \t|92.25 \t|\n| LR=0.01, w/o OrthP \t| 9.74 \t| 63.86 \t| 79.96 \t| 79.74 \t| 80.06 \t|85.79 \t|85.82 \t|86.11 \t|86.45 \t|86.53 \t| 85.95 \t|\n| LR=0.001, w/o OrthP \t| 9.74 \t| 9.74 \t| 9.74 \t| 12.09 \t|21.74 \t|27.95 \t|33.55 \t|35.92 \t|49.19 \t|65.50 \t|69.90 \t|\n\nAs seen in Tab. R4-#1, (1) the mean deviation-from-unity of using OrthP is smaller than not using, which is good since we know OrthP is useful; (2) but the problem is, the new metric does not align with the test accuracy. E.g., for each row in Tab. R4-#1, the mean deviation-from-unity is arising in general, which means bad, but the test accuracy is also arising, which means good. They are contradicting with each other. And particularly note the \"LR=0.001, w/o OrthP\" row, at Epoch 1~5, its test accuracy is very low. This correlates well with the low mean JSV metric (see Tab. #2 above);  but using mean deviation-from-unity, the metric value stays low at around 1, which should be regarded as good, but bad in reality.\n\nGiven above, the mean deviation-from-unity metric indeed has its merit, yet it may not be better than the mean JSV metric."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "0fGRCPJkMPj",
        "original": null,
        "number": 15,
        "cdate": 1637518346702,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637518346702,
        "tmdate": 1637596921334,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "YMX00Wvatt",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Responses to Reviewer YYie (R4) -- Part 1",
          "comment": "Thank Reviewer YYie (R4) for your very helpful comments and the generous credit to our nice analysis of previous papers! We address your concerns as follows. \n\n\n`R4-Weakness1`: Unfair experimental comparison.\n\n`R4-Response1`: What R4 suggests about the scratch training epochs has a point, yet with all due respect, we might not fully agree with it in terms of the \"fairness\". The reasons are:\n* The pretrained models are readily available in most practical applications. In fact, the task of pruning is mostly discussed in the context of *model compression*, where a large pretrained model typically exists already. We value the fairness in theory too but as a method for practical benefits, we should also consider the real application context.\n\n* Moreover, the large model is typically trained *once* and can be pruned into *multiple* small networks. For example, if we desire 5 efficient models with different sizes for devices with different resource budgets, then obtaining all the 5 efficient models using pruning needs $90+90\\times5=540$ epochs. In contrast, with the scratch scheme suggested by R4, it will need $180\\times5=900$ epochs. Then this would be unfair to pruning. There is no obvious reason that the \"fairness\" must be defined in the one-pruned-model case rather than the multiple-pruned-model case, especially when the latter is actually more practical and more useful.\n\n* We actually have done pretty much to maintain fairness. For example, the typical scratch results are obtained through the 90-epoch ImageNet training strategy (e.g., (Liu et al., 2019) uses this strategy). We particularly note this strategy is sub-optimal so upgrade it to the 120-epoch strategy. It improves the top-1 accuracy by 0.5~1% on average (which is a non-trivial improvement on ImageNet) against the 90-epoch scheme. The value of pruning is shown compared to these stronger baselines.\n\n`R4-Weakness2`: Unclear why hypothesis must be true.\n\n`R4-Response2`: We never claimed the proposed hypothesis *must* be true. We also understand a rigorous hypothesis needs particular deliberation and extensive empirical validation, so we are very careful with our wording in the paper. We actually only said the hypothesis is *plausible* (see \"This inspires us to the following plausible explanation...\" on page 7). By \"plausible\", we mean it makes some seemingly contradictory or random results (e.g., on the MLP-7-Linear, finetuning for 90 epochs, LR 0.01 is better than LR 0.001, while finetuning for 900 epochs, the inequation is turned over) now logically consistent and predictable.\n\nR4 states: \"In principle it should be clear that both two methods can produce models with similar generalization given *sufficiently strong optimizers*\", \"given that both methods solve the same overall optimization problem (i.e, minimizing loss of the small model), I am confused regarding why one would expect pruning + fine-tuning to generalize better than training from scratch, even in principle. For example, it is clear that given *infinite epochs and a suitable (well-tuned) optimizer* both methods should work equally well\". \n\nWe actually agree with R4 on all these \"in principle\" statements, yet we may want to emphasize we are dealing with *practical* systems. In practice, we may not have \"sufficiently strong optimizers\", \"infinite epochs\", etc. These conditions are attractive but they are non-trivial to have in practice. Specifically,\n\n* Solving \"the same overall optimization problem\" does not mean the two methods definitely reach similar performance in practice. E.g., for the same network training, different initialization schemes can lead to *starkly different* results (we particularly mention \"initialization\" as an example because pruning is essentially a way of initialization compared to scratch training). One concrete example is in Tab. R3-#2 and Tab. R3-#3 (see our response to R3 `R3-Response5`) above, kaiming_uniform and orthogonal initialization can lead to very different results (and thus conclusions). Note, kaiming_uniform is actually the default initialization scheme in pytorch. If we are not aware of the dynamical isometry issue discussed by our paper, we probably do *not even realize* it is a problem here, then we won't even try to solve it at all (via the longer training epochs, different initializations or stronger optimizers etc., even if these are practically available). \n\n* We agree with R4, in the ideal case (when the exact isometry is achieved), pruning and scratch training should lead to similar performance (you may see our response to R3 `R3-Response5`). However, for *practical* modern deep networks, exact isometry in initialization has not been achieved up to date. Then pruning still has a chance to be valuable."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "JNBVU3Tlcfh",
        "original": null,
        "number": 16,
        "cdate": 1637678983747,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637678983747,
        "tmdate": 1638067968539,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Thanks to All Reviewers! Draft Updated.",
          "comment": "Dear Reviewers,\n\nWe have updated the draft according to your comments. The changed parts are highlighted in **orange color** in the new draft pdf. There are the following main changes. All these changes are also in the response boxes below but more formally. All reviewers are *strongly* recommended to take a look.\n\n* Add a new section \"*How to Properly Look at the Mean JSV Metric for Dynamical Isometry*\" in Appendix D, since properly using mean JSV is a key to understanding this paper.\n* Remove the poorly-plotted Fig. 2 of mean JSV and test accuracy. Instead, use tables (Tabs. 4 and 5 on page 7) to present the first 10-epoch mean JSV and and test accuracy, where the correlation between mean JSV and test accuracy is more obvious. The analysis text and the boxed hypothesis are also updated accordingly, on page 7. (Fig. 2 is still kept in the Appendix, page 15, for reference)\n* Add a new section \"*Rectified Argument on Value of Pruning*\" (page 9) to address the newly-brought concern in Tab. 7, where the results show pruning has *no* value. We analyze the reason and update our claims on the value of pruning to a more rigorous version: \"`pruning has the potential to be valuable if the random initialization of scratch training cannot achieve exact isometry`\u201d. Three key takeaways are also presented.\n* Add a new section in Appendix E to clarify why the proposed StrongReg is related to dynamical isometry -- it works just like OrthP: (1) improves the mean JSV at the early stage of fine-tuning, (2) improves the test accuracy, (3) closes up the accuracy difference between LR 0.01 and 0.001.\n\nHope our efforts so far can help resolve your concerns! *We are more than happy to keep in communication with you in the following rebuttal period! Let us know if you have any further questions*. Thanks!\n\nSincerely,\n\nAuthors\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "qLaHLkc7gAt",
        "original": null,
        "number": 17,
        "cdate": 1637806231772,
        "mdate": 1637806231772,
        "ddate": null,
        "tcdate": 1637806231772,
        "tmdate": 1637806231772,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "IXfEyI0vgDI",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Still some concerns remain",
          "comment": "Thanks for the detailed response. Unfortunately, there are some concerns still remain.\n\n1. From the empirical analysis one cannot \"define\" that the large of the mean JSV can be regarded as \"around 1\". This statement is stretching to connect to the existing DI theory. I would strongly suggest rephrasing that \"in practice networks with JSV in the range (2-5) yield good empirical performance\".\n\n2. If a network from scratch can be tuned to match or exceed the performance of a pruned network, the 'value of pruning\" becomes questionable. This also relates to the \"Rectified Argument on Value of Pruning\". It seems the main message of the paper has changed during the rebuttal phase due to the new experiments.  \n\n3. Related to the above, \"pruning has the potential to be valuable if the random initialization of scratch training cannot achieve exact isometry\" what is meant by \"exact isometry\" in this statement? \n\n4. The L2 regularization does not directly incentives JSV to be close to 1, it simply encourages the magnitude of the weights to be small. From the experiments, if JSV becomes closer to 1 with strong L2 regularization, it is merely a side-effect. Further analysis is required to understand why is such behaviour emerging.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Reviewer_nWUS"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Reviewer_nWUS"
        ]
      },
      {
        "id": "g6DwZ65tmf",
        "original": null,
        "number": 18,
        "cdate": 1637997529957,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637997529957,
        "tmdate": 1638067990891,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "-kEDu9CDHa2",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Sincerely expecting further discussions with Reviewer VvbL (R3)",
          "comment": "Dear Reviewer VvbL,\n\nWe greatly thank you for the reviewing process so far! Given the ICLR final discussion deadline (11/29) is approaching, we *really* hope to have a further discussion with you to see if our responses solve your concerns. Thank you!!\n\nSincerely,\n\nAuthors\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "9KeQRYJjcFb",
        "original": null,
        "number": 19,
        "cdate": 1637997627691,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637997627691,
        "tmdate": 1638068011093,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "YMX00Wvatt",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Sincerely expecting further discussions with Reviewer YYie (R4)",
          "comment": "Dear Reviewer YYie,\n\nWe greatly thank you for the reviewing process so far! Given the ICLR final discussion deadline (11/29) is approaching, we *really* hope to have a further discussion with you to see if our responses solve your concerns. Thank you!!\n\nSincerely,\n\nAuthors"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "168KnlUGP3",
        "original": null,
        "number": 20,
        "cdate": 1638030343627,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638030343627,
        "tmdate": 1638134889972,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "qLaHLkc7gAt",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Responses to Left Concerns of Reviewer nWUS (R2)",
          "comment": "Dear Reviewer nWUS,\n\nThank you so much for the further comments! We have the following responses.\n\n1. We will rephrase it as suggested. Meanwhile, although the empirical study is not equivalent to a proven theory (which we admitted), yet, (1) notably, existing DI theory studies the *trainability/initialization* of a network, where researchers have the freedom to decide what should be the best value of mean JSV (which is 1, obviously), however, in this work, we use mean JSV to study the behaviors of the network *during training*, not initialization. Thus we need to *adapt* it for our use. It is actually the best we have right now to analyze a practical network during training (if R2 has a better way to define \"close to 1\" for a practical neural network during training, you may specifically let us know), (2) the empirical study is informative and consistent with our expectation, thus pertinent and useful to our analyses. \n\nWe are more than happy to make it more principled in the future, but considering this is still an early stage of a scientific exploration (our work is among the first one or two papers to examine network (filter) pruning via dynamical isometry), empirical studies are as important as theoretical proofs, too. Many theories start from empirical studies in the beginning. And note, the prior \"pruning has no value\" papers  (Liu et al., 2019) (Crowley et al., 2018) are also built upon empirical studies. \n\n2. R2 stated \"*It seems the main message of the paper has changed during the rebuttal phase due to the new experiments.*\" -- The main message \"pruning has value\" does *not* change *on the whole* (see `Response to Review VvbL (R3) -- Part 3`: *On the whole, we believe it is fair to say pruning has value considering the non-linear case is more practical*) even when we have the MLP-7-Linear scratch results because the network is too simple to be practical. What we do in the \"Rectified Argument on Value of Pruning\" is to make our message more *rigorous* (not \"changed\"), because clearly, without specific context, discussing pruning has value or not is meaningless (the former is closer to practical cases, though). **The more important may not be to have a \"one-sentence-fit-all\" conclusion about the value of pruning (we cannot either, actually, as we've shown), but to know what is the reason under the hood answering for the mixed and seemingly inconsistent results**. This can greatly help us understand pruning in a more principled way. In this regard, (Liu et al., 2019) (Crowley et al., 2018) are purely empirical studies, while our work presents a faithful framework via dynamical isometry to explain the mixed behaviors of pruning. We believe this can be fairly regarded as a non-trivial contribution. R1 also agrees with this point: *Though some issues exists and DI may not be the ultimate solution to the question, `the paper provides valuable thoughts for this filed`*.\n\n3. R2 stated: \"*pruning has the potential to be valuable if the random initialization of scratch training cannot achieve exact isometry\", what is meant by \"exact isometry\" in this statement?*\" -- We have this statement because of the MLP-7-Linear scratch results, where OrthP can achieve exact isometry.  The \"exact isometry\" is defined the usual way: mean JSV equals 1.\n\n4. R2 stated: \"*From the experiments, if JSV becomes closer to 1 with strong L2 regularization, it is merely a side-effect. Further analysis is required to understand why is such behaviour emerging.*\" -- Such behaviour emerging is because that network training itself can recover mean JSV (see page 7 of updated draft: \"*In Tab. 4, one important fact is that, the mean JSV can recover itself without any extra help during finetuning, regardless of different setups*\"). Just, in a normal manner of training, because of the weight dependency (as we explained in `R1-Response3`), the weight removal operation will damage the mean JSV anyway. Thus, we have to cut off this dependency. This is why we need StrongReg to peel off the weights that will be finally removed from the rest. Making them very small in magnitude is an effective way to cut off the dependancy. We believe such behavior is quite easy to understand with the above facts. \n\nMeanwhile, StrongReg serves as a starting point for inspiring more advanced DI recovery method. The major point of this paper is *not* DI recovery, but the proposed 4 hypotheses regarding the role of dynamcial isometry in filter pruning. This (plausible) theoretical-level understanding may be much more important than one specific DI recovery algorithm in our view. We hope R2 can pay more attention to our contribution there. Even if you think this paper may not be good enough for publication, if our responses so far help clear (some of) your concerns, could you kindly consider improving your score slightly?\n\nSincerely,\n\nAuthors"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "TmjV7MVWRiB",
        "original": null,
        "number": 21,
        "cdate": 1638113078177,
        "mdate": 1638113078177,
        "ddate": null,
        "tcdate": 1638113078177,
        "tmdate": 1638113078177,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "Ikn4F6SvA3",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Response",
          "comment": "`Weakness 1` I don't think practicality is an issue here, as we spend as much number of epochs pruning as we do training from scratch. For example, as in this paper, models are pruned for 90 epochs and the large model training itself takes 90 epochs. In any case, it is critical that the paper make the precise statement it intends to show or disprove. There is no mention of such practicality constraints in the paper as far as I can see. The hypothesis mentioned in section 2.1 is that \"Inheriting weights from a pretrained model in pruning has no value, i.e., training from scratch the small model can match (or outperform sometimes) the counterpart pruned from a big pretrained model\", which makes no statements regarding practicality.\n\nIt is also possible that the authors have misunderstood my argument. When 5 models are pruned in parallel, we would still need to compare against 180 epochs trained-from-scratch model as each such pruned model is only trained for 180 epochs each. The point is to equalize the number of optimizer iterations that these models have seen.\n\n`Weakness 2` I think the authors have misunderstood my point yet again. My point is that even if pruned models outperform models trained from scratch, under what conditions must we expect that to happen? For instance, if dynamical isometery is achieved, and one trains a model from scratch long enough, one would expect that pruned models work as well as models trained from scratch according to the ideal conditions. Again, I think one way to be clear about these is to **clearly** and **rigorously** state the hypothesis that the paper intends to prove or disprove, as claims regarding such optimizer practicality were never brought up in the original paper. Without this, one can very easily write endless follow up papers \"proving\" and \"disproving\" the same hypothesis. A clear hypothesis in your case would roughly state that \"under practical conditions such as blah blah, we expect pruning for x epochs to outperform training from scratch done for y epochs\".\n\n`Weakness 3` The argument that deviation-from-unity is not better than mean JSV is confusing. If dynamical isometery measures how close each of the spectral values are equal to one, then it is clear that deviation-from-unity measures **exactly** this phenomenon, which mean JSV misses. The authors bring up auxiliary issues that are thought to be related to the **outcomes** of dynamical isometery such as model performance and trainability, which is not the point at all. Nonetheless it is nice to see results with deviation-from-unity as well.\n\n`Weakness 4` Regarding mean JSV and convergence, my point is that there is no underlying theory connecting both concepts. The paper claims some correlations between both, but this means that high mean JSV need not imply good convergence and so on. \n\n> A good metric indeed needs more deliberation. Right now, we do not have a better alternative\n\nThere is indeed a perfectly good way to detect convergence, which I also mentioned earlier, which is to look at the loss gradients w.r.t. parameters, which must be equal to zero."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Reviewer_YYie"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Reviewer_YYie"
        ]
      },
      {
        "id": "f9HC26wk2Fa",
        "original": null,
        "number": 22,
        "cdate": 1638132737990,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638132737990,
        "tmdate": 1638133977071,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "TmjV7MVWRiB",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Thanks for Your Further Comments!",
          "comment": "`Weakness1`: *\"There is no mention of such practicality constraints in the paper as far as I can see\"* -- What we initially reported is the most typical pruning comparison setting. Since R4 mentioned the 180-epoch comparison setup in the first place, we point out this is *not* the typical way in the pruning community, hence the emphasis on practicality. The practicality consideration, (we thought), is obvious to see given pruning is a topic for improving DNN efficiency in real-world applications. Thanks for letting us know that this actually is not so obvious in our paper right now. We'll explicitly state we focus on the practicality of pruning algorithms and the \"value of pruning\" argument is treated under this practical context.\n\n\"*When 5 models are pruned in parallel, we would still need to compare against 180 epochs trained-from-scratch model as each such pruned model is only trained for 180 epochs each.*\" -- We think we understand this point correctly. This is why we mentioned if using the scratch training scheme suggested by R4, training 5 pruned models from scratch takes 180x5=900 epochs in total. \n\n\"*The point is to equalize the number of optimizer iterations that these models have seen.*\" -- We understand this and fully agree with it actually. Yet, this is exactly where we mentioned the necessity of considering practicality. We do not think this is the \"point\" (at least for this paper), because even under this setting scratch training is shown no worse than pruning, it answers little to the argument of \"value of pruning\" *in practice*. Again, we still see our paper as one focusing on the *practical* value of pruning in the real world.  This is why we keep emphasizing what we reported is the most practical or typical comparison setting in the community right now, also the typical way of pruning being applied in industry.\n\n`Weakness 2`: Thanks for this point! We fully agree with it. Even during this rebuttal, we also feel we are getting more precise about what should be rigorously stated. This is why we added the section ''Rectified Argument on Value of Pruning\" to make our statements more rigorous.\n\nMeanwhile, we hope R4 can see that compared to the previous \"rethinking the value of pruning\" papers (Liu et al., 2019) (Crowley et al., 2018), which are purely empirical studies, our work has presented a more principled framework via dynamical isometry to make the \"value of pruning\" argument *more rigorous* already. We believe this can be fairly regarded as a non-trivial contribution. R1 also agrees with this point: *\"Though some issues exists and DI may not be the ultimate solution to the question, `the paper provides valuable thoughts for this filed`\"*. So we sincerely hope R4 can see the value of this paper and kindly improve your score slightly even though you think it is not good enough for publication.\n\n`Weakness 3`: \"*If dynamical isometry measures how close each of the spectral values are equal to one, then it is clear that deviation-from-unity measures exactly this phenomenon, which mean JSV misses.*\" -- We also fully agree with this point. Again, this is where we emphasize practicality. Dynamical isometry is studied for the *trainability/initialization* of deep networks, where researchers have the freedom to decide what should be the best value of mean JSV (which is 1, obviously). However, when the network starts training, no one can ensure the mean JSVs always keep at 1 during training. R4's statement \"it is clear that deviation-from-unity measures exactly this phenomenon\" should be \"it is clear that deviation-from-unity measures exactly this phenomenon *at initialization*\", while our paper discusses the mean JSV  behaviors *during training*. This is also why the deviation-from-unity metric does not work well for our case, as we've shown.\n\n`Weakness 4`: \"*there is no underlying theory connecting both concepts*\" -- We fully agree that mean JSV has *no* relation with convergence (what we said is \"mean JSV implies the *easiness* of convergence\", not convergence per se). This is also why we report the *first 10-epoch* mean JSV and accuracies (in merely 10 epochs, the model is not converged at all). R4 may misread our responses. \n\n\"*There is indeed a perfectly good way to detect convergence, which I also mentioned earlier, which is to look at the loss gradients w.r.t. parameters, which must be equal to zero.*\" -- Thanks for bringing this up again. But notably, we are not needing a tool to detect *convergence* (even by looking at the learning curves, we can easily see if a model is converged or not). We need a tool to detect the *easiness* of convergence, which implies the potential of a model given N-epoch (like 90-epoch) finetuning. This is actually what mean JSVs faithfully describe, as we've shown in `How to properly look at the mean JSV metric for dynamical isometry` (better mean JSV, fewer epochs to reach accuracy like 92%) . \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "bha0c__ww0O",
        "original": null,
        "number": 23,
        "cdate": 1638144595724,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638144595724,
        "tmdate": 1638144697158,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "dw_b-uhNpNt",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Further Comments",
          "comment": "Thanks for experiments in Tab R3-#2 and Tab R3-#3. I found them very interesting. Based on the authors' claim, the \"value of pruning\" mainly lies in nonlinear networks, where initializing with \"pruning+strong reg\" may achieve better isometry than existing initialization methods. \n\nAs the authors later update their claim to be \"pruning will (probably) have no values\" in the linear case, the current paper arrangement becomes very confusing to me. I personally think it might be more appropriate to make \"strong reg\" the main contribution and take the linear case as a motivating observation.\n\nAlso, several new claims in Section 5 need more rigorous supports (also pointed out by other reviewers). For example, 1) more evidence on \"pruning+strong reg\" can achieve better isometry than existing initialization and pruning methods, and 2) more analysis on why \"pruning+strong reg\" achieves better isometry.\n\nWith these being said, I believe this paper has potential and is very interesting, but need further investigations on its claims. I will keep my original rating."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Reviewer_VvbL"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Reviewer_VvbL"
        ]
      },
      {
        "id": "TpiIFVh0Iq1",
        "original": null,
        "number": 24,
        "cdate": 1638218850553,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638218850553,
        "tmdate": 1638219109282,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "bha0c__ww0O",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Thanks for Your Further Comments!",
          "comment": "Dear Reviewer VvbL,\n\n*As the authors later update their claim to be \"pruning will (probably) have no values\" in the linear case, the current paper arrangement becomes very confusing to me.* -- Notably, we are updating our claim, making it more rigorous and specific, not *changing* it. Even without the MLP-7-Linear scratch results, in our original version, we still see examples against our \"pruning has value\" argument (e.g., on ImageNet, ResNet18, pruning ratio 0.9, 0.95, scratch is better than pruning). If our original paper arrangement is not \"very confusing\" (not mentioned previously), after adding the new section, it only makes our claim more specific and rigorous (not changing any of our major claims), thus should not be \"very confusing\" either.\n\n*I personally think it might be more appropriate to make \"strong reg\" the main contribution and take the linear case as a motivating observation* -- Although we still believe the major contribution of this paper is the analysis framework of dynamical isometry instead of StrongReg (note, only after we identify the dynamical isometry issue in filter pruning, then comes the necessity/legitimacy of dynamical isometry *recovery* by StrongReg. Currently, we are still at the \"identifying problem\" stage), we greatly thank you (and other reviewers) for the very constructive comments so far!  \n\nSincerely,\n\nAuthors"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "4tkIvTULlLU",
        "original": null,
        "number": 25,
        "cdate": 1638253437388,
        "mdate": 1638253437388,
        "ddate": null,
        "tcdate": 1638253437388,
        "tmdate": 1638253437388,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "f9HC26wk2Fa",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Thank you for the updates",
          "comment": "I appreciate the effort taken by the authors to update the paper to incorporate reviewer comments, and I will increase my score accordingly. However I still do not feel the paper makes rigorous arguments and experiments to back its core argument regarding the effectiveness of pruning, I still cannot recommend acceptance."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Reviewer_YYie"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Reviewer_YYie"
        ]
      },
      {
        "id": "bBSOzSh_t71",
        "original": null,
        "number": 26,
        "cdate": 1638282150197,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638282150197,
        "tmdate": 1638282386096,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "4tkIvTULlLU",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Thanks for Your Feedback",
          "comment": "We greatly thank R4 for your kind words and for improving the score. \n\nRegarding \"*I still do not feel the paper makes rigorous arguments and experiments to back its core argument regarding the effectiveness of pruning*\", we feel we may need to make a little further remarks. Note, this should *not* be taken as disagreeing with R4's opinion, just sharing a few of our thoughts.\n\nWe are desperate for \"rigorous\" (this is why when we have the MLP-7-Linear scratch results requested by R3, we immediately realize our claim should be updated further. Our efforts in the empirical study concerning how to define \"close to 1\" for dynamical isometry can be another example. To our best knowledge, no previous works have attempted to do so), but for any scientific exploration, there is a process to be rigorous step by step. Right now, many concepts are not clear enough, which we admit, and these are the open problems in the community. Yet, compared to previous works in this line (the two \"rethinking the value of pruning\" papers (Liu et al., 2019) (Crowley et al., 2018)), this work has become much more rigorous (simply put -- *Yes, it is not perfect, but getting better*). We hope this contribution can be fairly recognized. \n\n\"*Lack experiments to back its core argument regarding the effectiveness of pruning*\" -- Up to now, we already know we cannot really make any reliable comment regarding whether pruning has value unless we know what is the specific network/initialization scheme/pruning ratio etc. Yet, practically, the most straightforward way to evaluate this argument is to run experiments on large-scale dataset with modern networks. In this regard, even in our original paper, we have reported *extensive* experiments on ImageNet (Tab. 1, Tab. 2) with different network archs (residual network: ResNet18/34, and single-branch network: VGG11), across a large spectrum of pruning ratios (0.1 to 0.95). Results: **17/20 experiments** are against the argument in (Liu et al., 2019) (Crowley et al., 2018)). We believe this is pretty strong to support our \"pruning has value\" claim (note, logically, even one single counter-example is enough for refuting \"pruning has no value\"; right now, we have 17/20). \n\nFinally, this is what we responded to R2, which we think is also pertinent here: *the more important may not be to have a \"one-sentence-fit-all\" conclusion about the value of pruning (we cannot either, actually, as we've shown), but to know what is the reason under the hood answering for the mixed and seemingly inconsistent results*.\n\nSincerely,\n\nAuthors\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "pk1uEeM7vsUU",
        "original": null,
        "number": 1,
        "cdate": 1642696842329,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696842329,
        "tmdate": 1642696842329,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The paper's primary contributions are:\n* Contrary to previous claims, the authors empirically show that inheriting the weights after pruning can be beneficial when using *larger* fine-tuning learning rates than previously done.\n* As an explanation, the authors provide suggestive results showing that pruning breaks dynamical isometry, which they claim explains why larger learning rates are needed.\n* They propose a regularization-based technique to recover dynamical isometry on modern residual CNNs.\n\nGenerally, reviewers were positive about the ideas in the paper, however, even after the rebuttal 3/4 reviewers did not find the arguments were clear or strongly supported yet. One issue that came up several times is a request for more investigation of StrongReg+pruning. At this time, I have to recommend rejection, but I encourage the authors to follow up on the reviewers suggestions and submit to a future venue."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "YMX00Wvatt",
        "original": null,
        "number": 1,
        "cdate": 1635755973818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635755973818,
        "tmdate": 1638253670038,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This work takes a second look at the set of papers (Crowley et al 2018, Liu et al 2019) which claim that there is no generalization benefit offered by pruning, i.e., training a large network first and then pruning performs identically or worse than training the same pruned network architecture from scratch. This work claims that these previous works do not set learning rates during fine-tuning correctly while performing their experiments and hence these claims are invalid. The paper also offers an explanation for this phenomenon via dynamical isometery theory.",
          "main_review": "1) **Nice analysis of previous papers**: It is commendable that this paper points out issues with two previous papers (Crowley et al 2018, Liu et al 2019) who make methodological errors by not tuning the learning rate for fine-tuning, and use the results of these erroneous experiments to make general statements about the efficacy of pruning (regardless of whether these statements are true or not). However from the perspective of these previous papers, it seems they had a fixed small fine-tuning budget of 20 epochs, and did not want to exceed this as we require number of fine-tuning epochs to be much smaller than the number of training epochs. While I do think that this requirement is arbitrary, it helps understand the claims of the previous papers.\n\n2) **Unfair experimental comparison**: In Table 1, it seems that the small Resnet models (A,B) trained from scratch are trained for 120 epochs, while the best prune-finetuned models are trained for 90 epochs. However also accounting for the training of the original large Resnet-34 model, the total number of training epochs are 90+90=180 epochs. Hence it seems that the prune+finetuned models are trained overall for a larger number of epochs, which may be a confounding factor which explains their better results. I would *strongly* suggest that the experiments be run such that the number of training epochs for \"training small model from scratch\" and \"large model training with pruning and finetuning\" be equalized for a fair comparison.      \n\n3) **Unclear why hypothesis must be true**: This paper rest on the hypothesis that training a large model followed by pruning and fine-tuning offers improved generalization benefits over training a small model from scratch. In principle it should be clear that both two methods can produce models with similar generalization given sufficiently strong optimizers, as the only difference between them is the initialization of the small model. Even in practice, we often are able to train models such that the train loss is close to zero (or sufficiently small), regardless of whether we train small models from scratch or fine-tune pruned models.  \nHence given that both methods solve the same overall optimization problem (i.e, minimizing loss of the small model), I am confused regarding why one would expect pruning + fine-tuning to generalize better than training from scratch, even in principle. For example, it is clear that given infinite epochs and a suitable (well-tuned) optimizer both methods should work equally well. Is the paper's hypothesis a claim about the speed of optimization, that pruning+fine-tuning is a fast way to obtain a small model? If so, how does this change after having accounted for the training of the dense model itself (which is usually slower to train)?\n\n4) **Incorrect metric for measuring Dynamical Isometry**: In equation 1, the paper uses mean singular values as a metric for assessing dynamical isometry. However if we require that all singular values are equal to one, then the metric to use would be the sum of (squared) deviations from unity of each singular value (\\sum_i (\\Sigma_{ii} - 1)^2). In such a metric, zero corresponds to dynamical isometry being achieved. However the metric of mean singular values has no such meaning as the standard deviation can be large. I am hence confused regarding why the paper chose to use the mean JSV metric, and if there is no strong reason for doing so, I would *strongly* recommend re-running experiments with the deviations-from-unity metric.\n\n5) **Relationship between mean JSVs and convergence unclear**: Hypotheses 1-4 and the boxed explanation talk about the \"recovery\" of mean JSVs and their relationship to network convergence. I am confused regarding why the behaviour of mean JSVs during or after training is deemed meaningful. I understand that too small or too large mean JSVs must be avoided at initialization as they can cause vanishing or exploding gradients, but can increasing mean JSVs from ~1 to ~5 (as in Fig 2) be deemed meaningful? It is thus not clear why mean JSV (or any measure of dynamical isometry) can be used to measure model convergence (ref. to boxed explanation in page 7). For example, if gradient-norm regularization is used, then the model would converge and yet have small JSVs. The only measure of model convergence is the gradient norm of the loss w.r.t. weights, and other measures are meaningless unless explicitly shown otherwise. ",
          "summary_of_the_review": "This paper does not make a solid case for the hypothesis that \"pruning+finetuning produces models with better generalization than training small models from scratch\", as there are issues with both the experimental and the analytical studies. In the experimental setup, the comparisons are not performed fairly, while in the analytical parts use flawed metric to measure model convergence and dynamical isometry. Even fundamentally, it is unclear why the proposed hypothesis must be true in the first place. For all these reasons, I would recommend rejection.\n\nPost rebuttal update: The discussions did not change my opinion of the paper. However I appreciate the efforts taken by the authors to update the paper in light of reviewer suggestions, particularly to make the core argument more rigorous. Although I do not think the paper still succeeds at this, I am increasing my score slightly to reflect this change.",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Reviewer_YYie"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Reviewer_YYie"
        ]
      },
      {
        "id": "-kEDu9CDHa2",
        "original": null,
        "number": 2,
        "cdate": 1635801830055,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635801830055,
        "tmdate": 1635801830055,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper challenges an existing argument that \u201cinheriting the weights of the pruned network is not necessary for fine-tuning\u201d. This paper provides empirical evidence that the previous experiments are not carried out with proper learning rates and training epochs to ensure the network convergence. \n\nThe authors conjecture that fine-tuning pruned network requires larger learning rates and longer training epochs because its dynamical isometry is broken. As a result, large learning rates (if training for shorter epochs) or long training epochs are required for the recovery of dynamic isometry. \n\nThen the authors propose OrthP to re-initialize the pruned weights to completely recover the dynamic isometry in simple MLP. With OrthP, the fine-tuning process is less sensitive to different learning rate and training epochs. \n",
          "main_review": "Strength: \n\n1) This paper brings new insights for the role of the \u201cweights\u201d in the network pruning.\n2) This paper re-examines the established arguments with careful designed experiments over wide range of pruning ratios.\n3) The proposed technique is clearly motivated by empirical observations and is very effective on MLP networks under large pruning ratios.\n4) This paper is easy to follow.\n\nWeakness:\n\n1) Table 1,2 shows that fine-tuning a pruned network requires much more training time and tuning efforts to achieve a satisfying result. Although the author claims that OrthP can \u201ccomplete\u201d resolve the dynamical isometry issue, the pruned network converges still only after prolonged training epochs (90 and 900 epochs). Does that means the pruned networks weights are more difficult to optimize than random initialized ones, which contradicts the \"the value of weights\" claim?\n2) The application area of the proposed OrthP is limited to MLP (Table 6). Although the author proposed a Strong L2 Regularization for more complicated networks, this section seems to be \u201cunfinished\u201d \u2014 the connection with dynamical isometry seems a little bit weak to me, can the authors provides more elaborations? This seems to be more of practical interests.\n3) I have some confusions regarding the experiment designs and presented results (see the Questions section below). \n\nQuestions:\n\n1)  For Table 1: In \u201cOur rerun\u201d,  are \u201cScratch\u201d results trained under 120 epochs with a decayed LR schedule? For Table 2, what are the hyper-parameters for the \u201cScratch\u201d results, are they carefully tuned?\n2) Have the authors investigated the JSV of randomly initialized sparse networks? I am not sure if applying OrthP initialization or other orthogonal regularization methods on \u201cScratch\u201d can also boost its performance. I believe this would be a fairer comparison to support \"the value of weights\" claim.\n3) Table 5: Can the authors also provide the \u201cScratch\u201d results and their hyper-parameters? I am asking because the reasons stated in Weakness 1) \u2014 if the \u201cScratch\u201d result is comparable to OrthP without using prolonged epochs, then this means even when the dynamical isometry is recovered, the pruning selected weights need more training time to optimize than randomly initialized ones. Also, in Table 2, it looks like \u201cScratch\u201d results work well under large PR, which is exactly the case in Table 5.\n4) Related to 3), from Figure 2 and Table 5, it looks like OrthP takes the role of a large training epochs to recover dynamical isometry. It would be nice to see if OrthP works for non-prolonged epochs, e.g., < 90.",
          "summary_of_the_review": "I like this paper in that it thoroughly investigates existing arguments on the value of pruning weights. The perspective of OrthP is not that novel, but is simple and effective on MLP under large pruning ratios. \n\nMy major concern is in the experiments design and results part. I would consider raising my score if my questions can be addressed.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Reviewer_VvbL"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Reviewer_VvbL"
        ]
      },
      {
        "id": "LdBwgvSpwzf",
        "original": null,
        "number": 3,
        "cdate": 1635918954587,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635918954587,
        "tmdate": 1635918954587,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper argues pruning a pretrained network and finetuning is better than training a sparse network from scratch and tries to connect the fine-tuning phase properties to dynamical isometry (DI) literature. The empirical observation makes a case for inheriting the pretrained weights.",
          "main_review": "# Strengths\n\n1. The empirical observation that inheriting pretrained weights is better than training a sparse network from scratch if the fine-tuning phase is carefully tuned.\n2. The empirical analysis on fine-tuning hyperparameters (LR, #epochs) might be useful to practitioners.\n\n# Weaknesses\n\n1. lack of novelty: connection to DI and pruning breaks DI is known [Lee et al 2020, analysed at initialization] and analysing for a pretrained network is straightforward and claiming it as a contribution is weak.\n\n2. The hypothesis on page 7 connecting larger LR in fine-tuning and DI is dubious and not backed by any theory or experiments. In fig.2 plots mean JSV increases and that does not mean improved DI. As DI theory suggests to have mean JSV around 1.\n\n3. The comparison against scratch does not seem fair. Usually, fine-tuning phase is much shorter compared to training from scratch but in this case fine-tuning is done for almost the same no of epochs as the scratch version. Is it possible that the scratch version could be tuned to improve the performance?\n\n4. In section 4, the L2 regularization of pruned weights is regarded as a method for DI recovery which in my opinion is unsubstantiated.",
          "summary_of_the_review": "There are many unsubstantiated claims (or weak statements) in the paper. ",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Reviewer_nWUS"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Reviewer_nWUS"
        ]
      },
      {
        "id": "6sA-fgy87ZM",
        "original": null,
        "number": 4,
        "cdate": 1636303312964,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636303312964,
        "tmdate": 1636303312964,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Review",
        "content": {
          "summary_of_the_paper": "There are recent works questioning the value of inheriting weights in structured neural network pruning as it is empirically observed training from scratch can match or outperform finetuning a pruned model. This paper mainly includes three components: 1) the authors reinvestigated the problem and demonstrates that the conclusion is inaccurate because of improperly small finetuning learning rates. They show finetuning with pruned weights actually outperforms training from scratch, when larger learning rates and longer training epochs are adopted. 2)  the authors explored dynamical isometry (DI) to understand how finetuning LR affects the final performance. They show that weight pruning breaks dynamical isometry and finetuning can recover it and a larger LR can recover faster. 3) They proposed to fully recover dynamical isometry in fitler pruning before finetuning.\n",
          "main_review": "The paper is well written and organized. The effect of learning rate on fine-tuning stage of pruning is not thoroughly investigated and this paper provides an in-time and thorough study. My major concerns are as follows:\n\n- Pruning as poor initialization: the concept of initialization needs to be cleared. Does the pruning in Fig 1 refer to weights pruning for both networks? It might be true if it is weights pruning as different pruning ratios correspond to the same weights dimensions with different values. However, filter pruning results in different architectures and different pruning results are not different initializations with respect to the same architecture. \n\n- The dynamic isometry might explain the easiness of optimization, however, the relationship with the generalization is not quite clear. There might be other metrics such as flatness might show a similar trend as JSV. A comparison with other metrics for generalization could be more persuasive.\n\n- The analysis with MLP-7 and MNIST might be too simple for practical usage, and this method does not generalize to more practical non-linear convolutional neural networks. The authors propose strong regularization helps, however, no explanation on why this method helps and how it connects with previous analysis. Does this method work for MLP-7 and how is it connected to DI?",
          "summary_of_the_review": "The paper carefully studies recent thoughts on the meaning of pruning and made some rigorous investigations. Though some issues exists and DI may not be the ultimate solution to the question, the paper provides valuable thoughts for this filed, including fair experimental comparison and new explanations. Therefore I recommend to accept.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Reviewer_bhQQ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Reviewer_bhQQ"
        ]
      },
      {
        "id": "F4vSYSfb6N6",
        "original": null,
        "number": 8,
        "cdate": 1637369881198,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637369881198,
        "tmdate": 1637679729495,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "[Response to All Reviewers] How to properly look at the mean JSV metric for dynamical isometry (1/2)",
          "comment": "We use these abbreviations to mention different reviewers: `R1`: bhQQ, `R2`: nWUS, `R3`: VvbL, `R4`: YYie.\n\n**Thanks to all the reviewers for your constructive comments!** We find several reviewers are curious about how the mean JSV is related to dynamical isometry. Thanks for letting us know this critical concept is not clearly treated in our paper. Here we elaborate more. All the reviewers are strongly encouraged to look at this section.\n\nDI (dynamical isometry) is defined by mean JSV close to 1 in (Saxe et al., 2014) (we are aware that, rigorously in (Saxe et al., 2014), DI describes the distribution of *all* JSVs. Mean JSV is only an average sketch of the distribution. But this approximation is accurate enough for analysis, also used by (Lee et al., 2020) (in fact, we are directly inspired by (Lee et al., 2020) for using this metric). In other words, **if a network has mean JSV close to 1, we can say this network has dynamical isometry**.\n\nThen a non-trivial technical question is: **When we deal with practical DNNs in the real world, how close is the so-called \u201cclose to 1\u201d**? To our best knowledge, there is no outstanding theory to *quantify* this (if any reviewer disagrees, you may suggest specific references), so we resort to empirical analysis, specifically on the MLP-7-Linear network used in our paper.\n\n In Tab. #1 below, we present the mean JSV of MLP-7-Linear network on MNIST under different pruning ratios, along with their accuracies before and after fine-tuning.\n\n**Tab. #1. Mean JSV and accuracies of MLP-7-Linear on MNIST under different pruning ratios. Each result is randomly run for 3 times. We report the mean accuracy and (std).**\n\n| pruning ratio (PR) \t| 0 \t| 0.1 \t| 0.2 \t| 0.3 \t| 0.4 \t| 0.5 \t| 0.6 \t| 0.7 \t| 0.8 \t| 0.9 \t|\n|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|\n| mean JSV \t| 2.4987 \t| 1.7132 \t| 0.9993 \t| 0.5325 \t| 0.2711 \t| 0.1180 \t| 0.0452 \t| 0.0151 \t| 0.0040 \t| 0.0004 \t|\n| Accuracy before finetuning (%) \t| 92.77 \t| 91.35 \t| 78.88 \t| 62.21 \t| 32.14 \t| 11.47 \t| 9.74 \t| 9.74 \t| 9.74 \t| 9.74 \t|\n| Accuracy after finetuning (%) \t| / \t| 92.82 (0.05) \t| 92.80 (0.04) \t| 92.80 (0.01) \t| 92.77 (0.01) \t| 92.77 (0.02) \t| 92.77 (0.00) \t| 92.78 (0.02) \t| 91.37 (0.03) \t| 87.82 (0.03) \t|\n\nAs seen, there is a clear trend: larger pruning ratio, smaller mean JSV, lower accuracy (before or after finetuning). Particularly note *the mean JSV range where the pruned network can be finetuned back to the original accuracy (92.77%)*, which is **>=0.0151**. This means, for networks with mean JSV >= 0.0151, in spite that their immediate accuracies (without finetuning) can be distinct (e.g., 91.35% at PR 0.1 vs. 9.74% at PR 0.7), intrinsically, they are equivalently potential after finetuning. By this \u201c*trainable with equivalent potential*\u201d rule, **if a mean JSV lies in the range of >=0.0151, we can regard it as \u201cclose to 1\u201d because they can do just as well as 1** (we do not need to pay as much attention to what is the upper limit of mean JSV in defining \"close to 1\", because in practice, a normally trained network *rarely* present a *very large* mean JSV by our observation, but is *quite likely* to have a *very small* mean JSV (0.0004 at PR 0.9 in Tab. #1 is an example).\n\nThen, for Fig. 2 in our paper, the most important point we want to convey is actually in the *early stage (epoch < 10)*. We have realized that Fig. 2 is not well-plotted in a proper scale, so here we use a table instead, which should be more straightforward to see:\n\n\n\n\n\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "JNBVU3Tlcfh",
        "original": null,
        "number": 16,
        "cdate": 1637678983747,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637678983747,
        "tmdate": 1638067968539,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Official_Comment",
        "content": {
          "title": "Thanks to All Reviewers! Draft Updated.",
          "comment": "Dear Reviewers,\n\nWe have updated the draft according to your comments. The changed parts are highlighted in **orange color** in the new draft pdf. There are the following main changes. All these changes are also in the response boxes below but more formally. All reviewers are *strongly* recommended to take a look.\n\n* Add a new section \"*How to Properly Look at the Mean JSV Metric for Dynamical Isometry*\" in Appendix D, since properly using mean JSV is a key to understanding this paper.\n* Remove the poorly-plotted Fig. 2 of mean JSV and test accuracy. Instead, use tables (Tabs. 4 and 5 on page 7) to present the first 10-epoch mean JSV and and test accuracy, where the correlation between mean JSV and test accuracy is more obvious. The analysis text and the boxed hypothesis are also updated accordingly, on page 7. (Fig. 2 is still kept in the Appendix, page 15, for reference)\n* Add a new section \"*Rectified Argument on Value of Pruning*\" (page 9) to address the newly-brought concern in Tab. 7, where the results show pruning has *no* value. We analyze the reason and update our claims on the value of pruning to a more rigorous version: \"`pruning has the potential to be valuable if the random initialization of scratch training cannot achieve exact isometry`\u201d. Three key takeaways are also presented.\n* Add a new section in Appendix E to clarify why the proposed StrongReg is related to dynamical isometry -- it works just like OrthP: (1) improves the mean JSV at the early stage of fine-tuning, (2) improves the test accuracy, (3) closes up the accuracy difference between LR 0.01 and 0.001.\n\nHope our efforts so far can help resolve your concerns! *We are more than happy to keep in communication with you in the following rebuttal period! Let us know if you have any further questions*. Thanks!\n\nSincerely,\n\nAuthors\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper174/Authors"
        ]
      },
      {
        "id": "pk1uEeM7vsUU",
        "original": null,
        "number": 1,
        "cdate": 1642696842329,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696842329,
        "tmdate": 1642696842329,
        "tddate": null,
        "forum": "p4H9QlbJvx",
        "replyto": "p4H9QlbJvx",
        "invitation": "ICLR.cc/2022/Conference/Paper174/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The paper's primary contributions are:\n* Contrary to previous claims, the authors empirically show that inheriting the weights after pruning can be beneficial when using *larger* fine-tuning learning rates than previously done.\n* As an explanation, the authors provide suggestive results showing that pruning breaks dynamical isometry, which they claim explains why larger learning rates are needed.\n* They propose a regularization-based technique to recover dynamical isometry on modern residual CNNs.\n\nGenerally, reviewers were positive about the ideas in the paper, however, even after the rebuttal 3/4 reviewers did not find the arguments were clear or strongly supported yet. One issue that came up several times is a request for more investigation of StrongReg+pruning. At this time, I have to recommend rejection, but I encourage the authors to follow up on the reviewers suggestions and submit to a future venue."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}