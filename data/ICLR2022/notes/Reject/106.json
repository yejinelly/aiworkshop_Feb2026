{
  "id": "dDARN-TCiA",
  "original": "HvMjmrHi2AK",
  "number": 106,
  "cdate": 1632875429089,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875429089,
  "tmdate": 1676330688245,
  "ddate": null,
  "content": {
    "title": "Stochastic Reweighted Gradient Descent",
    "authorids": [
      "~Ayoub_El_Hanchi1",
      "~Chris_J._Maddison1",
      "david.stephens@mcgill.ca"
    ],
    "authors": [
      "Ayoub El Hanchi",
      "Chris J. Maddison",
      "David Alan Stephens"
    ],
    "keywords": [
      "Stochastic gradient descent",
      "Finite-sum optimization",
      "Variance reduction",
      "Importance sampling"
    ],
    "abstract": "Importance sampling is a promising strategy for improving the convergence rate of stochastic gradient methods. It is typically used to precondition the optimization problem, but it can also be used to reduce the variance of the gradient estimator. Unfortunately, this latter point of view has yet to lead to practical methods that  improve the asymptotic error of stochastic gradient methods. In this work, we propose stochastic reweighted gradient (SRG), a variance-reduced stochastic gradient method based solely on importance sampling that can improve on the asymptotic error of stochastic gradient descent (SGD) in the strongly convex and smooth case. We show that SRG can be extended to combine the benefits of both importance-sampling-based preconditioning and variance reduction. When compared to SGD, the resulting algorithm can simultaneously reduce the condition number and the asymptotic error, both by up to a factor equal to the number of component functions. We demonstrate improved convergence in practice on $\\ell_2$-regularized logistic regression problems.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "hanchi|stochastic_reweighted_gradient_descent",
    "pdf": "/pdf/abd946d638111f1056c3f3d0411781c549ce157a.pdf",
    "one-sentence_summary": "We introduce the first importance-sampling-based variance reduction algorithm for finite-sum optimization with convergence rate guarantees under standard assumptions.",
    "supplementary_material": "/attachment/db505f90cec5978f222c8100e72ab993bf090f0d.zip",
    "_bibtex": "@misc{\nhanchi2022stochastic,\ntitle={Stochastic Reweighted Gradient Descent},\nauthor={Ayoub El Hanchi and Chris J. Maddison and David Alan Stephens},\nyear={2022},\nurl={https://openreview.net/forum?id=dDARN-TCiA}\n}",
    "venue": "ICLR 2022 Submitted",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "dDARN-TCiA",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 17,
    "directReplyCount": 6,
    "revisions": true,
    "replies": [
      {
        "id": "4rAJlxML4l",
        "original": null,
        "number": 1,
        "cdate": 1635808872852,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635808872852,
        "tmdate": 1635808872852,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "dDARN-TCiA",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This work proposes a variance-reduced method called stochastic reweighted gradient (SRG) method which is based on importance sampling.  SRG improves on the of stochastic gradient descent error for the strongly convex and smooth objective functions. The authors extended  also SRG to SRG+ to combine the importance-sampling-based preconditioning and variance reduction. Numerical experiments are provided to assess the numerical efficiency of these methods.",
          "main_review": "\nOne of the biggest challenge in this paper is the assumption for SRG+ that we know all the Lipschitz constants of f_i s. For linear & logistic regressions, I agree that we can know them. But for many problems, especially in real world scenarios, it will be difficult to either compute these values or use them to do sampling. More comments on this is needed in the paper.\n\nI would like to see more applications and experiments than those in the paper. The analysis does not convince me that importance sampling is truly helpful. Does this matter? Should I try to use this? Do I have any hope of using it in settings that are large-scale enough that I want to simply stream the data? \n\nSince there is extra computation at each iteration compared to SGD (computation of norms, updates....), then experiments with run time in x-axis make more sense?\n\nWhat about minibatch versions of your methods, do they support this? how? comments on this are needed.\n\nWhat is the sensitivity of your methods to theta?",
          "summary_of_the_review": "See above",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "None",
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Reviewer_N3uR"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Reviewer_N3uR"
        ]
      },
      {
        "id": "FqQDa6y6nto",
        "original": null,
        "number": 2,
        "cdate": 1635828418669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635828418669,
        "tmdate": 1635828418669,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "dDARN-TCiA",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper considers minimizing the finite sum problem with SGD with non-uniform sampling. \nThe optimal sampling weight at a given point is achieved via minimizing the variance of stochastic gradient which needs the norm of the gradient of each function at that point. To approximate these sampling weights and also be efficient this paper proposes to take the approach similar to SAGA and use auxiliary memory with the size of number of functions and update one of the stored values in each iterate similar to SAGA. Using stale and stored gradient norms gives an upper bound to the stochastic gradient variance that could be minimized similarly. Since this new upper bound contains two terms, they propose a mixture of non-uniform and uniform sampling to make both terms small. They analyze SGD with the proposed sampling scheme and show that the sub-optimality upper bound is n (the number of functions)-times smaller than the upper bound of SGD with uniform sampling. They also propose another weighting scheme that mixes uniform and non-uniformes schemes proportioned with gradient norms and smoothness parameters of each function. \n",
          "main_review": "1- The paper is well-written and it is easy to follow and walk through the proofs. \n\n2- The major comment is that the analysis is for very limiting settings. It would be more practical to analyze the method for non-convex settings to see the effectiveness of the method in practice for training NNs. It would be useful to apply this method to NN training. \n\n3- For the experiments, it is not clear how many times each experiment has been run and if the result is the average of those.   \n\n",
          "summary_of_the_review": "The major comment is that the analysis is for very limiting settings. It would be more practical to analyze the method for non-convex settings to see the effectiveness of the method in practice for training NNs. It would be useful to apply this method to NN training. \n",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Reviewer_wvfK"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Reviewer_wvfK"
        ]
      },
      {
        "id": "X-8tmWkSmxd",
        "original": null,
        "number": 3,
        "cdate": 1635863072243,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635863072243,
        "tmdate": 1635863198858,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "dDARN-TCiA",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The authors propose the stochastic reweighted gradient (SRG) algorithm, which is based on importance sampling in SGD to reduce the variance for the SGD method. The authors also extend SRG to combine the benefits of both importance-sampling-based preconditioning and variance reduction. The convergence rate is studied and the authors show that the proposed method can achieve a better asymptotic error than SGD. ",
          "main_review": "There is a rich literature in the domain for SGD based on importance sampling and conditional variances approximation. The paper combines these two ideas, and I believe that it is useful for reducing the asymptotic error and computational complexities. I have some concerns as follows.\n1.\tI might be misunderstanding part of the proofs, but I hope the author could help me understand more on why Lemma 2 holds. In the algorithm, $p_k$ is calculated based on $q_k$, which is related to a binomial distribution with parameter $\\theta_k$. So the randomness of $q_k$ may have an impact on the expectation. However in the proof, the impact of $q_k$ is not involved, and $p_k$ seems to be treated as a constant. In other words, since the importance sampling process can be regarded as a kind of Bayes procedure, how does the distribution of the sampling probability impact the convergence?\n2.\tIn the experiments, the proposed method is compared with SGD+ (SGD with $L_i$-sampling) and SGD++ (SGD with partially biased sampling), how about comparing with importance sampling SGD with the optimal sample probabilities?\n",
          "summary_of_the_review": "I believe the paper has its value in combining importance sampling SGD with conditional variances approximation to reduce the asymptotic error as well as the computational complexity. I just have one question on the technical soundness of the convergence results.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Reviewer_i1ZH"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Reviewer_i1ZH"
        ]
      },
      {
        "id": "o8THLhYtxp8",
        "original": null,
        "number": 4,
        "cdate": 1635917453221,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635917453221,
        "tmdate": 1635917453221,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "dDARN-TCiA",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper uses importance sampling in SGD to improve performance.",
          "main_review": "The importance sampling procedure is new, however there have been many similar works in the past and this paper's results maybe very incremental.",
          "summary_of_the_review": "The importance sampling procedure is new, however there have been many similar works in the past and this paper's results maybe very incremental.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Reviewer_F7eT"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Reviewer_F7eT"
        ]
      },
      {
        "id": "SNBNZ6O-pUl",
        "original": null,
        "number": 2,
        "cdate": 1637301641361,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637301641361,
        "tmdate": 1637302619599,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "o8THLhYtxp8",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "Can you defend your claim that our paper is very incremental ?",
          "comment": "We laid out our contributions in the introduction and the relationship between our method and analysis to the literature in our related work section. Can you defend your claim that our paper is very incremental ?"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ]
      },
      {
        "id": "xso8EOGtAM",
        "original": null,
        "number": 3,
        "cdate": 1637301932764,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637301932764,
        "tmdate": 1637302415140,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "X-8tmWkSmxd",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "$p_k$ is constant conditional on $(i_t, b_t)_{t=0}^{k-1}$",
          "comment": "\\> *\"[...] the randomness of $q_k$ may have an impact on the expectation. However in the proof, the impact of $q_k$ is not involved, and $p_k$ seems to be treated as a constant [...] how does the distribution of the sampling probability impact the convergence?\"*\n\nThank you for the question. The randomness of $p_k$ (due to the randomness of $q_k$) does indeed have an impact on the *overall* expectation. The reason this does not interact directly with our proof is because our bound is on the *conditional* expectation of the second moment, conditioned on $(b_t, i_t)_{t=0}^{k-1}$. Conditioned on these random variables, $q_k$ is constant and so is $p_k$, and we can proceed with the analysis."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ]
      },
      {
        "id": "y4naDLGm27q",
        "original": null,
        "number": 4,
        "cdate": 1637302605008,
        "mdate": 1637302605008,
        "ddate": null,
        "tcdate": 1637302605008,
        "tmdate": 1637302605008,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "X-8tmWkSmxd",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "We do compare SRG with the optimal importance sampling probabilities.",
          "comment": "\\> *[...] how about comparing with importance sampling SGD with the optimal sample probabilities ?*\n\nWe compare SRG and SGD with the optimal importance sampling probabilities in the experiments depicted in Figure 2. We did not include this comparison in the experiments of Figure 1 since our goal there was to verify our theoretical claims rather than compare our methods to baselines."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ]
      },
      {
        "id": "OzF3xeZVTzT",
        "original": null,
        "number": 5,
        "cdate": 1637303307655,
        "mdate": 1637303307655,
        "ddate": null,
        "tcdate": 1637303307655,
        "tmdate": 1637303307655,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "FqQDa6y6nto",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "Limitedness of setting and extension to neural networks.",
          "comment": "\\> *\"The major comment is that the analysis is for very limiting settings.\"*\n\nWhile we agree that our setting (strongly-convex and smooth objectives) is somewhat limited, it is the standard setting under which new optimization algorithms are analyzed. Let us also point out that there is a large literature on importance sampling for variance reduction for SGD, yet our paper is the first that provides a clean analysis of such methods under the standard technical assumptions of smoothness and strong-convexity. We believe that this is significant and useful to the community on its own.\n\n\\> *\"It would be more practical to analyze the method for non-convex settings to see the effectiveness of the method in practice for training NNs. It would be useful to apply this method to NN training.\"*\n\nWe agree that an extension of our results to the non-convex case would be nice and possibly within reach using ideas in [1], but we left that to future work. In particular, It is not clear to us that such an analysis would uncover the practical behaviour of SRG on the training of NN. Experiments would be the most informative. \n\nHowever, our method is simply not efficiently implementable within current deep learning frameworks for large NNs that are used in practice. In particular, at each iteration, one would need to obtain the per-example gradients within each mini-batch to be able to calculate their norms. In current implementations, this is infeasible for all but the smallest NN architectures (holding such per-example gradients in working memory is not feasible, even for relatively small batch sizes).\n\nThe question of whether per-example gradient norms can be efficiently computed is a research question that is outside the scope of our work. We believe this to be possible for linear layers for example where the per-example gradients can be expressed as outer products. If you believe that the above explanation is useful for readers of the paper, we would happily include it in a new section of the appendix.\n\n[1] Reddi, S.J., Sra, S., P\u00f3czos, B., & Smola, A. (2016). Fast Incremental Method for Nonconvex Optimization. ArXiv, abs/1603.06159."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ]
      },
      {
        "id": "m0FdGOzDIPp",
        "original": null,
        "number": 6,
        "cdate": 1637303471756,
        "mdate": 1637303471756,
        "ddate": null,
        "tcdate": 1637303471756,
        "tmdate": 1637303471756,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "FqQDa6y6nto",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "Each experiment was run 10 times and the average of these runs is what is displayed.",
          "comment": "\\> *\"For the experiments, it is not clear how many times each experiment has been run and if the result is the average of those.\"*\n\nEach experiment was run 10 times and we display the average of the 10 runs. We have added this detail to our paper."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ]
      },
      {
        "id": "LIZ-7g7p5CB",
        "original": null,
        "number": 7,
        "cdate": 1637303567250,
        "mdate": 1637303567250,
        "ddate": null,
        "tcdate": 1637303567250,
        "tmdate": 1637303567250,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "4rAJlxML4l",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "Our main method does not require knowledge of the smoothness constants.",
          "comment": "\\> *\"One of the biggest challenges in this paper is the assumption for SRG+ that we know all the Lipschitz constants of f_i s.\"*\n\nThe main algorithm in the paper (SRG) does not require knowledge of the smoothness constants. SRG+ simply says that IF one has access to these constants, then one can leverage this knowledge to design better importance sampling distributions. If such knowledge is not available, then one can naturally fall back to SRG."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ]
      },
      {
        "id": "c2pylbo2xM",
        "original": null,
        "number": 8,
        "cdate": 1637303675726,
        "mdate": 1637303675726,
        "ddate": null,
        "tcdate": 1637303675726,
        "tmdate": 1637303675726,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "4rAJlxML4l",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "We added a mini-batch version of SRG.",
          "comment": "\\> *\"What about minibatch versions of your methods [?]\"*\n\nWe added a mini-batch version of SRG in the appendix."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ]
      },
      {
        "id": "AHbz0WgzLT",
        "original": null,
        "number": 9,
        "cdate": 1637303824393,
        "mdate": 1637303824393,
        "ddate": null,
        "tcdate": 1637303824393,
        "tmdate": 1637303824393,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "4rAJlxML4l",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "Are there any specific experiments that you believe would significantly contribute to supporting the claims of the paper ?",
          "comment": "\\> *\"I would like to see more applications and experiments than those in the paper.\"*\n\nAre there any specific experiments that you believe would significantly contribute to supporting the claims of the paper ? We'd be happy to consider running them if they strengthen our contribution. If referring to NN experiments, please see our comment above titled \"Limitedness of setting and extension to neural networks.\"."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ]
      },
      {
        "id": "s71CiLWSPQq",
        "original": null,
        "number": 10,
        "cdate": 1637303886841,
        "mdate": 1637303886841,
        "ddate": null,
        "tcdate": 1637303886841,
        "tmdate": 1637303886841,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "4rAJlxML4l",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "SRG introduces negligible overhead compared to SGD.",
          "comment": "*\"Since there is extra computation at each iteration compared to SGD (computation of norms, updates....), then experiments with run time in x-axis make more sense?\"*\n\nThe extra-computation needed by SRG is negligible. We discuss in the paper how SRG can be implemented using only an additional $O(\\log{n})$ floating point operations per iteration when compared to SGD. We did not compare runtime because our implementation was a non-optimized research implementation."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ]
      },
      {
        "id": "uxNtZKbfgoe",
        "original": null,
        "number": 11,
        "cdate": 1637304111748,
        "mdate": 1637304111748,
        "ddate": null,
        "tcdate": 1637304111748,
        "tmdate": 1637304111748,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "4rAJlxML4l",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "SRG should be used whenever possible.",
          "comment": "\\> *\"Does this matter? Should I try to use this?\"*\n\nUp to small constants, our analysis asserts that SRG will perform better than SGD when $\\sigma_*^2 \\ll \\sigma^2$. This is further supported by our experiments, particularly Figure 1 (left). Because $\\sigma_*^2 \\leq \\sigma^2$ always holds, and because the overhead of SRG is negligible, there is no reason not to use SRG instead of SGD."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ]
      },
      {
        "id": "lDj_qwL1O2",
        "original": null,
        "number": 12,
        "cdate": 1637304235267,
        "mdate": 1637304235267,
        "ddate": null,
        "tcdate": 1637304235267,
        "tmdate": 1637304235267,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "4rAJlxML4l",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "SRG is not too sensitive to $\\theta$ as long as it is reasonably large.",
          "comment": "\\> *\"What is the sensitivity of your methods to theta?\"*\n\nThe short answer is that our method is not too sensitive to $\\theta$ as long as it is reasonably large. A good default when using a constant step size is to set $\\theta = 1/2$ as we did in our experiments.\n\nOur theoretical results provide a more precise answer to this question (Theorem 1). The choice of theta controls a trade-off: a smaller $\\theta$ leads to a smaller asymptotic error, while a larger theta allows the use of a larger step size leading to a faster initial convergence.\n\nWe view $\\theta$ as a parameter that the user can set depending on their goal (rather than a hyperparameter which should be optimized over). Our suggestion would be to start the algorithm with the largest theta allowable ($\\theta$ = 1/2), then decrease it (jointly with the step size, as prescribed by our theory, see Theorem 1) as needed to achieve higher accuracy."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Authors"
        ]
      },
      {
        "id": "RPbzqoh7D8",
        "original": null,
        "number": 15,
        "cdate": 1638200709586,
        "mdate": 1638200709586,
        "ddate": null,
        "tcdate": 1638200709586,
        "tmdate": 1638200709586,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "dDARN-TCiA",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "After rebuttal",
          "comment": "I have read the rebuttals. The authors answered \"correctly\" some of my questions  and responded with general statements to others. I also read the other criticisms from other reviews. I still think this work has some merit as it is, but I cannot increase my score to more than 6."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Reviewer_N3uR"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Reviewer_N3uR"
        ]
      },
      {
        "id": "ohyZ2XrWK5",
        "original": null,
        "number": 1,
        "cdate": 1642696838113,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696838113,
        "tmdate": 1642696838113,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "dDARN-TCiA",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The reviewers and AC all find the presented approach interesting and promising. \n\nHowever, as pointed out in the reviews and as the authors recognized, the strongly convex + smooth objective setting considered is limited. Given the prevalence of non-convex settings in many practical applications and the rich related literature on the analysis of SGD and variants in the non-convex setting,  it would be highly desirable to (i) consider experiments on small NN architectures (since the method cannot accommodate larger architectures)  to gain some understanding of the value of the approach and (ii)  to try and extend the present analysis to the non-convex case. \n\nIt would also be valuable to perform experiments illustrating the impact of theta indicated by the theory."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "4rAJlxML4l",
        "original": null,
        "number": 1,
        "cdate": 1635808872852,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635808872852,
        "tmdate": 1635808872852,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "dDARN-TCiA",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This work proposes a variance-reduced method called stochastic reweighted gradient (SRG) method which is based on importance sampling.  SRG improves on the of stochastic gradient descent error for the strongly convex and smooth objective functions. The authors extended  also SRG to SRG+ to combine the importance-sampling-based preconditioning and variance reduction. Numerical experiments are provided to assess the numerical efficiency of these methods.",
          "main_review": "\nOne of the biggest challenge in this paper is the assumption for SRG+ that we know all the Lipschitz constants of f_i s. For linear & logistic regressions, I agree that we can know them. But for many problems, especially in real world scenarios, it will be difficult to either compute these values or use them to do sampling. More comments on this is needed in the paper.\n\nI would like to see more applications and experiments than those in the paper. The analysis does not convince me that importance sampling is truly helpful. Does this matter? Should I try to use this? Do I have any hope of using it in settings that are large-scale enough that I want to simply stream the data? \n\nSince there is extra computation at each iteration compared to SGD (computation of norms, updates....), then experiments with run time in x-axis make more sense?\n\nWhat about minibatch versions of your methods, do they support this? how? comments on this are needed.\n\nWhat is the sensitivity of your methods to theta?",
          "summary_of_the_review": "See above",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "None",
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Reviewer_N3uR"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Reviewer_N3uR"
        ]
      },
      {
        "id": "FqQDa6y6nto",
        "original": null,
        "number": 2,
        "cdate": 1635828418669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635828418669,
        "tmdate": 1635828418669,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "dDARN-TCiA",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper considers minimizing the finite sum problem with SGD with non-uniform sampling. \nThe optimal sampling weight at a given point is achieved via minimizing the variance of stochastic gradient which needs the norm of the gradient of each function at that point. To approximate these sampling weights and also be efficient this paper proposes to take the approach similar to SAGA and use auxiliary memory with the size of number of functions and update one of the stored values in each iterate similar to SAGA. Using stale and stored gradient norms gives an upper bound to the stochastic gradient variance that could be minimized similarly. Since this new upper bound contains two terms, they propose a mixture of non-uniform and uniform sampling to make both terms small. They analyze SGD with the proposed sampling scheme and show that the sub-optimality upper bound is n (the number of functions)-times smaller than the upper bound of SGD with uniform sampling. They also propose another weighting scheme that mixes uniform and non-uniformes schemes proportioned with gradient norms and smoothness parameters of each function. \n",
          "main_review": "1- The paper is well-written and it is easy to follow and walk through the proofs. \n\n2- The major comment is that the analysis is for very limiting settings. It would be more practical to analyze the method for non-convex settings to see the effectiveness of the method in practice for training NNs. It would be useful to apply this method to NN training. \n\n3- For the experiments, it is not clear how many times each experiment has been run and if the result is the average of those.   \n\n",
          "summary_of_the_review": "The major comment is that the analysis is for very limiting settings. It would be more practical to analyze the method for non-convex settings to see the effectiveness of the method in practice for training NNs. It would be useful to apply this method to NN training. \n",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Reviewer_wvfK"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Reviewer_wvfK"
        ]
      },
      {
        "id": "X-8tmWkSmxd",
        "original": null,
        "number": 3,
        "cdate": 1635863072243,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635863072243,
        "tmdate": 1635863198858,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "dDARN-TCiA",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The authors propose the stochastic reweighted gradient (SRG) algorithm, which is based on importance sampling in SGD to reduce the variance for the SGD method. The authors also extend SRG to combine the benefits of both importance-sampling-based preconditioning and variance reduction. The convergence rate is studied and the authors show that the proposed method can achieve a better asymptotic error than SGD. ",
          "main_review": "There is a rich literature in the domain for SGD based on importance sampling and conditional variances approximation. The paper combines these two ideas, and I believe that it is useful for reducing the asymptotic error and computational complexities. I have some concerns as follows.\n1.\tI might be misunderstanding part of the proofs, but I hope the author could help me understand more on why Lemma 2 holds. In the algorithm, $p_k$ is calculated based on $q_k$, which is related to a binomial distribution with parameter $\\theta_k$. So the randomness of $q_k$ may have an impact on the expectation. However in the proof, the impact of $q_k$ is not involved, and $p_k$ seems to be treated as a constant. In other words, since the importance sampling process can be regarded as a kind of Bayes procedure, how does the distribution of the sampling probability impact the convergence?\n2.\tIn the experiments, the proposed method is compared with SGD+ (SGD with $L_i$-sampling) and SGD++ (SGD with partially biased sampling), how about comparing with importance sampling SGD with the optimal sample probabilities?\n",
          "summary_of_the_review": "I believe the paper has its value in combining importance sampling SGD with conditional variances approximation to reduce the asymptotic error as well as the computational complexity. I just have one question on the technical soundness of the convergence results.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Reviewer_i1ZH"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Reviewer_i1ZH"
        ]
      },
      {
        "id": "o8THLhYtxp8",
        "original": null,
        "number": 4,
        "cdate": 1635917453221,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635917453221,
        "tmdate": 1635917453221,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "dDARN-TCiA",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper uses importance sampling in SGD to improve performance.",
          "main_review": "The importance sampling procedure is new, however there have been many similar works in the past and this paper's results maybe very incremental.",
          "summary_of_the_review": "The importance sampling procedure is new, however there have been many similar works in the past and this paper's results maybe very incremental.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Reviewer_F7eT"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Reviewer_F7eT"
        ]
      },
      {
        "id": "RPbzqoh7D8",
        "original": null,
        "number": 15,
        "cdate": 1638200709586,
        "mdate": 1638200709586,
        "ddate": null,
        "tcdate": 1638200709586,
        "tmdate": 1638200709586,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "dDARN-TCiA",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Official_Comment",
        "content": {
          "title": "After rebuttal",
          "comment": "I have read the rebuttals. The authors answered \"correctly\" some of my questions  and responded with general statements to others. I also read the other criticisms from other reviews. I still think this work has some merit as it is, but I cannot increase my score to more than 6."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper106/Reviewer_N3uR"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper106/Reviewer_N3uR"
        ]
      },
      {
        "id": "ohyZ2XrWK5",
        "original": null,
        "number": 1,
        "cdate": 1642696838113,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696838113,
        "tmdate": 1642696838113,
        "tddate": null,
        "forum": "dDARN-TCiA",
        "replyto": "dDARN-TCiA",
        "invitation": "ICLR.cc/2022/Conference/Paper106/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Reject",
          "comment": "The reviewers and AC all find the presented approach interesting and promising. \n\nHowever, as pointed out in the reviews and as the authors recognized, the strongly convex + smooth objective setting considered is limited. Given the prevalence of non-convex settings in many practical applications and the rich related literature on the analysis of SGD and variants in the non-convex setting,  it would be highly desirable to (i) consider experiments on small NN architectures (since the method cannot accommodate larger architectures)  to gain some understanding of the value of the approach and (ii)  to try and extend the present analysis to the non-convex case. \n\nIt would also be valuable to perform experiments illustrating the impact of theta indicated by the theory."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}