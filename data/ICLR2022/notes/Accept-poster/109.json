{
  "id": "RAW9tCdVxLj",
  "original": "j2WxIdmmd0",
  "number": 109,
  "cdate": 1632875429439,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875429439,
  "tmdate": 1676330688096,
  "ddate": null,
  "content": {
    "title": "Zero-CL: Instance and Feature decorrelation for negative-free symmetric contrastive learning",
    "authorids": [
      "~Shaofeng_Zhang1",
      "~Feng_Zhu1",
      "~Junchi_Yan2",
      "~Rui_Zhao6",
      "~Xiaokang_Yang1"
    ],
    "authors": [
      "Shaofeng Zhang",
      "Feng Zhu",
      "Junchi Yan",
      "Rui Zhao",
      "Xiaokang Yang"
    ],
    "keywords": [
      "Self supervised learning",
      "representation learning"
    ],
    "abstract": "For self-supervised contrastive learning, models can easily collapse and generate trivial constant solutions. The issue has been mitigated by recent improvement on objective design, which however often requires square complexity either for the size of instances ($\\mathcal{O}(N^{2})$) or feature dimensions ($\\mathcal{O}(d)^2$). To prevent such collapse, we develop two novel methods by decorrelating on different dimensions on the instance embedding stacking matrix, i.e., \\textbf{I}nstance-wise (ICL) and \\textbf{F}eature-wise (FCL) \\textbf{C}ontrastive \\textbf{L}earning. The proposed two methods (FCL, ICL) can be combined synthetically, called Zero-CL, where ``Zero'' means negative samples are \\textbf{zero} relevant, which allows Zero-CL to completely discard negative pairs i.e., with \\textbf{zero} negative samples. Compared with previous methods, Zero-CL mainly enjoys three advantages: 1) Negative free in symmetric architecture. 2) By whitening transformation, the correlation of the different features is equal to zero, alleviating information redundancy. 3) Zero-CL remains original information to a great extent after transformation, which improves the accuracy against other whitening transformation techniques. Extensive experimental results on CIFAR-10/100 and ImageNet show that Zero-CL outperforms or is on par with state-of-the-art symmetric contrastive learning methods.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "zhang|zerocl_instance_and_feature_decorrelation_for_negativefree_symmetric_contrastive_learning",
    "pdf": "/pdf/545b532dc138dbfc098e9384dbc805e96fceed40.pdf",
    "one-sentence_summary": "We develop two contrastive learning methods to prevent collapses in symmetric architecture without negative pairs.",
    "data": "",
    "_bibtex": "@inproceedings{\nzhang2022zerocl,\ntitle={Zero-{CL}: Instance and Feature decorrelation for negative-free symmetric contrastive learning},\nauthor={Shaofeng Zhang and Feng Zhu and Junchi Yan and Rui Zhao and Xiaokang Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RAW9tCdVxLj}\n}",
    "venue": "ICLR 2022 Poster",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "RAW9tCdVxLj",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 16,
    "directReplyCount": 7,
    "revisions": true,
    "replies": [
      {
        "id": "mGzn80xGLCO",
        "original": null,
        "number": 1,
        "cdate": 1634857069766,
        "mdate": null,
        "ddate": null,
        "tcdate": 1634857069766,
        "tmdate": 1634857069766,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper addresses the collapse problem during self-supervised contrastive learning. ICL and FCL methods are proposed to decorrelate instances and features. Zero-Contrastive Learning can discard negative pairs with advantages in negative free, reducing correlation of different features to zero and retaining information during transformation. Promising experimental results are demonstrated on CIFAR and ImageNet.",
          "main_review": "+ This paper dedicates to whitening transformation with a ZCA-based solution. It is intuitive to apply the whitening to both instance and feature dimensions, which makes the objective function of ICL and FCL consistent.\n+ Paper is well presented and easy to follow.\n\n- The key concerns came from the symmetric framework. Despite the fancy and consistent solution, the problem is down to the different properties of feature dimensions and instances. For image classification, we believe the ideal feature representation should have decorrelated non-redundant dimensions. However, the correlation between instances is inevitable - the inner-class correlation should be encouraged. Is a pure orthogonal matrix that pushes every instance unique the best solution? Should it be a \"block-based\" matrix with ones filled in the same class?\n- Another big concern is about the claimed contribution in complexity. It is argued the second term in Eq(13) results in square order complexity. And the improvement is made by the replacement of the whitened embedding. I am not super clear why this can reduce the computational complexity.\n\n\n- Eq (9) seems an error: should it be Z_{i,d}^{A,Ins}\\dot Z_{i,d}^{B,Ins}?\n- Decorrelating for redundancy removal can often result in low-rank effects (it is equivalent to implicit feature selection or sampling) in the transformed matrix. It would be good to discuss and potential properties and problems resulting from the low-rank issues.\n- In the literature review, the background, properties and advantages of ZCA compared to PCA and Cholesky should be extended to clarify the motivation of using ZCA.\n- Eq(4) the \"1\" should be in bold to denote an identity matrix?\n",
          "summary_of_the_review": "Overall, it is a good paper. I am keen to understand why the symmetric framework can be consistently applied to features and instances given they have different properties in the contexts of image classification. I am also not clear about the complexity reduction, which seems to be claimed as a very important contribution in this paper.\n\nMinor issues include some typos in equations and the discussion of low-rank property and ZCA background.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "No",
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Reviewer_1y87"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Reviewer_1y87"
        ]
      },
      {
        "id": "XxP2_zzb4lv",
        "original": null,
        "number": 2,
        "cdate": 1635886373932,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635886373932,
        "tmdate": 1638237466251,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a novel contrastive loss by whitening the embedding vectors in two ways: along the instance dimension and along the feature dimension. The results are comparable to recent works.\n\n",
          "main_review": "This loss function is somewhat new. However, the justification is relatively weak. The work is good, though at this stage, the authors have only been able to show it gives satisfactory results. It is unclear whether this is just an incremental work among the recent advances.\n\nThis formulation is still using other samples as negative samples. The negative samples are unrelated to the positive samples  in the loss function. This is nice but in theory it is still quite similar to the recent contrastive learning approaches.\n\nTypo: eq 9 Z -> H.",
          "summary_of_the_review": "The paper is satisfactory.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Reviewer_kq6q"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Reviewer_kq6q"
        ]
      },
      {
        "id": "IOdyh2GAiI3",
        "original": null,
        "number": 3,
        "cdate": 1635894465249,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635894465249,
        "tmdate": 1635894465249,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper is concerned with preventing the model collapse in the self-supervised learning scenario using contrastive losses. The main addition is an adaptation to the existing formulations that performs instance or feature whitening, avoiding the use of negative examples than can be expensive to store and to compute the similarity. Experiments on the selected datasets show that the method generally matches the current top-performing methods.",
          "main_review": "Pro:\n+ The proposed formulation can be integrated into various methods as a plug-and-play component.\n+ The approach is clean and well backed-up both theoretically and experimentally.\n+ In-depth ablation study\n\nCons/Quest:\n- How frequent is the mode collapse to trivial solutions in practice for the competing methods? Most of this approaches are quite stable in practice even without using negative samples (i.e. SimSiam).\n- What is the actual advantage in wall-clock training time? Does the lower complexity in theory translates well in practice?\n- In Table 1, what is the explanation behind the fact that the proposed approach lags behind as the number of iteration increases?\n- Can this approach scale well to larger models too?\nMinor: \n- Section 2, a few paragraphs don't start with a capital letter.",
          "summary_of_the_review": "Overall the idea looks clear and efficient, it's not clear however how problematic for the current approaches the collapse is and some additional clarifications could strengthen the work.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Reviewer_AaoY"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Reviewer_AaoY"
        ]
      },
      {
        "id": "-ivbKlyGZsc",
        "original": null,
        "number": 4,
        "cdate": 1635906312995,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635906312995,
        "tmdate": 1635906312995,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper presents a new self-supervised learning objective that aims to further adapt the previous decorrelation-based [1] contrastive learning method [Bardes, 2021], which largely alleviates the trivial solutions in SSL. Compared with prior arts, the proposed Zero-ICL/FCL are constructed from mainly the aspects of instance and feature-wise. In particular, Zero-CL requires no negative samples; feature-wise FCL discards the redundancy term by feature-wise whitening, and the proposed ICL prevents the collapse of contrastive learning effectively. Quantitative evaluations validate that Zero-CL leads to on-par performances with previous state-of-the-art results.\n\n",
          "main_review": "This work presents a good extension on the basis of the previous regularization-based SSL method [1], and further, proposes Zero-CL from the instance and feature-wise aspects. The empirical experiment shows only incremental performance gain over the prior arts like Barlow Twins, yet the proposed Zero-CL has an obvious complexity advantage as it's not dependent on the feature dimension. \n\nSome technical doubts regard the Zero-CL:\n1. It seems the parameter \\lambda regulates the weight of feature/instance terms, is there any intuition behind the setting of lambda=1?\n\n2.  According to what is shown in Figure 3, L_{fea} has a consistent Acc@1 trend with Barlow Twins (larger dimension yields better results) but there does have a seemingly optimal hidden-size (1024 as shown in the Figure). Does this indicate that there's a conflict between the instance/feature-wise objective? In such a case, how could one decide the best-hidden size besides empirical experiences?\n\n[1] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.",
          "summary_of_the_review": "- The paper is very constructed and has a clear motivation, comprehensive ablative studies, and good analytic discussions.\n\n- The only concern from the reviewer is whether the complexity advantage is a sufficient contribution over the previous regularization method [1], given that Zero-CL shows only incremental performance gain. \n\n- The exhibited theoretical analysis of Zero-CL with previous efforts gives direct and clear comparisons. \n\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Reviewer_GfmZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Reviewer_GfmZ"
        ]
      },
      {
        "id": "GmWY6M4e-Cx",
        "original": null,
        "number": 2,
        "cdate": 1636533894911,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636533894911,
        "tmdate": 1636533937026,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "-ivbKlyGZsc",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer GfmZ",
          "comment": "Thanks for your constructive suggestions!\n\n**Intuition of $\\lambda$.** We find that both Zero-ICL and Zero-FCL can work independently, where the performance of the two methods are somewhat similar. Hence, we just simply set the $\\lambda=1$. We also try $\\lambda=0.5$ and $\\lambda=2$. However, we get similar results. Hence, we think Zero-CL is not sensitive to $\\lambda$.\n\n**Best hidden size and batch size.** In our experiments, we find Zero-ICL shows similar behavior to SimCLR [3], while Zero-FCL shows similar behavior to Barlow Twins [1]. The two methods are methodologically orthogonal and we try to combine them to seek for the best of the two worlds. Admittedly, in its current form there is no notable improvement (see Table 2 and 3) for which the reason we agree that there still calls for more detailed technical design that may not be able to be covered in this conference paper. Hence, we recommend to use Zero-ICL and Zero-FCL separately. \n\n**Complexity advantage.** VICReg [2] proposes three terms, which are variance, invariance and covariance terms. The invariance term requires $\\mathcal{O}(N)$ complexity and the variance term requires $\\mathcal{O}(N)$ complexity. However, the covariance term requires $\\mathcal{O}(d^2)$ complexity (see the covariance loss in page 16 in <https://arxiv.org/pdf/2105.04906.pdf>). Where Zero-FCL doesn't depend on pair-wise similarity, which only has invariance term. Hence, only $\\mathcal{O}(d)$ complexity is required.\n\n[1] Zbontar J, Jing L, Misra I, et al. Barlow twins: Self-supervised learning via redundancy reduction[J]. arXiv preprint arXiv:2103.03230, 2021.\n\n[2] Bardes A, Ponce J, LeCun Y. Vicreg: Variance-invariance-covariance regularization for self-supervised learning[J]. arXiv preprint arXiv:2105.04906, 2021.\n\n[3] Chen T, Kornblith S, Norouzi M, et al. A simple framework for contrastive learning of visual representations[C]//International conference on machine learning. PMLR, 2020: 1597-1607."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ]
      },
      {
        "id": "8EZEpy4oC-4",
        "original": null,
        "number": 3,
        "cdate": 1636534147443,
        "mdate": 1636534147443,
        "ddate": null,
        "tcdate": 1636534147443,
        "tmdate": 1636534147443,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "IOdyh2GAiI3",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer AaoY",
          "comment": "Thanks for your constructive suggestions!\n\n**Frequency of mode collapse.** We conduct several experiments of evaluating model collapses in Table 4, where our method does not collapse under both w/o and w/ negatives. At least in our numeric experiments (Zero-ICL and Zero-FCL), The collapse frequency is **0**. However, SimCLR [2] and Barlow Twins [1] will completely collapse [3] and dimensionally collapse [3] w/o negatives.\n\n**Wall-clock time.** Since the forward propagation and backpropagation of backbone will take lots of time, here we give spending time of loss calculation to show the difference. For Zero-FCL, it takes 1.61855 seconds over 4k iterations and for Barlow Twins [1], it takes 4.82446 seconds over 4k iterations. Note that the data is from $d=2048$ (one single 1080Ti GPU), and if the hidden dimension is larger, the gap will become larger (due to quadratic and linear order).\n\n**Lags behind as the number of iterations.** Our method converges with a faster rate than previous asymmetric and negative-requiring methods (see Table 3, also discussed in ablation study). Hence, our method gets better results in 100 epochs, while gets lower results in 400 epochs.\n\n**Larger model.** In our main study, we follow Barlow Twins [1], which mainly uses ResNet-50 as backbone. Note that our method reduces the quadratic complexity of objective function to linear, which is more scalable than Barlow Twins [1] and SimCLR [2]. Per your request, we will give results of ResNet152 and ResNet101 as soon as possible (hopefully during the rebuttal period, but subject to the time and resource constraint).\n\n**Typos.** We will carefully check the typos and modify in our next version. \n\n[1] Zbontar J, Jing L, Misra I, et al. Barlow twins: Self-supervised learning via redundancy reduction[J]. arXiv preprint arXiv:2103.03230, 2021.\n\n[2] Chen T, Kornblith S, Norouzi M, et al. A simple framework for contrastive learning of visual representations[C]//International conference on machine learning. PMLR, 2020: 1597-1607.\n\n[3] Jing L, Vincent P, LeCun Y, et al. Understanding Dimensional Collapse in Contrastive Self-supervised Learning[J]. arXiv preprint arXiv:2110.09348, 2021."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ]
      },
      {
        "id": "r1AfPolc6aS",
        "original": null,
        "number": 4,
        "cdate": 1636534235330,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636534235330,
        "tmdate": 1636546194265,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "XxP2_zzb4lv",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer kq6q",
          "comment": "Thanks for your constructive suggestions!\n\n**Main review.** We are sorry to write the objective function in this formulation, which aims to better understanding. However, it seems this formulation does not emphasize the differences between our method and previous methods. Consider the relation of Zero-ICL and SimCLR. Given two set of embeddings (view A and view B) $Z^{A}=\\{z_1^{A}, z_2^{A}, \\cdots z_n^{A}\\}$ and  $Z^{B}=\\{z_1^{B}, z_2^{B}, \\cdots, z_n^{B}\\}$. The later one (SimCLR [1]) calculates the similarity ($z_i^{A}, z_j^B, 1 \\leq i \\leq N, 1 \\leq j \\leq N$) of each two embeddings of the two sets, which will takes $\\mathcal{O}(N^2)$ time complexity. However, in our Zero-ICL, we first transform the two matrix via whitening to obtain $H^{A}=\\{h_1^{A}, h_2^{A}, \\cdots, h_n^{A}\\}$ and $H^{B}=\\{h_1^{B}, h_2^{B}, \\cdots, h_n^{B}\\}$. Then we only maximize the similarity of $h_i^{A}, h_i^{B}, 1\\leq i\\leq N$, which only takes $\\mathcal{O}(N)$ complexity.\n\n**typos.** Thanks for your good suggestion. we will correct the typo.\n\n[1] Chen T, Kornblith S, Norouzi M, et al. A simple framework for contrastive learning of visual representations[C]//International conference on machine learning. PMLR, 2020: 1597-1607."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ]
      },
      {
        "id": "sNPmkazwuwo",
        "original": null,
        "number": 5,
        "cdate": 1636534403010,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636534403010,
        "tmdate": 1636685763144,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "mGzn80xGLCO",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 1y87",
          "comment": "Thanks for your suggestions. Due to some misunderstandings, we will give explanations in detail. \n\n**Key concerns of the symmetric framework.** From the paper [3], we know that the uniformity part and alignment part of InfoNCE [4]. However, it also brings some misunderstandings. Please take an in-depth consideration, if we push three $l_2$ normalized vectors in a unit circle (2-$d$) uniformly, the angle of each two vectors is equal to $120^{\\circ}$. Then, for in 3-d space, there are maximal 6 vectors that can be uniformly distributed in the unit sphere with equal angles [5] and the angle is about $116.6^{\\circ}$. Then, how about 256, 1024 even 4096 dimensions? For 3-d space, if we force pulling more than 6 vectors to the sphere uniformly, the angles between each two vectors are not equal, which is your main concerns (images with similar semantic information should share similar embeddings). Note that in our experiments, the dimension $d$ and batch size $N$ are in the same order of magnitude. Hence, even if uniform distribution (objective of InfoNCE), the angles between every two vectors are also equal to one constant. For Zero-ICL, the angle is $90^{\\circ}$ (orthogonal), which indicates the reason why Zero-ICL can work well. However, we think the \"block-based\" idea is cool, which is worthy to explore in future work.\n\n**Complexity.** For instance-wise methods (similar for feature-wise), consider the relation of Zero-ICL and SimCLR. Given two set of embeddings (view A and view B) $Z^{A}=\\{z_1^{A}, z_2^{A}, \\cdots z_n^{A}\\}$ and  $Z^{B}=\\{z_1^{B}, z_2^{B}, \\cdots, z_n^{B}\\}$. The later one (SimCLR [1]) calculates the similarity ($z_i^{A}, z_j^B, 1 \\leq i \\leq N, 1 \\leq j \\leq N$) of each two embeddings of the two sets, which will takes $\\mathcal{O}(N^2)$ time complexity. However, in our Zero-ICL, we first transform the two matrix via whitening to obtain $H^{A}=\\{h_1^{A}, h_2^{A}, \\cdots, h_n^{A}\\}$ and $H^{B}=\\{h_1^{B}, h_2^{B}, \\cdots, h_n^{B}\\}$. Then we only maximize the similarity of $h_i^{A}, h_i^{B}, 1\\leq i\\leq N$, which only takes $\\mathcal{O}(N)$ complexity. One of the concerns is the complexity of whitening transformation. However, thanks to the work randomized SVD, we can calculate the whitening matrix with a very fast speed.\n\n**Typos in Eq. 9** Thanks for your careful reading. We have corrected the typo.\n\n**low-rank issue.** It's a classical problem, i.e., the input matrix is not full-rank or low-rank. One of the solution is modifying Eq. 7 to $W=E(\\Lambda_{C}+\\lambda I)^{-1/2}E^{\\top}$.\n\n**Literature review.** We will add some discussion to PCA and Cholesky-based methods.\n\n**Question in Eq. 4.** The formulation is copied from Barlow Twins [1]. Actually, it should be 1, since $C_{ij}$ and $C_{ii}$ are numeric numbers but not vector or matrix.\n\n[1] Zbontar J, Jing L, Misra I, et al. Barlow twins: Self-supervised learning via redundancy reduction[J]. arXiv preprint arXiv:2103.03230, 2021.\n\n[2] Chen T, Kornblith S, Norouzi M, et al. A simple framework for contrastive learning of visual representations[C]//International conference on machine learning. PMLR, 2020: 1597-1607.\n\n[3] Wang T, Isola P. Understanding contrastive representation learning through alignment and uniformity on the hypersphere[C]//International Conference on Machine Learning. PMLR, 2020: 9929-9939.\n\n[4] Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding[J]. arXiv preprint arXiv:1807.03748, 2018.\n\n[5] Jiang Z, Tidor J, Yao Y, et al. Equiangular lines with a fixed angle[J]. Annals of Mathematics, 2021, 194(3): 729-743."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ]
      },
      {
        "id": "g6zxeNCMWJT",
        "original": null,
        "number": 6,
        "cdate": 1636869432963,
        "mdate": 1636869432963,
        "ddate": null,
        "tcdate": 1636869432963,
        "tmdate": 1636869432963,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "r1AfPolc6aS",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "This linear time argument may not be correct.",
          "comment": "since it takes more work to decorrelate $Z^A$ and $Z^B$."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Reviewer_kq6q"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Reviewer_kq6q"
        ]
      },
      {
        "id": "v_pp2DasyB",
        "original": null,
        "number": 7,
        "cdate": 1636870553050,
        "mdate": 1636870553050,
        "ddate": null,
        "tcdate": 1636870553050,
        "tmdate": 1636870553050,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "g6zxeNCMWJT",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer kq6q",
          "comment": "Thanks for your quick reply!\n\nHi, here we only discuss the complexity of the objective function. While from $Z$->$H$, it only takes about one second for $10^{6} \\times 10^{5}$ matrix by random SVD ($2048 \\times 2048$ in our experiments) [https://research.fb.com/blog/2014/09/fast-randomized-svd/]. Besides, after obtaining the whitening matrix, the $\\mathbf{H}$ is calculated by $\\mathbf{H} = \\mathbf{W}.detach() \\mathbf{Z}$, which is a matrix multiplication without gradient backpropagation.\n\n We will clarify it in our revised version. Thank you for your reminder!"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ]
      },
      {
        "id": "uD1h6btW26",
        "original": null,
        "number": 8,
        "cdate": 1636925309707,
        "mdate": 1636925309707,
        "ddate": null,
        "tcdate": 1636925309707,
        "tmdate": 1636925309707,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "v_pp2DasyB",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "The complexity.",
          "comment": "Right, but the complexity of matrix multiplication is usually $O(N^3)$.\nBut the overall complexity may not be lower than SimCLR."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Reviewer_kq6q"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Reviewer_kq6q"
        ]
      },
      {
        "id": "xGK_LqLrFz7",
        "original": null,
        "number": 9,
        "cdate": 1636940622852,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636940622852,
        "tmdate": 1636954819497,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "uD1h6btW26",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer kq6q.",
          "comment": "Thanks for your quick reply!\n\nThe complexity of matrix multiplication is usually $O(N^3)$, and by some optimization, it can be reduced to $O(N^{2.xx})$. However, the multiplication of $Z$ and whitening matrix $W$ does not require gradient, which only takes the forward propagation complexity, i.e., $O(N^{2}d)$. Then, consider SimCLR (from line 24 in [https://github.com/leftthomas/SimCLR/blob/master/main.py]), we first concatenate two matrices and use the multiplication, which takes $O(4N^{2}d)$. Even if we don't consider the exp and softmax operations, the backpropagation will also take at least $O(4N^{2}d)$ complexity. \n\nIn our experiments, Zero-CL with 50 epochs on ImageNet takes about 85154 seconds, Barlow Twins takes about 108561 seconds. SimCLR takes about 112716 seconds. The experiments are conducted on 16 1080Ti GPUs with ResNet-50 backbones. \n\nBut you are right, if we consider the overall time complexity, the gap between the two methods (SimCLR, Zero-ICL) is only a constant term $O(C N^{2} d)$. However, our method shows a faster convergence rate and is also more robust to hidden dimensions and batch size (see ablation study).\n\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ]
      },
      {
        "id": "TIOOBcKp8ev",
        "original": null,
        "number": 10,
        "cdate": 1637042039943,
        "mdate": 1637042039943,
        "ddate": null,
        "tcdate": 1637042039943,
        "tmdate": 1637042039943,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "General response to reviews",
          "comment": "First of all, we sincerely appreciate all the comments and suggestions of reviewers!\n\nHere we report some modifications in the revised version.\n\nModify typos $Z->H$ in Eq. 9 (Reviewer kq6q and 1y87).\n\nModify \"complexity\" to \"complexity of objectives\" (Reviewer kq6q).\n\nAdd properties and advantages of ZCA literature review (Reviewer 1y87).\n\nDiscuss low-rank conditions after Eq. 2 (Reviewer 1y87).\n\nWe feel sorry we may not completely solve the concern of Reviewer 1y87 (orthogonal on instance dimension) since it's a standard open problem in mathematics (Equiangular lines). At present, scientists can only give the number of equiangular lines (at most 344 lines) on spaces below the 43 dimensions [1]. However, in contrastive learning, almost all the methods set the output dimension as 256 or larger. However, even in 43 dimensions, if the batch size is lower than 344, the orthogonal solution in one mini-batch can be sub-optimal, since we can ignore the unequal angles due to the uniformity property of InfoNCE [2]. \n\n[1] Greaves G R W, Syatriadi J, Yatsyna P. Equiangular lines in Euclidean spaces: dimensions 17 and 18[J]. arXiv preprint arXiv:2104.04330, 2021.\n\n[2] Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding[J]. arXiv preprint arXiv:1807.03748, 2018.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ]
      },
      {
        "id": "Q-49Hhy5KPA",
        "original": null,
        "number": 11,
        "cdate": 1637941673480,
        "mdate": 1637941673480,
        "ddate": null,
        "tcdate": 1637941673480,
        "tmdate": 1637941673480,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "Look forward reply to our rebuttal and update",
          "comment": "Dear reviewers, thanks for your comments and constructive suggestions which have inspired us a lot to improve the paper. We are sincerely looking forward to your reply and we could provide more information if needed."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ]
      },
      {
        "id": "4Pa8_vUFKoq",
        "original": null,
        "number": 13,
        "cdate": 1638236446790,
        "mdate": 1638236446790,
        "ddate": null,
        "tcdate": 1638236446790,
        "tmdate": 1638236446790,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "mGzn80xGLCO",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "Look forward to your further reply.",
          "comment": "Dear reviewer, thanks again for your comments and valuable suggestions.\n\nCurrently, we have made further clarification in the updated manuscript to address your concerns.\n\n**Instance-wise solutions:** we analyze in one mini-batch, the optimal solution is angles of each two embeddings equal to a constant (due to the given literature about equiangular lines in the previous reply). Considering InfoNCE, the solution in one mini-batch is also pulling the embeddings uniform (angles of each two embeddings are equivalent), which is consistent with our instance-wise whitening. The only difference is the angle, where Zero-ICL sets $90^{\\circ}$, while the optimal solution equals one constant. For further clarify, please re-read our previous response.\n\n**Complexity:** we argue the linear-order complexity in the objective function since both Zero-ICL and Zero-FCL do not require pair-wise similarity calculations. The analysis and wall-clock time are given in reply to Reviewer kq6q and AaoY, respectively.\n\nThen, we also add the discussion to PCA-based method and low-rank effects in our modified manuscripts. We sincerely look forward to your reply and we could provide more information if needed. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ]
      },
      {
        "id": "X3JfXMimIE",
        "original": null,
        "number": 1,
        "cdate": 1642696838184,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696838184,
        "tmdate": 1642696838184,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "The initial reviews for this paper were somewhat diverging, however the paper did not receive any significant negative criticism to push it towards below the acceptance threshold. The reviewers have found some minor issues about the paper. Following the reviewer recommendations, the meta reviewer recommends acceptance."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "mGzn80xGLCO",
        "original": null,
        "number": 1,
        "cdate": 1634857069766,
        "mdate": null,
        "ddate": null,
        "tcdate": 1634857069766,
        "tmdate": 1634857069766,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper addresses the collapse problem during self-supervised contrastive learning. ICL and FCL methods are proposed to decorrelate instances and features. Zero-Contrastive Learning can discard negative pairs with advantages in negative free, reducing correlation of different features to zero and retaining information during transformation. Promising experimental results are demonstrated on CIFAR and ImageNet.",
          "main_review": "+ This paper dedicates to whitening transformation with a ZCA-based solution. It is intuitive to apply the whitening to both instance and feature dimensions, which makes the objective function of ICL and FCL consistent.\n+ Paper is well presented and easy to follow.\n\n- The key concerns came from the symmetric framework. Despite the fancy and consistent solution, the problem is down to the different properties of feature dimensions and instances. For image classification, we believe the ideal feature representation should have decorrelated non-redundant dimensions. However, the correlation between instances is inevitable - the inner-class correlation should be encouraged. Is a pure orthogonal matrix that pushes every instance unique the best solution? Should it be a \"block-based\" matrix with ones filled in the same class?\n- Another big concern is about the claimed contribution in complexity. It is argued the second term in Eq(13) results in square order complexity. And the improvement is made by the replacement of the whitened embedding. I am not super clear why this can reduce the computational complexity.\n\n\n- Eq (9) seems an error: should it be Z_{i,d}^{A,Ins}\\dot Z_{i,d}^{B,Ins}?\n- Decorrelating for redundancy removal can often result in low-rank effects (it is equivalent to implicit feature selection or sampling) in the transformed matrix. It would be good to discuss and potential properties and problems resulting from the low-rank issues.\n- In the literature review, the background, properties and advantages of ZCA compared to PCA and Cholesky should be extended to clarify the motivation of using ZCA.\n- Eq(4) the \"1\" should be in bold to denote an identity matrix?\n",
          "summary_of_the_review": "Overall, it is a good paper. I am keen to understand why the symmetric framework can be consistently applied to features and instances given they have different properties in the contexts of image classification. I am also not clear about the complexity reduction, which seems to be claimed as a very important contribution in this paper.\n\nMinor issues include some typos in equations and the discussion of low-rank property and ZCA background.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "No",
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Reviewer_1y87"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Reviewer_1y87"
        ]
      },
      {
        "id": "XxP2_zzb4lv",
        "original": null,
        "number": 2,
        "cdate": 1635886373932,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635886373932,
        "tmdate": 1638237466251,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a novel contrastive loss by whitening the embedding vectors in two ways: along the instance dimension and along the feature dimension. The results are comparable to recent works.\n\n",
          "main_review": "This loss function is somewhat new. However, the justification is relatively weak. The work is good, though at this stage, the authors have only been able to show it gives satisfactory results. It is unclear whether this is just an incremental work among the recent advances.\n\nThis formulation is still using other samples as negative samples. The negative samples are unrelated to the positive samples  in the loss function. This is nice but in theory it is still quite similar to the recent contrastive learning approaches.\n\nTypo: eq 9 Z -> H.",
          "summary_of_the_review": "The paper is satisfactory.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Reviewer_kq6q"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Reviewer_kq6q"
        ]
      },
      {
        "id": "IOdyh2GAiI3",
        "original": null,
        "number": 3,
        "cdate": 1635894465249,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635894465249,
        "tmdate": 1635894465249,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper is concerned with preventing the model collapse in the self-supervised learning scenario using contrastive losses. The main addition is an adaptation to the existing formulations that performs instance or feature whitening, avoiding the use of negative examples than can be expensive to store and to compute the similarity. Experiments on the selected datasets show that the method generally matches the current top-performing methods.",
          "main_review": "Pro:\n+ The proposed formulation can be integrated into various methods as a plug-and-play component.\n+ The approach is clean and well backed-up both theoretically and experimentally.\n+ In-depth ablation study\n\nCons/Quest:\n- How frequent is the mode collapse to trivial solutions in practice for the competing methods? Most of this approaches are quite stable in practice even without using negative samples (i.e. SimSiam).\n- What is the actual advantage in wall-clock training time? Does the lower complexity in theory translates well in practice?\n- In Table 1, what is the explanation behind the fact that the proposed approach lags behind as the number of iteration increases?\n- Can this approach scale well to larger models too?\nMinor: \n- Section 2, a few paragraphs don't start with a capital letter.",
          "summary_of_the_review": "Overall the idea looks clear and efficient, it's not clear however how problematic for the current approaches the collapse is and some additional clarifications could strengthen the work.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Reviewer_AaoY"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Reviewer_AaoY"
        ]
      },
      {
        "id": "-ivbKlyGZsc",
        "original": null,
        "number": 4,
        "cdate": 1635906312995,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635906312995,
        "tmdate": 1635906312995,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper presents a new self-supervised learning objective that aims to further adapt the previous decorrelation-based [1] contrastive learning method [Bardes, 2021], which largely alleviates the trivial solutions in SSL. Compared with prior arts, the proposed Zero-ICL/FCL are constructed from mainly the aspects of instance and feature-wise. In particular, Zero-CL requires no negative samples; feature-wise FCL discards the redundancy term by feature-wise whitening, and the proposed ICL prevents the collapse of contrastive learning effectively. Quantitative evaluations validate that Zero-CL leads to on-par performances with previous state-of-the-art results.\n\n",
          "main_review": "This work presents a good extension on the basis of the previous regularization-based SSL method [1], and further, proposes Zero-CL from the instance and feature-wise aspects. The empirical experiment shows only incremental performance gain over the prior arts like Barlow Twins, yet the proposed Zero-CL has an obvious complexity advantage as it's not dependent on the feature dimension. \n\nSome technical doubts regard the Zero-CL:\n1. It seems the parameter \\lambda regulates the weight of feature/instance terms, is there any intuition behind the setting of lambda=1?\n\n2.  According to what is shown in Figure 3, L_{fea} has a consistent Acc@1 trend with Barlow Twins (larger dimension yields better results) but there does have a seemingly optimal hidden-size (1024 as shown in the Figure). Does this indicate that there's a conflict between the instance/feature-wise objective? In such a case, how could one decide the best-hidden size besides empirical experiences?\n\n[1] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.",
          "summary_of_the_review": "- The paper is very constructed and has a clear motivation, comprehensive ablative studies, and good analytic discussions.\n\n- The only concern from the reviewer is whether the complexity advantage is a sufficient contribution over the previous regularization method [1], given that Zero-CL shows only incremental performance gain. \n\n- The exhibited theoretical analysis of Zero-CL with previous efforts gives direct and clear comparisons. \n\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Reviewer_GfmZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Reviewer_GfmZ"
        ]
      },
      {
        "id": "TIOOBcKp8ev",
        "original": null,
        "number": 10,
        "cdate": 1637042039943,
        "mdate": 1637042039943,
        "ddate": null,
        "tcdate": 1637042039943,
        "tmdate": 1637042039943,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "General response to reviews",
          "comment": "First of all, we sincerely appreciate all the comments and suggestions of reviewers!\n\nHere we report some modifications in the revised version.\n\nModify typos $Z->H$ in Eq. 9 (Reviewer kq6q and 1y87).\n\nModify \"complexity\" to \"complexity of objectives\" (Reviewer kq6q).\n\nAdd properties and advantages of ZCA literature review (Reviewer 1y87).\n\nDiscuss low-rank conditions after Eq. 2 (Reviewer 1y87).\n\nWe feel sorry we may not completely solve the concern of Reviewer 1y87 (orthogonal on instance dimension) since it's a standard open problem in mathematics (Equiangular lines). At present, scientists can only give the number of equiangular lines (at most 344 lines) on spaces below the 43 dimensions [1]. However, in contrastive learning, almost all the methods set the output dimension as 256 or larger. However, even in 43 dimensions, if the batch size is lower than 344, the orthogonal solution in one mini-batch can be sub-optimal, since we can ignore the unequal angles due to the uniformity property of InfoNCE [2]. \n\n[1] Greaves G R W, Syatriadi J, Yatsyna P. Equiangular lines in Euclidean spaces: dimensions 17 and 18[J]. arXiv preprint arXiv:2104.04330, 2021.\n\n[2] Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding[J]. arXiv preprint arXiv:1807.03748, 2018.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ]
      },
      {
        "id": "Q-49Hhy5KPA",
        "original": null,
        "number": 11,
        "cdate": 1637941673480,
        "mdate": 1637941673480,
        "ddate": null,
        "tcdate": 1637941673480,
        "tmdate": 1637941673480,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Official_Comment",
        "content": {
          "title": "Look forward reply to our rebuttal and update",
          "comment": "Dear reviewers, thanks for your comments and constructive suggestions which have inspired us a lot to improve the paper. We are sincerely looking forward to your reply and we could provide more information if needed."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper109/Authors"
        ]
      },
      {
        "id": "X3JfXMimIE",
        "original": null,
        "number": 1,
        "cdate": 1642696838184,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696838184,
        "tmdate": 1642696838184,
        "tddate": null,
        "forum": "RAW9tCdVxLj",
        "replyto": "RAW9tCdVxLj",
        "invitation": "ICLR.cc/2022/Conference/Paper109/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "The initial reviews for this paper were somewhat diverging, however the paper did not receive any significant negative criticism to push it towards below the acceptance threshold. The reviewers have found some minor issues about the paper. Following the reviewer recommendations, the meta reviewer recommends acceptance."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}