{
  "id": "5xEgrl_5FAJ",
  "original": "p8-m-AeCCB3",
  "number": 42,
  "cdate": 1632875424455,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875424455,
  "tmdate": 1697934971854,
  "ddate": null,
  "content": {
    "title": "BiBERT: Accurate Fully Binarized BERT",
    "authorids": [
      "~Haotong_Qin1",
      "~Yifu_Ding2",
      "~Mingyuan_Zhang1",
      "~Qinghua_YAN1",
      "~Aishan_Liu1",
      "dangqingqing@baidu.com",
      "~Ziwei_Liu1",
      "~Xianglong_Liu2"
    ],
    "authors": [
      "Haotong Qin",
      "Yifu Ding",
      "Mingyuan Zhang",
      "Qinghua YAN",
      "Aishan Liu",
      "Qingqing Dang",
      "Ziwei Liu",
      "Xianglong Liu"
    ],
    "keywords": [
      "Network Binarization",
      "Model Compression",
      "BERT",
      "NLP"
    ],
    "abstract": "The large pre-trained BERT has achieved remarkable performance on Natural Language Processing (NLP) tasks but is also computation and memory expensive. As one of the powerful compression approaches, binarization extremely reduces the computation and memory consumption by utilizing 1-bit parameters and bitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit weight, embedding, and activation) usually suffer a significant performance drop, and there is rare study addressing this problem. In this paper, with the theoretical justification and empirical analysis, we identify that the severe performance drop can be mainly attributed to the information degradation and optimization direction mismatch respectively in the forward and backward propagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate the performance bottlenecks. Specifically, BiBERT introduces an efficient Bi-Attention structure for maximizing representation information statistically and a Direction-Matching Distillation (DMD) scheme to optimize the full binarized BERT accurately. Extensive experiments show that BiBERT outperforms both the straightforward baseline and existing state-of-the-art quantized BERTs with ultra-low bit activations by convincing margins on the NLP benchmark. As the first fully binarized BERT, our method yields impressive 56.3 times and 31.2 times saving on FLOPs and model size, demonstrating the vast advantages and potential of the fully binarized BERT model in real-world resource-constrained scenarios.",
    "one-sentence_summary": "Our BiBERT, for the first time, presents a promising route towards the accurate fully binarized BERT (with 1-bit weight, embedding, and activation) and gives impressive 56.3 times and 31.2 times saving on FLOPs and model size, respectively.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "qin|bibert_accurate_fully_binarized_bert",
    "pdf": "/pdf/09aef2ecce1fcaf41eaa870ad5afc7e4d3222dad.pdf",
    "supplementary_material": "/attachment/251338709ffa2ac9f911a7383a2772d80a7b5b6d.zip",
    "data": "",
    "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2203.06390/code)",
    "_bibtex": "@inproceedings{\nqin2022bibert,\ntitle={Bi{BERT}: Accurate Fully Binarized {BERT}},\nauthor={Haotong Qin and Yifu Ding and Mingyuan Zhang and Qinghua YAN and Aishan Liu and Qingqing Dang and Ziwei Liu and Xianglong Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5xEgrl_5FAJ}\n}",
    "venue": "ICLR 2022 Poster",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "5xEgrl_5FAJ",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 30,
    "directReplyCount": 9,
    "revisions": true,
    "replies": [
      {
        "id": "4inq0vvKBiQ",
        "original": null,
        "number": 1,
        "cdate": 1635851421837,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635851421837,
        "tmdate": 1638254401781,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper addresses the problem of fully binarizing BERT model including the network weights, embeddings and activations. Through theoretical and empirical analysis, the authors find: 1) the direct binarization of softmax-ed attention matrix is problematic; 2) distillation of the attention scores from the full-precision teacher can be harmful to the binarized model. Therefore, they propose two different methods to alleviate the issues. One is the Bi-Attention which directly quantizes the attention matrix and excludes softmax. The other is Direction-Matching Distillation which chooses to distill the covariance matrices of query/key/value rather than the attention matrix. Experimental results on GLUE benchmark show that BiBERT model outperforms previous quantized BERT models on the full binarization setting.\nThis main contribution of the paper is that it is the first work on full binarization of BERT models. With detailed analysis and observations, it proposed two effective methods to improve the training of the binarized model, and achieve good performance on some of the NLU tasks.",
          "main_review": "Overall, the paper is well-motivated and easy to follow. To the best of my knowledge, this is the first work on full quantization (weights-embedding-activation) of the BERT model to 1-bit (denoted as 1-1-1), considering that previous work such as BinaryBERT only achieves 1-1-4 quantization. Binarizing all the activations without losing much performance is challenging. By making detailed analysis of the performance variances in binarizing intermediate outputs at each step and the different distillation losses, the authors find the major problems that hurt the model performance most. Two new methods are proposed to avoid or alleviate the problem in training the quantized model, which are theoretical sound and shows substantial improvements to the accuracies on downstream tasks. The proposed BiBERT model also surpasses the previous BERT quantization model on the setting of 1-1-1 quantization.\n\nThe weaknesses of the BiBERT model are as follows: \n1) The performance of BiBERT on GLUE is still far behind the full precision model (67.0 vs 82.3). This large performance gap may hinder application of the BiBERT model in many real scenarios (e.g., CoLA, STS-B tasks). While the BinaryBERT with 1-1-4 quantization, which has similar model size and a little bit higher computation as the BiBERT, can achieve GLUE score of 81.9 with almost neglectable performance loss. This raises the question why full binarization of the BERT model is necessary or preferred. One possible way to justify this is to enlarge the network size while keeping it binary and see if the performance can consistently improve.\n2) In the experiments, the authors only report BinaryBERT (1-1-2 and 1-1-1) and TernaryBERT (2-2-1 and 2-2-2) which are not present in their original papers. The author should clarify in the paper how the results are obtained. Are the results from reimplementation of their algorithms or based on some open-source code/models? Are the reimplementation correct? On CoLA task, BinaryBERT and TenaryBERT simply seem to diverge, and the baseline quantization method without Bi-Attention and DMD also outperform BinaryBERT and TenaryBERT.\n3) The author report Q2BERT and Q-BERT on the 2-2-8 setting, while the TernaryBERT have much higher performance (72.9) on it. Why not report these results?  \nOverall, I think the BiBERT is still not applicable to some NLU tasks given severe performance drop. Some of the experimental results are not convincing (or biased) regarding the baseline of BinaryBERT and TenaryBERT.\n",
          "summary_of_the_review": "The paper tries to tackle a challenging problem of binarizing BERT and find effective ways to improve the performance. However, the model performances on some tasks are still far from good and comparison in the experiments are not convincing. Therefore, my current recommendation is weak reject unless the author can clarify the concerns.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_TtMP"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_TtMP"
        ]
      },
      {
        "id": "YkLvJCjGB0q",
        "original": null,
        "number": 2,
        "cdate": 1635855850648,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635855850648,
        "tmdate": 1635855850648,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces an accurate fully binarized (i.e., 1-bit weight, embedding, and activation)  BERT, BiBERT, as a robust model compression method. It saves 56.3\u00d7 and 31.2\u00d7 on FLOPs and model size for real-world devices. Moreover, it addresses the substantial performance issues in the straightforward full binarization method. To tackle the performance drop issues, it proposes Bi-Attention and Direction Matching Distillation (DMD) mechanism.  The paper contains an evaluation of the proposed model on GLUE benchmark and a comparison with SOTA quantized BERT models.\n",
          "main_review": "Strengths:\n1. The paper is well constructed and easy to go through the idea; encompasses rigorous theoretical justification of the approaches.\n2. This paper addresses the crucial problems in fully binarized BERT and introduces BiBERT which is the very first attempt to implement an accurate fully binarized BERT.\n3. In this framework, they incorporate a Bi-Attention approach which maximizes the information entropy to mitigate the information degradation. And a Direction Matching Distillation (DMD) mechanism which resolves the optimization direction mismatch.\n4. BiBERT also impressively obtain savings on FLOPs and model size which is a great advantage for real-world edge devices.\n5. It also contains an ablation study on the GLUE benchmark which validates the contribution of the individual components.\n\nWeaknesses:\n1. The proposed method didn\u2019t properly address the performance drop due to the binarization of activation. Results on the GLUE benchmark showed a drastic performance drop in comparison to the related approaches and full precision BERT.\n2. In sec 2.1, it is evident that a stochastic sign function is used to binarize the BERT, therefore, why the deterministic function is used in sec 3.2.1 is not clear.\n3. In table 1,2, 3, we see that data augmentation method has been utilized for experimental purposes. But it is not properly addressed/clarified in sec 4.1 and 4.2 why it is employed and what is the significance of this approach here.\n4. Also, these two closely related works should be critically analyzed in the experiment section:\n- Bai, H., Zhang, W., Hou, L., Shang, L., Jin, J., Jiang, X., ... & King, I. (2020). Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701.\n- Zhang, W., Hou, L., Yin, Y., Shang, L., Chen, X., Jiang, X., & Liu, Q. (2020). Ternarybert: Distillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812.\n5. Figure 2, 3, 4 are not mentioned in appropriate sections to help understand the analysis.\n\nSome minor issues and typos:\n1. There is no full stop(.) at the end of each caption of the figures and tables.\n2. At the end of the 2nd paragraph of the Introduction: \u201cSo far, previous studies have pushed down the weight and embedding to be binarized, but none of them have ever achieved to binarize BERT with 1-bit activation accurately.\u201d -> please refer to the specific previous studies.\n3. Section 2, first paragraph: BRET -> BERT\n4. Section 2.2: which can be unobstructed applied -> which can be unobstructedly applied\n5. Appendix B.2: There is no citation for DynaBERT.\n",
          "summary_of_the_review": "This paper portrays the related performance issues of straightforward fully binarized BERT and introduces the idea of BiBERT as a solution. The paper has proper experimental proofs and analysis to justify the framework. It might be a positive inclusion towards an accurate fully binarized BERT architecture. The proposed method obtained impressive savings on FLOPs and model size. However, it seems that the related SOTA approaches are missing in the comparative performance analysis section, and I would really be looking to see this in a conference paper that meets ICLR's high standards.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_84Py"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_84Py"
        ]
      },
      {
        "id": "PH_M7xJQ-dj",
        "original": null,
        "number": 3,
        "cdate": 1635859927630,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635859927630,
        "tmdate": 1635859927630,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes to binarize both the weights and activations of the BERT model. The authors find that binarizing MHA causes the most significant accuracy drop and simple distillation on MHA by minimizing the MSE between the attention scores of the full-precision model and the quantized model can harm the performance sometimes. Thus the authors propose new binary representations that maximize entropy and distillation methods that maintain direction. Empirical results on the GLUE benchmark show that the method achieves good performance \nwhen the weights are binarized and the activation is binarized/ternarized.\n",
          "main_review": "The authors stated that the bi-linear quantization function in Equation (2) is motivated by Rastegati et al., 2016 and Qin et al., 2021.  However, (2) is quite different from the quantization methods in both papers. Specifically,\n- in Rastegati et al., 2016, both the weights and activations are not zero-meaned before quantization and have scaling factors. \n- in Qin et al., 2021, the weights are both zero-meaned and uni-normalized. \nCan the authors specify in more detail the derivation of (2), and the connection/difference with the two papers?\n\nThe theory part also requires some more clarifications. Specifically,\n- Theorem 2 is based on the assumption that B_Q and B_K are entropy maximized. However, it is not clear why this assumption holds. \n- Even though the softmax function is order-preserving, maximizing the entropy of sign(A - \\phi(\\tau, A)) is not equivalent to the same as the original problem in (9).\n- Moreover, given the implication from Theorem 2 that the optimal \\phi(\\tau) is zero,  the binarization of the attention scores  sign(A - \\phi(\\tau, A)) =  sign(A) does not equal bool(A) in equation (11).\n\nThe experiments also require some more clarification. According to the original papers of BinaryBERT and TernaryBERT, the activations are only quantized to as low as 4 bits. However, in Tables 2-3, activations of these two methods are quantized to 1 or 2 bits. How are the activations quantized for these two methods?\n\nMinors:\n- Figure 3: missing the full stop at the end of the caption.",
          "summary_of_the_review": "This paper is overall structured clearly and easy to follow. However, the motivation, theory part and experiments require some further clarifications.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_tpUQ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_tpUQ"
        ]
      },
      {
        "id": "Yz7RXhNQPvg",
        "original": null,
        "number": 4,
        "cdate": 1635873170196,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635873170196,
        "tmdate": 1635908341349,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper focuses on the full binarization of BERT. Since current pre-trained models achieve promising results on NLP tasks, it is a trend to apply these models to real-world applications with constraint computation resources.  Quantization is one of the prime choices to reduce computation requirements. In recent years, BERT quantization has made significant progress. However, training binary BERT is still an important but challenging question. This paper finds that the worse performance of binary BERT can be mainly attributed to information degradation and optimization direction mismatch respectively in the forward and backward propagation. To address these problems, the authors propose two solutions, Bi-Attention and Direction-Matching Distillation. Experimental results on GLUE benchmark demonstrate that the proposed work outperforms several popular baselines. Also, ablation studies verify the effectiveness of the proposed solutions.",
          "main_review": "Strength:\n\nThe idea is well-motivated and easy to follow. The paper starts from information theory and focuses on the mutual information between binarized and full-precision representations. Considering that the ideal binarized representation should preserve the given full-precision counterparts as much as possible, it is very natural to maximize the mutual information between two representations. \n\nThe authors also find that the direction mismatch hinders the accurate optimization of the fully binarized BERT. This finding is interesting and well-motivated. \n\nExperiment results show that the proposed approaches achieve much better results than previous baselines. \n\nWeakness:\n\n I have noticed that [1] reported higher results in their paper.  It would be better to explain why the higher results are not reported in this paper. \n\nThe work of BinaryBERT uses 1-bit weight quantization and 4-bit activation quantization. It not only achieves better results than the proposed approach,  but also almost reaches the results of the full-precision model.  Since BinaryBERT and BiBERT have similar parameter sizes, the empirical contribution of BiBERT is a little bit marginal to me. \n\n[1] BinaryBERT: Pushing the Limit of BERT Quantization\n",
          "summary_of_the_review": "It is a well-motivated paper presenting an alternative understanding of the challenges of training binarized BERT. The empirical contribution is a little bit marginal since there is a large gap between the proposed approach and the best result in [1].",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_T1kK"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_T1kK"
        ]
      },
      {
        "id": "o1BbEHHCSji",
        "original": null,
        "number": 5,
        "cdate": 1635923923075,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635923923075,
        "tmdate": 1638167282379,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a full binarization of BERT (including 1-bit activation), called BiBERT. The key observation of the paper is that binary activation becomes a big challenge for the attention weights in self-attention. To solve this problem, the paper proposes a new attention binarization approach that maximizes the information entropy. The paper further proposes a Direction-Matching Distillation (DMD) scheme which encourages the similarity pattern matrices of keys, values, and hidden states in the original BERT and BiBERT to be minimized. Experimental results on GLUE datasets show that BiBERT significantly outperforms other models under the full binarization scheme.",
          "main_review": "The paper proposes a full binarization of BERT (including 1-bit activation), which is a very challenging task. The proposed bi-attention for maximum information entropy is a very interesting design for fully binary attention models. My question here is in section 2.1, when analyzing the effect of each distillation term, whether the authors used a fully binarized attention map which only contains 1, just like the Figure 4(b). In that case, I don't understand the meaning of attention distillation in Figure 3 since the binary attention map is fixed.\n\nIt is also interesting to see that the activation distillation can cause the optimization direction when the quantization becomes more aggressive, which well motivates the proposed direction-matching distillation term.\n\nThe experimental results are solid and impressive. In the 1-1-1 setting in which the paper mainly focused, the proposed BiBERT model achieves significant improvement over the 1-1-1 baseline model.",
          "summary_of_the_review": "The paper is well-motivated, novel, and well supported by the experiments.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_yAoP"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_yAoP"
        ]
      },
      {
        "id": "E1ykFngA4SI",
        "original": null,
        "number": 1,
        "cdate": 1636897840739,
        "mdate": 1636897840739,
        "ddate": null,
        "tcdate": 1636897840739,
        "tmdate": 1636897840739,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Additional Discussion Encouraged",
          "comment": "Dear Reviewers,\n\ncan you please take a look at each other's reviews? Your reviews currently straddle the decision boundary and it would be good to make sure you have considered all the perspectives provided. Please update your reviews (at least to acknowledge that you have read all reviews).\n\nThanks,\nYour Area Chair"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Area_Chair_3Z5s"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Area_Chair_3Z5s"
        ]
      },
      {
        "id": "qCuLKlVdoIV",
        "original": null,
        "number": 2,
        "cdate": 1636946846741,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636946846741,
        "tmdate": 1637605348729,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "General Response",
          "comment": "We are grateful for the ACs' and reviewers' positive feedback towards BiBERT. To assist a clearer understanding of our paper, we summarize our main contributions below:\n\nWe present BiBERT, the first accurate fully binarized BERT for efficient deep learning on NLP tasks, to alleviate the resource constraint for large pre-trained BERTs that run on resource-limited devices. Different from the existing BERTs with quantized integer parameters or with only binarized weight and embedding, fully binarized parameters of BiBERT bring extremely efficient XNOR-Bitcount bitwise operations and lower storage usage, which allows BERTs to be compressed and deployed on limited computing resources and ultra-low power devices, such as mobile phones and cameras. And applications that require real-time interaction and fast response can run steadily on these source-limited devices. However, the compact 1-bit parameters cause the fully binarized neural network to suffer extremely limited representation capabilities, and the optimization process is hindered by the discrete function, which makes the design of accurate fully binarized networks a great challenge. \n\nIn this paper, we identify that the severe performance drop of the fully binarized BERT baseline based on existing techniques can be mainly attributed to the information degradation and optimization direction mismatch respectively in the forward and backward propagation. To solve these problems, BiBERT introduces an efficient Bi-Attention structure for maximizing representation information statistically and a Direction-Matching Distillation (DMD) scheme to optimize the full binarized BERT accurately. BiBERT outperforms both the straightforward baseline and existing state-of-the-art quantized BERTs with ultra-low bit activations by convincing margins on the NLP benchmark. We highlight that as the first fully binarized BERT, our method yields impressive ${56.3\\times}$ and ${31.2\\times}$ saving on FLOPs and model size, demonstrating the vast advantages and potential of the fully binarized BERT model in real-world resource-constrained scenarios.\n\nWe also update our manuscripts; the change we made includes:\n\n- In Section 4.2, we add and discuss the performance of BiBERT on compact architectures (TinyBERT-6L and TinyBERT-4L), which shows that our BiBERT has outstanding performance on various architectures;\n- In Section 4.2, we add more discussions and analysis of related methods (especially BinaryBERT and TernaryBERT);\n- In Appendix A.6, we add relevant discussions and proofs about the equivalence of information entropy maximization;\n- In Appendix B.2, we add relevant details about the implementation of the comparison method (BinaryBERT and TernaryBERT) under 1-bit and 2-bit activation settings;\n- In Section 4.2 and Appendix C.1, we add the results and related discussion of strengthened baselines.\n- We carefully correct our references in the revised version.\n\nFor the detailed explanation, please see our responses to each reviewer.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "-6PDgKnfHB",
        "original": null,
        "number": 3,
        "cdate": 1636946891737,
        "mdate": 1636946891737,
        "ddate": null,
        "tcdate": 1636946891737,
        "tmdate": 1636946891737,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "o1BbEHHCSji",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer yAoP",
          "comment": "We are deeply grateful for the reviewer\u2019s support of our work and we thank the reviewer for the constructive and helpful suggestions. We provide additional discussions below:\n\n**Q1:** My question here is in section 2.1, when analyzing the effect of each distillation term, whether the authors used a fully binarized attention map which only contains 1, just like the Figure 4(b). In that case, I don't understand the meaning of attention distillation in Figure 3 since the binary attention map is fixed.\n\n**A1:** We clarify that, as in Eq. (6) presents, the fully binarized BERT baseline distills the full-precision attention scores $\\mathbf{A}$ obtained by Eq. (4) instead of the binarized attention weight (attention map) $\\mathbf{B_A^s}$ (or $\\mathbf{B_A}$). Thus, though the forward representation of the binarized attention weight is fixed as shown in Figure 4, the distillation loss of the attention score still updates the student network through backward propagation.\n\nIn the fully binarized BERT baseline introduced in Section 2, the distillation loss of the attention score is adopted following the settings in [1,2]. The original intention of this design is to enable the quantized student network to utilize the knowledge in the attention scores of the full-precision teacher BERT. However, our observation experiments show that the distillation of attention score instead hinders the training of the student BERT in the full binarization (as shown in Figure 3(b)), which is further verified in Section 3 to be caused by the optimization direction mismatch in the complete binarization of the attention structure. This phenomenon motivates us to design the novel Direction-Matching Distillation (DMD) scheme for fully binarized BERT, which distills the upstream query $\\mathbf Q$ and key $\\mathbf K$ instead of attention score in our DMD to utilize its knowledge and alleviate direction mismatch. The experiments show that DMD provides the matching optimization direction (Figure 5(b)) and significantly improves the performance.\n\n**Reference**\n\n[1] Zhang, Wei, et al. \"TernaryBERT: Distillation-aware Ultra-low Bit BERT.\" EMNLP. 2020.\n\n[2] Jiao, Xiaoqi, et al. \"TinyBERT: Distilling BERT for Natural Language Understanding.\" EMNLP Findings. 2020."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "hXZdspQDX58",
        "original": null,
        "number": 4,
        "cdate": 1636946961098,
        "mdate": 1636946961098,
        "ddate": null,
        "tcdate": 1636946961098,
        "tmdate": 1636946961098,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "Yz7RXhNQPvg",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer T1kK",
          "comment": "We would like to first express our deep appreciation for your comments, and our response are as follows:\n\n\n\n**Q1:** I have noticed that [1] reported higher results in their paper. It would be better to explain why the higher results are not reported in this paper.\n\nThe work of BinaryBERT uses 1-bit weight quantization and 4-bit activation quantization. It not only achieves better results than the proposed approach, but also almost reaches the results of the full-precision model. Since BinaryBERT and BiBERT have similar parameter sizes, the empirical contribution of BiBERT is a little bit marginal to me.\n\nBinaryBERT: Pushing the Limit of BERT Quantization\n\n**A1:** We first need to state that the fully binarized network with binarized weight and activation (and embedding) is completely different from the binarized weight network since the former far surpasses the latter in computation and energy efficiency [1,2]. Fully binarized parameters bring extremely efficient XNOR-Bitcount bitwise operations [2] with ultra-low energy consumption (achieve ${89\\times}$ energy efficiency) [3] and lower storage usage. Therefore, the full binarization allows the network to be compressed and deployed on limited computing resources and ultra-low power devices, such as mobile phones and cameras, and also be suitable for applications that require real-time interaction and fast response. However, the extremely limited representation capabilities and the difficult discrete optimization make the accurate fully binarized networks a great challenge. As our experiments show, when we push the other existing methods to fully binarized (such as BinaryBERT [4]), the accuracy of these models drops sharply and does not even converge on some tasks.\n\nThe BiBERT is the first full binarization method for BERT models that applies 1-bit weights, activations, and embeddings, and thereby saves extreme  ${56.3\\times}$ computational FLOPs and ${31.2\\times}$ storage compared to the full-precision BERT. As a closely related competitor, BinaryBERT [4] only binarizes the weight and embedding while keeping activation to 4-bit or 8-bit, which results in executing the more computation-consuming integer operation rather than bitwise operation. For example, 1-1-4 BinaryBERT suffers ${3.8\\times}$ FLOPs consumption compared to 1-1-1 BiBERT. Therefore, BiBERT has a huge improvement in computing efficiency.  \n\nTo be fair, all the networks in the comparative experiments should be set to the same 1-1-1 bit-width to have a similar computation and storage level, otherwise, the accuracy comparison does not make sense. For BinaryBERT, the 4-bit activation setting brings a stronger representation with ${3.8\\times}$ computational consumption than BiBERT, which reasonably makes the accuracy of the network far exceed that with 1-bit activation. Our BiBERT not only surpasses all existing methods at the 1-1-1 bit setting but also surpasses some of them with higher activation bit-widths, such as 1-1-2 bit BinaryBERT (${2.0\\times}$ FLOPs consumption) and 2-2-2 bit TernaryBERT (${3.8\\times}$ FLOPs consumption), which more forcefully proves that our designs help the full binarized BERT to achieve higher accuracy.\n\n\n\n**Reference**\n\n[1] Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. \"Binaryconnect: Training deep neural networks with binary weights during propagations.\" NeurIPS. 2015.\n\n[2] Rastegari, Mohammad, et al. \"Xnor-net: Imagenet classification using binary convolutional neural networks.\" ECCV. 2016.\n\n[3] Chen, Gang, et al. \"PhoneBit: efficient gpu-accelerated binary neural network inference engine for mobile phones.\" DATE. 2020.\n\n[4] Bai, Haoli, et al. \"BinaryBERT: Pushing the Limit of BERT Quantization.\" ACL. 2021."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "YEHGb6lKivT",
        "original": null,
        "number": 5,
        "cdate": 1636947023072,
        "mdate": 1636947023072,
        "ddate": null,
        "tcdate": 1636947023072,
        "tmdate": 1636947023072,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "PH_M7xJQ-dj",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer tpUQ (1/3)",
          "comment": "We thank the reviewer for the feedback and comments. We respond to the concerns below:\n\n**Q1:** The authors stated that the bi-linear quantization function in Equation (2) is motivated by Rastegati et al., 2016 and Qin et al., 2021. However, (2) is quite different from the quantization methods in both papers. Specifically,\n\n- in Rastegati et al., 2016, both the weights and activations are not zero-meaned before quantization and have scaling factors.\n- in Qin et al., 2021, the weights are both zero-meaned and uni-normalized. Can the authors specify in more detail the derivation of (2), and the connection/difference with the two papers?\n\n**A1:** We are sorry for your confusion. For the bi-linear defined in Eq. (2), we follow XNOR-Net [1] to apply the scaling factors calculated from the mean of the absolute value to minimize the quantization error. The weight balance technique is also applied in XNOR-Net according to their official implementation though it is not mentioned in their paper (please refer to https://github.com/allenai/XNOR-Net/blob/4006d594fe0349a5ed4a5ba6795641d9eecb3aac/train.lua#L169 ).  And [2] first points out that the weight balance technique improves the binarized network by maximizing the information entropy of binarized parameters, which presents 1.3% gain (85.2% vs 86.5% [2]) on ResNet-20 architecture and CIFAR-10 dataset. Furthermore, in Table A1.1, we further evaluate that the bi-linear with zero-mean weight on some tasks that 1-1-1 BinaryBERT crashed (CoLA and RTE), which shows that it significantly improves the accuracy and allows the model to converge.\n\n**Table A1.1: Comparison results on CoLA and RTE**\n\n| Method                 | #Bit  | CoLA | RTE   |\n| ---------------------- | ----- | ---- | ----- |\n| BinaryBERT             | 1-1-1 | 0.0% | 52.7% |\n| BinaryBERT (bi-linear) | 1-1-1 | 8.3% | 54.2% |\n\nThe difference between the bi-linear we applied and XNOR-Net is that the scaling factor of activation is not used because it is dynamically calculated in real-time during inference which increases the computational cost. This is also a common practice in many recent binarization works [2,3,4].\n\n\n\n**Q2:** Theorem 2 is based on the assumption that B_Q and B_K are entropy maximized. However, it is not clear why this assumption holds.\n\n**A2:** Since the application of the bi-linear defined in Eq. (2) (and discussed in A1), the expected information entropy of both B_Q and B_K is maximized.\n\nAs shown in Eq. (3) and Eq. (4), Q and K are output by the corresponding bi-lieanr and then binarized by the sign function to obtain $\\mathbf{B_Q}$ and $\\mathbf{B_K}$. The application of zero-mean pre-binarized weight in the bi-linear layer maximizes the information entropy of the binarized activation (such as $\\mathbf{B_Q}$ and $\\mathbf{B_K}$) in fully binarized BERT as described in Section 3.2.1 (Paragraph 1), and the related proof is shown in Appendix A.1."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "TFFpPgy4V0z",
        "original": null,
        "number": 6,
        "cdate": 1636947064624,
        "mdate": 1636947064624,
        "ddate": null,
        "tcdate": 1636947064624,
        "tmdate": 1636947064624,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "YEHGb6lKivT",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer tpUQ (2/3)",
          "comment": "**Q3:** Even though the softmax function is order-preserving, maximizing the entropy of sign(A - \\phi(\\tau, A)) is not equivalent to the same as the original problem in (9).\n\n**A3:** We clarify that when the elements in tensor $\\mathbf A$ are assumed as independent and identically distributed, there exists a $\\phi(\\tau, \\mathbf A))$ making the entropy maximization of $\\operatorname{sign}(\\mathbf A - \\phi(\\tau, \\mathbf A))$ be equivalent to that of $\\operatorname{sign}(\\operatorname{softmax}(\\mathbf A) - \\tau)$. \n\nSpecifically, considering the order-preserving characteristic of both $\\operatorname{sign}$ and $\\operatorname{softmax}$, only the largest $n$ elements (the value of $n$ is related to the variable $k$ for specific matrix $\\mathbf A$ in Theorem 1) are binarized to $1$ while others are binarized to $-1$. Therefore, when $\\operatorname{sign}(\\operatorname{softmax}(\\mathbf A) - \\tau)$ is optimized to have the maximized information entropy, there exists a corresponding threshold $\\phi(\\tau, \\mathbf A)$ that maximizes the information entropy of $\\operatorname{sign}(\\mathbf A-\\phi(\\tau, \\mathbf A))$. In addition, the conclusion of Theorem 2 that $\\mathbf A\\sim\\mathcal N(0, D)$ suggests that a fixed threshold $\\phi(\\tau, \\mathbf A)=0$ maximizes the information entropy of the binarized attention weight.\n\n**Proof.** As shown in Eq. (8), the information entropy of $\\mathbf{\\hat{B}_{\\mathbf{w}}}^A =\\operatorname{sign}(\\operatorname{softmax}(\\mathbf{A})-\\tau)$ can be expressed as:\n\n$$\\mathcal H(\\mathbf{\\hat{B}_{\\mathbf{w}}}^A)=$$\n\n$$-\\left(\\int_{\\tau}^{\\infty} f(a^s) da^s\\right) \\log\\left(\\int_{\\tau}^{\\infty} f(a^s) da^s \\right)-\\left(\\int_{-\\infty}^{\\tau} f(a^s) da^s\\right) \\log\\left(\\int_{-\\infty}^{\\tau} f(a^s) da^s\\right),$$\n\nwhere $a^s = \\operatorname{softmax}_A(a)$ and $a \\in \\mathbf A$. Since $\\operatorname{softmax}$ function is strictly order-preserving, there exists a $\\phi(\\tau, \\mathbf A)$ makes $\\forall a>\\tau$, $a^s> \\phi(\\tau, \\mathbf A)$, and it thereby makes following equations established:\n\n$$\\int_{\\tau}^{\\infty} f(a^s) da^s=\\int_{\\phi(\\tau, A)}^{\\infty} g(a) da,$$\n\n$$\\int_{-\\infty}^{\\tau} f(a^s) da^s=\\int_{\\infty}^{\\phi(\\tau, A)} g(a) da,$$\n\nwhere $f$ and $g$ are the probability density functions of variable $a^s$ and $a$, respectively. And the information entropy  is of $\\mathbf{\\hat{B}_{\\mathbf{w}}}^A$ equal to that of $\\mathbf{B_A}$, and $\\mathbf{B_A} = \\operatorname{sign}(\\mathbf A-\\phi(\\tau, \\mathbf A))$. Therefore, when $\\tau$ is optimized to maximize information, the information of $\\mathbf {B_A}$ is also maximized under the threshold $\\phi(\\tau, \\mathbf A)$.\n\n\n\n**Q4:** Moreover, given the implication from Theorem 2 that the optimal \\phi(\\tau) is zero, the binarization of the attention scores sign(A - \\phi(\\tau, A)) = sign(A) does not equal bool(A) in equation (11).\n\n**A4:** We further clarify and prove that for binarization functions sign and bool, the conditions of information entropy maximum for binarized parameters are the same, and $\\phi(\\tau)=0$ in both cases. Based on this prerequisite, Bi-Attention applies the bool function to binarize attention weight for reviving the attention mechanism (in Section 3.2.2).\n\nSpecifically, since our bool function only redefined the value after binarization but keep the threshold of binarization at 0 (same as the sign function), the information entropy of binarized parameters can be expressed as:\n\n$\\max_{\\phi}  \\mathcal H(\\operatorname{sign}(A-\\phi)) = -(\\int_{\\phi}^{\\infty} f(x)) \\log (\\int_{\\phi}^{\\infty} f(x)) - (\\int_{-\\infty}^{\\phi} f(x)) \\log (\\int_{-\\infty}^{\\phi} f(x)),$    (1)\n\n$\\max_{\\phi} \\mathcal H(\\operatorname{bool}(A-\\phi)) = -(\\int_{\\phi}^{\\infty} f(x)) \\log (\\int_{\\phi}^{\\infty} f(x)) - (\\int_{-\\infty}^{\\phi} f(x)) \\log (\\int_{-\\infty}^{\\phi} f(x)).$    (2)\n\nSince $\\max_{\\phi} \\mathcal H(\\operatorname{sign}(A-\\phi))$ and $\\max_{\\phi} \\mathcal H(\\operatorname{bool}(A-\\phi))$ are trivially equal, the optimal $\\phi$ for (1) and (2) are the same. Theorem 2 further shows that an optimal value is 0.\n\nThe application of the bool function to binarize attention weight in Bi-Attention is for reviving the attention mechanism to capture crucial elements, and the above discussion shows that using it instead of sign function does not cut down the information entropy which is statistical maximized. Moreover, we point out that since the elements of binarized attention weight in the fully binarized BERT baseline are all 1 (all values before binarization are greater than 0), the discussion on its attention weight applies to any case of sign and bool functions (the latter is applied in the Bi-Attention of BiBERT), no matter it is from aspects of the information entropy or the specific value."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "42PNG1CKjZd",
        "original": null,
        "number": 7,
        "cdate": 1636947170697,
        "mdate": 1636947170697,
        "ddate": null,
        "tcdate": 1636947170697,
        "tmdate": 1636947170697,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "TFFpPgy4V0z",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer tpUQ (3/3)",
          "comment": "**Q5:** The experiments also require some more clarification. According to the original papers of BinaryBERT and TernaryBERT, the activations are only quantized to as low as 4 bits. However, in Tables 2-3, activations of these two methods are quantized to 1 or 2 bits. How are the activations quantized for these two methods?\n\n**A5:** Thanks for pointing out that. We have updated more details of BinaryBERT [5] and TernaryBERT [6] under 1-bit and 2-bit activation settings in the revision (Appendix B.2).\n\nBiBERT is the first work towards the fully binarized BERT with 1-bit weight, activation, and embedding, and enjoys extremely high computational efficiency and memory usage as we discussed in A1. To be fair, all the networks in the comparative experiments should be set to the same 1-1-1 bit-width to have a similar computation and storage level, otherwise, the accuracy comparison does not make sense. However, there is no existing BERT quantization method that binarizes activation, we thereby have to use the most closely related works BinaryBERT and TernaryBERT as the major competitors, which quantize weight and embedding to ultra-low bit-width (1-bit or 2-bit), and further reduce the activation to 1-bit or 2-bit to compare the accuracy rate under similar computational and storage efficiency. The quantization processes of activation follow official or fair implementation as much as possible.\n\nFor BinaryBERT, except 4-bit and 8-bit activation settings mentioned in their paper [5], the official code provides the implementations for 1-bit or 2-bit activation settings, which are Binary-Weight-Network (BWN) and Ternary-Weight-Network (TWN), respectively, and we completely follow the released code.\n\nSpecifically, we apply Binary-Weight-Network (BWN) and Ternary-Weight-Network (TWN) methods to activation under 1-bit and 2-bit input settings, respectively. The formulations and implementations are presented in [5] and official codes, respectively:\n\nBWN: $\\mathcal{Q}(a_{i}^{b})=\\beta \\cdot \\operatorname{sign}\\left(a_{i}^{b}\\right), \\beta=\\frac{1}{n}\\left\\|\\mathbf{a}^{b}\\right\\|_{1},$\n\nTWN: $\\mathcal{Q}\\left(a_{i}^{t}\\right)=\\beta \\cdot \\operatorname{sign}\\left(a_{i}^{t}\\right), \\left|a_{i}^{t}\\right| \\geq \\Delta; 0,  \\left|a_{i}^{t}\\right|<\\Delta.$\n\nBinaryBERT 1-bit & 2-bit input: [https://github.com/huawei-noah/Pretrained-Language-Model/blob/1dbaf58f17b8eea873d76aa388a6b0534b9ccdec/BinaryBERT/transformer/utils_quant.py](https://github.com/huawei-noah/Pretrained-Language-Model/blob/1dbaf58f17b8eea873d76aa388a6b0534b9ccdec/BinaryBERT/transformer/utils_quant.py#L428) (in the act_quant_fn function)\n\nFor TernaryBERT, since there is no recommended setting under 1-bit and 2-bit input in its official code, we use the same implementation as BinaryBERT to quantize activation (BWN for 1-bit and TWN for 2-bit) for fair comparison.\n\nIn fact, existing work [5] has also pointed out that quantizing the activation is of great challenge and the ultra-low bit-width activation quantization on the existing method may lead to too bad results, which is exactly the problem our BiBERT devotes to solve.  Compared with TernaryBERT and BinaryBERT, under the 1-bit input setting, our BiBERT applies a simple sign function to binarize activation, while the former two apply real-time calculated scaling factors. Therefore, benefitting from the well-designed structure and distillation scheme, our BiBERT achieved better results with a smaller amount of computation.\n\n\n\n**Q6:** Minors: Figure 3: missing the full stop at the end of the caption.\n\n**A6:** Thanks for pointing out. We revised the issue in the revision and carefully revised the full manuscript.\n\n\n\n**Reference**\n\n[1] Rastegari, Mohammad, et al. \"Xnor-net: Imagenet classification using binary convolutional neural networks.\" ECCV. 2016.\n\n[2] Qin, Haotong, et al. \"Forward and backward information retention for accurate binary neural networks.\" CVPR. 2020.\n\n[3] Liu, Zechun, et al. \"Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm.\" ECCV. 2018.\n\n[4] Liu, Zechun, et al. \"Reactnet: Towards precise binary neural network with generalized activation functions.\" ECCV. 2020.\n\n[5] Bai, Haoli, et al. \"BinaryBERT: Pushing the Limit of BERT Quantization.\" ACL. 2021.\n\n[6] Zhang, Wei, et al. \"TernaryBERT: Distillation-aware Ultra-low Bit BERT.\" EMNLP. 2020."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "r8FhIVuorVn",
        "original": null,
        "number": 8,
        "cdate": 1636947264402,
        "mdate": 1636947264402,
        "ddate": null,
        "tcdate": 1636947264402,
        "tmdate": 1636947264402,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "YkLvJCjGB0q",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 84Py (1/2)",
          "comment": "We thank for your professional and insightful comments. Our response to your suggestion can be found below:\n\n\n\n**Q1:** The proposed method didn\u2019t properly address the performance drop due to the binarization of activation. Results on the GLUE benchmark showed a drastic performance drop in comparison to the related approaches and full precision BERT.\n\n**A1:** Although BiBERT has extreme computational savings and achieves significant progress on most tasks, we admit that there is still a significant gap in accuracy between the fully-binarized and full-precision BERTs. In fact, full binarization presents a great potential for large-scale language models. And there is a huge demand for NLP models to run on limited computing resources and ultra-low power devices, such as mobile phones and cameras. Recently, fully binarized networks have been widely studied in many fields (e.g., image classification [1], data generation [2], object detection [3], point cloud processing [4]) for the extremely compact 1-bit parameters and efficient bitwise operations. These models achieve real-time interaction and fast response after quantization on source-limited devices. Compared to these successful practices, large pre-trained language models have greater demands for model quantization since the larger parameter sizes and more computational cost. Moreover, the fully binarized networks are also constantly approaching SOTA accuracy in these fields [4,5,6] with the proposing of well-designed architectures and training schemes. \n\nOur BiBERT, for the first time, presents a promising route towards the accurate fully binarized BERT for NLP. As analyzed in our paper, fully binarizing BERTs under the existing architecture and training scheme (such as the BinaryBERT and fully binarized BERT baseline) leads to severe information degradation and optimization of mismatch directions, which leads to a sharp drop in the accuracy and even divergence in certain tasks. As a pioneer work, BiBERT first realizes the convergence of fully binarized BERT by significantly relieving the above issues, while enjoying ultra-high computational FLOPs reduction and storage savings ($56.3\\times$ and $31.2\\times$, respectively), and also far exceeds the fully binarized models obtained by existing BERT quantization methods. We believe that the fully binarized BERT will develop in the future, and will continue to approach SOTA accuracy while maintaining ultra-high computing and storage efficiency. \n\n\n\n**Q2:** In sec 2.1, it is evident that a stochastic sign function is used to binarize the BERT, therefore, why the deterministic function is used in sec 3.2.1 is not clear.\n\n**A2:** We clarify the sign function used in our whole paper is deterministic as defined in Eq. (1) of Section 2.1. \n\nIn the papers proposing STE [7] and some early binarized networks [8], the stochastic sign function was applied or mentioned. However, in recent and the most proposed binarized networks [1,2,3,4,5,6], the deterministic function is routinely adopted for the advantages of computational efficiency and hardware implementation. Therefore, we use a deterministic binarization function in the forward propagation and STE in the backward propagation.\n\n\n\n**Q3:** In table 1,2, 3, we see that data augmentation method has been utilized for experimental purposes. But it is not properly addressed/clarified in sec 4.1 and 4.2 why it is employed and what is the significance of this approach here.\n\n**A3:** Thank you for pointing it out. We further clarify the method and significance of data augmentation in the revision. \n\nWe apply the data augmentation proposed in [9] as a strategy to increase the diversity of data available for the training models, which is orthogonal to model binarization and beneficial to all the BERT training methods. Previous studies like BinaryBERT [10] and TernaryBERT [11] also use the same data augmentation method as one of the default settings in the experiment. Thus, we evaluated BiBERT and other methods on augmented data for fair comparison. Moreover, for comprehensive comparison, we also include experiments without data augmentation. Experiments show that BiBERT performs better than other methods whether data augmentation is used or not. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "olM7maLYjrs",
        "original": null,
        "number": 9,
        "cdate": 1636947302579,
        "mdate": 1636947302579,
        "ddate": null,
        "tcdate": 1636947302579,
        "tmdate": 1636947302579,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "r8FhIVuorVn",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 84Py (2/2)",
          "comment": "**Q4:** Also, these two closely related works should be critically analyzed in the experiment section:\n\n- Bai, H., Zhang, W., Hou, L., Shang, L., Jin, J., Jiang, X., ... & King, I. (2020). Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701.\n- Zhang, W., Hou, L., Yin, Y., Shang, L., Chen, X., Jiang, X., & Liu, Q. (2020). Ternarybert: Distillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812.\n\n**A4:** Thanks for your constructive suggestions. We add more discussions and deeper analysis about these methods [10,11] in the experiment section.\n\nOur BiBERT surpasses existing methods on BERT$_\\text{BASE}$ architecture by a wide margin in the average accuracy, and first achieves convergence under ultra-low bit activation on some tasks, such as CoLA, MRPC, and RTE. While under ultra-low bit activation, the accuracy of TernaryBERT and BinaryBERT decreases severely which also does not improve under the 2-2-2 setting (about $4\\times$ FLOPs and $2\\times$ storage usage increase). On some specific tasks like MRPC, higher bit-widths and consumption did not even make TernaryBERT and BinaryBERT converge. We also noticed that BiBERT lags behind Q-BERT (2-8-8) and TernaryBERT (2-2-2) on MNLI and STS-B (with DA) while surpassing them on other tasks, indicating the fully binarized BERT may have greater improvement potential in these tasks.\n\n\n\n**Q5:** Figure 2, 3, 4 are not mentioned in appropriate sections to help understand the analysis.\n\n**A5:** Thanks for pointing it out. We have rearranged the figure positions to better help the illustration.\n\n\n\n**Q6:** Some minor issues and typos.\n\n**A6:** Thanks for pointing it out. We have carefully revised these issues and refined our paper in the revision. \n\n\n\n**Reference**\n\n[1] Martinez, Brais, et al. \"Training binary neural networks with real-to-binary convolutions.\" ICLR. 2019.\n\n[2] Bird, Thomas, Friso Kingma, and David Barber. \"Reducing the Computational Cost of Deep Generative Models with Binary Neural Networks.\" ICLR. 2020.\n\n[3] Wang, Ziwei, et al. \"BiDet: An efficient binarized object detector.\" CVPR. 2020.\n\n[4] Qin, Haotong, et al. \"BiPointNet: Binary Neural Network for Point Clouds.\" ICLR. 2020.\n\n[5] Liu, Zechun, et al. \"Reactnet: Towards precise binary neural network with generalized activation functions.\" ECCV. 2020.\n\n[6] Xu, Yixing, et al. \"Learning Frequency Domain Approximation for Binary Neural Networks.\" NeurIPS. 2021.\n\n[7] Bengio, Yoshua, Nicholas L\u00e9onard, and Aaron Courville. \"Estimating or propagating gradients through stochastic neurons for conditional computation.\" arXiv preprint arXiv:1308.3432 (2013).\n\n[8] Courbariaux, Matthieu, et al. \"Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1.\" arXiv preprint arXiv:1602.02830 (2016).\n\n[9] Jiao, Xiaoqi, et al. \"TinyBERT: Distilling BERT for Natural Language Understanding.\" EMNLP Findings. 2020.\n\n[10] Bai, Haoli, et al. \"BinaryBERT: Pushing the Limit of BERT Quantization.\" ACL. 2021.\n\n[11] Zhang, Wei, et al. \"TernaryBERT: Distillation-aware Ultra-low Bit BERT.\" EMNLP. 2020."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "1rlEtM6kZZv",
        "original": null,
        "number": 10,
        "cdate": 1636947341074,
        "mdate": 1636947341074,
        "ddate": null,
        "tcdate": 1636947341074,
        "tmdate": 1636947341074,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "4inq0vvKBiQ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer TtMP (1/3)",
          "comment": "We thank the reviewer for the constructive feedback and comments. We respond to the concerns below:\n\n\n\n**Q1:** The performance of BiBERT on GLUE is still far behind the full precision model (67.0 vs 82.3). This large performance gap may hinder application of the BiBERT model in many real scenarios (e.g., CoLA, STS-B tasks). While the BinaryBERT with 1-1-4 quantization, which has similar model size and a little bit higher computation as the BiBERT, can achieve GLUE score of 81.9 with almost neglectable performance loss. This raises the question why full binarization of the BERT model is necessary or preferred. One possible way to justify this is to enlarge the network size while keeping it binary and see if the performance can consistently improve.\n\n**A1:** We need to state that the fully binarized (1-1-1 bit-width) BERT has great computational efficiency advantages benefiting from the extremely efficient bitwise operations in the inference, while the BERT with binarized weight and 4 or 8-bit activation (such as 1-1-4/1-1-8 BinaryBERT [1]) conducts integer operations which is far less efficient. Fully binarized parameters bring extremely efficient XNOR-Bitcount bitwise operations [2] with ultra-low energy consumption (achieve $89\\times$ energy efficiency) [3] and lower storage usage. The full binarization allows the network to be compressed and deployed on limited computing resources and ultra-low power devices, such as mobile phones and cameras, and also enables the applications that require real-time interaction and fast response to run steadily on edge devices. However, the extremely limited representation capabilities and the difficult discrete optimization make the accurate fully binarized networks a great challenge. \n\nBiBERT is a fully binarized network that applies 1-bit weight, activation, and embedding, and utilizes bitwise operations to achieve a $56.3\\times$ saving in FLOPs compared to full-precision BERT. By contrast, BinaryBERT [1] keeps activations to 4-bit which suffers integer operations and $3.8\\times$ FLOPs consumption compared to BiBERT. Therefore, BiBERT is significant since it first realizes the convergence of fully binarized BERT, which enjoys high computational FLOPs reduction and storage-saving ($56.3\\times$ and $31.2\\times$, respectively). We admit that the performance of BiBERT on GLUE is still behind the full precision model, but it far exceeds the fully binarized models obtained by existing BERT quantization methods. And just like the model binarization widely studied in other fields (e.g. image classification, object detection, point cloud processing), full binarization of NLU models is prospective, and the accuracy will constantly approach SOTA with deeper studies in the future."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "EgDFY-3cX5L",
        "original": null,
        "number": 11,
        "cdate": 1636947385741,
        "mdate": 1636947385741,
        "ddate": null,
        "tcdate": 1636947385741,
        "tmdate": 1636947385741,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "1rlEtM6kZZv",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer TtMP (2/3)",
          "comment": "**Q2 (1):** In the experiments, the authors only report BinaryBERT (1-1-2 and 1-1-1) and TernaryBERT (2-2-1 and 2-2-2) which are not present in their original papers. The author should clarify in the paper how the results are obtained. Are the results from reimplementation of their algorithms or based on some open-source code/models? Are the reimplementation correct? \n\n**A2 (1):** Thanks for pointing out that. We update more details about the implementations of BinaryBERT [1] and TernaryBERT [4] under 1-bit and 2-bit activation settings in the revision (Appendix B.2).\n\nBiBERT is the first work towards the fully binarized BERT with 1-bit weight, activation, and embedding, and enjoys extremely high computational efficiency and memory usage as we discussed in A1. To be fair, all the networks in the comparative experiments should be set to the same 1-1-1 bit-width to have a similar computation and storage level, otherwise, the accuracy comparison does not make sense. However, there is no existing BERT quantization method that binarizes activation, we thereby have to use the most closely related works BinaryBERT and TernaryBERT as the major competitors, which quantize weight and embedding to ultra-low bit-width (1-bit or 2-bit), and further reduce the activation to 1-bit or 2-bit to compare the accuracy rate under similar computational and storage efficiency. The quantization processes of activation follow official or fair implementation as much as possible.\n\nFor BinaryBERT, though it is not mentioned in their paper, the official code provides an implementation for 1 or 2-bit inputs quantization, which is Binary-Weight-Network (BWN) and Ternary-Weight-Network (TWN) respectively. And we completely follow the released code. The implementations of BWN and TWN can be formulated as follows: \n\nBWN: $\\mathcal{Q}(a_{i}^{b})=\\beta \\cdot \\operatorname{sign}\\left(a_{i}^{b}\\right), \\beta=\\frac{1}{n}\\left\\|\\mathbf{a}^{b}\\right\\|_{1},$\n\nTWN: $\\mathcal{Q}\\left(a_{i}^{t}\\right)=\\beta \\cdot \\operatorname{sign}\\left(a_{i}^{t}\\right), \\left|a_{i}^{t}\\right| \\geq \\Delta; 0,  \\left|a_{i}^{t}\\right|<\\Delta.$\n\nPlease refer to BinaryBERT official repo for implementation details: https://github.com/huawei-noah/Pretrained-Language-Model/blob/1dbaf58f17b8eea873d76aa388a6b0534b9ccdec/BinaryBERT/transformer/utils_quant.py#L425\n\nFor TernaryBERT, since there is no recommended setting under 1-bit and 2-bit input in its official code, we use the same implementation as BinaryBERT to quantize activation (BWN for 1-bit and TWN for 2-bit) for fair comparison.\n\nIn fact, existing work [1] has also pointed out that quantizing the activation is of great challenge and the ultra-low bit-width activation quantization on the existing method may lead to too bad results, which is exactly the problem our BiBERT devotes to solve.  Compared with TernaryBERT and BinaryBERT, under the 1-bit input setting, our BiBERT applies a simple sign function to binarize activation, while the former two apply real-time calculated scaling factors. Therefore, benefitting from the well-designed structure and distillation scheme, our BiBERT achieved better results with a smaller amount of computation."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "Fg771hU713",
        "original": null,
        "number": 12,
        "cdate": 1636947417780,
        "mdate": 1636947417780,
        "ddate": null,
        "tcdate": 1636947417780,
        "tmdate": 1636947417780,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "EgDFY-3cX5L",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer TtMP (3/3)",
          "comment": "**Q2 (2):** On CoLA task, BinaryBERT and TenaryBERT simply seem to diverge, and the baseline quantization method without Bi-Attention and DMD also outperform BinaryBERT and TenaryBERT.\n\n**A2 (2):** For the results of TernaryBERT (2-2-1 and 2-2-2) and BinaryBERT (1-1-2 and 2-2-1) CoLA and other diverged tasks (such as RTE), the severely limited representation ability and optimization difficulties caused by the ultra-low bit activation are the main reasons for the sharp drop in the accuracy, while the direct application of existing methods cannot solve these problems.\n\nSpecifically, compared with 1-1-1 BinaryBERT, our fully binarized BERT baseline applies the bi-linear defined in Eq. (2) throughout the network, which significantly improves the binarized network by maximizing the information entropy of binarized weight and activation. Our further experiments show the effectiveness of bi-linear with zero-mean weight on some tasks that 1-1-1 BinaryBERT crashed (CoLA and RTE) in Table A2.1, which shows that it significantly improves the accuracy and allows the model to converge.\n\n**Table A2.1: Comparison results on CoLA and RTE**\n\n| Method                 | #Bit  | CoLA | RTE   |\n| ---------------------- | ----- | ---- | ----- |\n| BinaryBERT             | 1-1-1 | 0.0% | 52.7% |\n| BinaryBERT (bi-linear) | 1-1-1 | 8.3% | 54.2% |\n\nMoreover, as analyzed in our paper, though the information entropy maximized bi-linear is applied, fully binarizing BERTs under the existing architecture and training scheme (such as the 1-1-1 BinaryBERT and fully binarized BERT baseline) leads to severe information degradation and optimization of mismatch directions, which results in a sharp drop in the accuracy and even divergence in certain tasks. BiBERT further significantly relieves the above issues and improves accuracy, which makes it a pioneer work of the accurate fully binarized BERT.\n\n\n\n**Q3:** The author report Q2BERT and Q-BERT on the 2-2-8 setting, while the TernaryBERT have much higher performance (72.9) on it. Why not report these results?\n\n**A3:** As mentioned in A2(1), all BERTs for comparison should be set to similar bit-widths for fairness, and we use the most closely related works BinaryBERT and TernaryBERT as the major competitors in our paper. The 2-2-8 TernaryBERT truly performs better in terms of accuracy, but it suffers about $30\\times$ FLOPs computation and $2\\times$ and storage usage compared with the fully binarized BERT, which leads to an unfair comparison. Though the 1-1-1 BiBERT even surpasses the 2-2-8 Q-BERT and QBERT due to its well-designed architecture and training scheme (we directly quote the results in [4]), it does not mean that BiBERT is able to exceed all methods under 2-2-8 bit-width setting.\n\n\n\n**Reference**\n\n[1] Bai, Haoli, et al. \"BinaryBERT: Pushing the Limit of BERT Quantization.\" ACL. 2021.\n\n[2] Rastegari, Mohammad, et al. \"Xnor-net: Imagenet classification using binary convolutional neural networks.\" ECCV. 2016.\n\n[3] Chen, Gang, et al. \"PhoneBit: efficient gpu-accelerated binary neural network inference engine for mobile phones.\" DATE. 2020.\n\n[4] Zhang, Wei, et al. \"TernaryBERT: Distillation-aware Ultra-low Bit BERT.\" EMNLP. 2020.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "4AW1DBmC7bh",
        "original": null,
        "number": 1,
        "cdate": 1637152648168,
        "mdate": 1637152648168,
        "ddate": null,
        "tcdate": 1637152648168,
        "tmdate": 1637152648168,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Public_Comment",
        "content": {
          "title": "About visualization of the results of proposed Bi-Attention structure",
          "comment": "Dear authors, thanks for your cool work. I'm really curious about the visualization of the attention weight obtained from the proposed Bi-Attention structure (Eq.11). Can you kindly show any examples if time permits\uff1f"
        },
        "signatures": [
          "~Jack_Ma3"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "~Jack_Ma3"
        ]
      },
      {
        "id": "LKnvJu9OhJ",
        "original": null,
        "number": 13,
        "cdate": 1637240150721,
        "mdate": 1637240150721,
        "ddate": null,
        "tcdate": 1637240150721,
        "tmdate": 1637240150721,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "olM7maLYjrs",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Authors",
          "comment": "Thanks to the authors for the response. They clarified most of my questions. But the main concern remains the same i.e. comparison with SOTA approaches instead of setting them with BiBERT settings. I think this paper should address the comparison with the SOTA methods as a standard paper. The inclusion of the SOTA methods can help demonstrate the differences and highlight your novel contribution towards the binarization of activation. Otherwise, it seems that the paper is too biased to your findings. \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_84Py"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_84Py"
        ]
      },
      {
        "id": "rBQKC5vvc2",
        "original": null,
        "number": 14,
        "cdate": 1637286312074,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637286312074,
        "tmdate": 1637286358147,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "LKnvJu9OhJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Re: Response to Authors",
          "comment": "Thanks for your positive and very helpful feedback! We fully agree with your suggestions and further add and compare the results of existing SOTA BERT quantization methods (1-1-4 BinaryBERT [1] and 2-2-8 TernaryBERT [2] which are reported in their original papers) in Table 2 and Table 3 of our new revision. \n\nAs the results show, though the BERTs quantized by existing methods [1,2] almost maintain the accuracy under 4-bit or above bit-widths activation, they sharply crash when the activation is binarized/quantized to 1-bit and 2-bit. Moreover, even though BinaryBERT binarizes the weight and embedding to 1-bit for the first time, activation binarization can further bring up to $3.8\\times$ of computational FLOPs saving and allows the model to apply extreme efficient bitwise operations instead of integer operations. The above facts make the fully binarized BERT (activation binarization of BERT) an approach with both significant attraction and challenge.\n\nAs a pioneer work, BiBERT presents a promising route towards the accurate fully binarized BERT (with 1-bit weight, embedding, and activation) for the first time. The experiments show that our BiBERT outperforms existing quantized BERTs with 1-bit/2-bit activation by convincing margins. And though a significant gap still exists with the full-precision model, BiBERT presents the ultra-high $56.3\\times$ and $31.2\\times$ computational FLOPs and storage savings, respectively. Thus, this work confirms that full binarization is one of the most promising compression approaches for large-scale pre-trained BERT that significantly alleviates the deployment difficulties of BERTs in real-world hardware with limited computation and storage resources.\n\n[1] Bai, Haoli, et al. \"BinaryBERT: Pushing the Limit of BERT Quantization.\"\u00a0ACL. 2021.\n\n[2] Zhang, Wei, et al. \"TernaryBERT: Distillation-aware Ultra-low Bit BERT.\"\u00a0EMNLP. 2020."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "BgXEByc1uDd",
        "original": null,
        "number": 15,
        "cdate": 1637301198236,
        "mdate": 1637301198236,
        "ddate": null,
        "tcdate": 1637301198236,
        "tmdate": 1637301198236,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "-6PDgKnfHB",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to the Authors",
          "comment": "I have read the other reviews and the authors' responses. In fact, the author's response does not address my concerns on the baseline binarized BERT.\n\nFrom the authors' response, it seems to me that all the baseline methods (including the ones that this paper converts into binarized activation versions) use a fixed all one attention map. Such kind of map is just an average pooling of all tokens and would be meaningless to be adopted as an attention map. I'm afraid this would result in under-estimating the performance of vanilla fully binarized BERT and previous approaches in the fully binarized setting.\n\nI'm going to temporarily decrease my score until the authors can provide comparisons to baselines with binarized attention in more meaningful settings (for example, directly use a threshold to control 50% (or 30% / 70%) elements in the attention map to be zeros. Since this paper is the first paper to develop a fully binarized version of BERT, setting up good baselines would be very important."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_yAoP"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_yAoP"
        ]
      },
      {
        "id": "aVHJol3mtT",
        "original": null,
        "number": 16,
        "cdate": 1637604640581,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637604640581,
        "tmdate": 1637604952199,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "BgXEByc1uDd",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Re: Response to Authors (1/3)",
          "comment": "**Q1:** From the authors' response, it seems to me that all the baseline methods (including the ones that this paper converts into binarized activation versions) use a fixed all one attention map. Such kind of map is just an average pooling of all tokens and would be meaningless to be adopted as an attention map. I'm afraid this would result in under-estimating the performance of vanilla fully binarized BERT and previous approaches in the fully binarized setting.\n\nI'm going to temporarily decrease my score until the authors can provide comparisons to baselines with binarized attention in more meaningful settings (for example, directly use a threshold to control 50% (or 30% / 70%) elements in the attention map to be zeros. Since this paper is the first paper to develop a fully binarized version of BERT, setting up good baselines would be very important.\n\n\n\n**A1:** We thank you for your constructive feedback. We compare and discuss more solutions that diversify the values of binarized weights (instead of the all one attention map), including the baselines with 50% zero binarized attention weight, with the asymmetric quantization function, and with mean value threshold. We add the results and related discussion in Section 4.2 and Appendix C.1 in our revision.\n\n## **1. Reasons of building the original baseline**\n\nWhen we built the fully binarized BERT baseline, we considered the following reasons and chose the original baseline settings shown in the paper:\n\n(1) Additional inference computation should be avoided to the greatest extent compared with the BERTs obtained by direct full binarization. We discard the scaling factor for activation in the bi-linear unit since it requires real-time updating during inference and increases floating-point operations. In the attention structure, we directly binarize the activation without re-scaling or mean-shifting.\n\n(2) Since there is no previous work to fully binarize BERT, the existing representative binarization techniques are chosen to build the baseline. The binarization function with a fixed 0 threshold is applied to the original definition of the binarized neural network [1] and is used by default in most binarization works [2,3,4], we thereby initially apply this function when building the fully binarized BERT baseline. Besides, it also helps to more objectively evaluate the benefits of maximizing information entropy in the attention structure.\n\nTherefore, the fully binarized baseline under the current BERT architecture applies the representative techniques and almost without additional floating-point computation.\n\n## **2. Different solutions for attention weight**\n\nCausing by the application of softmax, the attention possibilities in the attention structure follow the probability distribution and (attention weight) are all one after binarization. In fact, we were aware of the problem when building the fully binarized baseline, and had evaluated two solutions by existing quantization techniques.\n\nThe first solution is to degenerate the asymmetric quantization function (usually applied to 2-8 bit quantization [5,6]) to the 1-bit case, since the method is originally designed to deal with the imbalance of value distribution in quantization:\n\n**Solution 1** (Baseline$_\\text{aysm}$): \n\n$Q(x)=\\max(x), x\\ge0.5(\\max(x)+\\min(x)); \\min(x), \\text{otherwise}.$  (A1.1)\n\nThe other solution is to simply use the mean of the elements as the threshold [1,2]:\n\n**Solution 2** (Baseline$_{\\mu}$): \n\n$Q(x)=\\operatorname{bool}(\\mathbf x-\\tau), \\quad \\tau = \\mu(\\mathbf x)$,  (A1.2)\n\nwhere $\\mu(\\cdot)$ denotes the mean value.\n\nWe also thank the reviewers for providing the Solution 3, which is to use the threshold limiting the zero attention weight to a certain percentage (such as 50%):\n\n**Solution 3** (Baseline$_{0.5}$): \n\n$Q(\\mathbf x)=\\operatorname{bool}(\\mathbf x-\\tau), \\quad \\tau = Q_{0.5}(\\mathbf x)$,  (A1.3)\n\nwhere $Q_{0.5}(\\cdot)$ denotes the quantile of 50% percentage (median).\n\nThe above three solutions are all able to alleviate the problem of fixed all one attention weight. We present the results of these solutions in Table A1.1 and further discuss them in detail."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "GRR_bF78goe",
        "original": null,
        "number": 17,
        "cdate": 1637604745544,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637604745544,
        "tmdate": 1637604916645,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "aVHJol3mtT",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Re: Response to Authors (2/3)",
          "comment": "Table A1.1 Comparison results without data augmentation.\n\n| Solution  | Quant                  | #Bits | $\\Delta$FLOPs(G) | MNLI          | QQP      | QNLI     | SST-2    | CoLA     | STS-B    | MRPC     | RTE      | Avg.     |\n| --------- | ---------------------- | ----- | ---------------- | ------------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| Original  | Baseline               | 1-1-1 | 0                | 45.8/47.0     | 73.2     | 66.4     | 77.6     | 11.7     | 7.6      | 70.2     | 54.1     | 50.4     |\n|           | BinaryBERT             | 1-1-1 | 0                | 35.6/35.3     | 63.1     | 51.5     | 53.2     | 0        | 6.1      | 68.3     | 52.7     | 40.6     |\n| Solution1 | Baseline$_\\text{asym}$ | 1-1-1 | 0.074 (15%)      | 45.1/46.3     | 72.9     | 64.3     | 72.8     | 4.6      | 9.8      | 68.3     | 53.1     | 48.6     |\n| Solution2 | Baseline$_{\\mu}$       | 1-1-1 | 0.076 (16%)      | 48.2/49.5     | 73.8     | 68.7     | 81.9     | 16.9     | 11.5     | 70.0     | 54.9     | 52.8     |\n| Solution3 | Baseline$_{0.5}$      | 1-1-1 | 0.076 (16%)      | 47.7/49.1     | 74.1     | 67.9     | 80.0     | 14.0     | 11.5     | 69.8     | 54.5     | 52.1     |\n|           | BinaryBERT$_{0.5}$    | 1-1-1 | 0.076 (16%)      | 39.2/40.0     | 66.7     | 59.5     | 54.1     | 4.3      | 6.8      | 68.3     | 53.4     | 43.5     |\n| Ours      | BiBERT                 | 1-1-1 | **0**            | **59.3/60.0** | **82.4** | **70.2** | **86.9** | **25.3** | **33.5** | **72.9** | **58.5** | **61.0** |\n\n### **2.1. Discussion of Solution 1**\n\nAs shown in Eq. (A1.1), since the dynamically threshold ($0.5(\\max(\\mathbf x)+\\min(\\mathbf x))$) is applied in this quantizer, the elements of attention weight are quantized to $\\max(\\mathbf x)$ and $\\min(\\mathbf x)$ instead of a same value. However, the asymmetrical binarized values obtained by this quantizer make the binarized BERT hard to apply bitwise operations and lose the efficiency advantage brought by binarization (0.074G (15%) additional FLOPs), so that this practice is also not used in existing binarization works.\n\nAnd as the results show, the accuracy of the binarized BERT is also worse than the existing baseline (Baseline 50.4% vs. Baseline$_\\text{asym}$ 48.6% on average, Row 2 and 4 in Table A1.1).\n\n### **2.2. Discussion of Solution 2**\n\nAs the effect of this solution on the weight parameter [1,2], it can ensure diversity of binarized attention weight elements instead of all 1 ($\\min(\\mathbf x) \\le \\tau \\le \\max(\\mathbf x)$). But the premise of this practice is the Gaussian (symmetric) distribution of floating-point parameters before binarization [2], while the attention score follows an asymmetric probability distribution, causing the values of binarized activation to imbalanced. \n\nTherefore, the improvement of Baseline$_\\mu$ in terms of accuracy is also limited (as shown in Row 2 and 5 in Table A1.1), and this solution also increases 0.076G (16%) computational FLOPs compared to the original baseline."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "zTzjK1kg6nd",
        "original": null,
        "number": 18,
        "cdate": 1637604878449,
        "mdate": 1637604878449,
        "ddate": null,
        "tcdate": 1637604878449,
        "tmdate": 1637604878449,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "GRR_bF78goe",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Re: Response to Authors (3/3)",
          "comment": "### **2.3. Discussion of Solution 3**\n\nThe fixed 50% percentage of zero binarized attention weights suggested by the reviewer are good baseline settings that help to further clarify the entropy maximization motivation of the Bi-Attention in BiBERT. As shown in Table A1.1 (Row 6 and 7), the results of Baseline and BinaryBERT under this setting are significantly improved by 1.7% and 2.9% on average compare the original results (Row 2 and 3), respectively. And from the perspective of information entropy, a 50% percentage of zero attention weights can also ensure maximum information. We present the detailed ablation results (10%-90% zero attention weight) on STS-B in Table A1.2, which show the model maximizing information entropy achieves the best results.\n\nTable A1.2 Ablation results with 10%~90% zero attention weight on STS-B.\n\n| Percentage | 10%  | 30%  | 50%       | 70%   | 90%   |\n| ---------- | ---- | ---- | --------- | ----- | ----- |\n| Entropy    | 0.47 | 0.88 | **1.0**   | 0.88  | 0.47  |\n| Accuracy   | 8.5% | 8.6% | **11.5%** | 10.8% | 10.0% |\n\nThough forcing a certain ratio of zero attention weights improves the baseline, there exists shortcomings for fully binarized BERTs. First, the calculation of quantile thresholds relies on real-time computation or sorting, which increases computation of the fully binarized BERT in inference (about 0.076G (16%) additional FLOPs). Moreover, the practice of 50% quantile threshold should be regarded as the solution to a more stringent optimization problem (rather than the optimization problem in Eq. (9) of the paper) since it further constrains and maximizes the entropy of each tensor of binarized attention weight instead of optimizing the overall distribution. Thus, the quantile threshold is more restrictive for the binarized attention weights and limits their representation capability, causing the results of obtained fully binarized BERTs lower than those of models applying Bi-Attention.\n\n## **3. Conclusion**\n\nAlthough the above three solutions improve accuracy to a certain extent, they only bring limited improvements while destroying the advantages of the fully binarized network on computational efficiency. As analyzed in our paper, the existing attention structure is not suitable for directly fully binarizing BERT, which is also the reason we specially designed the Bi-Attention structure for the fully binarized BERT. However, considering the reason mentioned by the reviewer that the fixed all one attention weight causes itself and related distillation to fail, and also to more comprehensively compare with our method, we add the results of these solutions (as strengthened baselines) to the paper as recommended by the reviewer.\n\n\n\n\n**Reference**\n\n[1] Rastegari, Mohammad, et al. \"Xnor-net: Imagenet classification using binary convolutional neural networks.\" ECCV. 2016.\n\n[2] Qin, Haotong, et al. \"Forward and backward information retention for accurate binary neural networks.\" CVPR. 2020.\n\n[3] Wang, Ziwei, et al. \"BiDet: An efficient binarized object detector.\" CVPR. 2020.\n\n[4] Liu, Zechun, et al. \"Reactnet: Towards precise binary neural network with generalized activation functions.\" ECCV. 2020.\n\n[5] Bai, Haoli, et al. \"BinaryBERT: Pushing the Limit of BERT Quantization.\" ACL. 2021.\n\n[6] Zhang, Wei, et al. \"TernaryBERT: Distillation-aware Ultra-low Bit BERT.\" EMNLP. 2020."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "trdXcsV4UHP",
        "original": null,
        "number": 20,
        "cdate": 1638151390255,
        "mdate": 1638151390255,
        "ddate": null,
        "tcdate": 1638151390255,
        "tmdate": 1638151390255,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "42PNG1CKjZd",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Authors",
          "comment": "I thank the authors for the detailed feedback. However, my concerns regarding the connection with XNOR-Net and fair comparison with BinaryBERT and TernaryBERT still exist. Specifically, (1) though XNOR-Net uses its weight balance in its implementation, the scaling should be calculated based on the zero-meaned weights. (2) I checked the official implementation of BianryBERT https://github.com/huawei-noah/Pretrained-Language-Model/blob/1dbaf58f17b8eea873d76aa388a6b0534b9ccdec/BinaryBERT/transformer/utils_quant.py#L434. The authors clearly commented on the code they do not suggest using BWN or TWN for activation quantization. Instead, for 2-bit activations, they have options of using uniform quantization or LSQ. For fair comparison with BinaryBERT, the authors should use the suggested 2-bit activation quantization. Otherwise it is hard to tell whether the proposed method still outperforms BinaryBERT under a fair setting."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_tpUQ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_tpUQ"
        ]
      },
      {
        "id": "kgPn2F8ae_2",
        "original": null,
        "number": 21,
        "cdate": 1638167375546,
        "mdate": 1638167375546,
        "ddate": null,
        "tcdate": 1638167375546,
        "tmdate": 1638167375546,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "o1BbEHHCSji",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Authors",
          "comment": "I appreciate the authors' efforts in addressing my concerns. With the comparison to stronger baselines (instead of all-one attention), my main concern is addressed so I decide to raise my score back to 8."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_yAoP"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_yAoP"
        ]
      },
      {
        "id": "fLrdwBKDro9",
        "original": null,
        "number": 22,
        "cdate": 1638210354521,
        "mdate": 1638210354521,
        "ddate": null,
        "tcdate": 1638210354521,
        "tmdate": 1638210354521,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "trdXcsV4UHP",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Re: Response to Authors (1/2)",
          "comment": "Thank you for your feedback and suggestions.\n\n**Q1:** Concerns regarding the connection with XNOR-Net: Although XNOR-Net uses its weight balance in its implementation, the scaling should be calculated based on the zero mean weight.\n\n**A1:** Thank you for pointing it out. Compared with XNOR-Net, the bi-linear in our BiBERT applies the same weight balance, while some differences still exist: (1) the scaling factor of activation in bi-linear is removed for the computational efficiency since it will cause an up to 0.3 GFLOPs (43%) computation increment in fully binarized BERTs; (2) the other minor difference is that bi-linear calculates the scaling factor before the weight balance (while XNOR-Net calculates it afterward). When building the baseline, we found that calculating the scaling factor for weight before balance usually leads to better results (e.g., 11.7% vs. 11.1% for baselines on CoLA). The benefits seem to come from more significant feature heterogeneity brought by larger values of scaling factors, which improves fully binarized BERTs in mitigating information degradation. We thus apply this setting to build all fully binarized BERTs. However, whether calculating scaling factors before or after weight balance does not affect the theory part of BiBERT. And the experiments in Table RA1.1 also show little difference in accuracy (BiBERT 61.0% vs. BiBERT$_\\text{scale}$ 60.9% on average). Some of our designs in BiBERT may eliminate the influence of rearranging the calculation. We will show more results and related discussions in the new revision.\n\nIn addition, the improvement brought by bi-linear mainly depends on the weight balance that maximizes the information of binarized activation and weight, which is discussed in A1 of the original response (1/3) and also practiced in XNOR-Net and IR-Net.\n\n**Table RA1.1 Comparison results without data augmentation.** \n\n| Quant                           | #Bits | FLOPs(G)        | MNLI          | QQP      | QNLI     | SST-2    | CoLA     | STS-B    | MRPC     | RTE      | Avg.     |\n| ------------------------------- | ----- | --------------- | ------------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| Baseline                        | 1-1-1 | 0.4             | 45.8/47.0     | 73.2     | 66.4     | 77.6     | 11.7     | 7.6      | 70.2     | 54.1     | 50.4     |\n| BinaryBERT                      | 1-1-1 | 0.4             | 35.6/35.3     | 63.1     | 51.5     | 53.2     | 0        | 6.1      | 68.3     | 52.7     | 40.6     |\n| BinaryBERT$_\\text{TWN}$ (paper) | 1-1-2 | 0.8 ($2\\times$) | 35.4/35.2     | 63.1     | 52.6     | 82.5     | 14.6     | 6.5      | 68.3     | 52.7     | 45.7     |\n| BinaryBERT$_\\text{LSQ}$         | 1-1-2 | 0.8 ($2\\times$) | 35.9/35.7     | 63.1     | 53.1     | 83.0     | 14.2     | 6.8      | 68.3     | 52.7     | 45.9     |\n| BinaryBERT$_\\text{uniform}$     | 1-1-2 | 0.8 ($2\\times$) | 35.9/35.5     | 63.1     | 52.5     | 82.1     | 14.5     | 6.1      | 68.3     | 52.7     | 45.6     |\n| BiBERT (paper)                  | 1-1-1 | **0.4**         | **59.3/60.0** | 82.4     | 70.2     | 86.9     | **25.3** | **33.5** | **72.9** | **58.5** | **61.0** |\n| BiBERT$_\\text{scale}$           | 1-1-1 | **0.4**         | 59.1/59.8     | **82.7** | **70.3** | **87.0** | 25.1     | 33.1     | 72.7     | 58.4     | 60.9     |"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "_2__Z_P_4P0",
        "original": null,
        "number": 23,
        "cdate": 1638210453000,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638210453000,
        "tmdate": 1638214914806,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "fLrdwBKDro9",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Re: Response to Authors (2/2)",
          "comment": "**Q2:** Concerns regarding the fair comparison with BinaryBERT and TernaryBERT: I checked the official implementation of BinaryBERT https://github.com/huawei-noah/Pretrained-Language-Model/blob/1dbaf58f17b8eea873d76aa388a6b0534b9ccdec/BinaryBERT/transformer/utils_quant.py#L434. The author clearly commented that they do not recommend using BWN or TWN for activation quantification codes. Instead, for 2-bit activation, they can choose to use unified quantization or LSQ. In order to make a fair comparison with BinaryBERT, the author should use the recommended 2-bit activation quantification. Otherwise, it is difficult to judge whether the proposed method is still better than BinaryBERT under a fair setting.\n\n**A2:** Thank you for your constructive suggestions. \n\nWe would like to first emphasize that BiBERT is the first fully binarized BERT with extremely high computational and storage savings. A completely fair comparison should be performed among models with the same bit width (1-1-1), however, the 1-1-1 models binarized by existing BERT quantization methods (such as BinaryBERT) suffer severe accuracy drops and even crash directly on many tasks. In contrast, 1-1-2 BinaryBERT has a computational cost of $2\\times$, but 1-1-1 BiBERT still surpasses it in terms of accuracy.\n\nMoreover, we also present the results of 1-1-2 BinaryBERT obtained by uniform (L434) and LSQ (L438) methods in Table RA1.1 following the reviewer's suggestions. The accuracy of 1-1-2 BinaryBERT$_\\text{uniform}$ and BinaryBERT$_\\text{LSQ}$ is similar to that of BinaryBERT$_\\text{TWN}$ (TWN 45.7% vs. uniform 45.6% & LSQ 45.9% on average). These results show that the bottleneck of 1-1-2 BinaryBERT performance is the limited representation capability caused by ultra-low bit-width activations, and also show that existing BERT quantization methods cannot be directly used to obtain BERTs with extremely low-bit activations (1-bit or 2-bit)."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "VP46-3WcB8h",
        "original": null,
        "number": 24,
        "cdate": 1638254383271,
        "mdate": 1638254383271,
        "ddate": null,
        "tcdate": 1638254383271,
        "tmdate": 1638254383271,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "Fg771hU713",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Response to Authors",
          "comment": "I thank the authors for the detailed feedback, although they havn't fully addressed my concern regarding the large performance drop of the BiBERT. And again, it is important to make the comparison fair and not biased toward your own method. Overall, I think the work has its merits."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_TtMP"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_TtMP"
        ]
      },
      {
        "id": "dzHa0uaPd4g",
        "original": null,
        "number": 1,
        "cdate": 1642696834629,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696834629,
        "tmdate": 1642696834629,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "This paper present a way to fully binarize a BERT model. The authors convincingly demonstrate that a naive binarization results in large quality losses and then propose amendments. It is pretty impressive that it is possible to get a fully binarized model to work at all.\nAt the same time, the quality losses are still significant and in practice one might prefer to use distillation (as long as the hardware doesn't require binarization). One could also envision combinations of the proposed technique (perhaps in the 1-1-4 setting) with distillation."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "4inq0vvKBiQ",
        "original": null,
        "number": 1,
        "cdate": 1635851421837,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635851421837,
        "tmdate": 1638254401781,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper addresses the problem of fully binarizing BERT model including the network weights, embeddings and activations. Through theoretical and empirical analysis, the authors find: 1) the direct binarization of softmax-ed attention matrix is problematic; 2) distillation of the attention scores from the full-precision teacher can be harmful to the binarized model. Therefore, they propose two different methods to alleviate the issues. One is the Bi-Attention which directly quantizes the attention matrix and excludes softmax. The other is Direction-Matching Distillation which chooses to distill the covariance matrices of query/key/value rather than the attention matrix. Experimental results on GLUE benchmark show that BiBERT model outperforms previous quantized BERT models on the full binarization setting.\nThis main contribution of the paper is that it is the first work on full binarization of BERT models. With detailed analysis and observations, it proposed two effective methods to improve the training of the binarized model, and achieve good performance on some of the NLU tasks.",
          "main_review": "Overall, the paper is well-motivated and easy to follow. To the best of my knowledge, this is the first work on full quantization (weights-embedding-activation) of the BERT model to 1-bit (denoted as 1-1-1), considering that previous work such as BinaryBERT only achieves 1-1-4 quantization. Binarizing all the activations without losing much performance is challenging. By making detailed analysis of the performance variances in binarizing intermediate outputs at each step and the different distillation losses, the authors find the major problems that hurt the model performance most. Two new methods are proposed to avoid or alleviate the problem in training the quantized model, which are theoretical sound and shows substantial improvements to the accuracies on downstream tasks. The proposed BiBERT model also surpasses the previous BERT quantization model on the setting of 1-1-1 quantization.\n\nThe weaknesses of the BiBERT model are as follows: \n1) The performance of BiBERT on GLUE is still far behind the full precision model (67.0 vs 82.3). This large performance gap may hinder application of the BiBERT model in many real scenarios (e.g., CoLA, STS-B tasks). While the BinaryBERT with 1-1-4 quantization, which has similar model size and a little bit higher computation as the BiBERT, can achieve GLUE score of 81.9 with almost neglectable performance loss. This raises the question why full binarization of the BERT model is necessary or preferred. One possible way to justify this is to enlarge the network size while keeping it binary and see if the performance can consistently improve.\n2) In the experiments, the authors only report BinaryBERT (1-1-2 and 1-1-1) and TernaryBERT (2-2-1 and 2-2-2) which are not present in their original papers. The author should clarify in the paper how the results are obtained. Are the results from reimplementation of their algorithms or based on some open-source code/models? Are the reimplementation correct? On CoLA task, BinaryBERT and TenaryBERT simply seem to diverge, and the baseline quantization method without Bi-Attention and DMD also outperform BinaryBERT and TenaryBERT.\n3) The author report Q2BERT and Q-BERT on the 2-2-8 setting, while the TernaryBERT have much higher performance (72.9) on it. Why not report these results?  \nOverall, I think the BiBERT is still not applicable to some NLU tasks given severe performance drop. Some of the experimental results are not convincing (or biased) regarding the baseline of BinaryBERT and TenaryBERT.\n",
          "summary_of_the_review": "The paper tries to tackle a challenging problem of binarizing BERT and find effective ways to improve the performance. However, the model performances on some tasks are still far from good and comparison in the experiments are not convincing. Therefore, my current recommendation is weak reject unless the author can clarify the concerns.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_TtMP"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_TtMP"
        ]
      },
      {
        "id": "YkLvJCjGB0q",
        "original": null,
        "number": 2,
        "cdate": 1635855850648,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635855850648,
        "tmdate": 1635855850648,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces an accurate fully binarized (i.e., 1-bit weight, embedding, and activation)  BERT, BiBERT, as a robust model compression method. It saves 56.3\u00d7 and 31.2\u00d7 on FLOPs and model size for real-world devices. Moreover, it addresses the substantial performance issues in the straightforward full binarization method. To tackle the performance drop issues, it proposes Bi-Attention and Direction Matching Distillation (DMD) mechanism.  The paper contains an evaluation of the proposed model on GLUE benchmark and a comparison with SOTA quantized BERT models.\n",
          "main_review": "Strengths:\n1. The paper is well constructed and easy to go through the idea; encompasses rigorous theoretical justification of the approaches.\n2. This paper addresses the crucial problems in fully binarized BERT and introduces BiBERT which is the very first attempt to implement an accurate fully binarized BERT.\n3. In this framework, they incorporate a Bi-Attention approach which maximizes the information entropy to mitigate the information degradation. And a Direction Matching Distillation (DMD) mechanism which resolves the optimization direction mismatch.\n4. BiBERT also impressively obtain savings on FLOPs and model size which is a great advantage for real-world edge devices.\n5. It also contains an ablation study on the GLUE benchmark which validates the contribution of the individual components.\n\nWeaknesses:\n1. The proposed method didn\u2019t properly address the performance drop due to the binarization of activation. Results on the GLUE benchmark showed a drastic performance drop in comparison to the related approaches and full precision BERT.\n2. In sec 2.1, it is evident that a stochastic sign function is used to binarize the BERT, therefore, why the deterministic function is used in sec 3.2.1 is not clear.\n3. In table 1,2, 3, we see that data augmentation method has been utilized for experimental purposes. But it is not properly addressed/clarified in sec 4.1 and 4.2 why it is employed and what is the significance of this approach here.\n4. Also, these two closely related works should be critically analyzed in the experiment section:\n- Bai, H., Zhang, W., Hou, L., Shang, L., Jin, J., Jiang, X., ... & King, I. (2020). Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701.\n- Zhang, W., Hou, L., Yin, Y., Shang, L., Chen, X., Jiang, X., & Liu, Q. (2020). Ternarybert: Distillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812.\n5. Figure 2, 3, 4 are not mentioned in appropriate sections to help understand the analysis.\n\nSome minor issues and typos:\n1. There is no full stop(.) at the end of each caption of the figures and tables.\n2. At the end of the 2nd paragraph of the Introduction: \u201cSo far, previous studies have pushed down the weight and embedding to be binarized, but none of them have ever achieved to binarize BERT with 1-bit activation accurately.\u201d -> please refer to the specific previous studies.\n3. Section 2, first paragraph: BRET -> BERT\n4. Section 2.2: which can be unobstructed applied -> which can be unobstructedly applied\n5. Appendix B.2: There is no citation for DynaBERT.\n",
          "summary_of_the_review": "This paper portrays the related performance issues of straightforward fully binarized BERT and introduces the idea of BiBERT as a solution. The paper has proper experimental proofs and analysis to justify the framework. It might be a positive inclusion towards an accurate fully binarized BERT architecture. The proposed method obtained impressive savings on FLOPs and model size. However, it seems that the related SOTA approaches are missing in the comparative performance analysis section, and I would really be looking to see this in a conference paper that meets ICLR's high standards.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_84Py"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_84Py"
        ]
      },
      {
        "id": "PH_M7xJQ-dj",
        "original": null,
        "number": 3,
        "cdate": 1635859927630,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635859927630,
        "tmdate": 1635859927630,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes to binarize both the weights and activations of the BERT model. The authors find that binarizing MHA causes the most significant accuracy drop and simple distillation on MHA by minimizing the MSE between the attention scores of the full-precision model and the quantized model can harm the performance sometimes. Thus the authors propose new binary representations that maximize entropy and distillation methods that maintain direction. Empirical results on the GLUE benchmark show that the method achieves good performance \nwhen the weights are binarized and the activation is binarized/ternarized.\n",
          "main_review": "The authors stated that the bi-linear quantization function in Equation (2) is motivated by Rastegati et al., 2016 and Qin et al., 2021.  However, (2) is quite different from the quantization methods in both papers. Specifically,\n- in Rastegati et al., 2016, both the weights and activations are not zero-meaned before quantization and have scaling factors. \n- in Qin et al., 2021, the weights are both zero-meaned and uni-normalized. \nCan the authors specify in more detail the derivation of (2), and the connection/difference with the two papers?\n\nThe theory part also requires some more clarifications. Specifically,\n- Theorem 2 is based on the assumption that B_Q and B_K are entropy maximized. However, it is not clear why this assumption holds. \n- Even though the softmax function is order-preserving, maximizing the entropy of sign(A - \\phi(\\tau, A)) is not equivalent to the same as the original problem in (9).\n- Moreover, given the implication from Theorem 2 that the optimal \\phi(\\tau) is zero,  the binarization of the attention scores  sign(A - \\phi(\\tau, A)) =  sign(A) does not equal bool(A) in equation (11).\n\nThe experiments also require some more clarification. According to the original papers of BinaryBERT and TernaryBERT, the activations are only quantized to as low as 4 bits. However, in Tables 2-3, activations of these two methods are quantized to 1 or 2 bits. How are the activations quantized for these two methods?\n\nMinors:\n- Figure 3: missing the full stop at the end of the caption.",
          "summary_of_the_review": "This paper is overall structured clearly and easy to follow. However, the motivation, theory part and experiments require some further clarifications.\n",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_tpUQ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_tpUQ"
        ]
      },
      {
        "id": "Yz7RXhNQPvg",
        "original": null,
        "number": 4,
        "cdate": 1635873170196,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635873170196,
        "tmdate": 1635908341349,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper focuses on the full binarization of BERT. Since current pre-trained models achieve promising results on NLP tasks, it is a trend to apply these models to real-world applications with constraint computation resources.  Quantization is one of the prime choices to reduce computation requirements. In recent years, BERT quantization has made significant progress. However, training binary BERT is still an important but challenging question. This paper finds that the worse performance of binary BERT can be mainly attributed to information degradation and optimization direction mismatch respectively in the forward and backward propagation. To address these problems, the authors propose two solutions, Bi-Attention and Direction-Matching Distillation. Experimental results on GLUE benchmark demonstrate that the proposed work outperforms several popular baselines. Also, ablation studies verify the effectiveness of the proposed solutions.",
          "main_review": "Strength:\n\nThe idea is well-motivated and easy to follow. The paper starts from information theory and focuses on the mutual information between binarized and full-precision representations. Considering that the ideal binarized representation should preserve the given full-precision counterparts as much as possible, it is very natural to maximize the mutual information between two representations. \n\nThe authors also find that the direction mismatch hinders the accurate optimization of the fully binarized BERT. This finding is interesting and well-motivated. \n\nExperiment results show that the proposed approaches achieve much better results than previous baselines. \n\nWeakness:\n\n I have noticed that [1] reported higher results in their paper.  It would be better to explain why the higher results are not reported in this paper. \n\nThe work of BinaryBERT uses 1-bit weight quantization and 4-bit activation quantization. It not only achieves better results than the proposed approach,  but also almost reaches the results of the full-precision model.  Since BinaryBERT and BiBERT have similar parameter sizes, the empirical contribution of BiBERT is a little bit marginal to me. \n\n[1] BinaryBERT: Pushing the Limit of BERT Quantization\n",
          "summary_of_the_review": "It is a well-motivated paper presenting an alternative understanding of the challenges of training binarized BERT. The empirical contribution is a little bit marginal since there is a large gap between the proposed approach and the best result in [1].",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_T1kK"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_T1kK"
        ]
      },
      {
        "id": "o1BbEHHCSji",
        "original": null,
        "number": 5,
        "cdate": 1635923923075,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635923923075,
        "tmdate": 1638167282379,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a full binarization of BERT (including 1-bit activation), called BiBERT. The key observation of the paper is that binary activation becomes a big challenge for the attention weights in self-attention. To solve this problem, the paper proposes a new attention binarization approach that maximizes the information entropy. The paper further proposes a Direction-Matching Distillation (DMD) scheme which encourages the similarity pattern matrices of keys, values, and hidden states in the original BERT and BiBERT to be minimized. Experimental results on GLUE datasets show that BiBERT significantly outperforms other models under the full binarization scheme.",
          "main_review": "The paper proposes a full binarization of BERT (including 1-bit activation), which is a very challenging task. The proposed bi-attention for maximum information entropy is a very interesting design for fully binary attention models. My question here is in section 2.1, when analyzing the effect of each distillation term, whether the authors used a fully binarized attention map which only contains 1, just like the Figure 4(b). In that case, I don't understand the meaning of attention distillation in Figure 3 since the binary attention map is fixed.\n\nIt is also interesting to see that the activation distillation can cause the optimization direction when the quantization becomes more aggressive, which well motivates the proposed direction-matching distillation term.\n\nThe experimental results are solid and impressive. In the 1-1-1 setting in which the paper mainly focused, the proposed BiBERT model achieves significant improvement over the 1-1-1 baseline model.",
          "summary_of_the_review": "The paper is well-motivated, novel, and well supported by the experiments.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Reviewer_yAoP"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Reviewer_yAoP"
        ]
      },
      {
        "id": "E1ykFngA4SI",
        "original": null,
        "number": 1,
        "cdate": 1636897840739,
        "mdate": 1636897840739,
        "ddate": null,
        "tcdate": 1636897840739,
        "tmdate": 1636897840739,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "Additional Discussion Encouraged",
          "comment": "Dear Reviewers,\n\ncan you please take a look at each other's reviews? Your reviews currently straddle the decision boundary and it would be good to make sure you have considered all the perspectives provided. Please update your reviews (at least to acknowledge that you have read all reviews).\n\nThanks,\nYour Area Chair"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Area_Chair_3Z5s"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Area_Chair_3Z5s"
        ]
      },
      {
        "id": "qCuLKlVdoIV",
        "original": null,
        "number": 2,
        "cdate": 1636946846741,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636946846741,
        "tmdate": 1637605348729,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Official_Comment",
        "content": {
          "title": "General Response",
          "comment": "We are grateful for the ACs' and reviewers' positive feedback towards BiBERT. To assist a clearer understanding of our paper, we summarize our main contributions below:\n\nWe present BiBERT, the first accurate fully binarized BERT for efficient deep learning on NLP tasks, to alleviate the resource constraint for large pre-trained BERTs that run on resource-limited devices. Different from the existing BERTs with quantized integer parameters or with only binarized weight and embedding, fully binarized parameters of BiBERT bring extremely efficient XNOR-Bitcount bitwise operations and lower storage usage, which allows BERTs to be compressed and deployed on limited computing resources and ultra-low power devices, such as mobile phones and cameras. And applications that require real-time interaction and fast response can run steadily on these source-limited devices. However, the compact 1-bit parameters cause the fully binarized neural network to suffer extremely limited representation capabilities, and the optimization process is hindered by the discrete function, which makes the design of accurate fully binarized networks a great challenge. \n\nIn this paper, we identify that the severe performance drop of the fully binarized BERT baseline based on existing techniques can be mainly attributed to the information degradation and optimization direction mismatch respectively in the forward and backward propagation. To solve these problems, BiBERT introduces an efficient Bi-Attention structure for maximizing representation information statistically and a Direction-Matching Distillation (DMD) scheme to optimize the full binarized BERT accurately. BiBERT outperforms both the straightforward baseline and existing state-of-the-art quantized BERTs with ultra-low bit activations by convincing margins on the NLP benchmark. We highlight that as the first fully binarized BERT, our method yields impressive ${56.3\\times}$ and ${31.2\\times}$ saving on FLOPs and model size, demonstrating the vast advantages and potential of the fully binarized BERT model in real-world resource-constrained scenarios.\n\nWe also update our manuscripts; the change we made includes:\n\n- In Section 4.2, we add and discuss the performance of BiBERT on compact architectures (TinyBERT-6L and TinyBERT-4L), which shows that our BiBERT has outstanding performance on various architectures;\n- In Section 4.2, we add more discussions and analysis of related methods (especially BinaryBERT and TernaryBERT);\n- In Appendix A.6, we add relevant discussions and proofs about the equivalence of information entropy maximization;\n- In Appendix B.2, we add relevant details about the implementation of the comparison method (BinaryBERT and TernaryBERT) under 1-bit and 2-bit activation settings;\n- In Section 4.2 and Appendix C.1, we add the results and related discussion of strengthened baselines.\n- We carefully correct our references in the revised version.\n\nFor the detailed explanation, please see our responses to each reviewer.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper42/Authors"
        ]
      },
      {
        "id": "4AW1DBmC7bh",
        "original": null,
        "number": 1,
        "cdate": 1637152648168,
        "mdate": 1637152648168,
        "ddate": null,
        "tcdate": 1637152648168,
        "tmdate": 1637152648168,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Public_Comment",
        "content": {
          "title": "About visualization of the results of proposed Bi-Attention structure",
          "comment": "Dear authors, thanks for your cool work. I'm really curious about the visualization of the attention weight obtained from the proposed Bi-Attention structure (Eq.11). Can you kindly show any examples if time permits\uff1f"
        },
        "signatures": [
          "~Jack_Ma3"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "~Jack_Ma3"
        ]
      },
      {
        "id": "dzHa0uaPd4g",
        "original": null,
        "number": 1,
        "cdate": 1642696834629,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696834629,
        "tmdate": 1642696834629,
        "tddate": null,
        "forum": "5xEgrl_5FAJ",
        "replyto": "5xEgrl_5FAJ",
        "invitation": "ICLR.cc/2022/Conference/Paper42/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "This paper present a way to fully binarize a BERT model. The authors convincingly demonstrate that a naive binarization results in large quality losses and then propose amendments. It is pretty impressive that it is possible to get a fully binarized model to work at all.\nAt the same time, the quality losses are still significant and in practice one might prefer to use distillation (as long as the hardware doesn't require binarization). One could also envision combinations of the proposed technique (perhaps in the 1-1-4 setting) with distillation."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}