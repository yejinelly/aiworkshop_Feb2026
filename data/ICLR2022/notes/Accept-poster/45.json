{
  "id": "w4cXZDDib1H",
  "original": "vegJXZ8jkj-",
  "number": 45,
  "cdate": 1632875424686,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875424686,
  "tmdate": 1697934971656,
  "ddate": null,
  "content": {
    "title": "ViDT: An Efficient and Effective Fully Transformer-based Object Detector",
    "authorids": [
      "~Hwanjun_Song2",
      "~Deqing_Sun2",
      "~Sanghyuk_Chun1",
      "~Varun_Jampani2",
      "~Dongyoon_Han1",
      "~Byeongho_Heo1",
      "~Wonjae_Kim1",
      "~Ming-Hsuan_Yang1"
    ],
    "authors": [
      "Hwanjun Song",
      "Deqing Sun",
      "Sanghyuk Chun",
      "Varun Jampani",
      "Dongyoon Han",
      "Byeongho Heo",
      "Wonjae Kim",
      "Ming-Hsuan Yang"
    ],
    "keywords": [
      "object detection",
      "vision transformer",
      "detection transformer"
    ],
    "abstract": "Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2AP owing to its high scalability for large models. We release the code and trained models at https://github.com/naver-ai/vidt.",
    "one-sentence_summary": "We integrate vision and detection transformers to build an efficient  and effective fully transformer-based object detector. ",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "song|vidt_an_efficient_and_effective_fully_transformerbased_object_detector",
    "pdf": "/pdf/264491cc60f75b87764768f41d002c6f4bee883e.pdf",
    "code": "",
    "data": "",
    "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2110.03921/code)",
    "_bibtex": "@inproceedings{\nsong2022vidt,\ntitle={Vi{DT}: An Efficient and Effective Fully Transformer-based Object Detector},\nauthor={Hwanjun Song and Deqing Sun and Sanghyuk Chun and Varun Jampani and Dongyoon Han and Byeongho Heo and Wonjae Kim and Ming-Hsuan Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=w4cXZDDib1H}\n}",
    "venue": "ICLR 2022 Poster",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "w4cXZDDib1H",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 24,
    "directReplyCount": 5,
    "revisions": true,
    "replies": [
      {
        "id": "_iW2hkqxOBU",
        "original": null,
        "number": 1,
        "cdate": 1635663449030,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635663449030,
        "tmdate": 1638266492950,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "w4cXZDDib1H",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes ViDT, a high-performance Detection Transformer with an impressive accuracy-speed trade-off. A lot of experiments as well as ablation studies are conducted to prove the effectiveness of the proposed detector. The design principle of ViDT can also generalize and inspire future detector design. Moreover, this paper also includes an in-depth analysis of several current Detection Transformer architectures.\n",
          "main_review": "#### Strengths\n\n- This paper is well motivated and clearly written. It proposes a high-performance Detection Transformer coined as ViDT, which synergizes the best of Swin Transformer, YOLOS, as well as DETR (Deformable DETR) architectural design and for the first time demonstrates that the Detection Transformer family can achieve a very competitive accuracy-speed trade-off in the challenging COCO object detection benchmark. A common pitfall is that some people believe Deformable DETR is \"fast\". Deformable DETR is indeed fast in terms of the convergence, *i.e.*, the total training epochs required by Deformable DETR are much smaller compared with the original DETR. However, Deformable DETR is not fast in terms of inference FPS, which is even slower than DETR. ViDT addresses the latency issues while maintaining a very high detection AP. It shows that Detection Transformer family also has the potential to be as strong as the highly-optimized detectors, *e.g.*, the YOLO series.\n\n- This paper proposes an efficient Detection Transformer design principle that can generalize well and inspire future Detection Transformer design, *i.e.*, the encoder part in current DETR & Deformable DETR design results in high latency. Inspired by YOLOS, this paper proposes to incorporate [DET] tokens in the earlier stage of the model, which can compensate for the effects of the encoder while maintaining high inference FPS. Therefore the Transformer encoder part can be ablated by adding [DET] tokens to stage-4 of the backbone. Meanwhile, this paper finds that the Transformer decoder part is crucial for fast convergence for several Detection Transformer instances, which cannot be easily removed.\n\n- This paper conducts a lot of experiments studying different configurations of DETR & Deformable DETR with ViT backbone, YOLOS, and the proposed ViDT in the same testbed. These results are quite meaningful and can be served as valuable references & baselines to the community as well as future work.\n\n  \n\n#### Weaknesses\n\n- I believe there is one missing detail about the model design. If the [DET] x [PATCH] attention starts at stage-4, I wonder what happens to the [DET] x [DET] attention? Is [DET] x [DET] attention performed across all previous stages? And what are the feature dimensions of [DET] tokens? Are the feature dims always the same as the stage-4 feature, or they are gradually raising during different stages. In a word, I think the details and configurations of [DET] x [DET] attention need to be clarified.\n\n\n--- Post rebuttal update ---\nAfter reading the rebuttal, other reviewers' comments, and the discussion, overall I think the detector presented in the paper is strong in terms of both precision and efficiency, and the advantages I mentioned above are valuable, especially the first one. I keep my original rating.",
          "summary_of_the_review": "Overall, I believe this is a meaningful paper from the perspective of both object detection performance and research. It will also inspire the design of [DET] token based framework of other object- and region- level downstream tasks, *e.g.*, instance segmentation, video instance segmentation as well as panoptic segmentation.\n",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Reviewer_JKxV"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Reviewer_JKxV"
        ]
      },
      {
        "id": "kIt5I7ZB2Yr",
        "original": null,
        "number": 2,
        "cdate": 1635838550918,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635838550918,
        "tmdate": 1638166994763,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "w4cXZDDib1H",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this paper, the authors propose an efficient transformer-based detector. Specifically,  the proposed detector introduce Reconfigured Attention Module (RAM) , encoder-free neck and token-based knowledge distillation to boost performance with low computational overhead. ",
          "main_review": "Strengths\n- Compared with other transformer-based detectors, the proposed detector is effective and efficient, which has a high performance with a fast speed.\n- The comparison with other transformer-based detectors is written well and readable.\n\nWeaknesses\n- Though the proposed detector is effective and efficient, the novelty is limited. To my knowledge, Reconfigured Attention Module (RAM) and token-based knowledge distillation is not novel. Also, other techniques, such neck structure and auxiliary decoding loss, are not original, which are borrowed from DETR and Deformable DETR. \n- There are some concerns about  Reconfigured Attention Module (RAM). First, there is little about the cross-attention between [DET] $\\times$ [PATCH]. Could you provide more details about how to process [DET] $\\times$ [DET] and [DET] $\\times$ [PATCH] at once? Second, the proposed RAM is not novel and may not have great contribution to the performance. RAM only has cross-attention between [DET] $\\times$ [PATCH] in the last stage. The function of this module is the same as the transformer decoder. For the rest stages,  there are two operations: [PATCH] $\\times$ [PATCH] and [DET] $\\times$ [DET]. [PATCH] $\\times$ [PATCH] is a default operation in Swin Transformer. To my knowledge, [DET] $\\times$ [DET] is not meaningful and may have no effect on performance. Could you provide an experiment that removing [DET] $\\times$ [DET] from the RAM module except the last stage?\n- In Table 4 (w. Neck), the detection performance drop largely if all the stages are not involved. In this setting (w. Neck), the transformer decoder also have interaction between [DET] $\\times$ [PATCH]. Thus, the performance drop is not only caused by the lack of interaction between [DET] $\\times$ [PATCH] in RAM. Could you provide an experiment that only interacting with {4} [PATCH] in the transformer decoder?\n- In Table 2, there is not an experiment about Deformable DETR + Swin-base. Could you provide this experiment? ",
          "summary_of_the_review": "I appreciate the effective and efficient of the proposed detector, but I prefer to reject this paper for the above weaknesses. If the authors solve my concerns, I would like to raise my rate.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Reviewer_vxqH"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Reviewer_vxqH"
        ]
      },
      {
        "id": "Sv7-nZmO6e3",
        "original": null,
        "number": 3,
        "cdate": 1635908621022,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635908621022,
        "tmdate": 1638221742692,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "w4cXZDDib1H",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper builds upon the recent advances in transformer based image classification methods (ViT variants) and detection methods (DETR variants). It argues that naively replacing the conv feature backbone in DETR with a ViT based one, is problematic due to (i) the quadratic complexity of the self attention module in ViT, and (ii) then again the attention module in the transformer encoder decoder part (which they call \"neck\"). To remedy, it proposes to build on top of Swin Transformer backbone, by proposing a novel reconfigured attention module (RAM), and further removing the encoder-decoder in the neck, replacing it with only a lightweight decoder.",
          "main_review": "The description of the cross-attention is a bit informal and hard to understand. There is a lot going on in Fig 3 which could be formally described for the readers benefit.\n\nYOLOS is not scalable because of global attention between [PATCH] tokens and [DET] tokens, however here the same issue arises with cross-attention (p4 last para) and is resolved by only considering global attention at the last stage. Could something similar be done with YOLOS? e.g. removing the global cross attention between patch and det in earlier stages. And if such global attention is removed in earlier stages, Swin type layers can also be used if I am not mistaken? In that case does the network correspond to YOLOS largely (without the neck?). This should be discussed as well.\n\nDETR (vanilla and deformable) results are not given with the base models. I understand that the FPS would be low, but the results should be reported. In addition the DeiT-tiny and DeiT-small should also be the distillation based training versions. Since they are +2 points better than the non distillation ones, one could suspect that the (a) Deformable DETR + DeiT-tiny will improve from 39.2 to ~41, which would be then higher than (b) ViDT+Swin-nano (which is comparable at 16M params vs. 18M for (a)). This would also be fair wrt to backbone, from Tab1 and Sec4(Algothims) discussion, it is mentioned that the DEIT tiny, small are eqv. to Swin nano and tiny.\n\nI am also wondering why are DETR(vanilla or Deformable) results with any base model (DeiT-base or Swin-base) not given in Tab2. While they might not be comparable wrt FPS, they should be given.\n\nSimilarly, the results with Conv backbones should also be given, since there are higher reported results in the literature. I wouldn't penalize the paper for not having \"state-of-the-art\" results. This request is so that the reader can be aware of the holistic landscape.\n\n\n--- Post rebuttal update ---\nI am not entirely convinced that a simpler baseline improvement to YOLOS can not be devised. But the proposed method is surely one way to go forward. The results and discussions could also be valuable to the readers. I am not upgrading my rating but I wouldn't argue strongly for rejections, specially when there is some support from the other reviewers.",
          "summary_of_the_review": "The paper is generally well written, however one of the main contribution, the reconfigurable attention module, should be more precisely and formally described, at least an appendix. Otherwise it is a little hard to get with Fig3 largely.\n\nThe contextualization of the paper wrt YOLOS is also something that could be improved. It seems that the model is more closely related to YOLOS than seems at first read.\n\nThe experiments should be made more complete with fair comparison for baseline backbones etc. And the results with conv backbones should also be given for completeness and benefit of the reader.\n\n",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Reviewer_cEv4"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Reviewer_cEv4"
        ]
      },
      {
        "id": "98iZhs7oy6k",
        "original": null,
        "number": 1,
        "cdate": 1637379033292,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637379033292,
        "tmdate": 1637655176126,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "w4cXZDDib1H",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "General Response",
          "comment": "Dear reviewers and meta reviewers,\n\nWe appreciate all the comments including 1) the proposed design principles can generalize well and inspire future work, and the results will be a good reference and baseline to the community (**Reviewer JKxV**); 2) the paper is generally well written and the reconfigure attention module is novel (**Reviewer cEv4**); 3) the proposed detector is effective and efficient and has a high performance with a fast speed (**Reviewer vxqH**).\n\nIn the following, we address the concerns from the reviewers. In addition, we include all the responses in the revised manuscript. For ease of reviewing, we highlight the added or revised text in **red color**.\n\nWe summarize the main changes as follows:\n\n- Add a computational complexity analysis in Appendix A.1  (**C1, C2**  by  **Reviewer cEv4**)\n- Replace the results of DeiT-tiny and DeiT-small backbones with those of their distillation versions in Figure 1, Table 1, and Table 2  (**C3**  by  **Reviewer cEv4**)\n- Add the results with DeiT-base and Swin-base backbones for (Deformable) DETR in Figure 1 and Table 2  (**C4**  by  **Reviewer cEv4** and **C6**  by **Reviewer vxqH**)\n- Add the comparison with CNN backbones in Appendix C.1 and Table 13 (**C5**  by  **Reviewer cEv4**)\n- Add more detailed explanation for RAM in Section 3.1 and Appendix A  (**C6**  by  **Reviewer cEv4**)\n- Add the detailed procedure for binding [DET]x[DET] and [DET]x[PATCH] attention in Appendix A.2.1 (**C2**  by  **Reviewer vxqH**)\n- Add an ablation study for the use of [DET]x[DET] attention in Appendix C.4 and Table 15 (**C3**  by  **Reviewer vxqH**)\n- Add the comparison with variations of existing pipelines in Appendix C.2 and Table 14  (**C4**  by  **Reviewer vxqH**)\n- Add the detail of embedding dimensions for [DET] tokens in Appendix A.2.2 (**C1**  by  **Reviewer JKxV**)\n\nShould you have any further questions or suggestions, please put your comments on OpenReview. We will address all the raised concerns according to the reviewing policy. \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "HU9wlJLlI0S",
        "original": null,
        "number": 2,
        "cdate": 1637379247775,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637379247775,
        "tmdate": 1637556158472,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "Sv7-nZmO6e3",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer cEv4 (1/3)",
          "comment": "We appreciate your valuable comments and suggestions. We hope that the concerns can be resolved through our clarifications in this response and the revised paper.\n\n---\n\n>**Main novelty of the method**\n\nViDT addresses two major challenges in ViT and DETR for efficient and effective object detection.\n\n**(1) Efficient Attention Module.** YOLOS highly relies on the canonical ViT (e.g., DeiT), which suffers from its quadratic computational complexity to the number of [PATCH] tokens. Although Swin Transformer is much more efficient than DeiT (linear complexity to the number of tokens), adopting its efficient principle to YOLOS is challenging. Note that YOLOS receives the [DET] + [PATCH] tokens and *the number of  [DET] tokens of YOLOS must be the same in all stages*. Swin Transformer, on the other hand, gradually reduces the number of tokens across the stages, which makes Swin Transformer not applicable to YOLOS. Thus, we propose a Reconfigured Attention Module (RAM) that decomposes 'global self-attention to the [PATCH] tokens with [DET] appended' to 'the *three* different attention operations,' namely [PATCH]x[PATCH], [DET]x[DET], and [DET]x[PATCH] attention. With this decomposition, RAM makes direct use of the Swin Transformer's efficient mechanism only to [PATCH]x[PATCH] attention, the computationally heaviest part due to # [DET] $<<$ # [PATCH]. Thanks to the proposed RAM, ViDT maintains a fixed scale of [DET] tokens for object detection but constructs hierarchical representations starting with small-sized image patches for [PATCH] tokens.\n\n**(2) Insight on Neck Components.** (Deformable) DETR achieves competitive AP on the COCO benchmark, but its neck decoder results in a high latency in detection. Inspired by YOLOS, we incorporate [DET] tokens in the ViT backbone to make it to be a *standalone* object detector, which can compensate for the effect of the neck encoder. Therefore, the computationally heaviest part (i.e., neck encoder) is successfully ablated from the detection pipeline without compromising detection accuracies. In addition, we find that the neck decoder cannot be easily removed unlike the neck encoder because it is crucial for a fast convergence and a high AP.\n\nWhile each component (e.g. RAM, Neck-free Decoder) may be seemingly simple, the design principle of ViDT is novel, and indeed realizes a highly efficient and effective fully Transformer-based object detector. As commented by Reviewer JKxV, this work inspires further Transformer-based network designs.\n\n---"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "5GoqH_3ImkP",
        "original": null,
        "number": 3,
        "cdate": 1637379369886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637379369886,
        "tmdate": 1637556542371,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "HU9wlJLlI0S",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer cEv4 (2/3)",
          "comment": "**Response to all the raised concerns**\n\n---\n\n> **C1.** *YOLOS is not scalable because of global attention between [PATCH] and [DET] tokens, however, here the same issue arises with cross-attention.*\n\nYes, YOLOS is not scalable because it performs a single global attention for the [PATCH] tokens with [DET] tokens appended. The computational complexity of this attention is $\\mathcal{O}(d^2({\\sf P} + {\\sf D}) + d({\\sf P} + {\\sf D})^2)$, which is *quadratic* to the number of [PATCH] tokens since # [DET] $<<$ # [PATCH], where ${\\sf {P}}$ and ${\\sf D}$ are the number of [PATCH] and [DET] tokens, respectively, and $d$ is the embedding dimension. For example, at the first stage, ${\\sf P}$ and ${\\sf D}$ are 66,650 and 100, respectively.\n\nHowever, cross-attention in our ViDT does not suffer from the scalability issue as *ViDT has a linear complexity to the number of the input patch tokens instead of quadratic complexity*. The proposed RAM decomposes the single global attention in YOLOS into the three different attention operations, (1) $[\\mathtt{PATCH}]\\times[\\mathtt{PATCH}]$ local self-attention with window partition, $\\mathcal{O}(d^2 {\\sf P} + d k^2 {\\sf P})$; (2) $[\\mathtt{DET}]\\times[\\mathtt{DET}]$ global self-attention, $\\mathcal{O}(d^2 {\\sf D} + d {\\sf D}^{2})$; (3) $[\\mathtt{DET}]\\times[\\mathtt{PATCH}]$ global cross-attention, $\\mathcal{O}(d^2 ({\\sf D} + {\\sf P}) + d {\\sf D} {\\sf P})$, where $k$ is the window size ($k << {\\sf P}, {\\sf D}$).  In total, the computational complexity of RAM is $\\mathcal{O}(d^2 ({\\sf D} + {\\sf P}) + dk^2 {\\sf P} + d {\\sf D}^{2} + d {\\sf D} {\\sf P})$, which is *linear* to the number of [PATCH] tokens because the local attention in Swin Transformer is applied for [PATCH]x[PATCH] attention. Please see the detailed complexity analysis in Appendix A.1, which is newly added in the revised paper.\n\nThe main complexity difference between YOLOS and ViDT comes from the [PATCH]x[PATCH] attention, i.e., *quadratic*: $\\mathcal{O}(d^2 {\\sf P} + d {\\sf P}^2)$ in YOLOS v.s. *linear*: $\\mathcal{O}(d^2 {\\sf P} + d k^2 {\\sf P})$ in ViDT.  In particular, the gap is significantly large at the first stage where ${\\sf P}$ is 66,650. In contrast, the complexity of [DET]x[PATCH]  attention, $\\mathcal{O}(dDP)$, is not the major part because it is linear to the number of [PATCH] tokens. Therefore, the quadratic complexity issue does not occur in RAM.\n\n---\n\n>**C2-1.** *Could something similar be done with YOLOS?, e.g., removing the cross-attention between [PATCH] and [DET].*\n\nAs discussed in C1, the computational bottleneck of YOLOS is the global [PATCH]x[PATCH] self-attention with quadratic complexity. Thus, removing the cross-attention between [DET] and [PATCH] tokens cannot make YOLOS have a linear complexity similar to ViDT, and also this change will negatively affect the detection accuracy of YOLOS.\n\n---\n\n>**C2-2.** *If such global cross-attention between [DET] and [PATCH] is removed in an earlier stage in YOLOS, Swin type layers can also be used if I am not mistaken? Then, does the proposed ViDT correspond to YOLOS largely (without the neck)?*\n\nYOLOS receives the append [DET] and [PATCH] tokens as the input. Here, the number of  [DET] tokens must be same across all the stages, so the Swin Layer cannot be applied to YOLOS because it gradually changes the number of tokens. That makes it challenging to incorporate  Swin's mechanism into YOLOS. The key idea in RAM for this challenge is decomposing the original attention into the proposed three different attentions and applying the Swin mechanism only into the [PATCH]x[PATCH] attention. Therefore, the proposed ViDT does not correspond to YOLOS. \n\nThe purpose of performing the cross-attention only at the last stage in ViDT is to make it more efficient. Although it has a linear complexity to the patch size, it could make the model slow when the number is extremely high, i.e., 66,650 tokens at the first stage.\n\n---\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "wNQX6Jta5W",
        "original": null,
        "number": 4,
        "cdate": 1637379526371,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637379526371,
        "tmdate": 1637664773752,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "5GoqH_3ImkP",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer cEv4 (3/3)",
          "comment": "---\n\n>**C3.** *DeiT-tiny and DeiT-small should also be the distillation-based training versions. This would be fair wrt to backbone because they are +2 points (ImageNet accuracy) better than the non-distillation ones.*\n\nWe agree that using the distilled version of DeiT-tiny and -small models is a fair comparison. We attach the detailed comparisons below. When replacing the scratch-trained backbones to the distillation-based backbones, there is an improvement in AP of $0.9-2.7$ but their FPS is significantly worse than ViDT due to (1) the quadratic complexity of DeiT and (2) the neck encoder in (Deformable) DETR. Figure 1, Table 1, and Table 2 in the paper have been updated with the results of the distilled DeiT models.\n\nAs pointed out by the reviewer, Deformable DETR (DeiT-tiny) improves from 39.2 to 40.8 (0.4 higher than ViDT (Swin-nano)). However, Deformable DETR (DeiT-tiny dist.) shows drastically slow (almost one third in the batch inference scenario) FPS compared to ViDT (Swin-nano); 16.3 FPS (Batch) for DeiT-tiny dist. vs. 45.8 FPS (Batch) for ViDT; and 12.4 FPS (Single) vs. 20.0 FPS (Single), where FPS (Single) and FPS (Batch) refer to inference with batch size 1 and 4, respectively.\n\n\n| Method | Backbone | Epochs | AP |AP_S | AP_M | AP_L | Params | FPS (Single) | FPS (Batch) |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |  :-----: | \n| `DETR` | DeiT-tiny | 50 | 29.1 |9.2 | 29.3 | 49.5 | 24M | 10.9 | 13.1 |\n| `DETR` | DeiT-tiny (dist.)| 50 | 30.0 | 9.9 | 30.8 | 50.6 | 24M | 10.9 | 13.1 |\n| `DETR` | DeiT-small | 50 | 30.8 | 10.5 | 31.0 |52.1 | 39M |  7.8 | 8.8 |\n| `DETR` | DeiT-small (dist.)| 50 | 32.4 | 11.3 | 33.5 |53.7 | 39M |  7.8 | 8.8 |\n| `Deformable DETR` | DeiT-tiny | 50 | 39.2 | 19.5 |41.5 | 58.0 | 18M | 12.4 | 16.3 |\n| `Deformable DETR` | DeiT-tiny (dist.)| 50 | 40.8 | 21.4 |43.4 | 58.2 | 18M | 12.4 | 16.3 |\n| `Deformable DETR` | DeiT-samll | 50 | 40.9 | 20.4 | 43.8 | 59.6 | 35M | 8.5 | 10.2 |\n| `Deformable DETR` | DeiT-samll (dist.)| 50 | 43.6 | 23.3 | 47.1 | 62.1 | 35M | 8.5 | 10.2 |\n| `ViDT` | Swin-nano | 50 | 40.4 | 23.2 | 42.5 | 55.8 | 16M | 20.0 | 45.8 |\n| `ViDT` | Swin-tiny | 50 | 44.8 | 25.9 | 47.6 | 62.1 | 38M | 17.2 | 26.5 |\n\n---\n\n>**C4.** *DETR (vanilla and deformable) results are not given with the base models (DeiT-base and Swin-base).*\n\nWe agree that (Deformable) DETRs with base models are good references although they do not perform well in terms of AP and FPS. During the rebuttal period, we train all the compared methods with DeiT- and Swin-base models, as summarized in the table below.  We have added all the results in the Table 2 of the revised paper. Note that DETR models have very slow FPS compared to our ViDT models with a similar model size.\n\n| Method  | Backbone | Epochs | AP |AP_S | AP_M | AP_L | Params | FPS (Single) | FPS (Batch) |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| `DETR` | DeiT-base | 50 | 37.1 | 14.7 | 39.4 | 52.9 | 0.1B | 4.3 | 4.9 |\n| `DETR` | Swin-base | 50 | 40.7 | 18.3 | 44.1 | 62.4 | 0.1B | 9.7 | 12.6 |\n| `Deformable DETR` | DeiT-base | 50 | 46.4 | 26.7 | 50.1 | 65.4 | 0.1B | 4.4 | 5.3 |\n| `Deformable DETR` | Swin-base | 50 | 51.4 | 34.5 | 55.1 | 67.5 | 0.1B | 4.8 | 5.4 |\n\n---\n\n>**C5.** *The results with Conv backbones should also be given so that readers can be aware of the holistic landscape.*\n\nTo show the holistic landscape, we have included the results with Conv backbones (ResNet-50), which are summarized in the table below. The AP values are borrowed from (Deformable) DETR papers, but FPS is calculated in our environment with a single NVIDIA V100 GPU. ViDT (Swin-tiny) achieves +3.5 (Single) and +7.1 (Batch) higher FPS than Deformable DETR (ResNet-50). In addition, it achieves AP higher than vanilla DETR with much smaller training epochs. Please see Appendix C.1 for more details.\n\n| Method  | Backbone | Epochs | AP |AP_S | AP_M | AP_L | Params | FPS (Single) | FPS (Batch) |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| `DETR` | ResNet-50 | 500 | 42.0 | 20.5 | 45.8 | 61.1 | 41M | 22.8 | 38.6 |\n| `DETR-DC5` | ResNet-50 | 500 | 43.3 | 22.5 | 47.3 | 61.1 | 41M | 12.8 | 14.2 |\n| `DETR-DC5` | ResNet-50 | 50 | 35.3 | 15.2 | 37.5 | 53.6 | 41M | 12.8 | 14.2 |\n| `Deformable DETR` | ResNet-50 | 50 | 45.4 | 26.8 | 48.3 | 64.7 | 40M | 13.7 | 19.4 |\n| `ViDT` | Swin-tiny | 50  | 44.8 | 25.9 | 47.6 | 62.1 | 38M | 17.2 | 26.5 |\n\n---\n\n>**C6.** *The reconfigurable attention module should be more precisely and formally described at least an appendix.*\n\nThanks for the suggestion. We add more details of the reconfigured attention model in Section 3.1. In addition,  We discuss the computational complexity and further algorithmic design of RAM in Appendix A.\n\n---"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "Zw2j2kFbRya",
        "original": null,
        "number": 5,
        "cdate": 1637380392891,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637380392891,
        "tmdate": 1637664958629,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "kIt5I7ZB2Yr",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer vxqH (1/3)",
          "comment": "We appreciate the valuable and reasonable concerns by Reviewer vxqH. We hope that they can be resolved through our clarifications in this response and the revised paper.\n\n---\n\n**Response to all the raised concerns**\n\n---\n\n>**C1.** *The novelty is limited. To my knowledge, Reconfigured Attention Module (RAM) and token-based knowledge distillation is not novel, and other techniques, such as neck structure and auxiliary decoding loss, are borrowed from Deformable DETR.*\n\nViDT addresses two major challenges in ViT and DETR for efficient and effective object detection.\n\n\n**(1) Efficient Attention Module.** YOLOS highly relies on the canonical ViT (e.g., DeiT), which suffers from its quadratic computational complexity to the number of [PATCH] tokens. Although Swin Transformer is much more efficient than DeiT (linear complexity to the number of tokens), adopting its efficient principle to YOLOS is challenging. Note that YOLOS receives the [DET] + [PATCH] tokens and *the number of  [DET] tokens of YOLOS must be the same in all stages*. Swin Transformer, on the other hand, gradually reduces the number of tokens across the stages, which makes Swin Transformer not applicable to YOLOS. Thus, we propose a Reconfigured Attention Module (RAM) that decomposes 'global self-attention to the [PATCH] tokens with [DET] appended' to 'the *three* different attention operations,' namely [PATCH]x[PATCH], [DET]x[DET], and [DET]x[PATCH] attention. With this decomposition, RAM makes direct use of the Swin Transformer's efficient mechanism only to [PATCH]x[PATCH] attention, the computationally heaviest part due to # [DET] $<<$ # [PATCH]. Thanks to the proposed RAM, ViDT maintains a fixed scale of [DET] tokens for object detection but constructs hierarchical representations starting with small-sized image patches for [PATCH] tokens.\n\n**(2) Insight on Neck Components.** (Deformable) DETR achieves competitive AP on the COCO benchmark, but its neck decoder results in a high latency in detection. Inspired by YOLOS, we incorporate [DET] tokens in the ViT backbone to make it to be a *standalone* object detector, which can compensate for the effect of the neck encoder. Therefore, the computationally heaviest part (i.e., neck encoder) is successfully ablated from the detection pipeline without compromising detection accuracies. In addition, we find that the neck decoder cannot be easily removed unlike the neck encoder because it is crucial for a fast convergence and a high AP.\n\nWhile each component (e.g. RAM, Neck-free Decoder) may be seemingly simple, the design principle of ViDT is novel, and indeed realizes a highly efficient and effective fully Transformer-based object detector. As commented by Reviewer JKxV, this work inspires further Transformer-based network designs.\n\nWe are not sure the papers that make the reviewer think RAM and token-based knowledge distillation are not novel.  We would like to address concerns about the issue when we know the papers that the reviewer refers to. \n\n\n---\n\n\n>**C2.** *Could you provide more details about how to process [DET]x[DET] and [DET]x[PATCH] attention at once?*\n\nBinding the two attention mechanisms is simple in terms of the implementation complexity. [DET]x[DET] and [DET]x[PATCH] attention generates a new [DET] token, which aggregates relevant contents in [DET] and [PATCH] tokens, respectively. Since the two attentions share exactly the same [DET] query embedding (obtained after the projection as shown in Figure 3), they can be processed at once by performing the scaled-dot product between $\\text{[DET]}_Q$ and $\\big[\\text{[DET]}_K, \\text{[PATCH]}_K\\big]$ embeddings, where $Q$, $K$ are the key and query, and $[\\cdot]$ is the concatenation. \nThen, the obtained attention map is applied to the $\\big[\\text{[DET]}_V, \\text{[PATCH]}_V\\big]$ embeddings, where $V$ is the value and $d$ is the embedding dimension.\n\n$i.e., \\text{[DET]}_{new} = \\text{Softmax}\\big(\\frac{\\text{[DET]}_Q \\big[\\text{[DET]}_K, ~\\text{[PATCH]}_K\\big]^{\\top}}{\\sqrt{d}})\\big[\\text{[DET]}_V, \\text{[PATCH]}_V\\big]$.\n\nThis is a widely used implementation in the recent Transformer-based architectures, such as YOLOS. We have included the discussion for [DET] tokens in Appendix A.2 in the revised paper. We will also make our code public upon acceptance.\n\n\n---"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "8TPtEGL-MZi",
        "original": null,
        "number": 6,
        "cdate": 1637380519989,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637380519989,
        "tmdate": 1637557260806,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "Zw2j2kFbRya",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer vxqH (2/3)",
          "comment": "---\n\n>**C3.** *The proposed RAM is not novel and may not have a great contribution to the performance, since RAM only has cross-attention between [DET] x [PATCH] attention in the last stage? The function of this module is the same as the transformer decoder.*\n\nRAM has similar attention operations as the Transformer decoder at the neck. However, the main difference between RAM and the transformer decoder is that RAM makes the existing ViT backbone to be a standalone object detector, which means the [DET]x[PATCH] attention affects [PATCH]x[PATCH] attention to extract a more suitable representation for detection in a synergistic manner. Therefore, we note that the improvements with RAM are remarkable although ViDT maintains [DET]x[PATCH] cross-attention at the last stage.\n\nAs shown in the table below, Deformable DETR is good w.r.t AP but shows very high latency due to the neck encoder component. If we simply remove the neck encoder, its AP drops drastically (-9.1) since fine-grained representation for object detection is difficult to obtain directly from the raw ViT backbone without using an additional neck encoder. However, ViDT compensates for the effect of the neck encoder by adding [DET] tokens into the body (backbone), thus successfully removing the computational bottleneck without compromising AP; it maintains +6.4 higher AP compared with the neck encoder-free Deformable DETR (the second row) while achieving similar FPS. Therefore, we conjecture that RAM contributes significantly to the performance w.r.t AP and FPS, especially for the trade-off between them. We included this discussion in Appendix C.2 and Table 14 of the revised paper.\n\n\n| Method | Neck Encoder | Backbone | Epochs | AP |AP_S | AP_M | AP_L | Params | FPS |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| `Deformable DETR` | O | Swin-nano | 50 | 43.1 | 25.9 | 45.2 | 59.4 | 17M | 7.0 |\n| `Deformable DETR` | X | Swin-nano | 50 | 34.0  | 18.0 | 36.3 | 18.4 | 14M | 22.4 |\n| `ViDT` |X | Swin-nano | 50  | **40.4 (+6.4)**  | 23.2 | 42.8  | 55.9 | 16M | **20.0 (-2.4)** |\n\n---\n\n>**C4.** *To my knowledge, [DET]x[DET] is not meaningful and may have no effect on performance. Could you provide an experiment that removes [DET]x[DET] from the RAM module except the last stage?*\n\nAs suggested, we conduct an ablation study by removing the [DET]x[DET] attention one by one from the bottom stage with results summarized in the table below. When all the [DET]x[DET] attention is removed (the last row), the AP drops by 0.7, which is a considerable performance degradation. On the other hand, as long as the self-attention is activated at the last two stages, all the strategies exhibit similar AP. Therefore, only keeping [DET]x[DET] self-attention at the last two stages can further increase FPS (+0.2) without any degradation in AP. This observation could be used as another design choice for the AP and FPS trade-off. Therefore, we believe that [DET]x[DET] attention is meaningful to use in RAM.\n\n| Stage 1 |  Stage 2 |  Stage 3 |  Stage 4 | Backbone | Epochs | AP | FPS  |\n| :-----: | :-----: | :-----: | :-----: | :-----: |  :-----: |  :-----: |  :-----: | \n| O|O| O| O|  Swin-nano | 50 | 40.4 | 20.0 |\n| |O| O| O|  Swin-nano | 50 | 40.3 | 20.1 |\n| | | O| O| Swin-nano | 50 | 40.4 | 20.2 |\n| | | | O|  Swin-nano | 50 | 40.1 | 20.3 |\n| | | | | Swin-nano | 50 | 39.7 | 20.4 | \n\n---\n\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "ixuJG4vGynw",
        "original": null,
        "number": 7,
        "cdate": 1637380623676,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637380623676,
        "tmdate": 1637557385064,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "8TPtEGL-MZi",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer vxqH (3/3)",
          "comment": "---\n\n>**C5.** *The transformer decoder also has interaction between [DET]x[PATCH] attention. Thus, the performance drop is not only caused by the lack of interaction between [DET]x[PATCH] in RAM. Could you provide an experiment that only interacts with [PATCH] in the body?*\n\nThe performance drop is not only caused by the lack of interaction between [DET]x[PATCH]. It is also associated with the use of spatial positional encoding, which greatly benefits object detection. As shown in Section 4.2.1, the spatial positional encoding helps in significant performance improvement, e.g., 5.0AP in Table 3 in our paper, or 7.8AP in Table 8 of the DETR paper. \n\nIn Deformable DETR, this positional encoding is only injected at the neck Transformer encoder, but ViDT removes this encoder due to its computational bottleneck for an efficient detector. To remedy this problem, we instead inject the positional encoding into the features associated with cross-attention in RAM, as can be seen from the left side of Figure 3. Therefore, if all the cross-attention is deactivated, there are two reasons for the performance degradation: (1) missing-cross attention (the lack of information change between [DET] and [PATCH] tokens) and (2) missing spatial encoding.\n\nIn addition, we have conducted the experiment -- interacting [PATCH] with [DET] only in the Transformer decoder (i.e., Swin-Transformer w.o. RAM + Transformer Decoder). The results are shown on the second row of the table for C3 above. In this case, the AP is only 34.0 due to the absence of the neck encoder. However, ViDT complements the effect of the neck encoder by appending [DET] tokens into the Swin Transformer backbone, achieving 40.4AP. Furthermore, if we only remove the sinusoidal positional encoding for the cross-attention in RAM, it shows 39.2AP, which is 1.2AP lower than ViDT.\n\n---\n\n>**C6.** There is not an experiment about Deformable DETR + Swin-base. \n\nWe agree that (Deformable) DETRs with base models are good references to readers, although their performances are not good in terms of AP and FPS. During the rebuttal period, we trained all the compared methods with DeiT- and Swin-base models, as summarized in the table below.  We have added all the results in Table 2 of the revised paper. Note that DETR models show catastrophically slow FPS compared to our ViDT models with a similar parameter size.\n\n| Method  | Backbone | Epochs | AP |AP_S | AP_M | AP_L | Params | FPS (Single) | FPS (Batch) |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| `DETR` | DeiT-base | 50 | 37.1 | 14.7 | 39.4 | 52.9 | 0.1B | 4.3 | 4.9 |\n| `DETR` | Swin-base | 50 | 40.7 | 18.3 | 44.1 | 62.4 | 0.1B | 9.7 | 12.6 |\n| `Deformable DETR` | DeiT-base | 50 | 46.4 | 26.7 | 50.1 | 65.4 | 0.1B | 4.4 | 5.3 |\n| `Deformable DETR` | Swin-base | 50 | 51.4 | 34.5 | 55.1 | 67.5 | 0.1B | 4.8 | 5.4 |\n\n---\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "LSAwrKOtpFB",
        "original": null,
        "number": 8,
        "cdate": 1637380707171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637380707171,
        "tmdate": 1637665900317,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "_iW2hkqxOBU",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer JKxV",
          "comment": "We appreciate the reviewer's constructive comments and positive feedback.\n\n---\n\n>**C1.**  *I wonder what happens to the [DET]x[DET] attention. Is [DET]x[DET] attention performed across all previous stages? And what are the feature dimensions of [DET] tokens? Are the feature dims always the same as stage 4, or they are gradually raising during different stages? I think the details and configurations of [DET]x[DET] attention need to be clarified.*\n\nIt is correct that  [DET]x[DET] attention is performed across all previous stages, and the feature dimension of [DET] tokens increases gradually like [PATCH] tokens. As for the patch token, its feature dimension is increased by concatenating nearby [PATCH] tokens in a grid. However, this mechanism is not applicable for [DET] tokens since we maintain the same number of [DET] tokens for detecting a fixed number of objects in a scene. Hence, we simply repeat a [DET] token multiple times along the feature dimension to increase its size. This allows [DET] tokens to reuse all the projection and normalization layers in Swin Transformer without any modification. We discuss the detailed configuration of [DET] tokens in Appendix A.2. As mentioned in the abstract, we will release our code to the public for reproducibility.\n\n---\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "NsaBZQW4Nz",
        "original": null,
        "number": 9,
        "cdate": 1637943180355,
        "mdate": 1637943180355,
        "ddate": null,
        "tcdate": 1637943180355,
        "tmdate": 1637943180355,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "Sv7-nZmO6e3",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Please let us know whether you have additional questions",
          "comment": "Dear Reviewer cEv4,\n\nWe appreciate your comments. We have provided more results/explanations based on your review. Please go over our response and let us know whether you have additional questions or not.\n\nThank you,"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "9AsSJ7WmeqK",
        "original": null,
        "number": 10,
        "cdate": 1637943232225,
        "mdate": 1637943232225,
        "ddate": null,
        "tcdate": 1637943232225,
        "tmdate": 1637943232225,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "kIt5I7ZB2Yr",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Please let us know whether you have additional questions",
          "comment": "Dear Reviewer vxqH,\n\nWe appreciate your comments. We have provided more results/explanations based on your review. Please go over our response and let us know whether you have additional questions or not.\n\nThank you,"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "Ym2Zfz8GtDS",
        "original": null,
        "number": 12,
        "cdate": 1638006830787,
        "mdate": 1638006830787,
        "ddate": null,
        "tcdate": 1638006830787,
        "tmdate": 1638006830787,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "9AsSJ7WmeqK",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Reviewer Response to Author Response",
          "comment": "Thanks for the great efforts in the rebuttal. The authors have resolved most of my concerns, but I still have a concern about C5. I need to clarify my question: could you provide an experiment that only interacting with **stage 4** [PATCH] in the transformer decoder? In the transformer decoder, there is a cross-attention between [DET] and [PATCH] of all stages. For early stages, the features may have little useful information for detection. With this large [PATH] set, it is hard to extract useful information. This is caused by softmax function, which produces positive value for each [PATH]. Maybe this is another reason for performance drop."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Reviewer_vxqH"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Reviewer_vxqH"
        ]
      },
      {
        "id": "v-GweTqyDap",
        "original": null,
        "number": 13,
        "cdate": 1638020488597,
        "mdate": 1638020488597,
        "ddate": null,
        "tcdate": 1638020488597,
        "tmdate": 1638020488597,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "Ym2Zfz8GtDS",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "We would like to fully address your concern!",
          "comment": "Dear Reviewer vxqH,\n\nThank you for the clarification of C5. We would like to fully address your concerns, so could you explain your question in more detail?\n\nAs you mentioned, there is a cross-attention for each layer in the Transformer decoder (we used **6 layers** of deformable transformer decoder).\n\nIn the deformable decoding layer, it receives **four levels** of hierarchical [PATCH] tokens generated from each stage of Swin Transformer (i.e., stages 1, 2, 3, 4 at the Swin backbone). Since the same hierarchical [PATCH] tokens are provided to all layers in the decoder, **the information in the [PATCH] tokens is exactly the same in all the decoding layers (note that there is no [PATCH]x[PATCH] attention in the decoder)**.  That is, the decoding layer aggregates multi-scale [PATCH] tokens to generate new [DET] tokens via cross-attention without affecting the [PATCH] information; it only decides which multi-scale [PATCH] tokens should be aggregated for better [DET] tokens.\n\nYour question \"only interacting with stage 4 [PATCH] in the transformer decoder\" means that only providing [PATCH] tokens at Stage 4 of the Swin backbone as the input to the transformer decoder? (i.e., single-scale cross attention with stage 4 [PATCH] tokens). \n\nPlease confirm that our understanding is correct.  \n\nIf this is correct, we do not benefit from using multi-scale features for object detection, which is crucial for detecting small-size objects in the scene. We will provide more results if our understanding is correct.\n\nThanks,"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "gvguDb5_5Et",
        "original": null,
        "number": 14,
        "cdate": 1638021468068,
        "mdate": 1638021468068,
        "ddate": null,
        "tcdate": 1638021468068,
        "tmdate": 1638021468068,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "v-GweTqyDap",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Reviewer Response to Author Response ",
          "comment": "Yes, your understanding is correct. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Reviewer_vxqH"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Reviewer_vxqH"
        ]
      },
      {
        "id": "hDa7ljGdaDj",
        "original": null,
        "number": 15,
        "cdate": 1638022374458,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638022374458,
        "tmdate": 1638022390080,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "gvguDb5_5Et",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Thank you for checking!",
          "comment": "Thank you for checking! \n\nIn this case, we carefully expect that the detection performance for small objects will rather drops drastically since we only use h/32 x w/32 (row resolution) [PATCH] tokens in the decoder. \n\nWe will provide the exact results for this until Nov 29th! (Training takes a few days)\n\nThanks,\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "6zDewbqiA0b",
        "original": null,
        "number": 17,
        "cdate": 1638100321920,
        "mdate": 1638100321920,
        "ddate": null,
        "tcdate": 1638100321920,
        "tmdate": 1638100321920,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "NsaBZQW4Nz",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Dear Reviewer cEv4",
          "comment": "We thank you for the precious review time and valuable comments/suggestions. We have provided corresponding responses and results, which we believe have covered your concerns: (1) detailed description for RAM including complexity analysis, (2) more fair DeiT models, (3) results with base models, and (4) comparison with CNN backbones.\n\nPlease take a close look at our paper and response, and let us know whether you have additional questions or not.\n\nThank you,"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "8XLSXcWdzKe",
        "original": null,
        "number": 19,
        "cdate": 1638165755769,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638165755769,
        "tmdate": 1638169918285,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "Ym2Zfz8GtDS",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "An experiment that only interacts with stage 4 [PATCH] in the transformer decoder",
          "comment": "Dear Reviewer vxqH,\n\nThank you for considering another aspect that could be contributing to performance degradation; too many [PATCH] tokens at the early layer in the decoder make it hard to extract useful information when conducting cross-attention. \n\nTo fully address your concern, we have conducted an experiment that only interacts with stage 4 [PATCH] tokens in the transformer decoder (i.e., **multi-scale** cross-attention -> **single-scale** cross-attention). We have tested four combinations to investigate the performance change when combined with RAM (the cross-attention in the backbone), as follows:\n\n(1) RAM: Cross-attention at Stage 4  \t/ Decoder: Cross-attention with multi-scale [PATCH] tokens\n\n(2) RAM: Cross-attention at Stage 4 \t/ Decoder: Cross-attention with single-scale Stage 4 [PATCH] tokens\n\n(3) RAM: No Cross-attention \t\t\t\t/ Decoder: Cross-attention with multi-scale [PATCH] tokens\n\n(4) RAM: No Cross-attention \t\t\t\t/ Decoder: Cross-attention with single-scale Stage 4 [PATCH] tokens\n\n\n|#| RAM| Decoder | Epochs | AP |AP_S | AP_M | AP_L | \n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |:-----: |\n| (1) | `{4}`  | `Multi-scale` \t| 50 | 40.4 | 23.2 | 42.5 | 55.8 |\n| (2) | `{4}`  | `Single-scale` | 50 | 30.3 (-10.1) | 14.6 (-8.6) | 32.8 (-9.7) | 45.9 (-9.9) | \n\n\n| # | RAM| Decoder | Epochs | AP |AP_S | AP_M | AP_L | \n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |:-----: |\n| (3) | `{ }`  | `Multi-scale` \t| 50 | 37.1 | 20.2 | 39.4 | 51.5 |\n| (4) | `{ }`  | `Single-scale` | 50 | 23.3 (-13.8) | 8.7 (-11.5) | 22.7 (-16.7) | 38.8 (-12.7)|\n\nOverall, only interacting single-scale (Stage 4) [Patch] tokens in the decoder rather drop APs significantly, which means that multi-scale cross-attention in the decoder is important to achieve high AP.  As you mentioned, if multi-scale features are used, the number of [PATCH] tokens is too many, but we found that **only a small number of sampled [PATCH] tokens participate in the deformable cross-attention regardless of the total number of [PATCH] tokens**. The number of sampled [PATCH] tokens $K$ (in the below Equation 1.) is set to be 4 for the Deformable attention.  Thus, only 8 (multi-head) x 4 (level of scale)  x 4 (K) = 128 [PATCH] tokens are used with multi-scale cross-attention, and only 8 (multi-head) x 1 (level of scale)  x 4 (K) = 32 [PATCH] tokens are used with single-scale cross-attention. These numbers are very small and constant regardless of the total number of tokens.\n\nTherefore, **we believe that using all the [PATCH] tokens from the four stages does not make it difficult to extract useful information**, and the benefit of using multi-scale features is very significant. \n\n`Equation 1. Deformable Cross-attention (Eq. (1) in the paper)`\n\n${\\rm MSDeformAttn}([\\mathtt{DET}], \\{ {x^{l}} \\}_{l=1}^{L})$ \n\n$=\\sum_{m=1}^{M}{W_m}\\Big[ \\sum_{l=1}^{L} \\sum_{k=1}^{K} A_{mlk}$ $ \\cdot {W_m^{\\prime}} {x^{l}}\\big(\\phi_{l}({p}) + \\Delta {p_{mlk}}\\big) \\Big]$.\n\n$L$ is the level of scales\n\n$K$ is the total number of sampled keys for content aggregation\n\n$M$ is the number of heads\n\n$\\phi_{l}({p})$ is the reference point of the $[\\mathtt{DET}]$ token re-scaled for the $l$-th level feature map\n\n$\\Delta{p_{mlk}}$ is the sampling offset for deformable attention\n\n$A_{mlk}$ is the attention weights of the $K$ sampled contents\n\n${W_m}$ and ${W_m^{\\prime}}$ are the projection matrices for multi-head attention\n\nThanks,"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "TsrKBY8OwO",
        "original": null,
        "number": 20,
        "cdate": 1638166959182,
        "mdate": 1638166959182,
        "ddate": null,
        "tcdate": 1638166959182,
        "tmdate": 1638166959182,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "8XLSXcWdzKe",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Reviewer Response to Author",
          "comment": "Thanks for the great efforts in the rebuttal. My concerns have been resolved, thus I will raise my score."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Reviewer_vxqH"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Reviewer_vxqH"
        ]
      },
      {
        "id": "ilNdLpKoyD-",
        "original": null,
        "number": 22,
        "cdate": 1638196988495,
        "mdate": 1638196988495,
        "ddate": null,
        "tcdate": 1638196988495,
        "tmdate": 1638196988495,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "NsaBZQW4Nz",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "A gentle reminder for Reviewer cEv4",
          "comment": "Dear Reviewer cEv4,\n\nWe appreciate your constructive comments for helping us to improve our paper in many aspects. This is a gentle reminder since the final discussion stage ends soon. \n\nWe would like to ask if Reviewer cEv4 may have any further questions regarding our submission so that we can still respond.\n\nThanks,"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "7O0tBldVGoY",
        "original": null,
        "number": 23,
        "cdate": 1638220767845,
        "mdate": 1638220767845,
        "ddate": null,
        "tcdate": 1638220767845,
        "tmdate": 1638220767845,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "wNQX6Jta5W",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Thank you for the clarifications and further results",
          "comment": "Dear Authors,\n\nSorry for the delay, and thank you for the clarifications.\n\nAlthough I still believe that alternate and simple modification to YOLOS algorithm could be done to improve the speed similar to the proposed method (which is perhaps the main improvement here), the method is one way of going forward. I might have been misunderstood a bit (regarding the global attention part), but the clarifications nonetheless do make things more clear.\n\nThank you also for the complete results, it makes the full landscape clear, giving some advantage to the proposed method in terms of speed for comparable performances.\n\nBest,\nGaurav"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Reviewer_cEv4"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Reviewer_cEv4"
        ]
      },
      {
        "id": "JKIDb3znZPY",
        "original": null,
        "number": 24,
        "cdate": 1638225889836,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638225889836,
        "tmdate": 1638226065884,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "7O0tBldVGoY",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "Thank you for the feedback!",
          "comment": "Dear Reviewer cEv4,\n\nWe really appreciate your feedback on our response. We agree that YOLOS could be improved in another way to resolve its heavy computation. Hence, we believe that the proposed reconfigured attention module (RAM) will inspire those future works and can be used as a good reference and baseline. Please note that this is the first work to incorporate the efficient mechanism of Swin Transformer to improve the YOLOS.\n\nIn addition, we would like to emphasize that **solving the slow performance of YOLOS is not everything for high detection performance**. As can be seen in the table below, the two neck-free detectors, YOLOS and its improved ViDT (w.o. Neck), shows relatively poor AP performance although ViDT (w.o. Neck) resolves the computational bottleneck in YOLOS.\n\nTo achieve high AP and fast training speed, **the integration with detection transformers (i.e., the neck components) is necessary**. Therefore, we proposed to integrate only the neck decoder (DETR) with our proposed detection backbone, which is justified by the two new observations: (1) the computationally heaviest part (i.e., neck encoder) is successfully ablated from the detection pipeline without compromising detection accuracies and (2) the neck decoder cannot be easily removed, unlike the neck encoder. Therefore, this integrated version called ViDT improves AP significantly while achieving high FPS.\n\n**The main novelty of this work is to synergize vision and detection transformers for the best trade-off between accuracy and efficiency.** The proposed ViDT shows that Detection Transformer family also has the potential to be as strong as the highly-optimized detector.\n\n|Method | Backbone | Epochs | AP |AP_S | AP_M | AP_L | Params | FPS (Single) | FPS (Batch) |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |:-----: |:-----: |:-----: |\n| `YOLOS` | DeiT-tiny\t| 150 | 30.4 | 12.4 | 31.8 | 48.2 | 6M | 28.1 | 31.3 |\n| `YOLOS` | DeiT-small| 150 | 36.1 | 15.6 | 38.4 | 55.3 | 30M | 9.3 | 11.8 |\n| `ViDT (w.o. Neck)` | Swin-nano  | 150 | 28.7 | 12.3 | 30.7 | 44.1 | 7M | 36.5 | 64.4 |\n| `ViDT (w.o. Neck)` | Swin-tiny  | 150 | 36.3 | 16.4 | 39.0 | 54.3 | 29M | 28.6 | 32.1 |\n| `ViDT`  | Swin-nano  | 50 | 40.4 | 23.2 | 42.5 | 55.8 | 16M | 20.0 | 45.8 |\n| `ViDT` | Swin-tiny  | 50 | 44.8 | 25.9 | 47.6 | 62.1 | 38M | 17.2 | 26.5 |\n\nThank you again for the constructive feedback, and let us know whether you have additional questions or not.\n\nBest,"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "qnRJu100aag",
        "original": null,
        "number": 1,
        "cdate": 1642696834656,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696834656,
        "tmdate": 1642696834656,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "w4cXZDDib1H",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "The paper introduces an object detection method that integrates vision and detection transformers through a novel Reconfigured Attention Module (RAM). Among other questions, the reviewers raised concerns about fair comparison with baselines, limited novelty of the RAM module, completeness of experiments, and missing details. The rebuttal adequately addressed these concerns with clarifications and additional experiments. R1 remained unconvinced that a simple modification to YOLOS could not be devised to improve the speed similar to the proposed method, but stated he/she wouldn\u2019t argue strongly for rejection. While this is a legitimate concern, the AC agrees with R2 and R3 that the paper has enough merits to be accepted at ICLR, as the results are strong and are likely to have significant practical value."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "_iW2hkqxOBU",
        "original": null,
        "number": 1,
        "cdate": 1635663449030,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635663449030,
        "tmdate": 1638266492950,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "w4cXZDDib1H",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes ViDT, a high-performance Detection Transformer with an impressive accuracy-speed trade-off. A lot of experiments as well as ablation studies are conducted to prove the effectiveness of the proposed detector. The design principle of ViDT can also generalize and inspire future detector design. Moreover, this paper also includes an in-depth analysis of several current Detection Transformer architectures.\n",
          "main_review": "#### Strengths\n\n- This paper is well motivated and clearly written. It proposes a high-performance Detection Transformer coined as ViDT, which synergizes the best of Swin Transformer, YOLOS, as well as DETR (Deformable DETR) architectural design and for the first time demonstrates that the Detection Transformer family can achieve a very competitive accuracy-speed trade-off in the challenging COCO object detection benchmark. A common pitfall is that some people believe Deformable DETR is \"fast\". Deformable DETR is indeed fast in terms of the convergence, *i.e.*, the total training epochs required by Deformable DETR are much smaller compared with the original DETR. However, Deformable DETR is not fast in terms of inference FPS, which is even slower than DETR. ViDT addresses the latency issues while maintaining a very high detection AP. It shows that Detection Transformer family also has the potential to be as strong as the highly-optimized detectors, *e.g.*, the YOLO series.\n\n- This paper proposes an efficient Detection Transformer design principle that can generalize well and inspire future Detection Transformer design, *i.e.*, the encoder part in current DETR & Deformable DETR design results in high latency. Inspired by YOLOS, this paper proposes to incorporate [DET] tokens in the earlier stage of the model, which can compensate for the effects of the encoder while maintaining high inference FPS. Therefore the Transformer encoder part can be ablated by adding [DET] tokens to stage-4 of the backbone. Meanwhile, this paper finds that the Transformer decoder part is crucial for fast convergence for several Detection Transformer instances, which cannot be easily removed.\n\n- This paper conducts a lot of experiments studying different configurations of DETR & Deformable DETR with ViT backbone, YOLOS, and the proposed ViDT in the same testbed. These results are quite meaningful and can be served as valuable references & baselines to the community as well as future work.\n\n  \n\n#### Weaknesses\n\n- I believe there is one missing detail about the model design. If the [DET] x [PATCH] attention starts at stage-4, I wonder what happens to the [DET] x [DET] attention? Is [DET] x [DET] attention performed across all previous stages? And what are the feature dimensions of [DET] tokens? Are the feature dims always the same as the stage-4 feature, or they are gradually raising during different stages. In a word, I think the details and configurations of [DET] x [DET] attention need to be clarified.\n\n\n--- Post rebuttal update ---\nAfter reading the rebuttal, other reviewers' comments, and the discussion, overall I think the detector presented in the paper is strong in terms of both precision and efficiency, and the advantages I mentioned above are valuable, especially the first one. I keep my original rating.",
          "summary_of_the_review": "Overall, I believe this is a meaningful paper from the perspective of both object detection performance and research. It will also inspire the design of [DET] token based framework of other object- and region- level downstream tasks, *e.g.*, instance segmentation, video instance segmentation as well as panoptic segmentation.\n",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Reviewer_JKxV"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Reviewer_JKxV"
        ]
      },
      {
        "id": "kIt5I7ZB2Yr",
        "original": null,
        "number": 2,
        "cdate": 1635838550918,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635838550918,
        "tmdate": 1638166994763,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "w4cXZDDib1H",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this paper, the authors propose an efficient transformer-based detector. Specifically,  the proposed detector introduce Reconfigured Attention Module (RAM) , encoder-free neck and token-based knowledge distillation to boost performance with low computational overhead. ",
          "main_review": "Strengths\n- Compared with other transformer-based detectors, the proposed detector is effective and efficient, which has a high performance with a fast speed.\n- The comparison with other transformer-based detectors is written well and readable.\n\nWeaknesses\n- Though the proposed detector is effective and efficient, the novelty is limited. To my knowledge, Reconfigured Attention Module (RAM) and token-based knowledge distillation is not novel. Also, other techniques, such neck structure and auxiliary decoding loss, are not original, which are borrowed from DETR and Deformable DETR. \n- There are some concerns about  Reconfigured Attention Module (RAM). First, there is little about the cross-attention between [DET] $\\times$ [PATCH]. Could you provide more details about how to process [DET] $\\times$ [DET] and [DET] $\\times$ [PATCH] at once? Second, the proposed RAM is not novel and may not have great contribution to the performance. RAM only has cross-attention between [DET] $\\times$ [PATCH] in the last stage. The function of this module is the same as the transformer decoder. For the rest stages,  there are two operations: [PATCH] $\\times$ [PATCH] and [DET] $\\times$ [DET]. [PATCH] $\\times$ [PATCH] is a default operation in Swin Transformer. To my knowledge, [DET] $\\times$ [DET] is not meaningful and may have no effect on performance. Could you provide an experiment that removing [DET] $\\times$ [DET] from the RAM module except the last stage?\n- In Table 4 (w. Neck), the detection performance drop largely if all the stages are not involved. In this setting (w. Neck), the transformer decoder also have interaction between [DET] $\\times$ [PATCH]. Thus, the performance drop is not only caused by the lack of interaction between [DET] $\\times$ [PATCH] in RAM. Could you provide an experiment that only interacting with {4} [PATCH] in the transformer decoder?\n- In Table 2, there is not an experiment about Deformable DETR + Swin-base. Could you provide this experiment? ",
          "summary_of_the_review": "I appreciate the effective and efficient of the proposed detector, but I prefer to reject this paper for the above weaknesses. If the authors solve my concerns, I would like to raise my rate.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Reviewer_vxqH"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Reviewer_vxqH"
        ]
      },
      {
        "id": "Sv7-nZmO6e3",
        "original": null,
        "number": 3,
        "cdate": 1635908621022,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635908621022,
        "tmdate": 1638221742692,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "w4cXZDDib1H",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper builds upon the recent advances in transformer based image classification methods (ViT variants) and detection methods (DETR variants). It argues that naively replacing the conv feature backbone in DETR with a ViT based one, is problematic due to (i) the quadratic complexity of the self attention module in ViT, and (ii) then again the attention module in the transformer encoder decoder part (which they call \"neck\"). To remedy, it proposes to build on top of Swin Transformer backbone, by proposing a novel reconfigured attention module (RAM), and further removing the encoder-decoder in the neck, replacing it with only a lightweight decoder.",
          "main_review": "The description of the cross-attention is a bit informal and hard to understand. There is a lot going on in Fig 3 which could be formally described for the readers benefit.\n\nYOLOS is not scalable because of global attention between [PATCH] tokens and [DET] tokens, however here the same issue arises with cross-attention (p4 last para) and is resolved by only considering global attention at the last stage. Could something similar be done with YOLOS? e.g. removing the global cross attention between patch and det in earlier stages. And if such global attention is removed in earlier stages, Swin type layers can also be used if I am not mistaken? In that case does the network correspond to YOLOS largely (without the neck?). This should be discussed as well.\n\nDETR (vanilla and deformable) results are not given with the base models. I understand that the FPS would be low, but the results should be reported. In addition the DeiT-tiny and DeiT-small should also be the distillation based training versions. Since they are +2 points better than the non distillation ones, one could suspect that the (a) Deformable DETR + DeiT-tiny will improve from 39.2 to ~41, which would be then higher than (b) ViDT+Swin-nano (which is comparable at 16M params vs. 18M for (a)). This would also be fair wrt to backbone, from Tab1 and Sec4(Algothims) discussion, it is mentioned that the DEIT tiny, small are eqv. to Swin nano and tiny.\n\nI am also wondering why are DETR(vanilla or Deformable) results with any base model (DeiT-base or Swin-base) not given in Tab2. While they might not be comparable wrt FPS, they should be given.\n\nSimilarly, the results with Conv backbones should also be given, since there are higher reported results in the literature. I wouldn't penalize the paper for not having \"state-of-the-art\" results. This request is so that the reader can be aware of the holistic landscape.\n\n\n--- Post rebuttal update ---\nI am not entirely convinced that a simpler baseline improvement to YOLOS can not be devised. But the proposed method is surely one way to go forward. The results and discussions could also be valuable to the readers. I am not upgrading my rating but I wouldn't argue strongly for rejections, specially when there is some support from the other reviewers.",
          "summary_of_the_review": "The paper is generally well written, however one of the main contribution, the reconfigurable attention module, should be more precisely and formally described, at least an appendix. Otherwise it is a little hard to get with Fig3 largely.\n\nThe contextualization of the paper wrt YOLOS is also something that could be improved. It seems that the model is more closely related to YOLOS than seems at first read.\n\nThe experiments should be made more complete with fair comparison for baseline backbones etc. And the results with conv backbones should also be given for completeness and benefit of the reader.\n\n",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Reviewer_cEv4"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Reviewer_cEv4"
        ]
      },
      {
        "id": "98iZhs7oy6k",
        "original": null,
        "number": 1,
        "cdate": 1637379033292,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637379033292,
        "tmdate": 1637655176126,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "w4cXZDDib1H",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Official_Comment",
        "content": {
          "title": "General Response",
          "comment": "Dear reviewers and meta reviewers,\n\nWe appreciate all the comments including 1) the proposed design principles can generalize well and inspire future work, and the results will be a good reference and baseline to the community (**Reviewer JKxV**); 2) the paper is generally well written and the reconfigure attention module is novel (**Reviewer cEv4**); 3) the proposed detector is effective and efficient and has a high performance with a fast speed (**Reviewer vxqH**).\n\nIn the following, we address the concerns from the reviewers. In addition, we include all the responses in the revised manuscript. For ease of reviewing, we highlight the added or revised text in **red color**.\n\nWe summarize the main changes as follows:\n\n- Add a computational complexity analysis in Appendix A.1  (**C1, C2**  by  **Reviewer cEv4**)\n- Replace the results of DeiT-tiny and DeiT-small backbones with those of their distillation versions in Figure 1, Table 1, and Table 2  (**C3**  by  **Reviewer cEv4**)\n- Add the results with DeiT-base and Swin-base backbones for (Deformable) DETR in Figure 1 and Table 2  (**C4**  by  **Reviewer cEv4** and **C6**  by **Reviewer vxqH**)\n- Add the comparison with CNN backbones in Appendix C.1 and Table 13 (**C5**  by  **Reviewer cEv4**)\n- Add more detailed explanation for RAM in Section 3.1 and Appendix A  (**C6**  by  **Reviewer cEv4**)\n- Add the detailed procedure for binding [DET]x[DET] and [DET]x[PATCH] attention in Appendix A.2.1 (**C2**  by  **Reviewer vxqH**)\n- Add an ablation study for the use of [DET]x[DET] attention in Appendix C.4 and Table 15 (**C3**  by  **Reviewer vxqH**)\n- Add the comparison with variations of existing pipelines in Appendix C.2 and Table 14  (**C4**  by  **Reviewer vxqH**)\n- Add the detail of embedding dimensions for [DET] tokens in Appendix A.2.2 (**C1**  by  **Reviewer JKxV**)\n\nShould you have any further questions or suggestions, please put your comments on OpenReview. We will address all the raised concerns according to the reviewing policy. \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper45/Authors"
        ]
      },
      {
        "id": "qnRJu100aag",
        "original": null,
        "number": 1,
        "cdate": 1642696834656,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696834656,
        "tmdate": 1642696834656,
        "tddate": null,
        "forum": "w4cXZDDib1H",
        "replyto": "w4cXZDDib1H",
        "invitation": "ICLR.cc/2022/Conference/Paper45/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "The paper introduces an object detection method that integrates vision and detection transformers through a novel Reconfigured Attention Module (RAM). Among other questions, the reviewers raised concerns about fair comparison with baselines, limited novelty of the RAM module, completeness of experiments, and missing details. The rebuttal adequately addressed these concerns with clarifications and additional experiments. R1 remained unconvinced that a simple modification to YOLOS could not be devised to improve the speed similar to the proposed method, but stated he/she wouldn\u2019t argue strongly for rejection. While this is a legitimate concern, the AC agrees with R2 and R3 that the paper has enough merits to be accepted at ICLR, as the results are strong and are likely to have significant practical value."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}