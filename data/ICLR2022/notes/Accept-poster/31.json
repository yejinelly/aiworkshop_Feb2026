{
  "id": "C1_esHN6AVn",
  "original": "50eEK9BvVt9",
  "number": 31,
  "cdate": 1632875423600,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875423600,
  "tmdate": 1676330692045,
  "ddate": null,
  "content": {
    "title": "Learning Synthetic Environments and Reward Networks for Reinforcement Learning",
    "authorids": [
      "~Fabio_Ferreira1",
      "~Thomas_Nierhoff1",
      "~Andreas_S\u00e4linger1",
      "~Frank_Hutter1"
    ],
    "authors": [
      "Fabio Ferreira",
      "Thomas Nierhoff",
      "Andreas S\u00e4linger",
      "Frank Hutter"
    ],
    "keywords": [
      "Synthetic Environments",
      "Synthetic Data",
      "Meta-Learning",
      "Reinforcement Learning",
      "Evolution Strategies",
      "Reward Shaping"
    ],
    "abstract": "We introduce Synthetic Environments (SEs) and Reward Networks (RNs), represented by neural networks, as proxy environment models for training Reinforcement Learning (RL) agents. We show that an agent, after being trained exclusively on the SE, is able to solve the corresponding real environment. While an SE acts as a full proxy to a real environment by learning about its state dynamics and rewards, an RN is a partial proxy that learns to augment or replace rewards. We use bi-level optimization to evolve SEs and RNs: the inner loop trains the RL agent, and the outer loop trains the parameters of the SE / RN via an evolution strategy. We evaluate our proposed new concept on a broad range of RL algorithms and classic control environments. In a one-to-one comparison, learning an SE proxy requires more interactions with the real environment than training agents only on the real environment. However, once such an SE has been learned, we do not need any interactions with the real environment to train new agents. Moreover, the learned SE proxies allow us to train agents with fewer interactions while maintaining the original task performance. Our empirical results suggest that SEs achieve this result by learning informed representations that bias the agents towards relevant states. Moreover, we find that these proxies are robust against hyperparameter variation and can also transfer to unseen agents.",
    "one-sentence_summary": "We propose an evolution-based approach to meta-learn synthetic neural environments and reward neural networks for reinforcement learning.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "ferreira|learning_synthetic_environments_and_reward_networks_for_reinforcement_learning",
    "pdf": "/pdf/706b0f26d6790fba3e56e1ae94751cce8f4a0789.pdf",
    "_bibtex": "@inproceedings{\nferreira2022learning,\ntitle={Learning Synthetic Environments and Reward Networks for Reinforcement Learning},\nauthor={Fabio Ferreira and Thomas Nierhoff and Andreas S{\\\"a}linger and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=C1_esHN6AVn}\n}",
    "venue": "ICLR 2022 Poster",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "C1_esHN6AVn",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 26,
    "directReplyCount": 6,
    "revisions": true,
    "replies": [
      {
        "id": "6ihfnhFq4V7",
        "original": null,
        "number": 1,
        "cdate": 1635607978649,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635607978649,
        "tmdate": 1638207685536,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "C1_esHN6AVn",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces Synthetic Environments and Synthetic Rewards as a bi-level optimization problem. The main idea is to find SEs and REs such that an RL agent trained under them will do well in a given, known target environment, without further training. ",
          "main_review": "Results:\n- In Figure 2, the performance seems very unstable: In particular, the reward for individual agents seems to fluctuate widely and even the average starts drooping after the 50th iteration in the arcobot task. It would be great to know what the source of this instability is. \n\nConceptual:\n- There are repeated claims in the paper that the synthetic environment \"focuses the learning\" on the relevant states or learns \"representations\" of the target environment (\"an informed representation of the target environment by\nmodifying the state distributions to bias agents towards relevant states\"). This claim is entirely unclear to me at the moment. In particular, there simply doesn't seem to be any reason that the SE should have much in common with the ground truth environment. The only condition is that _optimal policies_ for the two environments match. In particular, there will be many environments in which the optimal policy is likely much easier to learn than in the original Env. \nFor example, you can imagine a state transition dynamic that is entirely random but provides a large reward for the optimal action in each of the given states. Another option is to learn environment dynamics where the optimal action in each state deterministically transitions to an arbitrary next state, such that the agent will see all required states during training. \nIt would be great to provide any type of evidence that there is actually semantic meaning in the SE that matches the underlying ground truth environment.  \n\nOther comments:\n- In Figure 2, it seems like the legend is switched? I.e. dashed line should be the solved threshold?",
          "summary_of_the_review": "A very interesting paper with early results in a potentially interesting direction. It is currently unclear what this method allows to do that was previously impossible from a practical point of view, but there seems to be promise. \nI'd be open to raising my score if my concerns are addressed by the authors. \n\n\n\nUpdate: \nBased on the discussion phase and the rebuttal I have updated the score to a 6. I believe this paper has enough merit to be published at ICLR, if there is space. It is an intriguing piece of work, even though the practical utility is very unclear at this point. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_ktr8"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_ktr8"
        ]
      },
      {
        "id": "s4BAoDkpYWz",
        "original": null,
        "number": 2,
        "cdate": 1635863397328,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635863397328,
        "tmdate": 1635863397328,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "C1_esHN6AVn",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper aims to learn proxy environments (synthetic environments or SEs) and reward functions (reward networks or RNs), parameterized as neural networks, such that these proxy models provide beneficial transitions to make it more sample-efficient to learn a policy for a fixed target environment (referred to as the real environment). SEs replace the observed state and rewards during training, while reward networks provide a synthetic reward that is a function of the real reward, current state, and next state. The proposed method formulates this problem as a bi-level optimization, where the inner loop consists of standard RL under the proxy model, and the outer loop consists of NES with the aim of optimizing either the performance on the true, target environment (SEs) or a the number of training steps needed to reach a certain return threshold on the real environment. ",
          "main_review": "### Strengths\n- This paper extends prior work on meta-learning MDP transition dynamics for improving training performance on a target environment, by introducing a method that aims to learn both the state and reward transitions. This formulation is more general than prior works.\n- Experimental results are provided for multiple formulations of meta-learned MDP transition dynamics, spanning both joint state and reward networks (SEs) as well as various formulations of a reward network (potential and non-potential-based reward shaping parameterizations).\n- This paper includes experiments on transferring to different agent architectures and hyperparameter settings, which seems to be a novel experimental setting for meta-learning MDP dynamics.\n\n### Weaknesses\n- The motivation for learning SEs and RNs for singleton target environments is not convincing. While there seem to be some marginal gains in sample-efficiency, it would seem these gains are largely made irrelevant due to the additional training needed to train the environment in the first place. Moreover, if SEs and RNs require first training the agent multiple times on synthetic environments, why not just copy weights or use a form of policy distillation, e.g. kickstarting (Schmitt et al, 2018), rather than train the agent again? Further, it seems the basic ICM baseline already matches or outperforms the proposed RN approaches across experimental settings, further weakening the argument for using the more computationally expensive method proposed in this work.\n- Given the wide design space of possible ways of learning the state and reward transition dynamics for a target real environment, it is unclear why only these two (SE and RN) formulations were studied. Given that this is an empirical paper, the value of the findings rests largely on a clean experimental design that provides a convincing recommendation based on comprehensive experiments. The seemingly arbitrary choice for only considering the SE and RN formulation does not provide such a convincing recommendation for using either of these formulations.\n- RNs are effectively an ablation of SEs, where the state is not learned. However, the current experimental design does not neatly present these results as a clean ablation. This is because the RN experiments also separately consider various kinds of potential and non-potential-based reward shaping parameterizations for the learned reward. These parameterizations are not studied in combination with SEs.\n- Likewise, there is no corresponding ablation for when the reward is not learned, but only the state is learned, i.e. a \"state-transition network\" or perhaps a \"state-augmentation network.\"\n- It is not made clear why exactly SEs and RNs have to use different objectives when training.\n- The RN objective seems strange, as it seems to be optimizing for sample efficiency rather than performance. In contrast the SE objective optimizes for performance. This seems like a significant inconsistency between the training methology for the two types of models, making it hard to compare the relative merits of SEs and RNs.\n- The reward difference term of the RN objective also caps the maximum performance improvement compared to training on the real environment. The motivation for this objective design is not clear, as not taking the max between the reward difference and 0 should encourage learning RNs that lead to outperforming training on the real environment, rather than simply matching it.\n- Further, it seems that there is not a consistent ranking in terms of which parameterization for the RNs improves training performance on the real environment the most, and similarly, no consistent ranking between non-transfer and transfer scenarios for each environment.\n- Meanwhile, the SE results on CartPole and Acrobot would benefit from a direct comparison to the training curves for agents using the best hyperparameters, trained on the real environment. This would directly separate out the performance gains due to training on the  SE.\n- The comparison to previous work such as Zheng et al, 2020 seems quite hand-wavey. Zheng et al, 2020 in particular is quite similar to this work in terms of analyzing the effect of learning an RN and also includes more comprehensive experimental results in a more complex setting requiring recurrent RNs. The novel contributions in relation to these prior works seems to be around analyzing the impact of varying agent hyperparameters on transferring to alternative agent settings and architectures during evaluation. However, these transfer results do not seem consistently strong for any one method proposed.\n- Sampling agent hyperparameter settings from a diverse range seems crucial for the learned SEs and RNs to transfer to other agent configurations. The paper does not discuss how sensitive the proposed methods are to the agent hyperparameter ranges used during training.",
          "summary_of_the_review": "While this work proposes an interesting generalization of prior work on meta-learning MDP dynamics, the case made for these methods is weak, and the experimental results\u2014especially for SEs\u2014are uncompelling. Given the additional compute required by these methods, it seems that various forms of policy distillation or even training from scratch seem more efficient. Further, it is not clear why only certain formulations (SEs and RNs) of meta-learning neural transition dynamics were investigated, while others (like a state-only neural transition network) were not. Given these points, I recommend this paper in its current form for rejection.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_Q4vZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_Q4vZ"
        ]
      },
      {
        "id": "GbSZnNGlnN1",
        "original": null,
        "number": 3,
        "cdate": 1635929816463,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635929816463,
        "tmdate": 1635930005695,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "C1_esHN6AVn",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The authors propose a framework for learning to synthesize a proxy models for usually non-learned components of an RL loop: namely transition dynamics and reward as defined in the interacting environment. They learn the parameters of these model by \"meta-learning\" onto the real environment with a learning agent (with slight detail variants depending on what the learning objective is), showing that not only can this setup lead to successfully learn the proxy parameters, but also improve learning performance for the agents.\n",
          "main_review": "\nFirst and foremost, let me start by saying that I *really* like this manuscript.\n\n1. The writing is extremely clear & detailed, and makes the paper a pleasure to read. I also found that the authors tended to answer questions that were popping as I was reading paragraphs immediately thereafter, which is a likely sign of writing that has gone through a lot of careful iterations.\n\n2. The authors have gone through a lot of work to justify the choice of optimization methods, how and why the framework was setup in certain ways, and how the hyperparameters for a now fairly complex learning system were chosen. Considering the code release and the extremely detailed appendices, I suspect the work will be extremely easy to reproduce. Truly commendable, and a great example of what good ML research looks like (I also particularly enjoyed the frank discussion about the work's limitation due to choosing ES as an optimization method for the outer loop).\n\n3. The central idea of the method is relatively simple, but seems to be quite effective; it honestly borrows from a lot of previous literature (reward learning, population-based optimization, etc.), whilst effectively producing a novel take on learning syntethic environment models.\n\nSo, overall, I am extremely happy to argue for acceptance as it is. Now, let's look at what I perceive to be some of its weaknesses:\n\n4. The manuscript at times feels very dense, especially when looking at methodology and experimental details, and its understanding is greatly helped from the presence of a sizable appendix. It very much feels like the authors could have a feasibly split the paper into two, one on ES and a follow up on RN, providing more space to experimentally analyse ES and RN separately and make for a cleaner and easier review process.\n\n5. Some of the hypotheses presented to justify the improved agent training feel relatively handwave-y. Consider for instance the following quote (emphasis mine):\n\n>Optimizing SEs for the maximum cumulative reward automatically yielded fast training of agents as a side-product, likely due to **efficient synthetic state dynamics**.\n\nIt is implied that this meta-learning approach leads to generally discover better data-producing dynamics for the agents used in the system, but I wonder whether this is primarily due to the structure of the tasks employed in the experimental settings (as state space and transition dynamics of these simple control problems are _generally_ not very informative, and the _useful_ -- policy learning wise -- parts of these MDP are instead both easily discoverable via search and extremely informative). \n\nIn short, I wonder if we'd see similar improvements for tasks / environments where task difficulty is extremely affected by the emerging complexity of the environment, or whether we can actually show that this is a property of SEs / RNs when trained in this manner (which would be quite outstanding!). Although that said, the effecitveness of RNs do seem to indeed point to the latter hypothesis.\n\n\n### Nits / Additional comments\n\nFigure 2: the labels are inverted (dotted lines should be the thresholds).\n\nSection 6 (but generally in many parts of the paper): it is claimed that learning rewards is generally easier than learning transition functions -- would it be possible to find a reference for this? My personal opinion is that it should indeed be often the case, but that generally it is not true (e.g. imagine an MDP with a small transition matrix but an extremely stochastic reward function).",
          "summary_of_the_review": "\nThis is a good paper. It contains excellent writing, good research, and makes for a great example of what an ICLR paper should look like. The research problem and proposed methods are interesting and well placed in the literature, and the experimental section is exhaustive.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_pG56"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_pG56"
        ]
      },
      {
        "id": "kG-za7eEsmv",
        "original": null,
        "number": 4,
        "cdate": 1635979865295,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635979865295,
        "tmdate": 1638226104943,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "C1_esHN6AVn",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes to learn neural environment models they term Synthetic Environments, such that planning in the model with model-free RL results in good performance in the real environment.\nThe authors study learning full models as well as reward-only models they call Reward Networks, investigating the feasibility of learning these models as well as their robustness to different inner-loop algorithms and hyperparameters.",
          "main_review": "This is an interesting line of work and initial investigation. The exposition of the method and description of experiments are quite clear and complete. However, there are a few weaknesses.\n\nThe motivation for this work deserves more attention. The authors suggest that SE\u2019s could be used for AutoML, iterating on algorithms in expensive environments like robotics, agent or task analysis, or agent pre-training. However, I\u2019m not sure these are particularly well aligned with the authors\u2019 statement of the problem, implementation, or experiments.\nMost applications are predicated on the relative performance of various algorithms, when trained in the SE, being correlated with performance in the true environment.\nHowever, the problem formulation in eqn (1) describes maximising performance absolutely.\nThis is approximated in Alg. 1 with a generic TrainAgent function that approximates the argmax_theta. But it seems the specific TrainAgent functions chosen are highly relevant to potential use cases \u2013 it is not a generic stand-in for an argmax.\nThe authors certainly understand the relevance of this, as they study generalisation of SE\u2019s to different algorithms and their hyperparameters, and even address generalisation algorithmically by varying hyperparameters during training of the SEs. However, this is introduced as a brief comment in the experiment descriptions, rather than being central to the method.\nIt feels that to use SE\u2019s for the potential applications discussed, it would make more sense to reformulate the problem statement to explicitly account for different inner-loop algorithms.\nFurther, simply maximising the final performance of all inner-loop algorithms seems quite limited when applications involve comparing different algorithms.\nIt might be interesting to algorithmically address this use-case by changing the fitness function to account for the relative performance of different algorithms.\nHowever, it would also be useful to address this empirically given the current formulation of the objective, and the authors likely already have the data to do so, at least partially: does the performance of different HPs/algorithms *in the SEs* correlate with their performance on the true environment?\n\nIt may also be possible to motivate the work from a more traditional model-based RL point of view, which might be more closely aligned to the problem statement given in the paper. A closely related work is that of Nikishin et al [1] (who use implicit differentiation to address the bi-level optimisation). NB I don\u2019t fault the authors for not citing this, I believe it is only available as preprint. They argue that models learned only to further optimisation of a policy (as in this work's problem statement) may perform better than traditional models when capacity is limited. However, the ES-based bi-level optimisation is expensive, so it may be difficult to make a strong case for this approach.\n\nAnother weakness of the study is the simplicity of the environments used. I understand the authors may have limited computational resources, but it is difficult to know how highly to weight an empirical feasibility study carried out on such small-scale tasks. In the absence of more compelling empirical findings, it is more critical to explore the algorithmic choices and motivations.\n\nIn this vein, I did appreciate the investigation of which inductive biases might help during reward learning. Overall, I find the case for RNs is perhaps more intuitive: dense-ifying rewards once in an expensive optimisation to allow fast iteration later. Using an additive potential form even guarantees that this will not change the optimal behaviour \u2013 sadly, it seems this type of RN did not outperform the baseline much in the experiments.\n\nIn the RN case optimising for the speed of training makes sense, but I still find the chosen objective strange. Why not optimise the area-under-curve returns for the full inner-loop? Clipping at a somewhat arbitrary solution threshold seems like it would result in suboptimal RNs (this appears to have occurred in the half-cheetah experiment in the appendix).\n\nMinor comments:\n - In the SE experiments, it would be good to show the performance of the \u2018fixed\u2019 hyperparams evaluated with the same fixed hyperparams: this would show the \u2018generalisation gap\u2019 compared to evaluating with varied hyperparms as is currently shown.\n - Overall, many figures are a bit hard to parse. Try using more smoothing or subsampling for curves. For the barplots, maybe re-arrange them so the comparable bars (i.e. measuring the same metric, \u2018steps\u2019 or \u2018episodes\u2019, are next to each other).\n\n[1] \u201cControl-Oriented Model-Based Reinforcement Learning with Implicit Differentiation\u201d Nikishin et al 2021\n\n----------------------\n\nUpdate: the authors have addressed many of my concerns, ran some additional useful experiments, and clarified their reasoning on several points in the overall discussion with reviewers. I still think there is a bit of a gap between the big-picture motivation for the work and the instantiation we see studied empirically; the authors are working on an empirical investigation to help make that connection. However, even in its absence, I am sufficiently positive about the paper overall now to lean towards acceptance -- I found the study interesting and creative.\n\n",
          "summary_of_the_review": "While the work is interesting, I find the motivation a little strained at not well aligned with the problem statement or algorithmic instantiation. The empirical study is fairly clear but limited by the simplicity of the domains and the restriction largely to assessing feasibility rather than giving clearer indications of how the method could be used in practice.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_1Zys"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_1Zys"
        ]
      },
      {
        "id": "IbGTpdvCKSl",
        "original": null,
        "number": 2,
        "cdate": 1636823566778,
        "mdate": 1636823566778,
        "ddate": null,
        "tcdate": 1636823566778,
        "tmdate": 1636823566778,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "C1_esHN6AVn",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Common Comment to Reviewers",
          "comment": "We are very grateful for the helpful feedback and would like to thank the reviewers for their time and effort, and hope that they and their loved ones are safe and healthy. We will focus on each of the reviewer\u2019s concerns individually and are looking forward to a lively discussion."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "mfINjla2QH",
        "original": null,
        "number": 3,
        "cdate": 1636823975081,
        "mdate": 1636823975081,
        "ddate": null,
        "tcdate": 1636823975081,
        "tmdate": 1636823975081,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "kG-za7eEsmv",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 1Zys (Part 1)",
          "comment": "> The motivation for this work deserves more attention. [...] Most applications are predicated on the relative performance of various algorithms, when trained in the SE, being correlated with performance in the true environment. However, the problem formulation in eqn (1) describes maximising performance absolutely. [...] Further, simply maximising the final performance of all inner-loop algorithms seems quite limited when applications involve comparing different algorithms\n\nThank you for sharing this concern. We believe that there may be a misunderstanding. To clarify, we are not exchanging RL algorithms between outer loop iterations and thus we believe the TrainAgent function is a generic stand-in for an argmax. It is only *after* we have successfully optimized for an SE by executing Alg. 1 that we compare different RL algorithms on it. As you correctly pointed out in your Paper Summary, our work revolves around finding the SE models and in post-hoc evaluations we analyse transfer to different RL algorithms and their sensitivity to agent hyperparameter changes. Since these changing factors are only considered in the evaluation phase, we believe changing the problem formulation to account for these is not needed. However, changing the RL algorithms between outer loop iterations (also in addition to using pre-trained agents from previous outer loop iterations) is a very interesting idea that we already have on our future-work-stack. \n\nDid this address your concern and did we understand you correctly? We hope we were able to give some clarification on our motivation and experiment design and are happy to discuss further suggestions.\n\n\\\n&nbsp;\n\n> The authors certainly understand the relevance of this, as they study generalisation of SE\u2019s to different algorithms and their hyperparameters, and even address generalisation algorithmically by varying hyperparameters during training of the SEs. However, this is introduced as a brief comment in the experiment descriptions, rather than being central to the method.\n\nThank you for pointing this out. We have integrated this information into the method section (Section 3; \u201cAgents, Hyperparameters, and Sampling\u201d) and updated the manuscript accordingly (changes highlighted in red color).\n\n\\\n&nbsp;\n\n> Further, simply maximising the final performance of all inner-loop algorithms seems quite limited when applications involve comparing different algorithms. It might be interesting to algorithmically address this use-case by changing the fitness function to account for the relative performance of different algorithms.\n\nThank you for sharing this idea with us. We believe this proposed new objective is interesting to study and may improve transferability to unseen agents. We will take this into consideration for our future work on learning SEs.\n\n\\\n&nbsp;\n> However, it would also be useful to address this empirically given the current formulation of the objective, and the authors likely already have the data to do so, at least partially: does the performance of different HPs/algorithms in the SEs correlate with their performance on the true environment?\n\nWe thank you for this interesting idea. Unfortunately, we do not have this data yet. We are currently preparing multiple experiments and will post an update here with the results once we have them.\n\n\n\\\n&nbsp;\n> It may also be possible to motivate the work from a more traditional model-based RL point of view, which might be more closely aligned to the problem statement given in the paper. A closely related work is that of Nikishin et al [1] (who use implicit differentiation to address the bi-level optimisation). NB I don\u2019t fault the authors for not citing this, I believe it is only available as preprint. They argue that models learned only to further optimisation of a policy (as in this work's problem statement) may perform better than traditional models when capacity is limited. However, the ES-based bi-level optimisation is expensive, so it may be difficult to make a strong case for this approach.\n\nThank you for pointing out this related work and the suggestion to motivate our work through the lens of model-based RL (MBRL). Here, we would kindly point out that we provide an extensive discussion on the differences between our work and model-based RL in Appendix D. In summary, we decided against motivating our work based on MBRL because we see too many dissimilarities between both paradigms. As to the work of Nikishin et al. [1], we believe adopting ideas such as implicit differentiation from this paper is very valuable and provides interesting ideas for future work on gradient-based optimization for SEs. \n\nRegarding the SE model capacity, we have observed during hyperparameter optimization of the SE architecture, that its capacity did not seem to play a significant role. SE models seem to further the optimization of a policy irrelevant of whether the capacity was small or large."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "eVsuRF6YQTa",
        "original": null,
        "number": 4,
        "cdate": 1636824183786,
        "mdate": 1636824183786,
        "ddate": null,
        "tcdate": 1636824183786,
        "tmdate": 1636824183786,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "kG-za7eEsmv",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 1Zys (Part 2)",
          "comment": "> [Simplicity of the environments used]. \n\nThank you for sharing this concern. We would like to emphasize that we not only propose a new method for learning environments but that we also propose the concept of creating SEs in the first place, and because of that, starting with simple environments is beneficial for several reasons, e.g.:\n* they allow for interpretability, \n* they are well-understood, \n* they are simple enough to be reproducible in the community without requiring large computational resources, and \n* they significantly reduce evaluation time. \n\nHaving said that, we do agree that scalability to complex environments is an important issue that needs to be further investigated. In terms of SEs, we would like to highlight that we already chose environments with continuous state spaces where most of the related work uses simpler, discrete state spaces. For RNs, we would like to point out that we do already present results for the complex HalfCheetah environment. \nWhat is missing in our view is a complex environment, such as HalfCheetah, for SEs. Despite our limited compute resources, we are preparing the launch of experiments for training SEs on HalfCheetah, and given that the compute resources we get are sufficient, we are hopeful that we can present results at the end of the rebuttal period.\n\nFinally, we would like you to consider that most of the related work focussed on similarly or even less complex environments, such as grid worlds. For this reason, we believe that our empirical findings are very compelling. Not only do we show the feasibility of our method, but also report valuable insights into the learned SEs in terms of efficiency, agent transfer and an analysis of the distribution of synthetic states and rewards.\n\nWe hope we were able to address your concern and we would be very happy to discuss more about this point.\n\n\\\n&nbsp;\n\n> In the RN case optimising for the speed of training makes sense, but I still find the chosen objective strange. Why not optimise the area-under-curve returns for the full inner-loop? Clipping at a somewhat arbitrary solution threshold seems like it would result in suboptimal RNs (this appears to have occurred in the half-cheetah experiment in the appendix).\n\nThank you for opening the discussion about the RN objective. Regarding the arbitrary solution thresholds, we would like to point out that the solved-reward thresholds were not arbitrarily chosen but taken from the widely adopted Gym competition leaderboard setups (reported here: https://github.com/openai/gym/wiki/Table-of-environments)\n\nIn terms of the objective, we agree with you that this detail may be confusing to readers. Frankly speaking, the choice of this objective is not greatly motivated. We implemented this objective after we noticed the efficiency improvements from SEs. To test whether RNs could also yield such efficiency improvements, we chose to directly optimize for efficiency. Another reason for optimizing for efficiency directly was that the real environment is involved in the RN case and we pursued the goal to reduce real environment interactions. \n\nNevertheless, we now conducted a small study to contrast different objectives in the RN case and made the results available here (results reported across 6 pages): \n\nhttps://docs.google.com/document/d/1LLshom6G8gr-Si6-RKPYfnhHM7BYrCeKhkwEJn2BTrk/edit?usp=sharing\n\n\nIn these results, next to the area-under-the-curve (AUC) objective you mentioned, we report results for the RN objective that was used in the paper (denoted as \u201cReward Threshold\u201d), as well as for the reward maximization objective (denoted as \u201cmax reward\u201d). The overall conclusion from these results is that an AUC objective generally yields similar results as for the reward threshold objective, but in some cases it seems to reduce efficiency a bit. However, the overall significant efficiency gains remain. Moreover, in contrast to SEs, efficiency improvements in the RN case seem to arise more often when explicitly optimized for it (see Figures under \u201cAUC\u201d or \u201creward threshold\u201d) while maximizing reward (only tested for CartPole and Cliff environments) leads to efficiency improvements only in the CartPole but not in the Cliff environment.\n\nDid we address this concern appropriately? Again, we would be happy to discuss further; we also plan to include the results of the above objective study in the manuscript."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "bZfNCXs2tHm",
        "original": null,
        "number": 5,
        "cdate": 1636824208621,
        "mdate": 1636824208621,
        "ddate": null,
        "tcdate": 1636824208621,
        "tmdate": 1636824208621,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "kG-za7eEsmv",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 1Zys (Part 3)",
          "comment": "> Minor comments:\n> * In the SE experiments, it would be good to show the performance of the \u2018fixed\u2019 hyperparams evaluated with the same fixed hyperparams: this would show the \u2018generalisation gap\u2019 compared to evaluating with varied hyperparms as is currently shown.\n> * Overall, many figures are a bit hard to parse. Try using more smoothing or subsampling for curves. For the barplots, maybe re-arrange them so the comparable bars (i.e. measuring the same metric, \u2018steps\u2019 or \u2018episodes\u2019, are next to each other).\n\n\nThank you for pointing these out. Regarding the generalization gap, we will launch an experiment and report back once we have the results for it. With reference to the suggestion of improving the curve plots, we will also look into that. Regarding the barplots, we will test out your suggestion for re-arranging the bars and report back to you."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "3uZZLn5OXu",
        "original": null,
        "number": 6,
        "cdate": 1636824445174,
        "mdate": 1636824445174,
        "ddate": null,
        "tcdate": 1636824445174,
        "tmdate": 1636824445174,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "GbSZnNGlnN1",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer pG56 (Part 1)",
          "comment": "> First and foremost, let me start by saying that I really like this manuscript.\n\nWe are very grateful for your feedback and your encouraging words. Thank you! We will not repeat the parts you liked but focus on parts you mention could be improved. \n\n\\\n&nbsp;\n\n> The manuscript at times feels very dense, especially when looking at methodology and experimental details, and its understanding is greatly helped from the presence of a sizable appendix. It very much feels like the authors could have a feasibly split the paper into two, one on ES and a follow up on RN, providing more space to experimentally analyse ES and RN separately and make for a cleaner and easier review process.\n\nWe are very grateful for your helpful feedback. We would like to take your suggestion to heart and try to reduce the overall density in the mentioned sections. In order to do this effectively, we would welcome concrete suggestions; in our perspective, splitting the paper into two, one on SEs and one on RNs, is not a preferable option since our conclusions and claims are often derived and justified from observations made on both SE and RN. Also, while we are very grateful that you feel there is enough material in the paper for two papers, other reviewers are rather asking for more results not less.\n\n\\\n&nbsp;\n\n> Some of the hypotheses presented to justify the improved agent training feel relatively handwave-y. Consider for instance the following quote (emphasis mine):\n\u201cOptimizing SEs for the maximum cumulative reward automatically yielded fast training of agents as a side-product, likely due to efficient synthetic state dynamics.\u201d\n\n> It is implied that this meta-learning approach leads to generally discover better data-producing dynamics for the agents used in the system, but I wonder whether this is primarily due to the structure of the tasks employed in the experimental settings (as state space and transition dynamics of these simple control problems are generally not very informative, and the useful -- policy learning wise -- parts of these MDP are instead both easily discoverable via search and extremely informative).\n\n> In short, I wonder if we'd see similar improvements for tasks / environments where task difficulty is extremely affected by the emerging complexity of the environment, or whether we can actually show that this is a property of SEs / RNs when trained in this manner (which would be quite outstanding!). Although that said, the effecitveness of RNs do seem to indeed point to the latter hypothesis.\n\nThank you for our feedback. We are uncertain whether we understand your point fully, especially with regard to the property of being \u201cinformative\u201d. We believe the state space and transition dynamics of CartPole and Acrobot are informative in both a meta-learning and non-meta-learning setting as they can be purely described deterministically and analytically (e.g., do not rely on potentially approximate collision detection or contact simulation) and are bounded by clearly defined and deterministic termination and reward criteria (i.e. no stochasticity involved).\n\nIf the assumption is that the beneficial data-producing dynamics arise from the simplicity of the used control problems and not from our meta-learning formulation, we argue that we would see similar benefits in a (non-meta-learning) supervised-learning or model-based RL setup. Fortunately, we have conducted an extensive experiment to study exactly this question. For the results, we kindly refer to Appendix D. Overall, we conclude from this study that the efficiency is very likely to arise from the meta-learning scheme and is challenging to reproduce through purely supervised-learning despite the simplicity of the task. As you pointed out, we think the RN results back up this claim as well.\n\nWith this in mind, we nevertheless see high-complexity environments as an important step to justify our claim further. Despite our limited compute resources, we are preparing the launch of experiments on the complex HalfCheetah environment and given that these compute resources are sufficient, we are hopeful that we can present results at the end of the rebuttal period.\n\nPlease let us know if we understood / addressed your point correctly and also if we should rephrase or pull relevant information from the appendix into the main manuscript to make this point clearer.\n\n\\\n&nbsp;\n\n> Nits / Additional comments\n> Figure 2: the labels are inverted (dotted lines should be the thresholds).\n\nThank you very much for pointing this out. We have fixed the labels and updated the manuscript."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "ErKRc2Qx2yG",
        "original": null,
        "number": 7,
        "cdate": 1636824476858,
        "mdate": 1636824476858,
        "ddate": null,
        "tcdate": 1636824476858,
        "tmdate": 1636824476858,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "GbSZnNGlnN1",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer pG56 (Part 2)",
          "comment": "> Section 6 (but generally in many parts of the paper): it is claimed that learning rewards is generally easier than learning transition functions -- would it be possible to find a reference for this? My personal opinion is that it should indeed be often the case, but that generally it is not true (e.g. imagine an MDP with a small transition matrix but an extremely stochastic reward function).\n\nUnfortunately, we could not find such a reference but of course would be very happy to include it in the manuscript if any of the reviewers happen to know of one.\n\nWe completely agree with you that one can construct counter-examples where reward learning is much more difficult than state learning. However, in practice, for instance, in scenarios where complex physical laws are simulated, it is generally much more difficult to come up with such counter-examples because the simulated dynamics are non-trivial. Moreover, when considering control or robotics applications, dynamics are often given and cannot be modified but rewards usually can be adapted to solve a problem.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "7LnUREB0BS",
        "original": null,
        "number": 8,
        "cdate": 1636824695250,
        "mdate": 1636824695250,
        "ddate": null,
        "tcdate": 1636824695250,
        "tmdate": 1636824695250,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "s4BAoDkpYWz",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer Q4vZ (Part 1)",
          "comment": "> This paper extends prior work on meta-learning MDP transition dynamics for improving training performance on a target environment, by introducing a method that aims to learn both the state and reward transitions. This formulation is more general than prior works.\n\nWe are thankful for your comment. To better understand your perspective with regards to the below mentioned weaknesses, we would kindly ask you for references on prior work on meta-learning MDP transition dynamics that you would say our work extends. From our point of view, we are the first that explore meta-learning neural networks as proxies for transition dynamics of a target environment. We elaborated on this point extensively in the related work section and we would therefore greatly appreciate any pointers to prior work that we might have missed. \n\n\\\n&nbsp;\n\n> This paper includes experiments on transferring to different agent architectures and hyperparameter settings, which seems to be a novel experimental setting for meta-learning MDP dynamics.\n\nThank you for acknowledging our experimental contribution. To avoid confusion, we would like to kindly emphasize that, instead of studying the transfer to agent architectures, we studied the transfer to multiple agent algorithms.\n\n\\\n&nbsp;\n\n> The motivation for learning SEs and RNs for singleton target environments is not convincing. While there seem to be some marginal gains in sample-efficiency, it would seem these gains are largely made irrelevant due to the additional training needed to train the environment in the first place. Moreover, if SEs and RNs require first training the agent multiple times on synthetic environments, why not just copy weights or use a form of policy distillation, e.g. kickstarting (Schmitt et al, 2018), rather than train the agent again? \n\nThank you for communicating your concerns with us. While we agree that learning SEs is expensive, we respectfully disagree that improvements of 30-65% can be deemed marginal. More important to point out is that these gains are achieved without explicitly optimizing for it, which, in our opinion, is a very interesting finding. Moreover, we believe that focussing mostly on the inefficiency of our method does our work a disservice since it disregards the many other gains, for example, its ability to transfer to other agents and to train these efficiently or the possibility to do task analysis (Analyzing SE Behavior, Section 5.1) and interpretation. We did not intend to make a claim of end-to-end speedups; rather, we find it fascinating that it is possible to learn synthetic environments *at all* that allow for faster training, even of other agents. That finding is indeed one of the key contributions of our paper, not the particular (granted, quite costly) evolutionary algorithm for doing so. \n\nLastly, while we agree that loading agent weights from earlier outer loop iterations *could* make the optimization more efficient, this falls into the category of a speedup technique for the fundamentally novel task we\u2019re proposing in this paper. While this may indeed lead to speedups, we believe that in the first paper on SEs we should rather scientifically study SEs (like in our analysis) without directly introducing confounding factors due to a speedup technique. In particular, it is unclear to us a priori how the proposed speedup technique would affect the teaching capacity of SE models. For example, the efficiency gains could be lost since the SE may not learn to properly teach agents from scratch. As mentioned above in our response to Reviewer 1Zys, we do, however, believe that loading pre-trained weights is a good idea and we are considering it for future work on learning SEs.\n\nAll in all, while we share many of your concerns, we would appreciate it if we could work jointly together to improve the motivation and communication of our conclusions to make them clearer for you.\n\n\\\n&nbsp;\n\n> Further, it seems the basic ICM baseline already matches or outperforms the proposed RN approaches across experimental settings, further weakening the argument for using the more computationally expensive method proposed in this work.\n\nWe respectfully, but firmly, disagree here. The ICM baseline does not outperform the RNs. More specifically, in terms of 1) efficiency, the ICM never outperforms any RN variants and in terms of 2) final performance, the ICM only outperforms the RNs for one case (the Cliff environment in the varied hyperparameter setting, see Fig. 5)."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "0OpUsbtxmz3",
        "original": null,
        "number": 9,
        "cdate": 1636824861996,
        "mdate": 1636824861996,
        "ddate": null,
        "tcdate": 1636824861996,
        "tmdate": 1636824861996,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "s4BAoDkpYWz",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer Q4vZ (Part 2)",
          "comment": "> Given the wide design space of possible ways of learning the state and reward transition dynamics for a target real environment, it is unclear why only these two (SE and RN) formulations were studied. Given that this is an empirical paper, the value of the findings rests largely on a clean experimental design that provides a convincing recommendation based on comprehensive experiments. The seemingly arbitrary choice for only considering the SE and RN formulation does not provide such a convincing recommendation for using either of these formulations.\n\nWe welcome your comments on a potential downside in our experiment design. Overall, we think there are three choices of formulations to be considered. One can model the reward dynamics, the state dynamics or both. In our paper, we chose the former and latter which, in our point of view, does not constitute an arbitrary, but a logical choice. We are a bit puzzled by the statement about our experiment design and would appreciate it if you could make this more concrete. We would be very happy to engage in a discussion about any formulation that you would find more interesting to study and that would lead to a better experiment design. \n\n\\\n&nbsp;\n\n> RNs are effectively an ablation of SEs, where the state is not learned. However, the current experimental design does not neatly present these results as a clean ablation. This is because the RN experiments also separately consider various kinds of potential and non-potential-based reward shaping parameterizations for the learned reward. These parameterizations are not studied in combination with SEs.\n\nWe find your view point of RNs being an ablation of SEs very interesting and thank you for this input. While it is an interesting idea to present RNs as an ablation of SEs, we are unsure why phrasing RNs as a subset of the broader formulation of SEs would constitute a weakness. We would be happy to engage in a discussion with you about this.\n\nIn terms of the different parameterizations not being studied in the SE case, we believe there may be a misunderstanding. In particular, we want to point out that in the SE formulation we do not want to depend on rewards from the real environment but rather have them artificially generated. Consequently, by adopting the RN parameterization variants from Table 1 to the case of SEs, one would effectively end up only with the two variants denoted as \u201cExclusive\u201d to test for the SEs which do not depend on the real reward. However, the variant denoted as \u201cExclusive Non-potential RN\u201d from these two variants is essentially the SE case (with the addition that we ask the SE to produce artificial states along the artificial rewards). So this can actually be seen as an ablation of SEs without the dynamics component. The other variant we are left with is \u201cExclusive Potential RN\u201d. We acknowledge that testing this parameterization is indeed interesting in the SE case but we also point out that it does not fully follow the potential-based reward shaping theory as it does not augment a reward but produces a fully artificial one. This is the reason why we did not pursue this direction of research when studying SEs. Another reason why we did not investigate PBRS for SEs is that the theory usually provides guarantees only for reward augmentations and not artificial states. \n\nWe hope we were able to address your concerns and are open to engaging in a discussion on this.\n\n\\\n&nbsp;\n\n> Likewise, there is no corresponding ablation for when the reward is not learned, but only the state is learned, i.e. a \"state-transition network\" or perhaps a \"state-augmentation network.\"\n\nThank you for this suggestion. We actually tried this, but since in this case the state transitions differ, the original reward signal does not capture any meaning, and the reward rather would need to be adapted to the different state transitions (as in full SEs)."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "hATN8m66oBT",
        "original": null,
        "number": 10,
        "cdate": 1636824986663,
        "mdate": 1636824986663,
        "ddate": null,
        "tcdate": 1636824986663,
        "tmdate": 1636824986663,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "s4BAoDkpYWz",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer Q4vZ (Part 3)",
          "comment": "> It is not made clear why exactly SEs and RNs have to use different objectives when training. \n\n> The RN objective seems strange, as it seems to be optimizing for sample efficiency rather than performance. In contrast the SE objective optimizes for performance. This seems like a significant inconsistency between the training methology for the two types of models, making it hard to compare the relative merits of SEs and RNs.\n\n> The reward difference term of the RN objective also caps the maximum performance improvement compared to training on the real environment. The motivation for this objective design is not clear, as not taking the max between the reward difference and 0 should encourage learning RNs that lead to outperforming training on the real environment, rather than simply matching it.\n\n\nThank you for opening the discussion about the RN objective. We agree with you that this detail may be confusing to readers. Frankly speaking, the choice of this objective is not greatly motivated. We implemented this objective after we noticed the efficiency improvements from SEs. To test whether RNs could also yield such efficiency improvements, we chose to directly optimize for efficiency. Another reason for optimizing for efficiency directly was that the real environment is involved in the RN case and we pursued the goal to reduce real environment interactions. \n\nNevertheless, we now conducted a small study to contrast different objectives in the RN case and made the results available here: \n\nhttps://docs.google.com/document/d/1LLshom6G8gr-Si6-RKPYfnhHM7BYrCeKhkwEJn2BTrk/edit?usp=sharing\n\nThe overall conclusion from these results is that an AUC objective generally yields similar results as for the reward threshold objective, but in some cases it seems to reduce efficiency a bit. However, the overall significant efficiency gains remain. Moreover, in contrast to SEs, efficiency improvements in the RN case seem to arise more often when explicitly optimized for it (see Figures under \u201cAUC\u201d or \u201creward threshold\u201d) while maximizing reward (only tested for CartPole and Cliff environments) leads to efficiency improvements only in the CartPole but not in the Cliff environment\n\nWe hope by providing these results, we are able to convince you that a relative comparison between SEs and RNs is still possible. Did we address your concerns appropriately? Again, we would be happy to engage in a discussion with you.\n\n\\\n&nbsp;\n\n> Further, it seems that there is not a consistent ranking in terms of which parameterization for the RNs improves training performance on the real environment the most, and similarly, no consistent ranking between non-transfer and transfer scenarios for each environment.\n\nThis observation is correct. We share and describe it in our paper. However, it is unclear to us how this is a weakness of our work. We believe this is certainly something to study in more detail in the future.\n\n\\\n&nbsp;\n\n> Meanwhile, the SE results on CartPole and Acrobot would benefit from a direct comparison to the training curves for agents using the best hyperparameters, trained on the real environment. This would directly separate out the performance gains due to training on the SE.\n\nThank you for your comment. We believe the choice of hyperparameters is irrelevant as long as the identical hyperparameter configuration is used on the SE and the real environment to enable a fair comparison. In our case, we chose the default ones from the original papers. In fact, we have executed the SE performance experiment with optimal agent hyperparameters in both cases, i.e. agents trained on SEs and on the baselines and the results were identical, meaning that the same performance gaps were observable.\n\n\\\n&nbsp;\n\n> The comparison to previous work such as Zheng et al, 2020 seems quite hand-wavey. Zheng et al, 2020 in particular is quite similar to this work in terms of analyzing the effect of learning an RN and also includes more comprehensive experimental results in a more complex setting requiring recurrent RNs. \n\nWe again respectfully, but firmly, disagree here. As described in our related work section, Zheng et al. explore learning reward networks for gridworld-like environments only. Thus, we believe our continuous-control environments constitute a much more complex setting. More importantly, however, is that Zheng et al. focus on learning a reward as a distribution over tasks and in contrast to our work not on a single task. \n\nMoreover, their method facilitates exploration across episodes by taking into account the entire lifetime of an agent with the goal to distill knowledge about long-term exploration and exploitation into a reward function. Therefore, they not only consider a different objective but also their goal is different to our paper. All in all, we think that their work studies entirely different research questions.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "vYYb9BXEqnw",
        "original": null,
        "number": 11,
        "cdate": 1636825091748,
        "mdate": 1636825091748,
        "ddate": null,
        "tcdate": 1636825091748,
        "tmdate": 1636825091748,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "s4BAoDkpYWz",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer Q4vZ (Part 4)",
          "comment": "> The novel contributions in relation to these prior works seems to be around analyzing the impact of varying agent hyperparameters on transferring to alternative agent settings and architectures during evaluation. However, these transfer results do not seem consistently strong for any one method proposed.\n\nAs mentioned above, we are convinced that the novel contributions in relation to Zheng et al. revolve around: 1) learning state dynamics (SEs), 2) using more complex environments in the RN case, 3) a much richer analysis of agent transfer (SEs and RNs), and 4) showing robustness to hyperparameter variations (SEs and RNs). We agree with you that the transfer performance is only high within an agent family but we still see this particular result as intriguing. In particular, we want to mention that we did not apply any \u201ctricks\u201d nor did we put any effort into improving transfer performance but our results nevertheless indicate that transfer is possible.\n\n\\\n&nbsp;\n\n> Sampling agent hyperparameter settings from a diverse range seems crucial for the learned SEs and RNs to transfer to other agent configurations. The paper does not discuss how sensitive the proposed methods are to the agent hyperparameter ranges used during training.\n\nThank you for sharing this concern with us. We believe there may be an overestimation of the influence of varying the hyperparameters during the learning of SEs and RNs. First, one can see in Fig. 3 that the difference between varying hyperparameters and not varying hyperparameters during SE training is low (compare \u201ctrain: synth. HPs: varied\u201d with \u201ctrain: synth. HPs: fixed\u201d). Second, in the transfer case of teaching entirely new agents, we want to note that we evaluate the transferability of RN and SEs under very challenging conditions since we use completely unseen agents that also come from entirely different RL agent families (e.g. off-policy to on-policy or q-learning to actor-critic). Here, we strongly doubt that making the hyperparameter range more diverse during SE and RN training will adequately \u201cprepare\u201d an SE or RN model to transfer to an entirely different agent type. \n\nMoreover, we discuss the sensitivity of hyperparameter variation during SE/RN training repeatedly in our paper. Not only do we analyse the behavior of SE learning without varying the agent hyperparameters in Fig. 3, we also discuss this in several parts of the paper. Here are a few excerpts:\n* \u201cTraining DDQN agents on DDQN-trained SEs without varying the agent\u2019s HPs (green) during SE training clearly yielded the worst results. We attribute this to overfitting of the SEs to the specific agent HPs. Instead, when varying the agent HPs (orange) during SE training\u201d (Section 5.1; Evaluating the Performance of SE-trained Agents)\n* \u201cAs before, not varying the DDQN agent HPs during SE training reduces transfer performance (green).\u201d (Section 5.1; Transferability of SEs)\n* \u201cContrary to training SEs, we do not vary the agent HPs during RN training as we did not observe improvements in doing so.\u201d (end of Section 4)\n* \u201cLike in the CartPole case, we show three different SE training settings: agents trained on real env. with varying agent HPs (blue), on DDQN-trained SEs when varying agent HPs during NES runs (orange), on DDQN-trained SEs where the agent HPs were fixed during training of the SEs (green).\u201d (Appendix A.3)\n\nLastly, the choice of the diversity of our hyperparameter configurations is easily explained: the chosen lower value range for the sampled hyperparameters (Table 2) is simply equivalent to the default lower bound divided by three and similarly for the higher value range (multiplied by three).\n\nPlease let us know if the presentation of this information needs to be changed to be more accessible to a reader. We hope we were able to address your concerns appropriately and would be grateful to engage in a discussion with you about any remaining concerns. An indication about what it would take to convince you of our work\u2019s merit would also be highly appreciated.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "pL-K7K2fr2",
        "original": null,
        "number": 12,
        "cdate": 1636825202863,
        "mdate": 1636825202863,
        "ddate": null,
        "tcdate": 1636825202863,
        "tmdate": 1636825202863,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "6ihfnhFq4V7",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer ktr8",
          "comment": "> In Figure 2, the performance seems very unstable: In particular, the reward for individual agents seems to fluctuate widely and even the average starts drooping after the 50th iteration in the arcobot task. It would be great to know what the source of this instability is.\n\nThank you very much for pointing out this concern. We are sorry for not having discussed the reason for this in the main paper. Due to the space limitations we only addressed this in the appendix (Appendix A.2), where we point out that the variance arises from the stochasticity of natural gradients and from the sensitivity of RL agents to seeds and parameter initializations. If you think we should move this information to the main manuscript we would be very happy to do so. \n\n\\\n&nbsp;\n\n> There are repeated claims in the paper that the synthetic environment \"focuses the learning\" on the relevant states or learns \"representations\" of the target environment (\"an informed representation of the target environment by modifying the state distributions to bias agents towards relevant states\"). This claim is entirely unclear to me at the moment. In particular, there simply doesn't seem to be any reason that the SE should have much in common with the ground truth environment. The only condition is that optimal policies for the two environments match. In particular, there will be many environments in which the optimal policy is likely much easier to learn than in the original Env. For example, you can imagine a state transition dynamic that is entirely random but provides a large reward for the optimal action in each of the given states. Another option is to learn environment dynamics where the optimal action in each state deterministically transitions to an arbitrary next state, such that the agent will see all required states during training. It would be great to provide any type of evidence that there is actually semantic meaning in the SE that matches the underlying ground truth environment.\n\nWe are sorry to read that our made claims remain unclear to you. We thank you for sharing these concerns with us which we would like to address now. Most importantly, we want to emphasize that our optimization scheme does not bias the SEs to entail the true dynamics or reward or even have semantic meaning in accordance with the physical laws prevalent in the ground truth environment. It is because of this, that we believe that in our experiment results we do indeed not see evidence for semantic meaning that matches the ground truth environment. \n\nHere, we would also kindly refer to Appendix D where we study the case when training SEs with the objective of imitating the ground truth environment in a supervised learning (or model-based RL) setup. We believe the discussion given there may exactly address your doubts and clarify your concerns. Please let us know if anything else is unclear and requires further details or discussion. We would also be very happy to move missing information from the appendix to the main manuscript if you believe it would increase general understanding of what SEs learn.\n\n\\\n&nbsp;\n\n> In Figure 2, it seems like the legend is switched? I.e. dashed line should be the solved threshold?\n\nThank you for this remark. Indeed, the legend entries were switched; we have corrected this and updated the paper.\n\nIf we addressed your concerns, would you be kind enough to raise your score?\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "DShwhBSh0A",
        "original": null,
        "number": 13,
        "cdate": 1637077550146,
        "mdate": 1637077550146,
        "ddate": null,
        "tcdate": 1637077550146,
        "tmdate": 1637077550146,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "kG-za7eEsmv",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Experiment Update for Reviewer 1Zys (Generalization Gap)",
          "comment": "Dear reviewer,\n\nWe would like to report the CartPole results of the experiment you suggested with regards to the generalization gap and are happy to discuss them here. The results can be accessed in the following Google Doc: https://docs.google.com/document/d/1SItWlHtV4qGi6JfuuHnGJpi0aY_oI2mlld7qmDBuOAM/edit?usp=sharing\n\nWe used the fixed agent hyperparameters (reported in Appendix, Table 3) that were found for the SE optimization and repeated the evaluation procedure reported in the paper (Section 5.1, \u201cEvaluating the Performance of SE-trained Agents\u201d) with those fixed hyperparameters. \n\nOne can see that these agent hyperparameters lead to high scores under the real environment baseline and lower performance on the SEs when comparing them against the results in the paper. We believe the reason for high scores in the real environment stems from the fact that we executed our BOHB hyperparameter optimization with the goal that agents, in the end, would perform well in the real environment (after being trained on an SE). However, these hyperparameters seem to also result in needing more episodes than the default (sampled) hyperparameters. This may stem from the lower learning rates in the fixed HP case compared to sampled learning rates from Table 1.\n\nAlso, again we can observe that performance is higher when training agents on the SEs optimized with varying hyperparameters. We think this hyperparameter variation during training regularizes the SEs for higher generalization which might be beneficial on CartPole (for example, by coping better with the random initial start position of the pole). Overall, our results indicate a noticeable generalization gap of an average reward of ~50 when comparing agents trained on the SEs with the fixed and the varying hyperparameters.\n\nDo you feel we have addressed your experiment suggestion sufficiently enough?\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "PP-DjeuDoir",
        "original": null,
        "number": 17,
        "cdate": 1637418940481,
        "mdate": 1637418940481,
        "ddate": null,
        "tcdate": 1637418940481,
        "tmdate": 1637418940481,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "kG-za7eEsmv",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Performance Correlation Experiment",
          "comment": "> Reviewer 1Zys: However, it would also be useful to address this empirically given the current formulation of the objective, and the authors likely already have the data to do so, at least partially: does the performance of different HPs/algorithms in the SEs correlate with their performance on the true environment?\n\n> Paper Authors: We thank you for this interesting idea. Unfortunately, we do not have this data yet. We are currently preparing multiple experiments and will post an update here with the results once we have them. \n\nDear reviewer,\n\nWe would like to quickly give a status update on the correlation experiment that you suggested. We have launched the experiment runs for DDQN and DuelingDDQN and are hopeful that we can evaluate and post the results by tomorrow (Sunday).\n\nAlso, did you already find time to look at the generalization gap experiment results that we posted on Nov 16 (see below)? We would be very happy to receive feedback and would be looking forward to a fruitful discussion about it.\n\nThank you.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "iCiKEfax3BC",
        "original": null,
        "number": 20,
        "cdate": 1637689385907,
        "mdate": 1637689385907,
        "ddate": null,
        "tcdate": 1637689385907,
        "tmdate": 1637689385907,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "DShwhBSh0A",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Thanks",
          "comment": "Appreciate this additional experiment. I guess it would probably be even more informative if the fixed hyperparams were better-performing in this setting, but I understand the motivation for choosing the fixed HPs that came from your optimization procedure.\n\nAlthough the signal is a bit weak as a result, it at least indicates a bit more clearly what's going on in this case!"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_1Zys"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_1Zys"
        ]
      },
      {
        "id": "jL5syahnP0",
        "original": null,
        "number": 21,
        "cdate": 1637689868029,
        "mdate": 1637689868029,
        "ddate": null,
        "tcdate": 1637689868029,
        "tmdate": 1637689868029,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "eVsuRF6YQTa",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Thanks",
          "comment": "Regarding simplicity of environments; I do think there is information to take away from experiments on these environments. I disagree that discrete gridworlds are necessarily any simpler than simple continuous environments. But, I regard this factor as a minor issue overall; and believe there is a good quality paper to be written within the computational budget you have available.\n\nRegarding the RN objective: I'm aware the solution thresholds are established in gym, but still find them quite arbitrary regardless (of course, someone else's arbitrary decision is better than yours for unbiased research, but still can be unsatisfactory). So thank you for investigating this suggestion more thoroughly.\n\nCorrect me if I'm interpreting the results incorrectly, but it seems that in particular the non-potential RNs are working better on HalfCheetah with the AUC objective? This would make sense to me since the non-potential RNs would be the ones most affected by a thresholded objective, and the more complex Cheetah may be the most sensitive to this factor.\n\nRegardless, I believe this is a useful addition to the paper and will make the discussion of RNs feel a bit better connected to the overall motivation (by not tying them so closely to an objective which is so different from the SE one). This may also be of interest to reviewer Q4vZ."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_1Zys"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_1Zys"
        ]
      },
      {
        "id": "46HgNUPsMir",
        "original": null,
        "number": 22,
        "cdate": 1637690098309,
        "mdate": 1637690098309,
        "ddate": null,
        "tcdate": 1637690098309,
        "tmdate": 1637690098309,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "mfINjla2QH",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Appreciate discussion, clarification of concern.",
          "comment": "Clarification: I understand that you are not changing RL algorithms between outer-loop iterations. However, I think that not doing so undermines the motivations that you give for the work, some of which center on comparing different algorithms' performance in the SE, post-hoc. It's unclear why post-hoc performance of various algorithms would correspond to real-env performance, since only a single algorithm is used while learning the SE/RN. My challenge here is not to either the motivation per se, or the implementation, but rather the connection between them.\n\nI think the empirical study of performance correlations could go some way to bridging this gap.\n\nThank you for your discussion of the other points and pointing me to the relevant part of the Appendix."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_1Zys"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_1Zys"
        ]
      },
      {
        "id": "nxPf3dUIMoq",
        "original": null,
        "number": 23,
        "cdate": 1637690620781,
        "mdate": 1637690620781,
        "ddate": null,
        "tcdate": 1637690620781,
        "tmdate": 1637690620781,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "PP-DjeuDoir",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Appreciate followup, considering overall review",
          "comment": "I'd like to thank the authors for their engagement with the reviews. The additional results and clarifications definitely alleviate some of my concerns. I am also considering the points raised in the other reviews, and will likely amend my score shortly.\n\nI'd also like to echo another reviewer's comment about density and volume of information, and encourage the authors to think about streamlining the presentation further; perhaps aggregating results together or de-emphasising less-informative experiments to help the empirical section have clearer take-aways where possible."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_1Zys"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_1Zys"
        ]
      },
      {
        "id": "F0hE26NdwPl",
        "original": null,
        "number": 24,
        "cdate": 1637794239819,
        "mdate": 1637794239819,
        "ddate": null,
        "tcdate": 1637794239819,
        "tmdate": 1637794239819,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "vYYb9BXEqnw",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Thanks for the response",
          "comment": "I appreciate the authors' response. However, my main concern remains around the motivation for this setting and the additional insights with respect to prior work. I believe the results around transfer to novel agents via domain randomization over hyperparameters seems most interesting in justifying investment in this approach, and would encourage the authors to double down on this aspect of their findings in a future submission of this work. The current state of results around this aspect of the work seems inconsistent.\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_Q4vZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_Q4vZ"
        ]
      },
      {
        "id": "525tkFi_X0j",
        "original": null,
        "number": 25,
        "cdate": 1638015359538,
        "mdate": 1638015359538,
        "ddate": null,
        "tcdate": 1638015359538,
        "tmdate": 1638015359538,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "nxPf3dUIMoq",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Following up on the follow-up :-)",
          "comment": "We thank you in return for your response and the very interesting and lively discussion. We believe this is what a rebuttal should look like and highly appreciate the effort to provide us with this very detailed feedback. Hopeful that we have addressed and mitigated your initial concerns, frankly, we would like to ask you if you could increase your score (or, alternatively, we would be glad for any points we should address that would raise your score).\n\nIn the following, we address your remaining questions:\n\\\n&nbsp;\n\n> Appreciate this additional experiment. I guess it would probably be even more informative if the fixed hyperparams were better-performing in this setting, but I understand the motivation for choosing the fixed HPs that came from your optimization procedure.\nAlthough the signal is a bit weak as a result, it at least indicates a bit more clearly what's going on in this case!\n\nWe also think that these results are interesting and will include these together with the discussion in the updated version of the paper.\n\n\\\n&nbsp;\n\n\n> Regarding simplicity of environments; I do think there is information to take away from experiments on these environments. I disagree that discrete gridworlds are necessarily any simpler than simple continuous environments. But, I regard this factor as a minor issue overall; and believe there is a good quality paper to be written within the computational budget you have available.\n\n> Correct me if I'm interpreting the results incorrectly, but it seems that in particular the non-potential RNs are working better on HalfCheetah with the AUC objective? This would make sense to me since the non-potential RNs would be the ones most affected by a thresholded objective, and the more complex Cheetah may be the most sensitive to this factor.\n\nYes, we completely agree with your observation and also arrive at the same conclusion. Thank you for pointing this out. We will add this to the discussion of the HalfCheetah experiment.\n\n\\\n&nbsp;\n\n> Regardless, I believe this is a useful addition to the paper and will make the discussion of RNs feel a bit better connected to the overall motivation (by not tying them so closely to an objective which is so different from the SE one). This may also be of interest to reviewer Q4vZ.\n\nIndeed, we also think this is a very useful addition to the paper which makes the paper more sound by better connecting RNs to the motivation. Consequently, we would like to replace the existing RN figures in the main paper by the AUC figures that we presented in the Google Doc. and include a discussion on different RN objectives to the appendix.\n\n\\\n&nbsp;\n\n> Clarification: I understand that you are not changing RL algorithms between outer-loop iterations. However, I think that not doing so undermines the motivations that you give for the work, some of which center on comparing different algorithms' performance in the SE, post-hoc. It's unclear why post-hoc performance of various algorithms would correspond to real-env performance, since only a single algorithm is used while learning the SE/RN. My challenge here is not to either the motivation per se, or the implementation, but rather the connection between them.\n\n> I think the empirical study of performance correlations could go some way to bridging this gap.\n\n> Thank you for your discussion of the other points and pointing me to the relevant part of the Appendix.\n\n\nThank you for clarifying our misunderstanding. We agree that the performance correlation experiment would bridge this gap. Unfortunately, this experiment is still running. Measuring the correlations with the stochasticity in RL agents and the real environments requires a large number of evaluations to allow for solid conclusions with significant correlations. Given paper acceptance, we are confident that this experiment result will be available before the camera-ready deadline.\n\nNonetheless, we will include this in our future synthetic environment research."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "TiihJaeGD_6",
        "original": null,
        "number": 27,
        "cdate": 1638287853127,
        "mdate": 1638287853127,
        "ddate": null,
        "tcdate": 1638287853127,
        "tmdate": 1638287853127,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "F0hE26NdwPl",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "In hope of clarification",
          "comment": "Dear reviewer,\n\nConsidering the breadth and depth of our initial response and your relatively short response in return, we would like to point out that only through *actionable* feedback we are able to address your concerns successfully and ultimately improve our paper. Given your response, we are unsure about what would be steps to mitigate your concerns.\n\nWe are convinced that our proposed method is well-motivated and the evaluation is sound. We tried to address your concerns in detail above but your response did not touch on this at all. Therefore, we would kindly ask you to explain in detail whether, from your perspective, there exists a problem with the method or the types of evaluations we have studied.\n\nThank you.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "itXOJMEA9A",
        "original": null,
        "number": 28,
        "cdate": 1638288016529,
        "mdate": 1638288016529,
        "ddate": null,
        "tcdate": 1638288016529,
        "tmdate": 1638288016529,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "TiihJaeGD_6",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Please see previous comment",
          "comment": "Please see my previous comment, which highlighted the portions of my original review that your response failed to address."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_Q4vZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_Q4vZ"
        ]
      },
      {
        "id": "kg3RunKVOP",
        "original": null,
        "number": 1,
        "cdate": 1642696833905,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696833905,
        "tmdate": 1642696833905,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "C1_esHN6AVn",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "This paper proposes a new method for generating synthetic environments and reward networks for reinforcement learning tasks. This happens as a nested process: policies are learned in an inner loop, and environments are evolved in an outer loop. The environment representation is quite simple: the parameters of an MDP. Similarly, the reward networks are simply neural networks. Results show that the the learned environments and reward networks are reasonably good at decreasing policy training time by RL. The proposed method appears to be simple and quite general, and it would be interesting to see how it scales up to more complex environment representations.\n\nThe discussion around the paper centered on understanding various details of the method, and on the quality of the results. The reviewers generally agree that the paper is easy to read, and vary in their assessment of the significance of the results. It was pointed out that the generated environments are not necessarily similar to the base tasks, but it was nowhere claimed in the paper that they were. (In fact, it could be argued that the dissimilarity makes the method more interesting, given the good results of policy training.)\n\nI'm happy to recommend the paper for poster acceptance. If the results would have been more impressive, it could have been accepted for a more prominent presentation form; however, I believe that the method can yield better results in the future with more sophisticated environment representations."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "6ihfnhFq4V7",
        "original": null,
        "number": 1,
        "cdate": 1635607978649,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635607978649,
        "tmdate": 1638207685536,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "C1_esHN6AVn",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper introduces Synthetic Environments and Synthetic Rewards as a bi-level optimization problem. The main idea is to find SEs and REs such that an RL agent trained under them will do well in a given, known target environment, without further training. ",
          "main_review": "Results:\n- In Figure 2, the performance seems very unstable: In particular, the reward for individual agents seems to fluctuate widely and even the average starts drooping after the 50th iteration in the arcobot task. It would be great to know what the source of this instability is. \n\nConceptual:\n- There are repeated claims in the paper that the synthetic environment \"focuses the learning\" on the relevant states or learns \"representations\" of the target environment (\"an informed representation of the target environment by\nmodifying the state distributions to bias agents towards relevant states\"). This claim is entirely unclear to me at the moment. In particular, there simply doesn't seem to be any reason that the SE should have much in common with the ground truth environment. The only condition is that _optimal policies_ for the two environments match. In particular, there will be many environments in which the optimal policy is likely much easier to learn than in the original Env. \nFor example, you can imagine a state transition dynamic that is entirely random but provides a large reward for the optimal action in each of the given states. Another option is to learn environment dynamics where the optimal action in each state deterministically transitions to an arbitrary next state, such that the agent will see all required states during training. \nIt would be great to provide any type of evidence that there is actually semantic meaning in the SE that matches the underlying ground truth environment.  \n\nOther comments:\n- In Figure 2, it seems like the legend is switched? I.e. dashed line should be the solved threshold?",
          "summary_of_the_review": "A very interesting paper with early results in a potentially interesting direction. It is currently unclear what this method allows to do that was previously impossible from a practical point of view, but there seems to be promise. \nI'd be open to raising my score if my concerns are addressed by the authors. \n\n\n\nUpdate: \nBased on the discussion phase and the rebuttal I have updated the score to a 6. I believe this paper has enough merit to be published at ICLR, if there is space. It is an intriguing piece of work, even though the practical utility is very unclear at this point. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_ktr8"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_ktr8"
        ]
      },
      {
        "id": "s4BAoDkpYWz",
        "original": null,
        "number": 2,
        "cdate": 1635863397328,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635863397328,
        "tmdate": 1635863397328,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "C1_esHN6AVn",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper aims to learn proxy environments (synthetic environments or SEs) and reward functions (reward networks or RNs), parameterized as neural networks, such that these proxy models provide beneficial transitions to make it more sample-efficient to learn a policy for a fixed target environment (referred to as the real environment). SEs replace the observed state and rewards during training, while reward networks provide a synthetic reward that is a function of the real reward, current state, and next state. The proposed method formulates this problem as a bi-level optimization, where the inner loop consists of standard RL under the proxy model, and the outer loop consists of NES with the aim of optimizing either the performance on the true, target environment (SEs) or a the number of training steps needed to reach a certain return threshold on the real environment. ",
          "main_review": "### Strengths\n- This paper extends prior work on meta-learning MDP transition dynamics for improving training performance on a target environment, by introducing a method that aims to learn both the state and reward transitions. This formulation is more general than prior works.\n- Experimental results are provided for multiple formulations of meta-learned MDP transition dynamics, spanning both joint state and reward networks (SEs) as well as various formulations of a reward network (potential and non-potential-based reward shaping parameterizations).\n- This paper includes experiments on transferring to different agent architectures and hyperparameter settings, which seems to be a novel experimental setting for meta-learning MDP dynamics.\n\n### Weaknesses\n- The motivation for learning SEs and RNs for singleton target environments is not convincing. While there seem to be some marginal gains in sample-efficiency, it would seem these gains are largely made irrelevant due to the additional training needed to train the environment in the first place. Moreover, if SEs and RNs require first training the agent multiple times on synthetic environments, why not just copy weights or use a form of policy distillation, e.g. kickstarting (Schmitt et al, 2018), rather than train the agent again? Further, it seems the basic ICM baseline already matches or outperforms the proposed RN approaches across experimental settings, further weakening the argument for using the more computationally expensive method proposed in this work.\n- Given the wide design space of possible ways of learning the state and reward transition dynamics for a target real environment, it is unclear why only these two (SE and RN) formulations were studied. Given that this is an empirical paper, the value of the findings rests largely on a clean experimental design that provides a convincing recommendation based on comprehensive experiments. The seemingly arbitrary choice for only considering the SE and RN formulation does not provide such a convincing recommendation for using either of these formulations.\n- RNs are effectively an ablation of SEs, where the state is not learned. However, the current experimental design does not neatly present these results as a clean ablation. This is because the RN experiments also separately consider various kinds of potential and non-potential-based reward shaping parameterizations for the learned reward. These parameterizations are not studied in combination with SEs.\n- Likewise, there is no corresponding ablation for when the reward is not learned, but only the state is learned, i.e. a \"state-transition network\" or perhaps a \"state-augmentation network.\"\n- It is not made clear why exactly SEs and RNs have to use different objectives when training.\n- The RN objective seems strange, as it seems to be optimizing for sample efficiency rather than performance. In contrast the SE objective optimizes for performance. This seems like a significant inconsistency between the training methology for the two types of models, making it hard to compare the relative merits of SEs and RNs.\n- The reward difference term of the RN objective also caps the maximum performance improvement compared to training on the real environment. The motivation for this objective design is not clear, as not taking the max between the reward difference and 0 should encourage learning RNs that lead to outperforming training on the real environment, rather than simply matching it.\n- Further, it seems that there is not a consistent ranking in terms of which parameterization for the RNs improves training performance on the real environment the most, and similarly, no consistent ranking between non-transfer and transfer scenarios for each environment.\n- Meanwhile, the SE results on CartPole and Acrobot would benefit from a direct comparison to the training curves for agents using the best hyperparameters, trained on the real environment. This would directly separate out the performance gains due to training on the  SE.\n- The comparison to previous work such as Zheng et al, 2020 seems quite hand-wavey. Zheng et al, 2020 in particular is quite similar to this work in terms of analyzing the effect of learning an RN and also includes more comprehensive experimental results in a more complex setting requiring recurrent RNs. The novel contributions in relation to these prior works seems to be around analyzing the impact of varying agent hyperparameters on transferring to alternative agent settings and architectures during evaluation. However, these transfer results do not seem consistently strong for any one method proposed.\n- Sampling agent hyperparameter settings from a diverse range seems crucial for the learned SEs and RNs to transfer to other agent configurations. The paper does not discuss how sensitive the proposed methods are to the agent hyperparameter ranges used during training.",
          "summary_of_the_review": "While this work proposes an interesting generalization of prior work on meta-learning MDP dynamics, the case made for these methods is weak, and the experimental results\u2014especially for SEs\u2014are uncompelling. Given the additional compute required by these methods, it seems that various forms of policy distillation or even training from scratch seem more efficient. Further, it is not clear why only certain formulations (SEs and RNs) of meta-learning neural transition dynamics were investigated, while others (like a state-only neural transition network) were not. Given these points, I recommend this paper in its current form for rejection.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_Q4vZ"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_Q4vZ"
        ]
      },
      {
        "id": "GbSZnNGlnN1",
        "original": null,
        "number": 3,
        "cdate": 1635929816463,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635929816463,
        "tmdate": 1635930005695,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "C1_esHN6AVn",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The authors propose a framework for learning to synthesize a proxy models for usually non-learned components of an RL loop: namely transition dynamics and reward as defined in the interacting environment. They learn the parameters of these model by \"meta-learning\" onto the real environment with a learning agent (with slight detail variants depending on what the learning objective is), showing that not only can this setup lead to successfully learn the proxy parameters, but also improve learning performance for the agents.\n",
          "main_review": "\nFirst and foremost, let me start by saying that I *really* like this manuscript.\n\n1. The writing is extremely clear & detailed, and makes the paper a pleasure to read. I also found that the authors tended to answer questions that were popping as I was reading paragraphs immediately thereafter, which is a likely sign of writing that has gone through a lot of careful iterations.\n\n2. The authors have gone through a lot of work to justify the choice of optimization methods, how and why the framework was setup in certain ways, and how the hyperparameters for a now fairly complex learning system were chosen. Considering the code release and the extremely detailed appendices, I suspect the work will be extremely easy to reproduce. Truly commendable, and a great example of what good ML research looks like (I also particularly enjoyed the frank discussion about the work's limitation due to choosing ES as an optimization method for the outer loop).\n\n3. The central idea of the method is relatively simple, but seems to be quite effective; it honestly borrows from a lot of previous literature (reward learning, population-based optimization, etc.), whilst effectively producing a novel take on learning syntethic environment models.\n\nSo, overall, I am extremely happy to argue for acceptance as it is. Now, let's look at what I perceive to be some of its weaknesses:\n\n4. The manuscript at times feels very dense, especially when looking at methodology and experimental details, and its understanding is greatly helped from the presence of a sizable appendix. It very much feels like the authors could have a feasibly split the paper into two, one on ES and a follow up on RN, providing more space to experimentally analyse ES and RN separately and make for a cleaner and easier review process.\n\n5. Some of the hypotheses presented to justify the improved agent training feel relatively handwave-y. Consider for instance the following quote (emphasis mine):\n\n>Optimizing SEs for the maximum cumulative reward automatically yielded fast training of agents as a side-product, likely due to **efficient synthetic state dynamics**.\n\nIt is implied that this meta-learning approach leads to generally discover better data-producing dynamics for the agents used in the system, but I wonder whether this is primarily due to the structure of the tasks employed in the experimental settings (as state space and transition dynamics of these simple control problems are _generally_ not very informative, and the _useful_ -- policy learning wise -- parts of these MDP are instead both easily discoverable via search and extremely informative). \n\nIn short, I wonder if we'd see similar improvements for tasks / environments where task difficulty is extremely affected by the emerging complexity of the environment, or whether we can actually show that this is a property of SEs / RNs when trained in this manner (which would be quite outstanding!). Although that said, the effecitveness of RNs do seem to indeed point to the latter hypothesis.\n\n\n### Nits / Additional comments\n\nFigure 2: the labels are inverted (dotted lines should be the thresholds).\n\nSection 6 (but generally in many parts of the paper): it is claimed that learning rewards is generally easier than learning transition functions -- would it be possible to find a reference for this? My personal opinion is that it should indeed be often the case, but that generally it is not true (e.g. imagine an MDP with a small transition matrix but an extremely stochastic reward function).",
          "summary_of_the_review": "\nThis is a good paper. It contains excellent writing, good research, and makes for a great example of what an ICLR paper should look like. The research problem and proposed methods are interesting and well placed in the literature, and the experimental section is exhaustive.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_pG56"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_pG56"
        ]
      },
      {
        "id": "kG-za7eEsmv",
        "original": null,
        "number": 4,
        "cdate": 1635979865295,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635979865295,
        "tmdate": 1638226104943,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "C1_esHN6AVn",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes to learn neural environment models they term Synthetic Environments, such that planning in the model with model-free RL results in good performance in the real environment.\nThe authors study learning full models as well as reward-only models they call Reward Networks, investigating the feasibility of learning these models as well as their robustness to different inner-loop algorithms and hyperparameters.",
          "main_review": "This is an interesting line of work and initial investigation. The exposition of the method and description of experiments are quite clear and complete. However, there are a few weaknesses.\n\nThe motivation for this work deserves more attention. The authors suggest that SE\u2019s could be used for AutoML, iterating on algorithms in expensive environments like robotics, agent or task analysis, or agent pre-training. However, I\u2019m not sure these are particularly well aligned with the authors\u2019 statement of the problem, implementation, or experiments.\nMost applications are predicated on the relative performance of various algorithms, when trained in the SE, being correlated with performance in the true environment.\nHowever, the problem formulation in eqn (1) describes maximising performance absolutely.\nThis is approximated in Alg. 1 with a generic TrainAgent function that approximates the argmax_theta. But it seems the specific TrainAgent functions chosen are highly relevant to potential use cases \u2013 it is not a generic stand-in for an argmax.\nThe authors certainly understand the relevance of this, as they study generalisation of SE\u2019s to different algorithms and their hyperparameters, and even address generalisation algorithmically by varying hyperparameters during training of the SEs. However, this is introduced as a brief comment in the experiment descriptions, rather than being central to the method.\nIt feels that to use SE\u2019s for the potential applications discussed, it would make more sense to reformulate the problem statement to explicitly account for different inner-loop algorithms.\nFurther, simply maximising the final performance of all inner-loop algorithms seems quite limited when applications involve comparing different algorithms.\nIt might be interesting to algorithmically address this use-case by changing the fitness function to account for the relative performance of different algorithms.\nHowever, it would also be useful to address this empirically given the current formulation of the objective, and the authors likely already have the data to do so, at least partially: does the performance of different HPs/algorithms *in the SEs* correlate with their performance on the true environment?\n\nIt may also be possible to motivate the work from a more traditional model-based RL point of view, which might be more closely aligned to the problem statement given in the paper. A closely related work is that of Nikishin et al [1] (who use implicit differentiation to address the bi-level optimisation). NB I don\u2019t fault the authors for not citing this, I believe it is only available as preprint. They argue that models learned only to further optimisation of a policy (as in this work's problem statement) may perform better than traditional models when capacity is limited. However, the ES-based bi-level optimisation is expensive, so it may be difficult to make a strong case for this approach.\n\nAnother weakness of the study is the simplicity of the environments used. I understand the authors may have limited computational resources, but it is difficult to know how highly to weight an empirical feasibility study carried out on such small-scale tasks. In the absence of more compelling empirical findings, it is more critical to explore the algorithmic choices and motivations.\n\nIn this vein, I did appreciate the investigation of which inductive biases might help during reward learning. Overall, I find the case for RNs is perhaps more intuitive: dense-ifying rewards once in an expensive optimisation to allow fast iteration later. Using an additive potential form even guarantees that this will not change the optimal behaviour \u2013 sadly, it seems this type of RN did not outperform the baseline much in the experiments.\n\nIn the RN case optimising for the speed of training makes sense, but I still find the chosen objective strange. Why not optimise the area-under-curve returns for the full inner-loop? Clipping at a somewhat arbitrary solution threshold seems like it would result in suboptimal RNs (this appears to have occurred in the half-cheetah experiment in the appendix).\n\nMinor comments:\n - In the SE experiments, it would be good to show the performance of the \u2018fixed\u2019 hyperparams evaluated with the same fixed hyperparams: this would show the \u2018generalisation gap\u2019 compared to evaluating with varied hyperparms as is currently shown.\n - Overall, many figures are a bit hard to parse. Try using more smoothing or subsampling for curves. For the barplots, maybe re-arrange them so the comparable bars (i.e. measuring the same metric, \u2018steps\u2019 or \u2018episodes\u2019, are next to each other).\n\n[1] \u201cControl-Oriented Model-Based Reinforcement Learning with Implicit Differentiation\u201d Nikishin et al 2021\n\n----------------------\n\nUpdate: the authors have addressed many of my concerns, ran some additional useful experiments, and clarified their reasoning on several points in the overall discussion with reviewers. I still think there is a bit of a gap between the big-picture motivation for the work and the instantiation we see studied empirically; the authors are working on an empirical investigation to help make that connection. However, even in its absence, I am sufficiently positive about the paper overall now to lean towards acceptance -- I found the study interesting and creative.\n\n",
          "summary_of_the_review": "While the work is interesting, I find the motivation a little strained at not well aligned with the problem statement or algorithmic instantiation. The empirical study is fairly clear but limited by the simplicity of the domains and the restriction largely to assessing feasibility rather than giving clearer indications of how the method could be used in practice.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Reviewer_1Zys"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Reviewer_1Zys"
        ]
      },
      {
        "id": "IbGTpdvCKSl",
        "original": null,
        "number": 2,
        "cdate": 1636823566778,
        "mdate": 1636823566778,
        "ddate": null,
        "tcdate": 1636823566778,
        "tmdate": 1636823566778,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "C1_esHN6AVn",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Official_Comment",
        "content": {
          "title": "Common Comment to Reviewers",
          "comment": "We are very grateful for the helpful feedback and would like to thank the reviewers for their time and effort, and hope that they and their loved ones are safe and healthy. We will focus on each of the reviewer\u2019s concerns individually and are looking forward to a lively discussion."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper31/Authors"
        ]
      },
      {
        "id": "kg3RunKVOP",
        "original": null,
        "number": 1,
        "cdate": 1642696833905,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696833905,
        "tmdate": 1642696833905,
        "tddate": null,
        "forum": "C1_esHN6AVn",
        "replyto": "C1_esHN6AVn",
        "invitation": "ICLR.cc/2022/Conference/Paper31/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "This paper proposes a new method for generating synthetic environments and reward networks for reinforcement learning tasks. This happens as a nested process: policies are learned in an inner loop, and environments are evolved in an outer loop. The environment representation is quite simple: the parameters of an MDP. Similarly, the reward networks are simply neural networks. Results show that the the learned environments and reward networks are reasonably good at decreasing policy training time by RL. The proposed method appears to be simple and quite general, and it would be interesting to see how it scales up to more complex environment representations.\n\nThe discussion around the paper centered on understanding various details of the method, and on the quality of the results. The reviewers generally agree that the paper is easy to read, and vary in their assessment of the significance of the results. It was pointed out that the generated environments are not necessarily similar to the base tasks, but it was nowhere claimed in the paper that they were. (In fact, it could be argued that the dissimilarity makes the method more interesting, given the good results of policy training.)\n\nI'm happy to recommend the paper for poster acceptance. If the results would have been more impressive, it could have been accepted for a more prominent presentation form; however, I believe that the method can yield better results in the future with more sophisticated environment representations."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}