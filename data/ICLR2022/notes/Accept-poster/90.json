{
  "id": "JJCjv4dAbyL",
  "original": "dwfkHAKEisO",
  "number": 90,
  "cdate": 1632875427930,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875427930,
  "tmdate": 1676330689403,
  "ddate": null,
  "content": {
    "title": "Learning Discrete Structured Variational Auto-Encoder using Natural Evolution Strategies",
    "authorids": [
      "~Alon_Berliner1",
      "~Guy_Rotman1",
      "~Yossi_Adi1",
      "~Roi_Reichart1",
      "~Tamir_Hazan1"
    ],
    "authors": [
      "Alon Berliner",
      "Guy Rotman",
      "Yossi Adi",
      "Roi Reichart",
      "Tamir Hazan"
    ],
    "keywords": [
      "structured prediction",
      "derivative-free optimization",
      "variational autoencoder"
    ],
    "abstract": "Discrete variational auto-encoders (VAEs) are able to represent semantic latent spaces in generative learning. In many real-life settings, the discrete latent space consists of high-dimensional structures, and propagating gradients through the relevant structures often requires enumerating over an exponentially large latent space. Recently, various approaches were devised to propagate approximated gradients without enumerating over the space of possible structures. In this work, we use Natural Evolution Strategies (NES), a class of gradient-free black-box optimization algorithms, to learn discrete structured VAEs. The NES algorithms are computationally appealing as they estimate gradients with forward pass evaluations only, thus they do not require to propagate gradients through their discrete structures. We demonstrate empirically that optimizing discrete structured VAEs using NES is as effective as gradient-based approximations. Lastly, we prove NES converges for non-Lipschitz functions as appear in discrete structured VAEs.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "berliner|learning_discrete_structured_variational_autoencoder_using_natural_evolution_strategies",
    "pdf": "/pdf/31efa202599273e261068a5a41dffffc4fe9a922.pdf",
    "supplementary_material": "/attachment/bced5440302053278159398e732d35a70a01ed77.zip",
    "data": "",
    "_bibtex": "@inproceedings{\nberliner2022learning,\ntitle={Learning Discrete Structured Variational Auto-Encoder using Natural Evolution Strategies},\nauthor={Alon Berliner and Guy Rotman and Yossi Adi and Roi Reichart and Tamir Hazan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JJCjv4dAbyL}\n}",
    "venue": "ICLR 2022 Poster",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "JJCjv4dAbyL",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 10,
    "directReplyCount": 6,
    "revisions": true,
    "replies": [
      {
        "id": "v1eiNq_6CKq",
        "original": null,
        "number": 1,
        "cdate": 1635766652084,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635766652084,
        "tmdate": 1635766652084,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "JJCjv4dAbyL",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes and analyses the use of Natural Evolutionary Strategies (NES) to train a VAE with a discrete latent space representation (Spanning Trees).  NES provides a smoothed approximation of the discrete function for which the gradient can be easily computed.  This approach can be seen as an example of the REINFORCE algorithm, although it clearly shows superior performance to traditional REINFORCE implementations.  The authors show that not only is it competitive and often marginally better than state-of-the-art techniques for this problem, it is also much more straightforward and easier to generalise.",
          "main_review": "This is an elegant paper.  Although, NES algorithms have been used elsewhere, this is the first application to VAE training.  By demonstrating its feasibility and effectiveness the authors highlight the usefulness of the approach.  The discussion and analysis is informative and of a high scientific standard.  The use is the approach in this contexts requires the use of a number of sampling tricks which makes the approach quite subtle.\n\nOne could argue that this is just an application of NES to yet another application, but I think this would be unfair.  The REINFORCE algorithm has become a important, goto, algorithm in deep-learning.  Showing that NES provides a simple and efficient algorithm for solving non-differentiable problems is an important contribution worth making.\n\nThe paper is well written, accurate and to the point.",
          "summary_of_the_review": "This is in my view a good paper that deserves a place in ICLR.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Reviewer_qpkv"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Reviewer_qpkv"
        ]
      },
      {
        "id": "n7Gb_Eo-Q4c",
        "original": null,
        "number": 2,
        "cdate": 1635790665147,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635790665147,
        "tmdate": 1638261581652,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "JJCjv4dAbyL",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this paper, the authors proposed to use the Natural Evolution Strategy (NES) algorithm for learning discrete structured VAEs. This algorithm estimate gradients with forwarding pass evaluations only and do not require propagating gradients through their discrete structures. Authors showed empirically that optimizing discrete structured VAEs using NES is as effective as gradient-based approximations.",
          "main_review": "The strengths of the paper:\n- proposing another approach to optimization a neural network than through propagating gradients,\n- proving that NES converges for non-Lipschitz functions such as the objective function of a discrete VAE,\n- experimentally showing that gradient-free methods have similarly effective as perturb-and-parse gradient-based methods\n\nThe weaknesses of the paper:\n- optimizing only the mean of the distribution of parameters, a covariance matrix of the distribution of parameters is imposed by the user. The classical model VAE optimizes both these parameters.\n- too poor experiments, the authors showed experiments that presented effective optimization of the ELBO values and how the latent space size affects the method\u2019s run-time. It could be possible to show how the model reconstructs the images or generates them (this can be troublesome due to the lack of optimization of the VAE model variance). The results of such experiments can be compared to the results of the VQ-VAE method (https://arxiv.org/pdf/1711.00937.pdf or VQ-VAE 2 - https://arxiv.org/pdf/1906.00446.pdf).",
          "summary_of_the_review": "The paper is well written, the authors clearly describe the problem and the proposed solution that is computationally appealing because does not require propagating gradients. It is theoretical work, the experiments only show the effectiveness of the ELBO function optimization. The authors consider only optimization of the mean distribution of parameters and do not show other experiments that confirm the effectiveness of such optimization in generative models (see the section of the weaknesses of the paper). Hense I rate the work good but not sufficient for the ICLR conference.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Reviewer_ANuT"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Reviewer_ANuT"
        ]
      },
      {
        "id": "VMxJTOe9eX",
        "original": null,
        "number": 3,
        "cdate": 1635894746105,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635894746105,
        "tmdate": 1638249322648,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "JJCjv4dAbyL",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this paper, the authors use Natural Evolution Strategies (NES) to learn discrete structured VAE. They empirically show that optimizing discrete structured VAEs using NES is as effective as gradient-based approximations. And they also prove that NES converges for non-Lipschitz functions in discrete structured VAEs.",
          "main_review": "Strengths.\n\n1. The paper is well-organized. \n\n2. In this work, a gradient-free black-box optimization algorithm is explored for discrete structured VAEs. \nThey also experimentally show gradient-free methods are as effective as sophisticated gradient-based methods.\n\n3. They prove that the NES algorithm converges for non-Lipschitz functions. This is different from the contemporary trend that relies on Lipschitz functions.\n\nWeaknesses\n1. In VAES, there are two terms for ELBO: one is sampled log-likelihood term, the other is the KL-divergence term that measures the similarity of the auxiliary distribution q and the unknown distribution p.  A lot of people notice the importance of the second term. In this paper, the authors use gradient-free methods for the first term. However, the second term is missing in the algorithm. Or do I miss something? \n\n2. The gradient-free black-box optimization algorithm is generally with high variance (for example, refinance force algorithm). Unfortunately, there are no clear discussions on this. And how is the effect of the proposed method? Mirrored sampling is used in the experimental setup. It is not clear whether this sampling method is used in other methods. This leads to a question: whether there is a fair comparison.\n",
          "summary_of_the_review": "It would be great to provide more details on how to get Equation (2). Especially, why an argmax operation is added. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Reviewer_Fqt8"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Reviewer_Fqt8"
        ]
      },
      {
        "id": "Uo_pxXi5B5w",
        "original": null,
        "number": 4,
        "cdate": 1635965543283,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635965543283,
        "tmdate": 1638166647687,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "JJCjv4dAbyL",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The proposed paper presents a method for optimizing discrete structured VAEs using Natural Evolution Strategies (NES). The authors present a theoretical conclusion regarding algorithm convergence and extensive empirical results demonstrating that NES performs comparably to gradient-based methods, while being more computationally efficient.",
          "main_review": "**Strengths:**\n* The paper addresses an open problem in the space of discrete VAEs.\n* The paper is very well written, easy to follow, and compelling.\n* The presentation of the background material logically leads the reader into the proposed methodology, and the math is included as a meaningful and coherent addition to the story.\n* The experiments are extensive and convincing, demonstrating advantages over a wide range of baselines.\n\n**Weaknesses and suggestions for improvement:**\n* Some relevant works are missing from the literature review [1-6]. Contextualizing the proposed method in these works would make the overall paper stronger.\n* Since Eq. (2) is critical to the proposed method and the presented theoretical results, it would be helpful to highlight its importance when it is first defined. I recommend you clearly state that Eq. (2) is the objective function you will consider for the remainder of the paper, and elaborate on why it is non-negative (i.e., is it because of a ReLU at the end of the $f_{\\theta}$ network?). Additionally, please highlight in the proof of Lemma 1 where the non-negativity comes in.\n* I would appreciate the inclusion of a discussion of the bias introduced by the NES algorithm in Eq. (4). Further discussion on why $N$ forward passes in NES are more desirable than a sampling-based technique with $N$ samples would also be helpful. The experiments and the parallelization arguments contribute to this discussion, but a bit more emphasis and detail in the methods section would aid the reader to better understand the advantages of the proposed method.\n* I recommend the authors spend a bit more time explaining what $w_{1}$ and $w_{2}$ correspond to, i.e., the encoder and decoder parameters, respectively? At first read, I thought you were setting $N = 2$.\n* The statement on pages 4 and 5 that for any $T$, there exists a $t \\in \\{1, ..., T\\}$ for which the magnitude of the gradient is arbitrarily small needs to be corrected. Specifically, it should say something like for *T sufficiently large* the magnitude of the gradient is arbitrarily small.\n* In the experiments, additional discussion would be helpful for why SST might be expected to do better than NES in Table 1, and the disadvantages of SparseMap compared to NES.\n* Lastly, the paper needs to be proofread for typos, as I found a number of them while reviewing. Some examples include: 'over exponentially large latent space', missing period after Eq. (3), 'using NES algorithm' (missing 'the'), incorrect quotation marks for 'parameters' and \"Growth in N\", 'in contrast to contemporary trend' (missing 'the'), 'which must carefully designs', 'being simpler and robust' (missing 'more'), 'generic, flexible, and simpler to implement' (missing 'more'). The references need to be proofread for consistency, as well: some venues are missing, some venues are only presented as acronyms, while others are presented in full, incorrect capitalization ('Ai'), etc. If possible, avoid splitting equations across multiple lines in the text and capitalize 'Section' when referencing section numbers.\n\n[1] A. van den Oord, O. Vinyals, and K. Kavukcuoglu, \"Neural discrete representation learning,\" in NeurIPS, 2017.\n\n[2] A. Razavi, A. van den Oord, and O. Vinyals, \"Generating diverse high-fidelity images with VQ-VAE-2,\" in NeurIPS, 2019.\n\n[3] G. Correia, V. Niculae, W. Aziz, and A. Martins, \"Efficient marginalization of discrete and structured latent variables via sparsity,\" in NeurIPS, 2020.\n\n[4] M. Itkina, B. Ivanovic, R. Senanayake, M. J. Kochenderfer, and M. Pavone, \u201cEvidential sparsification of multimodal latent spaces in conditional variational autoencoders,\u201din NeurIPS, 2020.\n\n[5] P. Chen, M. Itkina, R. Senanayake, and M. J. Kochenderfer, \"Evidential softmax forsparse multimodal distributions in deep generative models,\" in NeurIPS, 2021.",
          "summary_of_the_review": "Overall, the authors present an interesting method for optimizing discrete VAEs with compelling theoretical and experimental results. The paper is well written and the story is coherent. My recommendation is to accept.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Reviewer_12Bq"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Reviewer_12Bq"
        ]
      },
      {
        "id": "yI_bPA4D5YU",
        "original": null,
        "number": 7,
        "cdate": 1636993166396,
        "mdate": 1636993166396,
        "ddate": null,
        "tcdate": 1636993166396,
        "tmdate": 1636993166396,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "Uo_pxXi5B5w",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer 12Bq",
          "comment": "We thank reviewer 12Bq for the constructive feedback and the detailed suggestions. We will update the paper accordingly and upload a revised version within a few days. The updates will be color-marked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Authors"
        ]
      },
      {
        "id": "T2amtYofxx3",
        "original": null,
        "number": 8,
        "cdate": 1636995425042,
        "mdate": 1636995425042,
        "ddate": null,
        "tcdate": 1636995425042,
        "tmdate": 1636995425042,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "n7Gb_Eo-Q4c",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Comment",
        "content": {
          "title": "Response to reviewer ANuT",
          "comment": "We thank reviewer ANuT for the comments and feedback. We are glad the reviewer thinks the paper is well written and that the problem and proposed solution are clearly described.\n \nIn the following, we address the reviewer concerns:\n\n**Learning the covariance matrix**\n\nWe thank the reviewer for emphasizing learning of the covariance matrix. We left such learning out to follow standard discrete VAEs baselines (e.g.,  [1, 2, 3, 4], Gumbel-Softmax, Gumbel-Sinkhorn, SparseMAP, Stochastic Softmax Tricks, Perturb and Parse) that learn only the mean parameters. We feel that comparing to baselines that do not naturally embed variance is unfavorable to the baselines. \n\nNevertheless, we conducted a set of experiments that show that learning the variance further improves NES performance. We used the VAE architecture described in Section B in the Appendix. The experiments were conducted on the binarized MNIST, FashionMNIST, and KMNIST datasets. The model was trained using the ADAM optimizer over $300$ epochs with $N=150$ and a constant learning rate of $10^{-3}$ for learning the mean and $5\\cdot 10^{-5}$ for learning the covariance. We compared the performance of NES when learning both the mean and covariance to the performance of NES when learning only the mean. In the following table, we report the negative ELBO of the different experiments (lower is better):\n\n| Method \\ Dataset        |  MNIST | FashionMNIST | KMNIST |\n|-------------------------|:------:|:------------:|:------:|\n| NES (mean)              | 167.12 |    225.36    | 293.35 |\n| NES (mean + covariance) | 166.23 |    224.09    | 291.63 |\n\nAs can be seen, jointly optimizing the mean and covariance leads to a minor improvement. We leave further exploration for future work.\n\n**Reconstructing/Generating images**\n\nIn our paper, we focused on VAEs whose latent space is discrete and structured. In our experimental validation we focused on the latent space of spanning trees and the challenge is to choose a spanning tree out of exponentially many spanning trees. We conducted a comprehensive comparison between NES and the leading methods for optimizing such systems and used relevant metrics such as edge precision, edge recall, unlabeled attachment score, run-time, and the ELBO to measure performance.\n\nIn addition, we investigated some properties of NES in a controlled setting, i.e., the effect of latent space size on model runtime, the effect of model size on the number of samples needed for optimization, etc. We don\u2019t claim that NES leads to better (or worse) image reconstruction compared to the baselines, as it\u2019s not the essence of our work. The main objective of our experiments was to measure the ability of NES in learning latent structures compared to gradient-based methods. Quoting reviewer 12Bq: \u201cThe experiments are extensive and convincing, demonstrating advantages over a wide range of baselines.\u201d\n\nRegarding the comparison to VQ-VAE, we agree that there is a similarity between the methods (both methods perform auto-encoding under the discrete variational setting), however, the goal is different. In our setting we **know** the structure of the latent space, e.g., the space of all possible spanning trees in a given graph, hence we do not perform unsupervised vector quantization as in VQ-VAE but rather use a predetermined quantization over the possible spanning trees. Comparing our method to VQ-VAE is unfavorable to VQ-VAE as its unsupervised vector quantization never results in valid spanning trees.  \n\n[1] Jang, Eric, Shixiang Gu, and Ben Poole. \"Categorical Reparameterization with Gumbel-Softmax\", in ICLR, 2017. \\\n[2] Mena, Gonzalo, et al. \"Learning latent permutations with gumbel-sinkhorn networks\", in ICLR, 2018. \\\n[3] Niculae, Vlad, et al. \"Sparsemap: Differentiable sparse structured inference\", in ICML, 2018. \\\n[4] Corro, Caio, and Ivan Titov. \"Differentiable perturb-and-parse: Semi-supervised parsing with a structured variational autoencoder\", in ICLR, 2019. \\\n[5] Paulus, Max B., et al. \"Gradient estimation with stochastic softmax tricks\", in NeurIPS, 2020.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Authors"
        ]
      },
      {
        "id": "KPGt224xL0P",
        "original": null,
        "number": 9,
        "cdate": 1636995501738,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636995501738,
        "tmdate": 1636995531986,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "v1eiNq_6CKq",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer qpkv",
          "comment": "We thank reviewer qpkv for the positive feedback. \\\nWe are glad the reviewer thinks that our paper is elegant and of a high scientific standard, as much thought went into it. \\\nLike the reviewer, we believe that showing that NES can solve a challenging non-differentiable problem (like optimizing discrete structured VAE) in an effective yet simple manner is an important contribution to the community."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Authors"
        ]
      },
      {
        "id": "oA-F6xHaSuH",
        "original": null,
        "number": 10,
        "cdate": 1636998479089,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636998479089,
        "tmdate": 1637610967639,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "VMxJTOe9eX",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer Fqt8",
          "comment": "We thank reviewer Fqt8 for the questions and feedback. \\\nIn the following, we address the reviewer concerns:\n\n**Optimizing the KL-divergence term**\n\nThe reviewer is correct. We didn\u2019t include the KL-divergence term in the algorithm for the sake of simplicity. In practice, the NES algorithm optimizes both terms. For the avoidance of doubt, in our experiments, we optimized both.\n\nFor completeness we provide the full objective function: \\\nAssuming that $p_\\theta (\\cdot)$ is the uniform distribution over the space of structures, the Gumbel-Max reparameterization trick let us derive the following approximation\n\n$\nKL(q_\\phi (\\cdot|x) || p_\\theta(\\cdot)) = E_{z \\sim q_\\phi (\\cdot|x)} \\big [\\log \\frac{q_\\phi (z|x)}{p_\\theta (z)} \\big ] \\approx E_{\\gamma \\sim \\mathcal{G}(h_\\phi(x))} [z^{*^{\\top}} h_{\\phi}(x) - \\log \\frac{1}{|\\mathcal{Z}|}],\n$\n\n\nwhere $z^* = \\arg\\max_{z \\in \\mathcal{Z}} \\{z^{\\top}\\gamma\\}$. The resulting NES objective is\n\n$E_{w \\sim N(\\mu, \\sigma^2 I)} E_{\\gamma \\sim \\mathcal{G}(h_{w_2}(x))} \\big [ - f_{w_1}(x, z^*) + z^{*^{\\top}} h_{w_2}(x) - \\log \\frac{1}{|\\mathcal{Z}|} \\big ]$.\n\nand its gradient takes the form of\n\n$E_{w \\sim N(0, I)} E_{\\gamma \\sim \\mathcal{G}(h_{\\mu_2 + \\sigma w_2}(x))} \\Big [\\frac{w}{\\sigma} \\big( -f_{\\mu_1 + \\sigma w_1}(x, z^*) + z^{*^{\\top}} h_{\\mu_2 +\\sigma w_2}(x) - \\log \\frac{1}{|\\mathcal{Z}|} \\big ) \\Big ]$,\n\nwhere $w = (w_1, w_2)$ is the concatenation of the two vectors $w_1, w_2$.\n \nWe thank the reviewer for this important question that highlights the need to include these equations. We will include them in the supplementary material in the updated version, which we will upload in a few days.\n\n**The variance of NES**\n\nAs stated by the reviewer, the score function estimator (REINFORCE) is known to suffer from high variance (as discussed in the background section). NES is an instance of REINFORCE that optimizes a smoothed version of the objective function by sampling from a distribution over parameter space (rather than the latent space). This distribution is commonly modeled as a multivariate Gaussian. A common practice for reducing the variance of NES is using the mirrored sampling technique where we use a single Gaussian noise vector to create two parameter sets, one by adding and the other by subtracting the noise vector. This technique is only possible for symmetric distributions (e.g., Gaussian) and unfortunately cannot be applied to  our baselines, as unlike NES, they are not based on learning a Gaussian distribution over the space of the parameters.\n \nNevertheless, various general techniques were devised to reduce REINFORCE variance in our baselines. In the latent structure recovery experiments (Section 4.1), we compare NES performance against the performance of four instances of REINFORCE, where each utilizes a different variance reduction technique. The results as described in Table 1 show that the variance of NES is low and, in some cases, even negligible compared to the variance of its competitors.\n\n**How to derive Equation 2**\n\nWe will provide the steps to derive Equation 2 in the supplementary material. \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Authors"
        ]
      },
      {
        "id": "s0pj2gDJat",
        "original": null,
        "number": 11,
        "cdate": 1637617725303,
        "mdate": 1637617725303,
        "ddate": null,
        "tcdate": 1637617725303,
        "tmdate": 1637617725303,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "JJCjv4dAbyL",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Comment",
        "content": {
          "title": "A revised version has been submitted",
          "comment": "We thank all reviewers for the valuable feedback we have received. The paper was revised accordingly, and the modifications were marked in blue. We have made the following changes:\n\n\u2022\tAdded the related works mentioned by reviewer 12Bq \\\n\u2022\tEmphasized the importance of Eq. (2) when it is first defined \\\n\u2022\tExplicitly described $f_{\\theta}$ \\\n\u2022\tDescribed where we use the non-negativity of $k(\\cdot)$ in the proof of Theorem 1 \\\n\u2022\tDiscussed the bias introduced by the NES algorithm \\\n\u2022\tDiscussed why $N$ forward passes in NES are more desirable than a sampling-based technique with $N$ samples \\\n\u2022\tAdded an explanation about what $w_1$ and $w_2$ correspond to \\\n\u2022\tCorrected the theoretical statements on pages 4 and 5 \\\n\u2022\tExplained why SST might be expected to do better than NES \\\n\u2022\tEmphasized the disadvantage of SparseMAP compared to NES \\\n\u2022\tFixed the typos \\\n\u2022\tAdded a Section describing the full objective function (which includes the KL-divergence term) to the Appendix \\\n\u2022\tAdded the derivation of Eq. (2) to the Appendix \n\nIf there are any questions left, please let us know before the discussion period ends.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Authors"
        ]
      },
      {
        "id": "OTZexpY6chm",
        "original": null,
        "number": 1,
        "cdate": 1642696836756,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696836756,
        "tmdate": 1642696836756,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "JJCjv4dAbyL",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "The authors propose to use genetic algorithms to learn variational autoencoders (VAEs) with discrete latent spaces. Specifically they employ natural evolution strategies (NES) to avoid backpropagating gradients through discrete variables. Experiments show how the proposed approach is competitive with the current state-of-the-art to train discrete VAEs.\n\nSome concerns arose from the review and discussion phases, these included confusion around the justification and derivation of NES for VAEs in the presentation and the limitation of the experiments. Authors were responsive and provided the reviewers the needed clarifications, an updated presentation in the revised paper and additional experimental results which ultimately were successful in raising the reviewers' scores towards full acceptance."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "v1eiNq_6CKq",
        "original": null,
        "number": 1,
        "cdate": 1635766652084,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635766652084,
        "tmdate": 1635766652084,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "JJCjv4dAbyL",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes and analyses the use of Natural Evolutionary Strategies (NES) to train a VAE with a discrete latent space representation (Spanning Trees).  NES provides a smoothed approximation of the discrete function for which the gradient can be easily computed.  This approach can be seen as an example of the REINFORCE algorithm, although it clearly shows superior performance to traditional REINFORCE implementations.  The authors show that not only is it competitive and often marginally better than state-of-the-art techniques for this problem, it is also much more straightforward and easier to generalise.",
          "main_review": "This is an elegant paper.  Although, NES algorithms have been used elsewhere, this is the first application to VAE training.  By demonstrating its feasibility and effectiveness the authors highlight the usefulness of the approach.  The discussion and analysis is informative and of a high scientific standard.  The use is the approach in this contexts requires the use of a number of sampling tricks which makes the approach quite subtle.\n\nOne could argue that this is just an application of NES to yet another application, but I think this would be unfair.  The REINFORCE algorithm has become a important, goto, algorithm in deep-learning.  Showing that NES provides a simple and efficient algorithm for solving non-differentiable problems is an important contribution worth making.\n\nThe paper is well written, accurate and to the point.",
          "summary_of_the_review": "This is in my view a good paper that deserves a place in ICLR.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Reviewer_qpkv"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Reviewer_qpkv"
        ]
      },
      {
        "id": "n7Gb_Eo-Q4c",
        "original": null,
        "number": 2,
        "cdate": 1635790665147,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635790665147,
        "tmdate": 1638261581652,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "JJCjv4dAbyL",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this paper, the authors proposed to use the Natural Evolution Strategy (NES) algorithm for learning discrete structured VAEs. This algorithm estimate gradients with forwarding pass evaluations only and do not require propagating gradients through their discrete structures. Authors showed empirically that optimizing discrete structured VAEs using NES is as effective as gradient-based approximations.",
          "main_review": "The strengths of the paper:\n- proposing another approach to optimization a neural network than through propagating gradients,\n- proving that NES converges for non-Lipschitz functions such as the objective function of a discrete VAE,\n- experimentally showing that gradient-free methods have similarly effective as perturb-and-parse gradient-based methods\n\nThe weaknesses of the paper:\n- optimizing only the mean of the distribution of parameters, a covariance matrix of the distribution of parameters is imposed by the user. The classical model VAE optimizes both these parameters.\n- too poor experiments, the authors showed experiments that presented effective optimization of the ELBO values and how the latent space size affects the method\u2019s run-time. It could be possible to show how the model reconstructs the images or generates them (this can be troublesome due to the lack of optimization of the VAE model variance). The results of such experiments can be compared to the results of the VQ-VAE method (https://arxiv.org/pdf/1711.00937.pdf or VQ-VAE 2 - https://arxiv.org/pdf/1906.00446.pdf).",
          "summary_of_the_review": "The paper is well written, the authors clearly describe the problem and the proposed solution that is computationally appealing because does not require propagating gradients. It is theoretical work, the experiments only show the effectiveness of the ELBO function optimization. The authors consider only optimization of the mean distribution of parameters and do not show other experiments that confirm the effectiveness of such optimization in generative models (see the section of the weaknesses of the paper). Hense I rate the work good but not sufficient for the ICLR conference.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Reviewer_ANuT"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Reviewer_ANuT"
        ]
      },
      {
        "id": "VMxJTOe9eX",
        "original": null,
        "number": 3,
        "cdate": 1635894746105,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635894746105,
        "tmdate": 1638249322648,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "JJCjv4dAbyL",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Review",
        "content": {
          "summary_of_the_paper": "In this paper, the authors use Natural Evolution Strategies (NES) to learn discrete structured VAE. They empirically show that optimizing discrete structured VAEs using NES is as effective as gradient-based approximations. And they also prove that NES converges for non-Lipschitz functions in discrete structured VAEs.",
          "main_review": "Strengths.\n\n1. The paper is well-organized. \n\n2. In this work, a gradient-free black-box optimization algorithm is explored for discrete structured VAEs. \nThey also experimentally show gradient-free methods are as effective as sophisticated gradient-based methods.\n\n3. They prove that the NES algorithm converges for non-Lipschitz functions. This is different from the contemporary trend that relies on Lipschitz functions.\n\nWeaknesses\n1. In VAES, there are two terms for ELBO: one is sampled log-likelihood term, the other is the KL-divergence term that measures the similarity of the auxiliary distribution q and the unknown distribution p.  A lot of people notice the importance of the second term. In this paper, the authors use gradient-free methods for the first term. However, the second term is missing in the algorithm. Or do I miss something? \n\n2. The gradient-free black-box optimization algorithm is generally with high variance (for example, refinance force algorithm). Unfortunately, there are no clear discussions on this. And how is the effect of the proposed method? Mirrored sampling is used in the experimental setup. It is not clear whether this sampling method is used in other methods. This leads to a question: whether there is a fair comparison.\n",
          "summary_of_the_review": "It would be great to provide more details on how to get Equation (2). Especially, why an argmax operation is added. ",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Reviewer_Fqt8"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Reviewer_Fqt8"
        ]
      },
      {
        "id": "Uo_pxXi5B5w",
        "original": null,
        "number": 4,
        "cdate": 1635965543283,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635965543283,
        "tmdate": 1638166647687,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "JJCjv4dAbyL",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The proposed paper presents a method for optimizing discrete structured VAEs using Natural Evolution Strategies (NES). The authors present a theoretical conclusion regarding algorithm convergence and extensive empirical results demonstrating that NES performs comparably to gradient-based methods, while being more computationally efficient.",
          "main_review": "**Strengths:**\n* The paper addresses an open problem in the space of discrete VAEs.\n* The paper is very well written, easy to follow, and compelling.\n* The presentation of the background material logically leads the reader into the proposed methodology, and the math is included as a meaningful and coherent addition to the story.\n* The experiments are extensive and convincing, demonstrating advantages over a wide range of baselines.\n\n**Weaknesses and suggestions for improvement:**\n* Some relevant works are missing from the literature review [1-6]. Contextualizing the proposed method in these works would make the overall paper stronger.\n* Since Eq. (2) is critical to the proposed method and the presented theoretical results, it would be helpful to highlight its importance when it is first defined. I recommend you clearly state that Eq. (2) is the objective function you will consider for the remainder of the paper, and elaborate on why it is non-negative (i.e., is it because of a ReLU at the end of the $f_{\\theta}$ network?). Additionally, please highlight in the proof of Lemma 1 where the non-negativity comes in.\n* I would appreciate the inclusion of a discussion of the bias introduced by the NES algorithm in Eq. (4). Further discussion on why $N$ forward passes in NES are more desirable than a sampling-based technique with $N$ samples would also be helpful. The experiments and the parallelization arguments contribute to this discussion, but a bit more emphasis and detail in the methods section would aid the reader to better understand the advantages of the proposed method.\n* I recommend the authors spend a bit more time explaining what $w_{1}$ and $w_{2}$ correspond to, i.e., the encoder and decoder parameters, respectively? At first read, I thought you were setting $N = 2$.\n* The statement on pages 4 and 5 that for any $T$, there exists a $t \\in \\{1, ..., T\\}$ for which the magnitude of the gradient is arbitrarily small needs to be corrected. Specifically, it should say something like for *T sufficiently large* the magnitude of the gradient is arbitrarily small.\n* In the experiments, additional discussion would be helpful for why SST might be expected to do better than NES in Table 1, and the disadvantages of SparseMap compared to NES.\n* Lastly, the paper needs to be proofread for typos, as I found a number of them while reviewing. Some examples include: 'over exponentially large latent space', missing period after Eq. (3), 'using NES algorithm' (missing 'the'), incorrect quotation marks for 'parameters' and \"Growth in N\", 'in contrast to contemporary trend' (missing 'the'), 'which must carefully designs', 'being simpler and robust' (missing 'more'), 'generic, flexible, and simpler to implement' (missing 'more'). The references need to be proofread for consistency, as well: some venues are missing, some venues are only presented as acronyms, while others are presented in full, incorrect capitalization ('Ai'), etc. If possible, avoid splitting equations across multiple lines in the text and capitalize 'Section' when referencing section numbers.\n\n[1] A. van den Oord, O. Vinyals, and K. Kavukcuoglu, \"Neural discrete representation learning,\" in NeurIPS, 2017.\n\n[2] A. Razavi, A. van den Oord, and O. Vinyals, \"Generating diverse high-fidelity images with VQ-VAE-2,\" in NeurIPS, 2019.\n\n[3] G. Correia, V. Niculae, W. Aziz, and A. Martins, \"Efficient marginalization of discrete and structured latent variables via sparsity,\" in NeurIPS, 2020.\n\n[4] M. Itkina, B. Ivanovic, R. Senanayake, M. J. Kochenderfer, and M. Pavone, \u201cEvidential sparsification of multimodal latent spaces in conditional variational autoencoders,\u201din NeurIPS, 2020.\n\n[5] P. Chen, M. Itkina, R. Senanayake, and M. J. Kochenderfer, \"Evidential softmax forsparse multimodal distributions in deep generative models,\" in NeurIPS, 2021.",
          "summary_of_the_review": "Overall, the authors present an interesting method for optimizing discrete VAEs with compelling theoretical and experimental results. The paper is well written and the story is coherent. My recommendation is to accept.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Reviewer_12Bq"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Reviewer_12Bq"
        ]
      },
      {
        "id": "s0pj2gDJat",
        "original": null,
        "number": 11,
        "cdate": 1637617725303,
        "mdate": 1637617725303,
        "ddate": null,
        "tcdate": 1637617725303,
        "tmdate": 1637617725303,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "JJCjv4dAbyL",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Official_Comment",
        "content": {
          "title": "A revised version has been submitted",
          "comment": "We thank all reviewers for the valuable feedback we have received. The paper was revised accordingly, and the modifications were marked in blue. We have made the following changes:\n\n\u2022\tAdded the related works mentioned by reviewer 12Bq \\\n\u2022\tEmphasized the importance of Eq. (2) when it is first defined \\\n\u2022\tExplicitly described $f_{\\theta}$ \\\n\u2022\tDescribed where we use the non-negativity of $k(\\cdot)$ in the proof of Theorem 1 \\\n\u2022\tDiscussed the bias introduced by the NES algorithm \\\n\u2022\tDiscussed why $N$ forward passes in NES are more desirable than a sampling-based technique with $N$ samples \\\n\u2022\tAdded an explanation about what $w_1$ and $w_2$ correspond to \\\n\u2022\tCorrected the theoretical statements on pages 4 and 5 \\\n\u2022\tExplained why SST might be expected to do better than NES \\\n\u2022\tEmphasized the disadvantage of SparseMAP compared to NES \\\n\u2022\tFixed the typos \\\n\u2022\tAdded a Section describing the full objective function (which includes the KL-divergence term) to the Appendix \\\n\u2022\tAdded the derivation of Eq. (2) to the Appendix \n\nIf there are any questions left, please let us know before the discussion period ends.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper90/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper90/Authors"
        ]
      },
      {
        "id": "OTZexpY6chm",
        "original": null,
        "number": 1,
        "cdate": 1642696836756,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696836756,
        "tmdate": 1642696836756,
        "tddate": null,
        "forum": "JJCjv4dAbyL",
        "replyto": "JJCjv4dAbyL",
        "invitation": "ICLR.cc/2022/Conference/Paper90/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "The authors propose to use genetic algorithms to learn variational autoencoders (VAEs) with discrete latent spaces. Specifically they employ natural evolution strategies (NES) to avoid backpropagating gradients through discrete variables. Experiments show how the proposed approach is competitive with the current state-of-the-art to train discrete VAEs.\n\nSome concerns arose from the review and discussion phases, these included confusion around the justification and derivation of NES for VAEs in the presentation and the limitation of the experiments. Authors were responsive and provided the reviewers the needed clarifications, an updated presentation in the revised paper and additional experimental results which ultimately were successful in raising the reviewers' scores towards full acceptance."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}