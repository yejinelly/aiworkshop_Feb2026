{
  "id": "YgPqNctmyd",
  "original": "uE-HOE56ErH",
  "number": 10,
  "cdate": 1632875422167,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875422167,
  "tmdate": 1676330692668,
  "ddate": null,
  "content": {
    "title": "Towards Building A Group-based Unsupervised Representation Disentanglement Framework",
    "authorids": [
      "~Tao_Yang9",
      "~Xuanchi_Ren1",
      "~Yuwang_Wang3",
      "~Wenjun_Zeng3",
      "~Nanning_Zheng1"
    ],
    "authors": [
      "Tao Yang",
      "Xuanchi Ren",
      "Yuwang Wang",
      "Wenjun Zeng",
      "Nanning Zheng"
    ],
    "keywords": [
      "Disentangled representation learning",
      "Group theory",
      "VAE"
    ],
    "abstract": "Disentangled representation learning is one of the major goals of deep learning, and is a key step for achieving explainable and generalizable models. The key idea of the state-of-the-art VAE-based unsupervised representation disentanglement methods is to minimize the total correlation of the joint distribution of the latent variables. However, it has been proved that their goal can not be achieved without introducing other inductive biases. The Group Theory based definition of representation disentanglement mathematically connects the data transformations to the representations using the formalism of group. In this paper, built on the group-based definition and inspired by the \\emph{n-th dihedral group}, we first propose a theoretical framework towards achieving unsupervised representation disentanglement. We then propose a model based on existing VAE-based methods to tackle the unsupervised learning problem of the framework. In the theoretical framework, we prove three sufficient conditions on model, group structure, and data respectively in an effort to achieve, in an unsupervised way, disentangled representation per group-based definition. With these conditions, we offer an option, from the perspective of the group-based definition, for the inductive bias that existing VAE-based models lack. Experimentally, we train 1800 models covering the most prominent VAE-based methods on five datasets to verify the effectiveness of our theoretical framework. Compared to the original VAE-based methods, these Groupified VAEs consistently achieve better mean performance with smaller variances.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "yang|towards_building_a_groupbased_unsupervised_representation_disentanglement_framework",
    "pdf": "/pdf/5394844d0eb50251cdb0af3ec98822f39236eb03.pdf",
    "one-sentence_summary": "In this paper, built on the group-based definition and inspired by the n-th dihedral group, we first propose a theoretical framework towards achieving unsupervised representation disentanglement.",
    "data": "",
    "_bibtex": "@inproceedings{\nyang2022towards,\ntitle={Towards Building A Group-based Unsupervised Representation Disentanglement Framework},\nauthor={Tao Yang and Xuanchi Ren and Yuwang Wang and Wenjun Zeng and Nanning Zheng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YgPqNctmyd}\n}",
    "venue": "ICLR 2022 Poster",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "YgPqNctmyd",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 32,
    "directReplyCount": 6,
    "revisions": true,
    "replies": [
      {
        "id": "GqC25fhEjkn",
        "original": null,
        "number": 1,
        "cdate": 1635210335680,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635210335680,
        "tmdate": 1635210335680,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "YgPqNctmyd",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposed a new theoretical framework for achieving unsupervised representation disentanglement based on $n$-th dihedral group. Further, it proved three sufficient conditions on the model, group structure, and data respectively. Evaluations on multiple benchmark datasets demonstrate the proposed framework achieves better mean performance with smaller variances compared to some VAE models.",
          "main_review": "Pros:\n1. Proposed a new theoretical framework for achieving unsupervised representation disentanglement based on $n$-th dihedral group.\n2. Provided theoretical analysis for the proposed framework via model and group structure\n3. Conducted extensive experiments to verify the proposed framework\n\nCons:\n1. Missing some latest works on disentanglement. The related work section is a bit weak since it only discussed the VAE models before the year 2019. In fact, there are a lot of related works about disentangled representation learning after 2019, such as TamingVAE, ControlVAE, and other works [1,2,3,4].\n2. Please also compare the proposed framework with the baselines in recent work as mentioned above.\n\n3. In Table 1, for dSprites dataset, the MIG score is much lower than the result illustrated in the literature $\\beta$-TCVAE and ControlVAE. Please clarify and explain it. \n\n[1] Srivastava, Akash, Yamini Bansal, Yukun Ding, Cole Hurwitz, Kai Xu, Bernhard Egger, Prasanna Sattigeri, Josh Tenenbaum, David D. Cox, and Dan Gutfreund. \"Improving the Reconstruction of Disentangled Representation Learners via Multi-Stage Modelling.\" arXiv preprint arXiv:2010.13187 (2020). \n\n[2] Shao, H., Yao, S., Sun, D., Zhang, A., Liu, S., Liu, D., ... & Abdelzaher, T. (2020, November). ControlVAE: Controllable variational autoencoder. In International Conference on Machine Learning (pp. 8655-8664). PMLR.\n\n[3] Kim, M., Wang, Y., Sahu, P., & Pavlovic, V. (2019). Bayes-factor-vae: Hierarchical bayesian deep auto-encoder models for factor disentanglement. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 2979-2987).\n\n[4] Lezama, J. (2018, September). Overcoming the disentanglement vs reconstruction trade-off via jacobian supervision. In International Conference on Learning Representations.",
          "summary_of_the_review": "This paper proposed a new theoretical framework for achieving unsupervised representation disentanglement. My concern is that this work only compares it with the baselines before 2018, so it is hard to say whether it outperforms the recent baselines. ",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_PA1y"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_PA1y"
        ]
      },
      {
        "id": "ts_zv_9FZUy",
        "original": null,
        "number": 2,
        "cdate": 1635426751619,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635426751619,
        "tmdate": 1637583508457,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "YgPqNctmyd",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes a theoretical framework for unsupervised representation disentanglement (in the sense of Higgins et al.) based on three constraints (group structure, data and model). It further describes a learning method for satisfying these constraints, and experimentally shows some performance gain on traditional disentanglement metrics. \n\nMy intuitive understanding of the constraints is a follows:\n- the model constraint is satisfied by construction of the agent group, whose structure is transferred from the latent space by the model.\n- the group structure constraint ensures that this transfer is an isomorphism, and thus that the agent group is actually a group.\n- the data constraint ensures that the agent group actually matches the ground truth generative factors. \n\nPlease clarify if one of my statements is incorrect.",
          "main_review": "Strengths:\n- The mathematical part of the paper and experiments appear sound.\n- The isomorphism loss is to my knowledge novel, and seems to bring performance gain across datasets and models.\n\nWeaknesses:\n-  The paper is at best ambiguous and at worst misleading concerning whether or not it claims to solve the problem of unsupervised representation disentanglement\n    - *\"we offer an option [...] for the inductive bias that existing VAE-based models lack\"*\n    - *\"Finally, we provide a learning model based on the existing VAE-based methods in an effort to fulfill the three conditions.\"*\n    - *\"we are the first to provide a theoretical framework to make the formal group-based mathematical definition of disentanglement practically applicable to unsupervised representation disentanglement.\"*\n   - *\"making the learned representation conform to the group- based definition without relying on the environment\"* \n\n   But the paper never theoretically shows that the data constraint is actually satisfied, and instead relies on a necessary condition of the data constraint. This greatly undermines the theoretical contribution of the paper.\n- I find the mathematical writing to be overall quite obfuscated, which complicates assessment of the theoretical contribution.\n\nAs actionnable feedback:\n- I would recommend to be more open about which theoretical guarantees the proposed framework actually provides.\n- Could you please give intuitive explanations of the role of the isomorphism loss, its novelty and comparison to related work, and why it brings this performance gain? ",
          "summary_of_the_review": "I find this paper to be overselling the importance of its theoretical contribution, and will therefore recommend rejection. I am looking forward to discussion with the authors, and will reconsider my rating if the authors respond to the two points listed as actionnable feedback in my main review. In particular I'd like to hear more about the isomorphism loss. ",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_M8Aa"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_M8Aa"
        ]
      },
      {
        "id": "L8MHP_cTued",
        "original": null,
        "number": 3,
        "cdate": 1635506194278,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635506194278,
        "tmdate": 1637908810007,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "YgPqNctmyd",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper provides a theoretical framework on the learning of group-based disentanglement representations. It proposes a method to learn a cyclic group representation with the Abel loss and Order loss based on VAEs. Experiments validate the effectiveness of the proposed method on improving existing disentanglement VAE models.",
          "main_review": "Strengths:\n1. The authors provide a theoretical framework on the learning of group-based disentangled representation, which is a step forward to the more general guaranteed disentanglement learning.\n2. The experimental results validate the disentanglement effect of the 'Groupify' operation on multiple base models and synthetic datasets.\n3. The proposed technique is compatible with all the information-theory-based baseline models. \n\nWeaknesses:\n1. The introduction of $\\Phi$ group is unnecessarily complex. Do the authors mean the $\\Phi$ group is a direct product of $m$ cyclic groups of order n (or even simpler, $m$-dim Torus)? Is each cyclic subgroup represented by $(sin((2\\pi z))/n), cos((2\\pi z)/n))$, with z being the output of the VAE encoder? The generator of each subgroup is the action of rotating each circle by step $2\\pi/n$? I see no necessity of incorporating more concepts from Group Theory like 'congruence class modulo n', 'n-th root unity group', 'additive group of integers modulon'. To me they are all the same in the context of this paper.\n2. The mention of 'n-th dihedral group' is not informative, or even misleading. Dihedral groups are generated by only two subgroups, rotation and flip, which is not very related to the group used in this method (a product of m cyclic groups of the same order n).\n3. The Abel loss and Order loss can only encourage the representation to maintain a group structure of m-dim torus. However, this does not solve the unidentifiability problem that motivates this method (the impossibility of disentanglement mentioned in the abstract, introduction section, and the paragraph before Sec. 4.1) because the ground-truth correspondence between the group action on the world space and its action effect on the image space is still unknown. The real problem here is not how to learn a group that is isomorphic to an m-dim cyclic group, but how to make sure the learned cyclic subgroups correspond to the interpretable variations shown in the image space. This paper proposes a method to learn a cyclic-group representation, which is valuable by itself in some way, but it still falls into the unidentifiability problem proposed in Locatello et al. 2019b.\n4. I don't think $\\phi\\phi^{-1}=e$ can replace $\\phi\\phi^{n-1}=e$ in the Order loss. $\\phi\\phi^{n-1}=e$ ensures the cyclic structure of order n, but $\\phi\\phi^{-1}=e$ holds in any group with arbitrary order. The equation $\\phi^{-1}=\\phi^{n-1}$ holds when the cyclic structure has **already been learned**, but $\\phi\\phi^{-1}=e$ alone cannot help the learning of a cyclic structure.\n5. It looks like the proposed VAE model is a special case of the Commutative Lie Group VAE proposed in https://arxiv.org/abs/2106.03375 with the learned group representation being replaced by a pre-defined direct product of cyclic groups. They should be more clearly discussed and compared in the main paper.\n6. Currently only the synthetic datasets provided in the open-sourced disentanglement library (https://github.com/google-research/disentanglement_lib) are used in the experiments. More results on some real-world datasets like CelebA should be provided to validate the model's generalization ability.\n7. The paper only shows the effectiveness of the method as an incremental technique to improve the existing disentanglement models, but does not show its stand-alone performance. If the proposed constraints are effective enough, they should outperform the existing SOTA models by being directly added to a vanilla VAE (as all the baseline models do). ",
          "summary_of_the_review": "I believe the theoretical contribution of this paper is valuable to the group-based disentanglement learning (or general disentanglement learning) community, but I still have concerns listed in the main review section. I currently rate this paper as weak reject, but I will be glad to improve my score if my concerns are properly solved.\n\n======\n\nAfter rebuttal:\nThanks for the authors' response to my comments. I think a large part of my concerns has been addressed. I will increase the score to 6 weak accept.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_YTd1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_YTd1"
        ]
      },
      {
        "id": "jqUCcSVuzEG",
        "original": null,
        "number": 4,
        "cdate": 1635883846248,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635883846248,
        "tmdate": 1635883846248,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "YgPqNctmyd",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper presents an unsupervised approach to achieve group-based disentangled representation learning, as opposed to existing environment-based approaches. A theoretical framework for the group-based VAE was proposed, and implementations of the group and isomorphism reported. Experiments were performed on four common benchmark datasets, with comparisons to several representative VAE-based disentanglement framework (beta-vae, annealVAE, factorVAE, beta-TCVAE), demonstrating the groupified VAE achieves better disentanglement in many metrics. Qualitative evaluations demonstrate the cyclic representation space learned by the groupified VAE.",
          "main_review": "Strength:\n\nThis paper appears to be the first in bringing and realizing the concept of group-based disentanglement in unsupervised VAE. The method was rigorously motivated and backed by theoretical analysis. The experiments were relatively thorough in terms of the datasets, metrics, and comparison disentanglement VAE methods considered. The improvement of metrics, while moderate to marginal in some cases, were consistent across most metrics and datasets.\n\nThe visuals of the cyclic representation shown in section 5.2 and Fig 5 are especially interesting, as well the latent space shown in Fig 3 demonstrating the benefit of the isomorphism loss.\n\nWeakness:\n\nSome of the metrics seem to be weaker than usually reported in the comparison methods. E.g., The MIG metrics reported for dSprites seemed to be rather low for each of the \"original\" VAE methods compared to those seen in literature (e.g., Fig 3 in [1]). Please clarify, especially since the margin of improvements in many metrics were less than 1 standard deviation of the statistics.\n\nWhile it is appreciated that the presented work considers a \"unsupervised\" learning of representations in comparison to existing works that uses the interaction with the environment as supervision to adopt the group-based definition, it would be still desirable to see a comparison of performance to understand what is the price to pay to go from a supervised to unsupervised setting. This is somewhat related to the comment above regarding avoiding using weak baselines.\n\nIn the final manuscript, please adjust the figures and tables such that they 1) appear in the order they're referred to in the text and 2) are spaced as close to the text description as possible. Right now Fig 4 and Fig 5 are referred to first, before Fig 3 and 2 are discussed in the text.\n",
          "summary_of_the_review": "This is an overall well written manuscript describing a novel method backed by in-depth theoretical analysis. The experiments were well designed and evaluations relatively thorough. There are some questions that can be addressed and improvements that can be made to the current manuscript, but overall I consider this to be an interesting work that will have good discussion potential for the community.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_AKvy"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_AKvy"
        ]
      },
      {
        "id": "P-WfzDb7UZt",
        "original": null,
        "number": 3,
        "cdate": 1636873194854,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636873194854,
        "tmdate": 1636986648686,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "jqUCcSVuzEG",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer AKvy",
          "comment": "Thanks for your extremely valuable comments. We appreciate the positive feedback for our work and are happy to include the suggested changes into the rebuttal version. We are looking forward to further discussion.\n\n1. The MIG metrics reported for dSprites seemed to be rather low for each of the \"original\" VAE methods compared to those seen in literature\n\nThe settings of our experiments are different from [a] (or other baselines). We follow [b] (The baseline performance is in Figure 13 of [b] appendix) to conduct experiments and verify our method taking into account the impact of hyperparameters and random seeds. This evaluation setting is well accepted in the literature (e.g.,Fig. 5 in [b], Fig.3 in [c]). Our method is effective across different hyperparameters and random seeds and better than other methods, including [a]. \n\nWe found out that the value of MIG of $\\beta$-TCVAE is incorrect. The values here are 0.15\u00b10.067 (original) and 0.21\u00b10.093 (groupified), we have updated the values in the rebuttal version (Highlighted with blue, please check).\n\n2. A comparison of performance to understand what is the price to pay to go from a supervised to unsupervised setting.\n\nThanks for this interesting suggestion. We fully agree with your insight and will add such an experiment in the final version later, as we finalize the experiments. \n\n3. Adjust the figures and tables.\n\nWe have followed your suggestions in the rebuttal version (Please check).\n\n[a] Chen, Ricky TQ, et al. \"Isolating sources of disentanglement in variational autoencoders.\" NeurIPS 2018.\n\n[b] Locatello, Francesco, et al. \"Challenging common assumptions in the unsupervised learning of disentangled representations.\" ICML, 2019.\n\n[c] Rhodes, Travers, and Daniel D. Lee. \"Local Disentanglement in Variational Auto-Encoders Using Jacobian $ L_1 $ Regularization.\" NeurIPS 2021.\n\n[d] H\u00e4lv\u00e4, Hermanni, et al. \"Disentangling Identifiable Features from Noisy Data with Structured Nonlinear ICA.\" NeurIPS 2021.\n\nIf the reference [1] in your comment is not the reference [a] here, please let us know."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "uupnBszALZu",
        "original": null,
        "number": 4,
        "cdate": 1636873767743,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636873767743,
        "tmdate": 1637418755333,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "L8MHP_cTued",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer YTd1 (Part 2)",
          "comment": "5. Commutative Lie Group VAE should be more clearly discussed and compared in the main paper.\n\nWe thank the reviewer for providing this related work, we will add the following discussions to the final version. First of all, we emphasize the building of a theoretical framework to use group definition towards solving unsupervised disentanglement. However, [a] 's contribution is more about a constraint in the latent space  ([a] proposed using the commutative Lie group to represent the latent space), which does not come from group-based definition. Specifically, the definitions of the group in our paper are different from [a]. We use the autoencoder's encoding action and decoding process to define the elements of the group, as Fig. 1 shows. In addition, our framework is compatible with their proposed method. The Lie group representation can be applied to extend our framework into a lie group version. \n\nIn addition, the motivation of \u201ccommutative\u201d of ours and [a] are totally different. In [a], the \u201ccommutative\u201d is motivated by the decomposition requirement of the Lie group (one-parameter subgroup decomposition in Sec. 4.2 in [a]). Each subgroup characterizes one kind of variation. Ours \u201ccommutative\u201d is motivated by the symmetry requirement of the group-based definition, which comes from the nature of factors.\n\n6. More results on some real-world datasets.\n\nThanks for the constructive suggestion. We have added the qualitative experimental results on CelebA in the rebuttal version (as done in [a, g]) and highlighted with blue, please check. We nevertheless want to emphasize here that we have conducted extensive experiments (as commented by other reviewers AKvy & PA1y) to demonstrate that the improvements of our theoretical framework are cross-data sets, cross-models, and cross hyper-parameters, our model\u2019s effectiveness is already validated with strong evidence. \n\n7. The paper does not show its stand-alone performance.\n\nThank you for raising the question about the stand alone performance of our model. This is an interesting concern. However, we can not fully agree with the comment that our method should outperform the existing SOTA models by being directly added to a vanilla VAE. The reasons are twofold:\n\n- According to our  theoretical framework, in order to achieve disentanglement, our method requires the minimization of TC,as stated in the second paragraph of section 4, \u201cthe minimization of total correlation is\u2026 to fulfill the data constraint to some extent for the unsupervised setting.\u201d  Technically, please note that since the baseline models propose a substituted objective for Total correlation minimization, they directly add their constraint to a vanilla VAE. However, we propose complementary techniques to those models rather than a substituted one, thus using our new constraints alone does not make sense.\n- Many other techniques are also based on the existing disentanglement models or regularization. For example,  Lie group VAE [ a] without hessian penalty [c] can not outperform the existing SOTA models (Tab. 4 in [a] showing that DCI 19.7, but BetaTCVAE DCI 35), and [b] requires beta-VAE objective function to work.\n\nHaving said that, we do believe it is an interesting ablation study. Therefore, we conduct and compare it (i.e., being directly added to a vanilla VAE) with Lie group VAE (without Hessian penalty) and vanilla VAE on dsprites. From the table below we observe that our model works even on vanilla VAE and is comparable to the Lie group VAE (without hessian penalty), under the same experiment setting as Lie group VAE.\n\n\n| Models | DCI | MIG |\n| :-----:| :----: | :----: |\n| vanilla VAE | 0.081\u00b10.041 | 0.078\u00b10.064 |\n| Lie group VAE | 0.197\u00b10.046 | **0.254\u00b10.061** |\n| Groupified       | **0.221\u00b10.042**  | 0.182\u00b1 0.041 |                \n\nNote that the results of the Lie group VAE report here is the best result after tuning the hyperparameters (see Tab.3 in [a]). Due to the time limitation, we use the default parameters of groupified vanilla VAE.\n\n[a] Xinqi Zhu, Chang Xu, and Dacheng Tao. Commutative lie group vae for disentanglement learning. ICML, 2021. \n\n[b] Shao, Huajie, et al. \"ControlVAE: Controllable variational autoencoder.\" ICML, 2020.\n\n[c] Peebles, William, et al. \"The hessian penalty: A weak prior for unsupervised disentanglement.\" ECCV, 2020.\n\n[d] Rhodes, Travers, and Daniel D. Lee. \"Local Disentanglement in Variational Auto-Encoders Using Jacobian $ L_1 $ Regularization.\" NeurIPS, 2021.\n[e] Ross, Andrew Slavin, and Finale Doshi-Velez. \"Benchmarks, Algorithms, and Metrics for Hierarchical Disentanglement.\" ICML, 2021.\n\n[f] Wei, Yuxiang, et al. \"Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation.\" ICCV, 2021.\n\n[g] Chen, Ricky TQ, et al. \"Isolating sources of disentanglement in variational autoencoders.\" NeurIPS, 2018.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "8Fabrp_Ld5l",
        "original": null,
        "number": 5,
        "cdate": 1636874351292,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636874351292,
        "tmdate": 1636979575196,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "L8MHP_cTued",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer YTd1 (Part 1)",
          "comment": "Thanks for providing constructive comments. Your concerns are addressed as below. We are looking forward to further discussion.\n\n1. The introduction of $\\Phi$ group is unnecessarily complex.\n\nWe regret to note that \u201c$\\Phi$ group is a direct product of m cyclic groups of order n\u201d is a misunderstanding of $\\Phi$. We would like to present a better way of understanding  $\\Phi$ from why we introduce $\\Phi$, and make further clarification about why we introduce those concepts.\n\nPlease note that our definition of $\\Phi$ is: 1) its elements are the process of encoding, action on latent space, and decoding. 2) the operation is function composition (As Fig. 1 shows). This group is determined/defined by the parameters of our VAE. In our paper, the group $G$ to which we constrain $\\Phi$ to be isomorphic is m cyclic groups of order n. (which is enforced by the optimization of the proposed isomorphism loss). Therefore, in order to make the above clear, the introduction of $\\Phi$ group is quite necessary.\n\nAlthough in group theory they are all isomorphic groups, we believe it is necessary to use these concepts in our paper for rigorousness and simplicity. \n- For the theoretical part, $Z$ is not a group (there is no operator), and we need to use 'congruence class modulo n' (same elements as in G) to define the elements in $Z$. We use 'element-wise addition' to define the group action $G$ on $Z$. In addition,  the \"action of rotating each circle by step $2\u03c0/n$\" is unfriendly to those readers unfamiliar with group theory (unlike 'element-wise addition', therefore here we use 'element-wise addition').\n- For the implementation part, we chose the n-th root unity group to instantiate these elements in $Z$ (whose elements are differentiable). The additive group of integers modulo n does not show up in the main manuscript. This name is only used to introduce $Z/nZ$ in the appendix.\n\n2. The mention of 'n-th dihedral group' is not informative, or even misleading.\n\nWe regret that you have a misunderstanding, and we have clarified it in the rebuttal version. Here we introduce Dihedral groups in an attempt to help readers better understand the relation between the elements of a group and disentangled factors. \nSpecifically, by \u201cinspiring by Dihedral groups,\u201d we mean the rotation and flip can be analogies to the factors in disentanglement, and they are also analogies to the generators in our settings. In addition, flip and rotation are transformations and also the elements of the group. These facts inspire us to construct the group $\\Phi$.\n\n3. The Abel loss and Order loss does not solve the unidentifiability problem that motivates this method.\n\nWe agree with you that it still falls into the unidentifiability problem proposed in Locatello et al. 2019b (Note that we never claim that we solved the problem), but we believe our theoretical framework and method have made a big step towards solving the problem from the perspective of group-based definition.  The reasons are twofold: \n- please note that the unidentifiability problem does not exist in the group-based definition. (Only identifiable solutions exist in the group-based definition) \n- Our theoretical framework presents three sufficient conditions for the definition. We proposed a solution that satisfied two of these conditions (model and structure) and met a necessary condition for the third one (data). \n\nIn addition, the unidentifiability problem is believed to be quite challenging in unsupervised settings. Note that the recent progress of unsupervised disentanglement also did not solve the unidentifiability problem [a, b, d, e, f] and even not discuss it [a, f]. They all fall into the unidentifiability problem. \n\n4. I don't think $\\varphi \\varphi^{\u22121}=e$ can replace $\\varphi \\varphi^{n\u22121}=e$, in the Order loss.\n\nThanks for pointing out\uff0cthe notation here is misleading. We have clarified this in the rebuttal version (highlighted with blue, please check). By using $\\varphi^{-1}$, we intended to represent the process in Fig. 7 (b), which is not the inverse element but an approximation of $\\varphi^{n-1}$. When the autoencoder can do the reconstruction well\uff0cthis approximation holds.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "_M1RJt3uEy",
        "original": null,
        "number": 6,
        "cdate": 1636875551576,
        "mdate": 1636875551576,
        "ddate": null,
        "tcdate": 1636875551576,
        "tmdate": 1636875551576,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "ts_zv_9FZUy",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer M8Aa (Part 2)",
          "comment": "2. Clarification of the Isomorphism Loss:\n\n- The role of Isomorphism Loss\n\nIntuitively speaking, we constrain the isomorphism relation to make the VAEs satisfy the symmetry requirement of the definition as the additional constraint for VAEs, and the symmetry requirement (commutative and cyclic) come from the nature of factors. \n\n- Its novelty and comparison to related work\n\nTo the best of our knowledge, we are the first to construct a group in this way (elements are image-to-image mapping and determined by the parameters of VAEs) and the first to derive a loss from the group-based definition in an unsupervised setting.\nThe way we construct group $\\Phi$: As shown in Fig. 1 (a), the group element $\\varphi$ (which is an image-to-image mapping) is the VAE-based mapping that encodes an image to latent space and conducts latent space operation then decodes back to the image space. The group operation is a mapping composition. \n\nThe derivation of Isomorphism Loss: As proved in Theorems 1, 2, 3, the sufficient condition of group structure constraint is that $\\Phi$ is isomorphic to $G$, and this condition is satisfied when Isomorphism Loss (Abel loss and Order loss) is optimized, which serves as an additional constraint on the original VAE-based systems to help optimize disentanglement.\n\nFrom the perspective of isomorphism loss, the most related work is [c] to the best of our knowledge. They proposed using the commutative Lie group to represent the latent space. They also require that the group used is commutative. However, we  have the following fundamental differences: \n\n1) Their group does not come from the group-based definition. Their target is to capture the variation in latent space by a Lie group rather than introducing the group-based definition into the unsupervised setting. \n2) Their group\u2019s elements are matrix, but ours are image-to-image mappings. Their group operation is matrix multiplication, but ours is function composition.\n\n- why it brings this performance gain\n\nThe intuition is that it provides a cycle consistency constraint under these different transformations (group action) of latent space to avoid the collapse of the manifold in the latent space (and reduce the solution space), as  Fig. 3 shows. \n\nIn other words, the symmetry property introduced by the isomorphism loss eliminates the distortion of the manifold in the latent space. Specifically, the Able loss constrains the latent space to stay unchanged under the exchange of two different $\\varphi_i$, The Order Loss constrains the latent space is cyclic under n compositions of $\\varphi_i$. The commutative and cyclic properties reduce the solution space of the VAEs. These properties come from the nature of disentangled factors.\n\n\n[a] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf, \u00a8 and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In ICML, 2019.\n\n[b] Learning group structure and disentangled representations of dynamical environments. NeurIPS, 2020.\n\n[c] Xinqi Zhu, Chang Xu, and Dacheng Tao. Commutative lie group vae for disentanglement learning. ICML, 2021. \n\nWe would appreciate it if you would let us know whether the above clarifies the novelty of our work and changes your assessment of the significance of our work.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "5_2-PLwtGP8",
        "original": null,
        "number": 7,
        "cdate": 1636875695145,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636875695145,
        "tmdate": 1636979801949,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "ts_zv_9FZUy",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer M8Aa (Part 1)",
          "comment": "Thanks for providing constructive comments. Your concerns are addressed below. We are looking forward to further discussion.\n\n\nWe do not intend to mislead the reviewers, and thanks for pointing out the ambiguity problem in our writing. We regret that it caused your misunderstanding.  We have the word \"towards\" in our title and other words like \"in an effort to,\" \"practically applicable,\" etc., in the Abstract & Introduction to indicate that we have not completely solved this problem.  We have made the point clearer in the abstract & introduction in the rebuttal version (highlighted with blue, please check) by stating we derive two sufficient conditions for the model and group constraints and a necessary condition for the data constraint. We believe that our rebuttal version can address the ambiguous problem well. \n\nIn the following, we provide the explanations and clarifications for your actionable feedback. We believe that our explanation can help you better understand our contribution.\n\n1. Which theoretical guarantees the proposed framework actually provides.\n\nWe are the first to theoretically explore one promising approach (group-based definition) for unsupervised disentanglement, which we believe is very valuable to this community, as also commented by reviewer AKvy. \n\nSpecifically, our **theoretical guarantee** is:  our theoretical framework narrows down the landscape of the problem by deriving three sufficient conditions of the group-based definition. Two of them (model, group constraint) can be sufficiently satisfied, and we derive a necessary condition for the third one (data constraint). Please note that, as pointed out in [b], it is not straightforward to reconcile the probabilistic inference methods with the group-based definition framework. Therefore, it is challenging to derive constraints from the group-based definition directly, but with our proposed framework, two of the constraints can be sufficiently satisfied, and the last/third one has a necessary condition. We believe (also pointed out by Reviewer AKvy) that it will have a good discussion potential for the community since our theoretical framework has the potential to derive sufficient conditions for the group-based definition in an unsupervised setting.  \n\nIn addition, our framework not only demonstrates that the previous VAE-based methods only consider the data constraint (which is in line with the \u201cimpossible\u201d conclusion in [a]), but also, it offers the answer of \u201cwhat inductive bias these VAEs lack\u201d from the perspective of group-based definition: i.e., model and group constraints and data constraint (existing VAEs only satisfy a necessary condition for data constraint).\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "lzDAsIyCNJ5",
        "original": null,
        "number": 8,
        "cdate": 1636876189025,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636876189025,
        "tmdate": 1636986629055,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "GqC25fhEjkn",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer PA1y",
          "comment": "Thanks for reviewing our paper and appreciating our idea. Your concerns are addressed   below. We are looking forward to further discussion.\n\n1. the MIG score is much lower than the result illustrated in the literature $\\beta$-TCVAE and ControlVAE.\n\nOur experimental settings are different from $\\beta$-TCVAE and ControlVAE. We follow [a] to conduct experiments and verify our method taking into account the impact of hyperparameters and random seeds. This evaluation setting is well accepted in the literature (e.g., Fig. 5 in [b], Fig.3 in [c]). Our method is effective across different hyperparameters and random seeds and better than other methods, including $\\beta$-TCVAE and ControlVAE.\n\nWe found out that the value of MIG of $\\beta$-TCVAE is incorrect. The values here are 0.15\u00b10.067 (original) and 0.21\u00b10.093 (groupified), we have updated the values in the rebuttal version (Highlighted with blue, please check).\n\n2. Missing some latest works on disentanglement.\n\nWe thank the reviewer for providing these related works, and we have followed your suggestion to discuss these works in our rebuttal version and highlighted with blue. We will conduct the experiments on ControlVAE [d] (2020), since 1) Taming VAE(2018), [e] (2018) and [f] (2019) are works before 2020 and 2) MS VAE [h] (2020) [1] is a preprint and did not release their code. Due to the time limit, here we only present the results on dsprites (under the following hyper-parameter settings and random seeds). For results on other datasets, we  will add them in the final version as we finalize these experiments. As the following table shows,   our model still works on ControlVAE. \n\nC_max 15, 25, 35\n\nRandom seeds: 0-9\n\ngroupified: True, False\n\nTotal: 3x10x2 = 60 runs\n\nOther parameters follow the best setting in [d]\n\n| Model | DCI | BetaVAE | MIG | FactorVAE |\n| :-------:| -----: | :----------: | :----: | :-------------: |\n| Original | 0.30 \u00b10.114    |       0.83\u00b10.084    |    0.15\u00b1**0.062**      |     0.62\u00b10.086  |\n| Groupified |  **0.46** \u00b1 **0.093**      |    **0.92**\u00b1**0.061**    |    **0.26**\u00b10.096     |     **0.73**\u00b1**0.072** |\n\nmean\u00b1std\n\n\n\n\n[a] Locatello, Francesco, et al. \"Challenging common assumptions in the unsupervised learning of disentangled representations.\" international conference on machine learning. PMLR, 2019.\n\n[b] Rhodes, Travers, and Daniel D. Lee. \"Local Disentanglement in Variational Auto-Encoders Using Jacobian $ L_1 $ Regularization.\" NeurIPS, 2021.\n\n[c] H\u00e4lv\u00e4, Hermanni, et al. \"Disentangling Identifiable Features from Noisy Data with Structured Nonlinear ICA.\" NeurIPS, 2021.\n\n[d] Shao, Huajie, et al. \"ControlVAE: Controllable variational autoencoder.\" ICML, 2020.\n\n[e] Lezama, J. Overcoming the disentanglement vs reconstruction trade-off via jacobian supervision. ICLR 2018.\n\n[f] Kim, M., Wang, Y., Sahu, P., & Pavlovic, V. Bayes-factor-vae: Hierarchical bayesian deep auto-encoder models for factor disentanglement. ICCV 2019.\n\n[h] Srivastava, Akash, Yamini Bansal, Yukun Ding, Cole Hurwitz, Kai Xu, Bernhard Egger, Prasanna Sattigeri, Josh Tenenbaum, David D. Cox, and Dan Gutfreund. Improving the Reconstruction of Disentangled Representation Learners via Multi-Stage Modelling. Preprint 2020.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "FjI_FyEW29w",
        "original": null,
        "number": 9,
        "cdate": 1636998197977,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636998197977,
        "tmdate": 1636998246465,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "YgPqNctmyd",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Looking forward to the discussion",
          "comment": "Dear all reviewers,\n\nWe are looking forward to further discussion.\n\nBest,\n\nPaper10 Authors"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "k9oF8dLjksh",
        "original": null,
        "number": 10,
        "cdate": 1637252853889,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637252853889,
        "tmdate": 1637255264236,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "5_2-PLwtGP8",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Discussion about the theoretical claims",
          "comment": "I thank the authors for clarifying most of their theoretical claims, and I see that this is has been taken into account in the new version of the submission. However, I still have a concern about the claim that *\"[the authors] offer an option for the inductive bias that existing VAE-based models lack\"*, which hasn't been modified. Indeed, I would argue that the main problem of existing VAE models (and the main motivation for searching good inductive biases) is that they also only satisfy a necessary condition for the data constraint. In order to support the claim that the submission *\"offers an option for the inductive bias these VAEs lack\u201d*, it would be necessary to also show experimentally or theoretically that the data constraint is solved. I would advise the authors to also rephrase this claim\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_M8Aa"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_M8Aa"
        ]
      },
      {
        "id": "7QYFn_s6TRG",
        "original": null,
        "number": 11,
        "cdate": 1637255351153,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637255351153,
        "tmdate": 1637511833017,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "k9oF8dLjksh",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Response to \"Discussion about the theoretical claims\"",
          "comment": "Thanks for your response and pointing this out, we agree with you that the claim still causes ambiguity here. We rephrased the claim to make the point clearer: \u201cprovides options from the perspective of group-based definition for the additional inductive bias in existing VAE-based methods.\u201d We have also modified the corresponding claim (also other similar claims) in our rebuttal version (highlighted with blue), please check.\n\nSpecifically, the modifications include (highlighted with blue, please check): \n- We changed the claim that \"However, it has been proved that ...\" into \"However, these models lack theoretical ...\" in Abstract.\n- We changed the claim that \"With the first two of the conditions satisfied ...\" into \"With the first two of the conditions satisfied ...as an additional inductive bias for ...\" in Abstract.\n- We changed the claim (also other similar claims) that \u201c... for the inductive bias these VAEs lack\u201d into \u201c \u2026 for the additional inductive bias in existing VAE-based methods\u201d in Introduction and also delete \u201cexisting VAE-based models only consider...\u201d in Sec.4. \n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "pUH21fc6m4k",
        "original": null,
        "number": 12,
        "cdate": 1637305786186,
        "mdate": 1637305786186,
        "ddate": null,
        "tcdate": 1637305786186,
        "tmdate": 1637305786186,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "8Fabrp_Ld5l",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "More Discussion",
          "comment": "1. About my \u2018misunderstanding\u2019: I understood $\\phi$ is defined on data space. But since it relies on the autoencoder mapping, which is already commonly used in all autoencoder-based disentanglement models, the only component that distinguishes $\\phi$ from existing models is the cyclic structure of the latent code, which is a product of cyclic groups, therefore I used \u2018$\\phi$ group is a direct product of m cyclic groups of order n\u2019 in my comment.\nThe overcomplexity problem: I don\u2019t remember the 'congruence class modulo n\u2019 has been mentioned in [a]. What new things have been brought here by re-describing the cyclic group into the \u2018integer modulo n\u2019 group? I still think the authors should stick to the group description in [a] which is simple and easy to understand, instead of reintroducing this group again with 'congruence classes modulo n\u2019 in Sec. 3.2, as it only causes me extra confusion though I am already familiar with the group definition in [a].\n\n3. (1) Yes, I understood it well. The group-based definition directly used the correspondence between the ground-truth group structure (in W state space) and the representation. So technically this can only \u2018describe\u2019 how the ideal solution should look like but it does not provide any method to achieve it.\n(2) After more detailed thinking, I want to make my point clearer.\nThe key to solving the unidentifiability in [b] is the condition (ii) in your proposed Theorem 1, which links the group $\\phi$ acting on data to the ground-truth generators. However, this data constraint modeling in this paper is solely modeled by the total correlation minimization. The inductive biases introduced in this paper are the group structure constraint and model constraint, which I believe are valuable by themselves, but not related to the inductive biases meant by [b] as they cannot contribute to the unidentifiable problem. I believe it is totally fine the model in this paper cannot solve the unidentifiability problem which is as you said very challenging, but the motivation and discussion in this paper try to convey that this paper proposes \u2018inductive biases\u2019 that are wanted by [b] to solve the unidentifiability problem, but the actual \u2018biases\u2019 (group structure constraint and model constraint) are not related to this problem, while the only key point \u2018data constraint\u2019 that can solve this unidentifiability problem is modeled in the same way as usual methods by the total correlation minimization. Therefore I believe the authors should emphasize the benefits of the group structure constraint and model constraint themselves, rather than trying to describe them as the lacked inductive biases mentioned in [b].\n\n[a] Irina Higgins, David Amos, David Pfau, Sebastien Racaniere,Loic Matthey, Danilo Rezende, Alexander Lerchner. \"Towards a Definition ofDisentangled Representations\u201d 2018\n[b] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf, \u00a8 and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In ICML, 2019."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_YTd1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_YTd1"
        ]
      },
      {
        "id": "xGG_XrSUIbF",
        "original": null,
        "number": 13,
        "cdate": 1637324726727,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637324726727,
        "tmdate": 1637511784135,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "pUH21fc6m4k",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Response to \"More Discussion\"",
          "comment": "1. We fully understand your response to \"the misunderstanding\", the statement makes sense in this context.\n\n2. Thanks for pointing out that we should stick to the group description in [a]. With this description, we indeed can describe our idea without \"congruence classes modulo n\". We have removed \"congruence classes modulo n\" in our rebuttal version (highlighted with blue in Sec 3.2 & 4.1, please check).\n\n3. Thanks for your detailed thinking, we fully agree with your insight and followed your suggestion that \"we should emphasize the benefits of the group structure constraint and model constraint themselves, rather than trying to describe them as lacking inductive biases\". We appreciate your insightful comments and suggestions that help a lot to make the point more precise.\n\nSpecifically, the modifications include (highlighted with blue, please check): \n- We removed \"congruence classes modulo n\" in Sec 3.2 & 4.1.\n- We changed the claim (also other similar claims) that \u201c... for the inductive bias these VAEs lack\u201d into \u201c \u2026 for the additional inductive bias in existing VAE-based methods\u201d in Introduction and also delete \u201cexisting VAE-based models only consider...\u201d in Sec.4. \n\n- We add \u201cThe inductive bias encourages ...\u201d in Introduction to emphasize the benefits (role) of the group structure constraint and model constraint.\n- We changed the claim that \"However, it has been proved that ...\" into \"However, these models lack theoretical ...\" in Abstract.\n- We changed the claim that \"With the first two of the conditions satisfied ...\" into \"With the first two of the conditions satisfied ...as an additional inductive bias for ...\" in Abstract.\n\n[a] Irina Higgins, David Amos, David Pfau, Sebastien Racaniere,Loic Matthey, Danilo Rezende, Alexander Lerchner. \"Towards a Definition of disentangled Representations\u201d 2018\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "8mLMqXVzU7y",
        "original": null,
        "number": 18,
        "cdate": 1637566981919,
        "mdate": 1637566981919,
        "ddate": null,
        "tcdate": 1637566981919,
        "tmdate": 1637566981919,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "k9oF8dLjksh",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Looking forward to hearing from you",
          "comment": "Dear Reviewer M8Aa,\n\nWe want to send you a friendly reminder for the discussion, since the second stage of discussion will be soon concluded.\n\nWe thank you again for your valuable comments, and we would appreciate it if you could reconsider the evaluation of our work based on our response. We are happy to extend our response if you have any other concerns left.\n\nThanks."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "kZjIE5BV0z",
        "original": null,
        "number": 19,
        "cdate": 1637567019889,
        "mdate": 1637567019889,
        "ddate": null,
        "tcdate": 1637567019889,
        "tmdate": 1637567019889,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "pUH21fc6m4k",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Looking forward to hearing from you",
          "comment": "Dear Reviewer YTd1,\n\nWe want to send you a friendly reminder for the discussion, since the second stage of discussion will be soon concluded.\n\nWe thank you again for your valuable comments, and we would appreciate it if you could reconsider the evaluation of our work based on our response. We are happy to extend our response if you have any other concerns left.\n\nThanks."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "1tAz5caQ_G",
        "original": null,
        "number": 21,
        "cdate": 1637567611166,
        "mdate": 1637567611166,
        "ddate": null,
        "tcdate": 1637567611166,
        "tmdate": 1637567611166,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "GqC25fhEjkn",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Looking forward to hearing from you",
          "comment": "Dear Reviewer PA1y,\n\nWe want to send you a friendly reminder for the discussion, since the second stage of discussion will be soon concluded.\n\nWe thank you again for your valuable comments, and we are happy to extend our response if you have any other concerns left.\n\nThanks."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "fv_Kblfh41S",
        "original": null,
        "number": 22,
        "cdate": 1637567677993,
        "mdate": 1637567677993,
        "ddate": null,
        "tcdate": 1637567677993,
        "tmdate": 1637567677993,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "jqUCcSVuzEG",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Looking forward to hearing from you",
          "comment": "Dear Reviewer AKvy,\n\nWe want to send you a friendly reminder for the discussion, since the second stage of discussion will be soon concluded.\n\nWe thank you again for your valuable comments, and we are happy to extend our response if you have any other concerns left.\n\nThanks."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "BwM-ErgRt_f",
        "original": null,
        "number": 23,
        "cdate": 1637582208250,
        "mdate": 1637582208250,
        "ddate": null,
        "tcdate": 1637582208250,
        "tmdate": 1637582208250,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "7QYFn_s6TRG",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Discussion about the theoretical claims - Final",
          "comment": "I thank the authors for these clarifications. I appreciate part of the modifications but I would still join Reviewer YTd1 in raising concerns about the multiple references to an **inductive bias**. The following paragraph Reviewer YTd1 reflects my opinion better than I could write myself\n\n*(2) After more detailed thinking, I want to make my point clearer. The key to solving the unidentifiability in [b] is the condition (ii) in your proposed Theorem 1, which links the group \u03d5 acting on data to the ground-truth generators. However, this data constraint modeling in this paper is solely modeled by the total correlation minimization. The inductive biases introduced in this paper are the group structure constraint and model constraint, which I believe are valuable by themselves, but not related to the inductive biases meant by [b] as they cannot contribute to the unidentifiable problem. I believe it is totally fine the model in this paper cannot solve the unidentifiability problem which is as you said very challenging, but the motivation and discussion in this paper try to convey that this paper proposes \u2018inductive biases\u2019 that are wanted by [b] to solve the unidentifiability problem, but the actual \u2018biases\u2019 (group structure constraint and model constraint) are not related to this problem, while the only key point \u2018data constraint\u2019 that can solve this unidentifiability problem is modeled in the same way as usual methods by the total correlation minimization. Therefore I believe the authors should emphasize the benefits of the group structure constraint and model constraint themselves, rather than trying to describe them as the lacked inductive biases mentioned in [b].*"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_M8Aa"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_M8Aa"
        ]
      },
      {
        "id": "ZhvcCiJ-0w6",
        "original": null,
        "number": 24,
        "cdate": 1637583454278,
        "mdate": 1637583454278,
        "ddate": null,
        "tcdate": 1637583454278,
        "tmdate": 1637583454278,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "_M1RJt3uEy",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Discussion about the isomorphism loss - Final",
          "comment": "After reading the author response and re-reading the submission, I still have trouble understanding the intuition of why the isomorphism loss brings this performance gain. Here is a detailed look at the authors explanations \n- *The intuition is that it provides a cycle consistency constraint under these different transformations (group action) of latent space* -> The mention of \"cycle consistency\" is quite vague\n- *to avoid the collapse of the manifold in the latent space (and reduce the solution space), as Fig. 3 shows.* -> What does the collapse of the manifold mean?\n- *In other words, the symmetry property introduced by the isomorphism loss eliminates the distortion of the manifold in the latent space.* ->  Again, what does the collapse of the manifold mean?\n- *Specifically, the Able loss constrains the latent space to stay unchanged under the exchange of two different \n\u03c6_i, The Order Loss constrains the latent space is cyclic under n compositions of \u03c6_i* -> this is mainly paraphrasing the mathematical definitions, which I understood\n- *The commutative and cyclic properties reduce the solution space of the VAEs.* -> To me this also looks like paraphrasing\n- *These properties come from the nature of disentangled factors.* -> This does not bring me much information\n\n**Final opinion**\nTo me, getting an intuitive idea of why the isomorphism loss brings this performance gain is crucial, as the isomorphism loss is the only model improvement of the submission.  With my current level of understanding, I don't feel confident in recommending acceptance, and I will stick to my reject recommendation. Considering (i) that the other reviewers have a more positive opinion (ii) that the authors have been responsive in the discussion phase, it is certainly possible that I didn't understand some central aspects of the submission, and I have changed my confidence level to 2. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_M8Aa"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_M8Aa"
        ]
      },
      {
        "id": "n1w3jGNnFSm",
        "original": null,
        "number": 25,
        "cdate": 1637588612499,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637588612499,
        "tmdate": 1637600280169,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "ZhvcCiJ-0w6",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer M8Aa",
          "comment": "Thanks for your response and re-reading. The intuition provided above is derived by inferring from the experimental results, which may make you feel that it is not so strong. Here, we would like to provide another  \u201ctop-level\u201d intuitive understanding of why isomorphism loss works. The group theory based definition is motivated by the symmetry property of the data. Based on this, we introduce the isomorphism loss to reorganize the latent space to reflect this symmetry property. Starting from the group-based theoretical definition of disentanglement, we get a quite practical isomorphism loss derived by mathematics proof step by step.  In our last response, we provide some detailed analysis on the experimental results, trying to bring a \u201cbottom-level\u201d intuitive understanding. Maybe it is better to have an intuitive understanding between the top-level and the bottom-level, which we can not provide currently. This may further indicate the value of our theoretical framework, it is hard to directly propose such an intuitive idea only from the perspective of practice itself."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "gt0uQkgn83o",
        "original": null,
        "number": 26,
        "cdate": 1637588650886,
        "mdate": 1637588650886,
        "ddate": null,
        "tcdate": 1637588650886,
        "tmdate": 1637588650886,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "BwM-ErgRt_f",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer M8Aa",
          "comment": "Thanks for your response. In our rebuttal version, by \u201cinductive bias\u201d, we mean we provide additional constraints from the perspective of group-based definition.  These constraints are valuable itself, which was agreed by Reviewer YTd1 and AKvy. We regret that the word \u201cinductive bias\u201d used here still caused your concern. We would like to modify the \u201cinductive bias\u201d into \u201cadditional constraints\u201d in our final paper if it is necessary to address this ambiguity.  Our main contribution is introducing the group-base definition into the unsupervised setting, it is valuable itself and has good discussion potential for the community (which was agreed by Reviewer AKvy). although we made an ambiguous statement on the \u201cinductive bias\u201d. (This has already been modified and can be further refined, and it will not hurt our main contribution).\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "xJPGwR8vG0",
        "original": null,
        "number": 27,
        "cdate": 1637600465542,
        "mdate": 1637600465542,
        "ddate": null,
        "tcdate": 1637600465542,
        "tmdate": 1637600465542,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "P-WfzDb7UZt",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Post-discussion comments",
          "comment": "Thanks for your response.\n\nThanks for clarifying on the discrepancy between the results and some published works may be due to the fact that the results were reported across multiple hyperparameters and random seeds. I did look into [b] and feel that there were still some difference in the reported numbers between Table 2 of the presented manuscript and those in [b], e.g., the numbers reported on Car3D in Fig 3 of [b] and dSprits and 3DShape in Fig 13 of [b]. I understand that such fluctuation may be due to the use of different hyperparameters and random seeds, which highlights the importance of a better way of presenting the \"improvement\" obtained by the groupified methods in this paper -- especially since the improvement was in most cases within 1 std away from the mean. If the results were highly overlapping and the improvement was less than the fluctuation itself, it'd not be very convincing. \n\nIn such cases, it may be more beneficial if the authors could control the factors causing fluctuation and then demonstrate the improvement of the presented method, e.g., comparing differences at different levels of regularization parameters.\n\nI also carefully read through the discussion among the authors and the other reviewers. While I personally are in favor of this manuscript, I understood the concerns of the other reviewers and could not strongly vouch against their concerns, especially on the points of inductive biases and the intuition of isomorphism loss. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_AKvy"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_AKvy"
        ]
      },
      {
        "id": "DFEvjTAHHC3",
        "original": null,
        "number": 28,
        "cdate": 1637600533434,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637600533434,
        "tmdate": 1637601391055,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "ZhvcCiJ-0w6",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Further clarification of Isomorphism Loss",
          "comment": "Although the reviewer has given the final option, considering that our clarification is still not clear enough for the reviewer to fully understand our intuition, we would like to give some further clarification on the intuition. We want to highlight Fig.5 in our rebuttal version.\n\n**What does the collapse of the manifold mean, and why cycle consistency makes sense?**\n\nBefore we answer the question, we first present the detailed process of plotting Fig. 5.\n\nIn order to have a global view of how the isomorphism loss leads to performance gains, we use the following way to visualize the learned latent space. For a given trained VAE-based model, 1) from the dataset, we select a set of images by traversing all the GT values of three different factors (x-position, y-position, scale), and images are labeled with continuously varying colors.  2) feed the images into the VAE-based model and get the corresponding representations. 3) for each representation we plot a point in 3D space,  whose 3D coordinates are the corresponding representation of x-position, y-position, scale, and whose color is the labeled color of the corresponding image. Thus we plot a manifold for the given VAE-based model as shown in Fig. 5. \n\n\nAn ideally disentangled case should be a cube with continuous color variation. \nWe find the manifold of those original VAEs with poor performance has the following phenomenon:  the manifold is not totally disorganized, but only has clear shape distortion.  (as shown in Fig. 5, (e,f,g,h)). If the distortion can be tackled, we can achieve better disentanglement. \n\nHowever, for the manifolds of Groupified VAEs, the distortion is suppressed. Considering that the physical meaning of isomorphism loss is that the latent space after different transformations should be well aligned, which is a kind of cycle consistency constraint (under different transformations, the results should be the same). The distorted spaces are usually asymmetry, resulting in a large value of the loss. By minimizing the loss, this phenomenon is well suppressed.  This observation is quite common, and we provide a lot of results in appendix L.\n\nTherefore, we derived a **conclusion**: the loss itself is a kind of cycle consistency to avoid the collapse of the manifold (latent space), which leads to factors entanglement in VAE models, as shown in Fig. 5 in the rebuttal version. \n\nSince this observation indeed helps us understand these gains, we believe this intuition can also help reviewers and other readers to intuitively understand why the isomorphism loss brings this performance gain."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "yKUtbTObxe4",
        "original": null,
        "number": 29,
        "cdate": 1637609963708,
        "mdate": 1637609963708,
        "ddate": null,
        "tcdate": 1637609963708,
        "tmdate": 1637609963708,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "lzDAsIyCNJ5",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "About MIG",
          "comment": "Thanks a lot for adding the latest work and adding new experiments. I still have two questions about the MIG score of ControlVAE and FactorVAE below.\n\n[1] I am wondering if you use the MIG of TensorFlow version from [a] to measure ControlVAE and FactorVAE?\n[2] Why do you set the C_max of ControlVAE to 15, 25, and 35? "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_PA1y"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_PA1y"
        ]
      },
      {
        "id": "g-7-Xf2FYO4",
        "original": null,
        "number": 31,
        "cdate": 1637664302426,
        "mdate": 1637664302426,
        "ddate": null,
        "tcdate": 1637664302426,
        "tmdate": 1637664302426,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "yKUtbTObxe4",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Response to \"About MIG\"",
          "comment": "Thanks for your response, for MIG, yes, we use the TensorFlow version from [a] (https://github.com/google-research/disentanglement_lib) and for ControlVAE, we use the model code provided by https://github.com/shj1987/ControlVAE-ICML2020 (official implementation). We follow the default setting as C_max = 25 (in Disentangling/main.py).  We follow [a] to set the hyperparameter interval to 10. \n\nWe can add more results under different settings if you think it is necessary.  For example,  C_max = XX, as reported in the ControlVAE paper. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "bny0BKbtQjn",
        "original": null,
        "number": 32,
        "cdate": 1637664725569,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637664725569,
        "tmdate": 1637664901969,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "pUH21fc6m4k",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Further response to \"More Discussion\"",
          "comment": "Dear Reviewer YTd1:\n\nAfter further thinking about your concern, we rewrote substantially (highlighted with blue) to address your concern that \u201cthe motivation and discussion in this paper try to convey that this paper proposes \u2018inductive biases\u2019 that are wanted by [b] to solve the unidentifiability problem\u201d. We believe that our modified version may address this \u201cambiguous problem\u201d well.\n\n- Remove the ambiguous statements related to the unidentifiability problem in Abstract & Introduction\n- Add some clarifications on that our work still falls into the unidentifiability problem in Related Works.\n- Remove the paragraph before Sec. 4.1.\n- Add \u201cwe only provide \u2026 as future work\u201d as our limitation in Conclusion.\n\nWe want to highlight the value of our theoretical framework. \u201cthe key to solving the unidentifiability in [b] is the condition (ii)\u201d also reflects the value of our theoretical framework. From the group-definition perspective, we make this point clearer by providing a theoretical guarantee from our proposed Theorem 1. \n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "P4xQ-jL-QiN",
        "original": null,
        "number": 34,
        "cdate": 1637667904857,
        "mdate": 1637667904857,
        "ddate": null,
        "tcdate": 1637667904857,
        "tmdate": 1637667904857,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "xJPGwR8vG0",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer AKvy",
          "comment": "Thank you for your response, and constructive suggestion. Due to the time limitation, we would like to follow your suggestion to present our results in final version."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "skbNECSP9qL",
        "original": null,
        "number": 35,
        "cdate": 1637908912005,
        "mdate": 1637908912005,
        "ddate": null,
        "tcdate": 1637908912005,
        "tmdate": 1637908912005,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "bny0BKbtQjn",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Thanks for your response",
          "comment": "Thanks for your response. I believe these modifications make the paper clearer. I will increase the score to 6 weak accept."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_YTd1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_YTd1"
        ]
      },
      {
        "id": "iGejGgSbFuI",
        "original": null,
        "number": 36,
        "cdate": 1638261445057,
        "mdate": 1638261445057,
        "ddate": null,
        "tcdate": 1638261445057,
        "tmdate": 1638261445057,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "skbNECSP9qL",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Thank you for updating the score!",
          "comment": "Your feedback was extremely valuable and helped improve the paper substantially."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "a-S5tigiJJS",
        "original": null,
        "number": 1,
        "cdate": 1642696832495,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696832495,
        "tmdate": 1642696832495,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "YgPqNctmyd",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "The submission provides a theoretical framework on the learning of group-based disentanglement representations and proposes a novel method to learn such representations.\n\nThe reviewers appreciated the novel perspective of the paper in introducing the concept of group-based disentanglement in unsupervised VAE. Furthermore, the approach was considered to be soundly theoretically motivated and experiments to be extensive. There was a lively discussion between reviewers and authors about certain ambiguities in the manuscript; however, they seem to have been largely resolved to the reviewers' satisfaction.\n\nWhile there was a reviewer with a very low confidence recommending rejection, this paper brings an indisputably interesting novel perspective to the learning of unsupervised representations and I thus recommend acceptance."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "GqC25fhEjkn",
        "original": null,
        "number": 1,
        "cdate": 1635210335680,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635210335680,
        "tmdate": 1635210335680,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "YgPqNctmyd",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposed a new theoretical framework for achieving unsupervised representation disentanglement based on $n$-th dihedral group. Further, it proved three sufficient conditions on the model, group structure, and data respectively. Evaluations on multiple benchmark datasets demonstrate the proposed framework achieves better mean performance with smaller variances compared to some VAE models.",
          "main_review": "Pros:\n1. Proposed a new theoretical framework for achieving unsupervised representation disentanglement based on $n$-th dihedral group.\n2. Provided theoretical analysis for the proposed framework via model and group structure\n3. Conducted extensive experiments to verify the proposed framework\n\nCons:\n1. Missing some latest works on disentanglement. The related work section is a bit weak since it only discussed the VAE models before the year 2019. In fact, there are a lot of related works about disentangled representation learning after 2019, such as TamingVAE, ControlVAE, and other works [1,2,3,4].\n2. Please also compare the proposed framework with the baselines in recent work as mentioned above.\n\n3. In Table 1, for dSprites dataset, the MIG score is much lower than the result illustrated in the literature $\\beta$-TCVAE and ControlVAE. Please clarify and explain it. \n\n[1] Srivastava, Akash, Yamini Bansal, Yukun Ding, Cole Hurwitz, Kai Xu, Bernhard Egger, Prasanna Sattigeri, Josh Tenenbaum, David D. Cox, and Dan Gutfreund. \"Improving the Reconstruction of Disentangled Representation Learners via Multi-Stage Modelling.\" arXiv preprint arXiv:2010.13187 (2020). \n\n[2] Shao, H., Yao, S., Sun, D., Zhang, A., Liu, S., Liu, D., ... & Abdelzaher, T. (2020, November). ControlVAE: Controllable variational autoencoder. In International Conference on Machine Learning (pp. 8655-8664). PMLR.\n\n[3] Kim, M., Wang, Y., Sahu, P., & Pavlovic, V. (2019). Bayes-factor-vae: Hierarchical bayesian deep auto-encoder models for factor disentanglement. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 2979-2987).\n\n[4] Lezama, J. (2018, September). Overcoming the disentanglement vs reconstruction trade-off via jacobian supervision. In International Conference on Learning Representations.",
          "summary_of_the_review": "This paper proposed a new theoretical framework for achieving unsupervised representation disentanglement. My concern is that this work only compares it with the baselines before 2018, so it is hard to say whether it outperforms the recent baselines. ",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_PA1y"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_PA1y"
        ]
      },
      {
        "id": "ts_zv_9FZUy",
        "original": null,
        "number": 2,
        "cdate": 1635426751619,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635426751619,
        "tmdate": 1637583508457,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "YgPqNctmyd",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes a theoretical framework for unsupervised representation disentanglement (in the sense of Higgins et al.) based on three constraints (group structure, data and model). It further describes a learning method for satisfying these constraints, and experimentally shows some performance gain on traditional disentanglement metrics. \n\nMy intuitive understanding of the constraints is a follows:\n- the model constraint is satisfied by construction of the agent group, whose structure is transferred from the latent space by the model.\n- the group structure constraint ensures that this transfer is an isomorphism, and thus that the agent group is actually a group.\n- the data constraint ensures that the agent group actually matches the ground truth generative factors. \n\nPlease clarify if one of my statements is incorrect.",
          "main_review": "Strengths:\n- The mathematical part of the paper and experiments appear sound.\n- The isomorphism loss is to my knowledge novel, and seems to bring performance gain across datasets and models.\n\nWeaknesses:\n-  The paper is at best ambiguous and at worst misleading concerning whether or not it claims to solve the problem of unsupervised representation disentanglement\n    - *\"we offer an option [...] for the inductive bias that existing VAE-based models lack\"*\n    - *\"Finally, we provide a learning model based on the existing VAE-based methods in an effort to fulfill the three conditions.\"*\n    - *\"we are the first to provide a theoretical framework to make the formal group-based mathematical definition of disentanglement practically applicable to unsupervised representation disentanglement.\"*\n   - *\"making the learned representation conform to the group- based definition without relying on the environment\"* \n\n   But the paper never theoretically shows that the data constraint is actually satisfied, and instead relies on a necessary condition of the data constraint. This greatly undermines the theoretical contribution of the paper.\n- I find the mathematical writing to be overall quite obfuscated, which complicates assessment of the theoretical contribution.\n\nAs actionnable feedback:\n- I would recommend to be more open about which theoretical guarantees the proposed framework actually provides.\n- Could you please give intuitive explanations of the role of the isomorphism loss, its novelty and comparison to related work, and why it brings this performance gain? ",
          "summary_of_the_review": "I find this paper to be overselling the importance of its theoretical contribution, and will therefore recommend rejection. I am looking forward to discussion with the authors, and will reconsider my rating if the authors respond to the two points listed as actionnable feedback in my main review. In particular I'd like to hear more about the isomorphism loss. ",
          "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "3: reject, not good enough",
          "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_M8Aa"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_M8Aa"
        ]
      },
      {
        "id": "L8MHP_cTued",
        "original": null,
        "number": 3,
        "cdate": 1635506194278,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635506194278,
        "tmdate": 1637908810007,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "YgPqNctmyd",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper provides a theoretical framework on the learning of group-based disentanglement representations. It proposes a method to learn a cyclic group representation with the Abel loss and Order loss based on VAEs. Experiments validate the effectiveness of the proposed method on improving existing disentanglement VAE models.",
          "main_review": "Strengths:\n1. The authors provide a theoretical framework on the learning of group-based disentangled representation, which is a step forward to the more general guaranteed disentanglement learning.\n2. The experimental results validate the disentanglement effect of the 'Groupify' operation on multiple base models and synthetic datasets.\n3. The proposed technique is compatible with all the information-theory-based baseline models. \n\nWeaknesses:\n1. The introduction of $\\Phi$ group is unnecessarily complex. Do the authors mean the $\\Phi$ group is a direct product of $m$ cyclic groups of order n (or even simpler, $m$-dim Torus)? Is each cyclic subgroup represented by $(sin((2\\pi z))/n), cos((2\\pi z)/n))$, with z being the output of the VAE encoder? The generator of each subgroup is the action of rotating each circle by step $2\\pi/n$? I see no necessity of incorporating more concepts from Group Theory like 'congruence class modulo n', 'n-th root unity group', 'additive group of integers modulon'. To me they are all the same in the context of this paper.\n2. The mention of 'n-th dihedral group' is not informative, or even misleading. Dihedral groups are generated by only two subgroups, rotation and flip, which is not very related to the group used in this method (a product of m cyclic groups of the same order n).\n3. The Abel loss and Order loss can only encourage the representation to maintain a group structure of m-dim torus. However, this does not solve the unidentifiability problem that motivates this method (the impossibility of disentanglement mentioned in the abstract, introduction section, and the paragraph before Sec. 4.1) because the ground-truth correspondence between the group action on the world space and its action effect on the image space is still unknown. The real problem here is not how to learn a group that is isomorphic to an m-dim cyclic group, but how to make sure the learned cyclic subgroups correspond to the interpretable variations shown in the image space. This paper proposes a method to learn a cyclic-group representation, which is valuable by itself in some way, but it still falls into the unidentifiability problem proposed in Locatello et al. 2019b.\n4. I don't think $\\phi\\phi^{-1}=e$ can replace $\\phi\\phi^{n-1}=e$ in the Order loss. $\\phi\\phi^{n-1}=e$ ensures the cyclic structure of order n, but $\\phi\\phi^{-1}=e$ holds in any group with arbitrary order. The equation $\\phi^{-1}=\\phi^{n-1}$ holds when the cyclic structure has **already been learned**, but $\\phi\\phi^{-1}=e$ alone cannot help the learning of a cyclic structure.\n5. It looks like the proposed VAE model is a special case of the Commutative Lie Group VAE proposed in https://arxiv.org/abs/2106.03375 with the learned group representation being replaced by a pre-defined direct product of cyclic groups. They should be more clearly discussed and compared in the main paper.\n6. Currently only the synthetic datasets provided in the open-sourced disentanglement library (https://github.com/google-research/disentanglement_lib) are used in the experiments. More results on some real-world datasets like CelebA should be provided to validate the model's generalization ability.\n7. The paper only shows the effectiveness of the method as an incremental technique to improve the existing disentanglement models, but does not show its stand-alone performance. If the proposed constraints are effective enough, they should outperform the existing SOTA models by being directly added to a vanilla VAE (as all the baseline models do). ",
          "summary_of_the_review": "I believe the theoretical contribution of this paper is valuable to the group-based disentanglement learning (or general disentanglement learning) community, but I still have concerns listed in the main review section. I currently rate this paper as weak reject, but I will be glad to improve my score if my concerns are properly solved.\n\n======\n\nAfter rebuttal:\nThanks for the authors' response to my comments. I think a large part of my concerns has been addressed. I will increase the score to 6 weak accept.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_YTd1"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_YTd1"
        ]
      },
      {
        "id": "jqUCcSVuzEG",
        "original": null,
        "number": 4,
        "cdate": 1635883846248,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635883846248,
        "tmdate": 1635883846248,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "YgPqNctmyd",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper presents an unsupervised approach to achieve group-based disentangled representation learning, as opposed to existing environment-based approaches. A theoretical framework for the group-based VAE was proposed, and implementations of the group and isomorphism reported. Experiments were performed on four common benchmark datasets, with comparisons to several representative VAE-based disentanglement framework (beta-vae, annealVAE, factorVAE, beta-TCVAE), demonstrating the groupified VAE achieves better disentanglement in many metrics. Qualitative evaluations demonstrate the cyclic representation space learned by the groupified VAE.",
          "main_review": "Strength:\n\nThis paper appears to be the first in bringing and realizing the concept of group-based disentanglement in unsupervised VAE. The method was rigorously motivated and backed by theoretical analysis. The experiments were relatively thorough in terms of the datasets, metrics, and comparison disentanglement VAE methods considered. The improvement of metrics, while moderate to marginal in some cases, were consistent across most metrics and datasets.\n\nThe visuals of the cyclic representation shown in section 5.2 and Fig 5 are especially interesting, as well the latent space shown in Fig 3 demonstrating the benefit of the isomorphism loss.\n\nWeakness:\n\nSome of the metrics seem to be weaker than usually reported in the comparison methods. E.g., The MIG metrics reported for dSprites seemed to be rather low for each of the \"original\" VAE methods compared to those seen in literature (e.g., Fig 3 in [1]). Please clarify, especially since the margin of improvements in many metrics were less than 1 standard deviation of the statistics.\n\nWhile it is appreciated that the presented work considers a \"unsupervised\" learning of representations in comparison to existing works that uses the interaction with the environment as supervision to adopt the group-based definition, it would be still desirable to see a comparison of performance to understand what is the price to pay to go from a supervised to unsupervised setting. This is somewhat related to the comment above regarding avoiding using weak baselines.\n\nIn the final manuscript, please adjust the figures and tables such that they 1) appear in the order they're referred to in the text and 2) are spaced as close to the text description as possible. Right now Fig 4 and Fig 5 are referred to first, before Fig 3 and 2 are discussed in the text.\n",
          "summary_of_the_review": "This is an overall well written manuscript describing a novel method backed by in-depth theoretical analysis. The experiments were well designed and evaluations relatively thorough. There are some questions that can be addressed and improvements that can be made to the current manuscript, but overall I consider this to be an interesting work that will have good discussion potential for the community.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Reviewer_AKvy"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Reviewer_AKvy"
        ]
      },
      {
        "id": "FjI_FyEW29w",
        "original": null,
        "number": 9,
        "cdate": 1636998197977,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636998197977,
        "tmdate": 1636998246465,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "YgPqNctmyd",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Official_Comment",
        "content": {
          "title": "Looking forward to the discussion",
          "comment": "Dear all reviewers,\n\nWe are looking forward to further discussion.\n\nBest,\n\nPaper10 Authors"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper10/Authors"
        ]
      },
      {
        "id": "a-S5tigiJJS",
        "original": null,
        "number": 1,
        "cdate": 1642696832495,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696832495,
        "tmdate": 1642696832495,
        "tddate": null,
        "forum": "YgPqNctmyd",
        "replyto": "YgPqNctmyd",
        "invitation": "ICLR.cc/2022/Conference/Paper10/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "The submission provides a theoretical framework on the learning of group-based disentanglement representations and proposes a novel method to learn such representations.\n\nThe reviewers appreciated the novel perspective of the paper in introducing the concept of group-based disentanglement in unsupervised VAE. Furthermore, the approach was considered to be soundly theoretically motivated and experiments to be extensive. There was a lively discussion between reviewers and authors about certain ambiguities in the manuscript; however, they seem to have been largely resolved to the reviewers' satisfaction.\n\nWhile there was a reviewer with a very low confidence recommending rejection, this paper brings an indisputably interesting novel perspective to the learning of unsupervised representations and I thus recommend acceptance."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}