{
  "id": "afoV8W3-IYp",
  "original": "KWSqSyzQtuH",
  "number": 68,
  "cdate": 1632875426396,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875426396,
  "tmdate": 1676330690295,
  "ddate": null,
  "content": {
    "title": "RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning",
    "authorids": [
      "~Xiaojian_Ma1",
      "~Weili_Nie1",
      "~Zhiding_Yu1",
      "~Huaizu_Jiang1",
      "~Chaowei_Xiao2",
      "~Yuke_Zhu1",
      "~Song-Chun_Zhu1",
      "~Anima_Anandkumar1"
    ],
    "authors": [
      "Xiaojian Ma",
      "Weili Nie",
      "Zhiding Yu",
      "Huaizu Jiang",
      "Chaowei Xiao",
      "Yuke Zhu",
      "Song-Chun Zhu",
      "Anima Anandkumar"
    ],
    "keywords": [
      "visual relational reasoning",
      "representation learning",
      "systematic generalization"
    ],
    "abstract": "Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "ma|relvit_conceptguided_vision_transformer_for_visual_relational_reasoning",
    "pdf": "/pdf/9ae93c86cdada9dfdeef3cf2c7ee77363fe51c7b.pdf",
    "one-sentence_summary": "We propose a novel concept-feature dictionary to enable two new concept-guided auxiliary tasks, which largely improve the model performances on visual relational reasoning, especially for systematic generalization.",
    "data": "",
    "_bibtex": "@inproceedings{\nma2022relvit,\ntitle={RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning},\nauthor={Xiaojian Ma and Weili Nie and Zhiding Yu and Huaizu Jiang and Chaowei Xiao and Yuke Zhu and Song-Chun Zhu and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=afoV8W3-IYp}\n}",
    "venue": "ICLR 2022 Poster",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "afoV8W3-IYp",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 4,
    "directReplyCount": 4,
    "revisions": true,
    "replies": [
      {
        "id": "hh-YdYbsWdW",
        "original": null,
        "number": 1,
        "cdate": 1635169292673,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635169292673,
        "tmdate": 1635169375772,
        "tddate": null,
        "forum": "afoV8W3-IYp",
        "replyto": "afoV8W3-IYp",
        "invitation": "ICLR.cc/2022/Conference/Paper68/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This work studies visual relationship reasoning and proposes a model based on the recent contrastive learning methods DINO and EsViT. Compared with the two predecessors, a feature dictionary is proposed to provide the online updated concept features for the teacher model instead. Correspondingly, two auxiliary tasks are introduced to drive semantic concept learning. Experiments on two large benchmarks show the efficacy of the proposed method. And extensive ablations are also made to probe the model components.",
          "main_review": "Pros:\n+ Non-trivial motivation to marry the ViT and contrastive learning to advance visual relation reasoning.\n+ Impressive improvements upon the HICO and GQA in the common setting and generalization setting.\n+ Detailed ablations of the proposed method.\n+ Discussion and results about the ViT and visual relation reasoning are inspiring.\n\nCons:\n- Novelty: Model and auxiliary follow the existing works like DINO, EsViT, Liu et.al. 2010, Wang et al., 2021c, etc. Though the feature dictionary is somewhat new, it is still essential to fully discuss the difference between it and the works including memory modules like Wu et al., 2018, Tian et al., 2020, even they are not based on Transformer.\n- Following the above point, this work proposes a useful pipeline by following the existing works, the empirical contribution is more visible than the insight novelty.\n- Compared with EsViT, the improvements are much more marginal, due to the less design difference.\n- Using the images from HICO, the HOI concepts are complex and suitable for relation reasoning. But HOI images usually contain multiple HOI human-verb-object pairs in one image. For example, in HICO, one image usually has more than 2 h-o pairs. So, during the training, what is the effect of the overlapped concepts in contrastive learning, e.g., the largely overlapped pos-neg concepts. Thus, if applying the proposed work to the instance-level concepts (one human-verb-object pair bbox), what would happen? \n- The comparison about the \"complex pre-processing stage\". This point seems unconvincing. In my opinion, these previous settings are a kind of design not for \"efficiency\". And if the proposed method has an obvious advantage on efficiency, supportive results should also be provided. So does the claim about the long-tailed data distribution. In HICO, the long-tailed distribution is very severe. Thus, a discussion about the alleviation of the long-tailed bias would make this work more solid.\n- \"reasoning hops can be found in Table 3.\" --> \"Figure 3\"?\n- \"using ViTs in eliminating the need for external object detectors\", this discussion is very interesting and inspiring, could you please give more analyses combining the discoveries of the proposed model?",
          "summary_of_the_review": "Overall, this is an interesting work to dig into the performance of ViT on visual relation reasoning and semantic concept learning. Some non-trivial contributions are made to reveal the effect of contrastive learning designs. And the results on the two benchmarks look promising. However, though studying a very different problem, the method contribution seems weak compared to the previous works. Thus, I think the rating of 6 is suitable for this version. I will adjust my rating according to the response and the discussions from the authors and the other reviewers.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "No.",
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper68/Reviewer_RiEe"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper68/Reviewer_RiEe"
        ]
      },
      {
        "id": "QC0rBfllYkO",
        "original": null,
        "number": 2,
        "cdate": 1635762929396,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635762929396,
        "tmdate": 1635762929396,
        "tddate": null,
        "forum": "afoV8W3-IYp",
        "replyto": "afoV8W3-IYp",
        "invitation": "ICLR.cc/2022/Conference/Paper68/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper extends the training objective of EsViT to make it better suited to visual relationship detection. This is done by replacing the teacher's features used in both local and global losses with features taken from a queue. The queue is populated with teacher features extracted from previous images. Furthermore, the queue is partitioned by \"concepts\", e.g. HOI labels or VQA concepts, so that the current student image can be compared with a previous teacher image that contains the same concept instead of any random image.\n\nThe novel queue-based formulation is tested in various experiments, both for visual relationship detection and visual question answering. Individual components of the method are ablated and discussed, namely the dictionary of queues, the EsViT-style local loss, and other hyperparameters.",
          "main_review": "Strengths:\n- Well-organized paper, it's clear which previous works the method is built upon and what the novel contributions are\n- A relatively simple implementation based on queues of previously-processed images is shown to improve empirical results on two challenging datasets (state-of-the-art mAP in HICO recognition, good results in GQA).\n- Well-detailed appendix, all the info about data pipeline and training hyperparameters should ensure full reproducibility of the results\n- Well-designed ablation studies that cover most of the questions that one can have while reading the description of the method\n\nWeaknesses:\n- The paper lacks a proper comparison with MoCo which is only mentioned briefly in section 4 (K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, \u201cMomentum Contrast for Unsupervised Visual Representation Learning,\u201d, https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html\n). In this paper, in fact, many of the concepts of RelViT are already present, almost with the same names, e.g. \"At the core of our approach is maintaining the dictionary as a queue of data samples\" (MoCo, section 3.2). The main differences are: MoCo uses a single queue and not many concept-based queues, the queue contains images and not features, teacher features are computed on the fly using the current parameters for the teacher.\n- The additional losses in the method are described as weakly-supervised, e.g. \"we use concepts as a source of weak supervision\" in section 4. I think this might be a misrepresentation of the method. In fact, for both use cases demonstrated in the paper, the concepts were extracted from the fully-supervised training labels of each dataset. For example, the basic training signal in HICO already includes the concepts used to build the dictionary, therefore it can not be represented as a form of weak supervision. I suggest rephrasing the relevant paragraphs so that it's clear that RelViT is not a weakly-supervised method but it makes better use of the strong labels present in a dataset.\n- Since the model is applied to relationship detection, I suggest adding experiments on VRD (C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei, \u201cVisual Relationship Detection with Language Priors\u201d http://arxiv.org/abs/1608.00187) and Unrel (J. Peyre, I. Laptev, C. Schmid, and J. Sivic, \u201cWeakly-supervised learning of visual relations\u201d http://arxiv.org/abs/1707.09472), which are two popular datasets for visual relationship detection\n- One ablation study on the concept-based queues is missing: using a single queue of old images. This would verify whether a replay buffer alone (akin to keeping track of multiple teachers) already brings an improvement over using the features of the current teacher only. However, maybe this is already present and corresponds to the \"None\" label in the plot, it's not very clear from the text whether None means no queue at all or a single non-concept-based queue.\n\nQuestions:\n- $L_{main}$ is left unspecified in section 2.2, it would help if the loss was detailed in full for the tasks used in the experiments. At a first read, I missed the fact that HICO is used for a recognition task (just predict the list of relationships present in an image) and not for detection (detect object bounding boxes and predict relationships). It might be my bias because I'm used to seeing HICO for detection, but also because $L_{main}$ for HICO is not properly specified.\n- For the local task, patch-based features from the student's image and the teacher's queued image are greedily matched in the loss. Is the match guaranteed to be 1:1? Is this desirable or not desirable? Can you add a discussion about this?\n- Is \"RelViT\" in figure 6 only RelViT or RelViT+EsVit? If it's only RelViT, without the local loss, why is it so good at matching local regions?\n\nMinor points:\n- Equation numbers would help reference specific parts of the paper\n- From section 2.2 it's not clear which part of the method corresponds to the experiment rows in table 1: EsViT, RelViT, RelViT+EsViT. The confusion arises also in other parts of the paper. I suggest making the similarities and differences explicit, e.g. with the table below\n\n|              | $L_{global}$ | $L_{local}$ | Compare `student(aug(img))` with |\n|--------------|--------------|-------------|----------------------------------|\n| DINO         | x            |             | `teacher(aug(img))`              |\n| EsViT        | x            | x           | `teacher(aug(img))`              |\n| RelViT       | x            |             | `queues[concept(img)].pop()`     |\n| RelVit+EsViT | x            | x           | `queues[concept(img)].pop()`     |",
          "summary_of_the_review": "This is a good paper in terms of ideas, writeup and experiments. My suggestions and critics would require just minor adjustments to the paper that can be done in the rebuttal phase. Unless major issues arise in the discussion with other reviewers, e.g. the method is not as novel as I thought due to related works that I am not aware of, my recommendation is to accept.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper68/Reviewer_dCw4"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper68/Reviewer_dCw4"
        ]
      },
      {
        "id": "nYGyK_IPM-o",
        "original": null,
        "number": 3,
        "cdate": 1635828335660,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635828335660,
        "tmdate": 1638095148901,
        "tddate": null,
        "forum": "afoV8W3-IYp",
        "replyto": "afoV8W3-IYp",
        "invitation": "ICLR.cc/2022/Conference/Paper68/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes a new concept-guided approach for visual relational reasoning. Particularly, it uses a concept-feature dictionary to store and retrieve the feature of an image by the corresponding visual concepts. Then, the concept-guided features are integrated into the training process with the global and local auxiliary tasks. Experimental results on HICO and GQA in both original and systematic settings evidenced the effectiveness of the proposed method against existing works.",
          "main_review": "Pros:\n- The proposed method is generic and easily incorporated to existing methods in a plug-and-play manner. \n- Empirical results on HICO and GQA show consistent improvements in performance over baselines. The proposed method also achieves some good results in systematic generalization settings.\n\n\nCons:\n\n(1) Proposed method\n- The scientific novelty of this paper seems limited. The overall structure of the proposed model is almost similar to the \"self-distillation with no labels\" approach (Caron et al.,2021) and EsViT (Li et al.,2021). The contribution is mostly about the use of their concept-feature dictionary as a visual knowledge base to support retrieving information on the fly. The use of the dictionary is more similar to the concept of knowledge base used by [1] rather than memory bank mechanisms as the proposed method only reads out information from the dictionary without refining the information stored in the dictionary.\n- The authors should explain more how they pick the suitable image feature f from a set of a queue Q_i. It is an important step in the proposed model, and the description for their two sampling tactics is not detailed enough.\n- Although the reported numbers look great, it is hard to explain why the visual concepts given by ViT are good for visual relation reasoning. ViT does not actually rely on actual \u201cvisual objects\u201d, for example, RoI poolings in Faster R-CNN. I think it is more about the more powerful representation learning pretrained on a large number of images which benefits all tasks regardless. \n\n(2) Experiments\n- My main concern about experimental results is that the improvement of the proposed model may come from the Visual Transformer backbone. It would be fair if the authors also evaluate their concept-guided approach with the ResNet-101 backbone or actual visual objects given by Faster R-CNN.\n- Even using the state-of-the-art ViT backbone, the performance of the proposed model is still worse than state-of-the-art methods on the GQA dataset in both original and systematic settings, and only has sightly improvement on the HICO dataset in the original setting.\n- CLOSURE probably is a better benchmark for evaluating systematic generalization.\n\n[1] Hudson DA, Manning CD. Compositional attention networks for machine reasoning. ICLR 2019.\n\n[2] Bahdanau, Dzmitry, et al. \"Closure: Assessing systematic generalization of clevr models.\" arXiv preprint arXiv:1912.05783 (2019).",
          "summary_of_the_review": "The paper appears to be limited in novelty and contribution.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper68/Reviewer_fbWL"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper68/Reviewer_fbWL"
        ]
      },
      {
        "id": "3euNyrUUNKF",
        "original": null,
        "number": 1,
        "cdate": 1642696836007,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696836007,
        "tmdate": 1642696836007,
        "tddate": null,
        "forum": "afoV8W3-IYp",
        "replyto": "afoV8W3-IYp",
        "invitation": "ICLR.cc/2022/Conference/Paper68/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "Three experts reviewed the paper and all recommended acceptance. Based on the reviewers' feedback, the decision is to recommend the paper for acceptance. However, the reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. For example, the discussion about or comparison with related works, clarity of the writing, suggested experiments, etc. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "hh-YdYbsWdW",
        "original": null,
        "number": 1,
        "cdate": 1635169292673,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635169292673,
        "tmdate": 1635169375772,
        "tddate": null,
        "forum": "afoV8W3-IYp",
        "replyto": "afoV8W3-IYp",
        "invitation": "ICLR.cc/2022/Conference/Paper68/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This work studies visual relationship reasoning and proposes a model based on the recent contrastive learning methods DINO and EsViT. Compared with the two predecessors, a feature dictionary is proposed to provide the online updated concept features for the teacher model instead. Correspondingly, two auxiliary tasks are introduced to drive semantic concept learning. Experiments on two large benchmarks show the efficacy of the proposed method. And extensive ablations are also made to probe the model components.",
          "main_review": "Pros:\n+ Non-trivial motivation to marry the ViT and contrastive learning to advance visual relation reasoning.\n+ Impressive improvements upon the HICO and GQA in the common setting and generalization setting.\n+ Detailed ablations of the proposed method.\n+ Discussion and results about the ViT and visual relation reasoning are inspiring.\n\nCons:\n- Novelty: Model and auxiliary follow the existing works like DINO, EsViT, Liu et.al. 2010, Wang et al., 2021c, etc. Though the feature dictionary is somewhat new, it is still essential to fully discuss the difference between it and the works including memory modules like Wu et al., 2018, Tian et al., 2020, even they are not based on Transformer.\n- Following the above point, this work proposes a useful pipeline by following the existing works, the empirical contribution is more visible than the insight novelty.\n- Compared with EsViT, the improvements are much more marginal, due to the less design difference.\n- Using the images from HICO, the HOI concepts are complex and suitable for relation reasoning. But HOI images usually contain multiple HOI human-verb-object pairs in one image. For example, in HICO, one image usually has more than 2 h-o pairs. So, during the training, what is the effect of the overlapped concepts in contrastive learning, e.g., the largely overlapped pos-neg concepts. Thus, if applying the proposed work to the instance-level concepts (one human-verb-object pair bbox), what would happen? \n- The comparison about the \"complex pre-processing stage\". This point seems unconvincing. In my opinion, these previous settings are a kind of design not for \"efficiency\". And if the proposed method has an obvious advantage on efficiency, supportive results should also be provided. So does the claim about the long-tailed data distribution. In HICO, the long-tailed distribution is very severe. Thus, a discussion about the alleviation of the long-tailed bias would make this work more solid.\n- \"reasoning hops can be found in Table 3.\" --> \"Figure 3\"?\n- \"using ViTs in eliminating the need for external object detectors\", this discussion is very interesting and inspiring, could you please give more analyses combining the discoveries of the proposed model?",
          "summary_of_the_review": "Overall, this is an interesting work to dig into the performance of ViT on visual relation reasoning and semantic concept learning. Some non-trivial contributions are made to reveal the effect of contrastive learning designs. And the results on the two benchmarks look promising. However, though studying a very different problem, the method contribution seems weak compared to the previous works. Thus, I think the rating of 6 is suitable for this version. I will adjust my rating according to the response and the discussions from the authors and the other reviewers.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "No.",
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper68/Reviewer_RiEe"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper68/Reviewer_RiEe"
        ]
      },
      {
        "id": "QC0rBfllYkO",
        "original": null,
        "number": 2,
        "cdate": 1635762929396,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635762929396,
        "tmdate": 1635762929396,
        "tddate": null,
        "forum": "afoV8W3-IYp",
        "replyto": "afoV8W3-IYp",
        "invitation": "ICLR.cc/2022/Conference/Paper68/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper extends the training objective of EsViT to make it better suited to visual relationship detection. This is done by replacing the teacher's features used in both local and global losses with features taken from a queue. The queue is populated with teacher features extracted from previous images. Furthermore, the queue is partitioned by \"concepts\", e.g. HOI labels or VQA concepts, so that the current student image can be compared with a previous teacher image that contains the same concept instead of any random image.\n\nThe novel queue-based formulation is tested in various experiments, both for visual relationship detection and visual question answering. Individual components of the method are ablated and discussed, namely the dictionary of queues, the EsViT-style local loss, and other hyperparameters.",
          "main_review": "Strengths:\n- Well-organized paper, it's clear which previous works the method is built upon and what the novel contributions are\n- A relatively simple implementation based on queues of previously-processed images is shown to improve empirical results on two challenging datasets (state-of-the-art mAP in HICO recognition, good results in GQA).\n- Well-detailed appendix, all the info about data pipeline and training hyperparameters should ensure full reproducibility of the results\n- Well-designed ablation studies that cover most of the questions that one can have while reading the description of the method\n\nWeaknesses:\n- The paper lacks a proper comparison with MoCo which is only mentioned briefly in section 4 (K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, \u201cMomentum Contrast for Unsupervised Visual Representation Learning,\u201d, https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html\n). In this paper, in fact, many of the concepts of RelViT are already present, almost with the same names, e.g. \"At the core of our approach is maintaining the dictionary as a queue of data samples\" (MoCo, section 3.2). The main differences are: MoCo uses a single queue and not many concept-based queues, the queue contains images and not features, teacher features are computed on the fly using the current parameters for the teacher.\n- The additional losses in the method are described as weakly-supervised, e.g. \"we use concepts as a source of weak supervision\" in section 4. I think this might be a misrepresentation of the method. In fact, for both use cases demonstrated in the paper, the concepts were extracted from the fully-supervised training labels of each dataset. For example, the basic training signal in HICO already includes the concepts used to build the dictionary, therefore it can not be represented as a form of weak supervision. I suggest rephrasing the relevant paragraphs so that it's clear that RelViT is not a weakly-supervised method but it makes better use of the strong labels present in a dataset.\n- Since the model is applied to relationship detection, I suggest adding experiments on VRD (C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei, \u201cVisual Relationship Detection with Language Priors\u201d http://arxiv.org/abs/1608.00187) and Unrel (J. Peyre, I. Laptev, C. Schmid, and J. Sivic, \u201cWeakly-supervised learning of visual relations\u201d http://arxiv.org/abs/1707.09472), which are two popular datasets for visual relationship detection\n- One ablation study on the concept-based queues is missing: using a single queue of old images. This would verify whether a replay buffer alone (akin to keeping track of multiple teachers) already brings an improvement over using the features of the current teacher only. However, maybe this is already present and corresponds to the \"None\" label in the plot, it's not very clear from the text whether None means no queue at all or a single non-concept-based queue.\n\nQuestions:\n- $L_{main}$ is left unspecified in section 2.2, it would help if the loss was detailed in full for the tasks used in the experiments. At a first read, I missed the fact that HICO is used for a recognition task (just predict the list of relationships present in an image) and not for detection (detect object bounding boxes and predict relationships). It might be my bias because I'm used to seeing HICO for detection, but also because $L_{main}$ for HICO is not properly specified.\n- For the local task, patch-based features from the student's image and the teacher's queued image are greedily matched in the loss. Is the match guaranteed to be 1:1? Is this desirable or not desirable? Can you add a discussion about this?\n- Is \"RelViT\" in figure 6 only RelViT or RelViT+EsVit? If it's only RelViT, without the local loss, why is it so good at matching local regions?\n\nMinor points:\n- Equation numbers would help reference specific parts of the paper\n- From section 2.2 it's not clear which part of the method corresponds to the experiment rows in table 1: EsViT, RelViT, RelViT+EsViT. The confusion arises also in other parts of the paper. I suggest making the similarities and differences explicit, e.g. with the table below\n\n|              | $L_{global}$ | $L_{local}$ | Compare `student(aug(img))` with |\n|--------------|--------------|-------------|----------------------------------|\n| DINO         | x            |             | `teacher(aug(img))`              |\n| EsViT        | x            | x           | `teacher(aug(img))`              |\n| RelViT       | x            |             | `queues[concept(img)].pop()`     |\n| RelVit+EsViT | x            | x           | `queues[concept(img)].pop()`     |",
          "summary_of_the_review": "This is a good paper in terms of ideas, writeup and experiments. My suggestions and critics would require just minor adjustments to the paper that can be done in the rebuttal phase. Unless major issues arise in the discussion with other reviewers, e.g. the method is not as novel as I thought due to related works that I am not aware of, my recommendation is to accept.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper68/Reviewer_dCw4"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper68/Reviewer_dCw4"
        ]
      },
      {
        "id": "nYGyK_IPM-o",
        "original": null,
        "number": 3,
        "cdate": 1635828335660,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635828335660,
        "tmdate": 1638095148901,
        "tddate": null,
        "forum": "afoV8W3-IYp",
        "replyto": "afoV8W3-IYp",
        "invitation": "ICLR.cc/2022/Conference/Paper68/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes a new concept-guided approach for visual relational reasoning. Particularly, it uses a concept-feature dictionary to store and retrieve the feature of an image by the corresponding visual concepts. Then, the concept-guided features are integrated into the training process with the global and local auxiliary tasks. Experimental results on HICO and GQA in both original and systematic settings evidenced the effectiveness of the proposed method against existing works.",
          "main_review": "Pros:\n- The proposed method is generic and easily incorporated to existing methods in a plug-and-play manner. \n- Empirical results on HICO and GQA show consistent improvements in performance over baselines. The proposed method also achieves some good results in systematic generalization settings.\n\n\nCons:\n\n(1) Proposed method\n- The scientific novelty of this paper seems limited. The overall structure of the proposed model is almost similar to the \"self-distillation with no labels\" approach (Caron et al.,2021) and EsViT (Li et al.,2021). The contribution is mostly about the use of their concept-feature dictionary as a visual knowledge base to support retrieving information on the fly. The use of the dictionary is more similar to the concept of knowledge base used by [1] rather than memory bank mechanisms as the proposed method only reads out information from the dictionary without refining the information stored in the dictionary.\n- The authors should explain more how they pick the suitable image feature f from a set of a queue Q_i. It is an important step in the proposed model, and the description for their two sampling tactics is not detailed enough.\n- Although the reported numbers look great, it is hard to explain why the visual concepts given by ViT are good for visual relation reasoning. ViT does not actually rely on actual \u201cvisual objects\u201d, for example, RoI poolings in Faster R-CNN. I think it is more about the more powerful representation learning pretrained on a large number of images which benefits all tasks regardless. \n\n(2) Experiments\n- My main concern about experimental results is that the improvement of the proposed model may come from the Visual Transformer backbone. It would be fair if the authors also evaluate their concept-guided approach with the ResNet-101 backbone or actual visual objects given by Faster R-CNN.\n- Even using the state-of-the-art ViT backbone, the performance of the proposed model is still worse than state-of-the-art methods on the GQA dataset in both original and systematic settings, and only has sightly improvement on the HICO dataset in the original setting.\n- CLOSURE probably is a better benchmark for evaluating systematic generalization.\n\n[1] Hudson DA, Manning CD. Compositional attention networks for machine reasoning. ICLR 2019.\n\n[2] Bahdanau, Dzmitry, et al. \"Closure: Assessing systematic generalization of clevr models.\" arXiv preprint arXiv:1912.05783 (2019).",
          "summary_of_the_review": "The paper appears to be limited in novelty and contribution.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper68/Reviewer_fbWL"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper68/Reviewer_fbWL"
        ]
      },
      {
        "id": "3euNyrUUNKF",
        "original": null,
        "number": 1,
        "cdate": 1642696836007,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696836007,
        "tmdate": 1642696836007,
        "tddate": null,
        "forum": "afoV8W3-IYp",
        "replyto": "afoV8W3-IYp",
        "invitation": "ICLR.cc/2022/Conference/Paper68/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "Three experts reviewed the paper and all recommended acceptance. Based on the reviewers' feedback, the decision is to recommend the paper for acceptance. However, the reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. For example, the discussion about or comparison with related works, clarity of the writing, suggested experiments, etc. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}