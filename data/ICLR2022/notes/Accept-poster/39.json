{
  "id": "-ngwPqanCEZ",
  "original": "TM7YjUV4NBE",
  "number": 39,
  "cdate": 1632875424220,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875424220,
  "tmdate": 1697934972454,
  "ddate": null,
  "content": {
    "title": "Representation-Agnostic Shape Fields",
    "authorids": [
      "~Xiaoyang_Huang1",
      "~Jiancheng_Yang3",
      "~Yanjun_Wang1",
      "~Ziyu_Chen2",
      "~Linguo_Li1",
      "~Teng_Li2",
      "~Bingbing_Ni3",
      "~Wenjun_Zhang3"
    ],
    "authors": [
      "Xiaoyang Huang",
      "Jiancheng Yang",
      "Yanjun Wang",
      "Ziyu Chen",
      "Linguo Li",
      "Teng Li",
      "Bingbing Ni",
      "Wenjun Zhang"
    ],
    "keywords": [
      "shape embedding",
      "3D deep learning",
      "shape classification and segmentation"
    ],
    "abstract": "3D shape analysis has been widely explored in the era of deep learning. Numerous models have been developed for various 3D data representation formats, e.g., MeshCNN for meshes, PointNet for point clouds and VoxNet for voxels. In this study, we present Representation-Agnostic Shape Fields (RASF), a generalizable and computation-efficient shape embedding module for 3D deep learning. RASF is implemented with a learnable 3D grid with multiple channels to store local geometry. Based on RASF, shape embeddings for various 3D shape representations (point clouds, meshes and voxels) are retrieved by coordinate indexing. While there are multiple ways to optimize the learnable parameters of RASF, we provide two effective schemes among all in this paper for RASF pre-training: shape reconstruction and normal estimation. Once trained, RASF becomes a plug-and-play performance booster with negligible cost. Extensive experiments on diverse 3D representation formats, networks and applications, validate the universal effectiveness of the proposed RASF. Code and pre-trained models are publicly available\\footnote{\\url{https://github.com/seanywang0408/RASF}}.",
    "pdf": "/pdf/3dd6d2a26afbbd24597f42af0a520b3b487fc536.pdf",
    "one-sentence_summary": "We propose a  generalizable and computation-efficient shape embedding layer for 3D deep learning, named Representation-Agnostic Shape Fields (RASF), to improve performance across different representations, backbones and down-stream tasks",
    "supplementary_material": "/attachment/77e2838b3b479e8a98a2b8f6f40ba11760b5e4dc.zip",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "huang|representationagnostic_shape_fields",
    "data": "",
    "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.10259/code)",
    "_bibtex": "@inproceedings{\nhuang2022representationagnostic,\ntitle={Representation-Agnostic Shape Fields},\nauthor={Xiaoyang Huang and Bingbing Ni and Jiancheng Yang and Yanjun Wang and Ziyu Chen and Linguo Li and Teng Li and Wenjun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=-ngwPqanCEZ}\n}",
    "venue": "ICLR 2022 Poster",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "-ngwPqanCEZ",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 10,
    "directReplyCount": 5,
    "revisions": true,
    "replies": [
      {
        "id": "QHoVl8D6LpS",
        "original": null,
        "number": 1,
        "cdate": 1634959073935,
        "mdate": null,
        "ddate": null,
        "tcdate": 1634959073935,
        "tmdate": 1634959073935,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "-ngwPqanCEZ",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper presents a method for 3D shape analysis by means of local shape embedding.  The key idea is to use a learnable multi-channel 3D grid to embed local shapes in the input 3D object, similar to word embedding in NLP; hence, the method can be pre-trained and applied for various downstream tasks.\n\nThe procedure is:\n(1) If the input is mesh-based or voxel-based, first generate sample points on mesh or in voxels; if input is point-based, no need to have this pre-processing;\n(2) For each point p, find a set of K nearest neighbors points, i.e., P_neigh; and\n(3) normalize { p, P_neigh } and take the result as an index to retrieve shape features from the 3D grid, etc.\n\nThe paper claims that the proposed method is generalizable (i.e., could be used in different 3D representations, backbones and downstream tasks) and computation-efficient shape embedding layer for 3D deep learning.",
          "main_review": "Strengths:\n\n(1) This work has a good motivation of trying to unify various 3D representations: meshes, point clouds, and voxels.\n\n(2) The design is simple and brings certain improvements to most (basic) networks employed in the experiments, and the method can be used a plugin for various method\n\n(3) The paper is well written and quite clear\n\n\nWeaknesses:\n\n(1) The first concern is on the motivation of the design. Though the paper claims \"generalizable\", the method simply re-samples points for mesh and voxel inputs, so that we can obtain points for any kind of inputs and then use these points for local shape extraction and embedding.  So, it seems to me that it is too strong to claim that the method is generalizable.\n\n(2) From the results shown in Section 4, the method seems to be effective mainly for earlier networks such as PointNet (2018) and MeshCNN (2019), which explore very local information in the input.\n\n(3) The idea of finding K nearest neighbors for feature extraction sounds very similar to EdgeConv in DGCNN, which improves over PointNet with features from local neighborhood of K nearest points. From Table 3, the improvement of the proposed method over DGCNN is quite marginal.\n\n(4) I am also concerned about the effectiveness of the design.  As shown in Table 8, the randomly-initialized RASF achieves comparable performance with the pre-trained one. If the backbone is changed from PointNet to DGCNN, it may be hard to see such improvement.",
          "summary_of_the_review": "Overall, I am lukewarm for this paper and slightly more on the negative side.\n\n\n--------------------------------------\n\n\nOther issues:\n\nI am not sure how sensitive the method is to K and the density of the point samples in the inputs.\n\n\nP.3:\n\ndescriptors( -> descriptors (\n\n(Sun et al., 2009) , -> (Sun et al., 2009),\n\n\nP.6:\n\na accuracy -> an accuracy\n\nShapenetPart -> ShapeNetPart\n\nrepresentation, We -> representation, we \n\nclassfication -> classification\n\n\nP.7:\n\nAdam(Kingma -> Adam (Kingma",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "Nil.",
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Reviewer_uSYH"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Reviewer_uSYH"
        ]
      },
      {
        "id": "6pzEIxdvk6",
        "original": null,
        "number": 2,
        "cdate": 1635530781093,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635530781093,
        "tmdate": 1635530781093,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "-ngwPqanCEZ",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper presents a generalizable shape embedding layer for 3D deep learning. The proposed shape embedding can be used for a number of 3D shape representations including point cloud, mesh, and voxel. The shape embeddings can be obtained via self-supervised learning, where the paper has experimented with shape reconstruction and normal estimation as the pre-training tasks. The pre-training would enable the proposed method to learn general shape embeddings. At deploy time, the 3D priors are retrieved using coordinate indexing. The experimental results have shown that the proposed method can provide a boost on various representations in different applications.",
          "main_review": "### Strength \n- The proposed shape embedding is general and can be plug-and-play in various 3D representations. \n- The proposed method has been extensively evaluated with different representations in a range of downstream applications, including classification, part segmentation, and semantic segmentation. All the experiments have witnessed a boost in performance when incorporating the proposed shape embedding, which indicates the effectiveness of the proposed approach.\n- The paper is well written and easy to follow.\n\n### Weakness\n- Though the performance can be boosted using the proposed method, the increase of the quantitative measurement seems to be a bit marginal. ",
          "summary_of_the_review": "The idea of introducing an embedding layer to the 3D shape analysis just as the NLP community is interesting and novel. \nThe proposed framework is general to many mainstream 3D representations and does not require many additional changes to existing 3D learning frameworks in order to be deployed. The experimental results are positive and have shown the effectiveness of the proposed method in a number of applications. Though the performance boost has been verified, the magnitude of the increase seems to be a bit marginal. \n\n",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "No.",
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Reviewer_quxB"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Reviewer_quxB"
        ]
      },
      {
        "id": "hFcLCvET5Eh",
        "original": null,
        "number": 3,
        "cdate": 1635795800424,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635795800424,
        "tmdate": 1635795800424,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "-ngwPqanCEZ",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This submissions introduces RASF (Representation Agnostic Shape Fields), an embedding layer that encodes local geometry and can be used for 3D deep learning with different input domains: point clouds, meshes or voxels. RASF is inspired by the idea of embedding layers in language models, where representations of tokens are indexed by one-hot vectors. \n\nIn this submissions the authors propose to implement RASF as volumetric latent embedding that is indexed via tri-linear sampling. For example, given an input pointcloud the process first extracts a local neighborhood around a point p and normalizes it, so that the point p becomes the origin. This pointcloud is then used to sample the volumetric latent embedding and afterwards a max-pooling operation is applied. The max-pooled feature becomes the representation of point p.\n\nThe experimental results show improvements when RASF is used on top of several baselines while incurring on a small additional computational complexity.",
          "main_review": "Overall I found the idea behind RASF compelling. The concept is simple and yet it does boost accuracy while keeping the added computational cost to a reasonable degree. \n\nStrengths:\n- Clear idea and exposition. I like that the authors didn't try to over-complicate the exposition of the idea.\n\n- Good evaluation on multiple geometry representations for different problems (reconstruction, normal estimation, segmentation, classification). It seems that adding RASF improves the accuracy for all the methods.\n\n\nWeaknesses:\n- Implementation details are unclear sometimes. Are RASF embedding layers used for all layers in pointnet for classification? I think the reader would benefit from an appendix where implementation details are clear.\n\n- The baselines used to add RASF on top might be outdated. It would be good if the authors could provide at least a couple of comparisons where baselines are state-of-the-art. Specially, those architectures using transformers (eg. https://arxiv.org/abs/2012.09688), which can learn both local and global geometry. I would like to see these comparisons before recommending this paper to be accepted.\n\n\nFinally, I have a couple of curiosities that I would like the authors to provide some feedback if possible:\n\n- Have you though about using RASF at different scales? One could easily do this by having multiple RASF embeddings which are queried by neigborhoods at different scales?\n\n- What would be the expected performance of a transfer learning task? Where you train RASF on point clouds and then test it on meshes? This could be an interesting direction for future work,",
          "summary_of_the_review": "This paper presents a conceptually simple approach that seems to boost performance for most baseline models for 3D deep learning. I like the exposition of the idea and the extensive experimental results. However, I'm concerned that the baselines chosen for comparison might be outdated. If the authors can provide results with updated baselines (specially those that use transformers as backbones) I would be happy to upgrade my current score.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Reviewer_ESBS"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Reviewer_ESBS"
        ]
      },
      {
        "id": "waWqYodR9_1",
        "original": null,
        "number": 4,
        "cdate": 1635906079541,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635906079541,
        "tmdate": 1635906256557,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "-ngwPqanCEZ",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a method for learning a latent spatial embedding of points in space to a feature space by pre-training on a pretext task (e.g., reconstruction). This embedding can the be used as part of any deep learning pipeline that inputs point coordinates as part of its architecture by first mapping the coordinates to their embedding. The authors demonstrate that utilizing these embeddings improves results of state-of-the-art learning methods on point clouds, meshes, and voxel grids.",
          "main_review": "This paper takes inspiration from word embeddings in NLP and applies the idea in a novel way to 3D deep learning. It is a simple idea, and it is exciting that it appears to be effective across various tasks and modalities. I think moving towards \"universal\" shape representations is an interesting direction for 3D deep learning, and this paper makes a nice step in that vein.\n\nIt is shown in the appendix that increasing the resolution and number channels in the embedding grid degrades quality, which is surprising. The authors claim that this is due to overfitting, but is this quantitatively validated? It would be nice to see metrics on the training set if this is indeed the case.\n\nThe authors should explicitly confirm that the learning set-ups are identical for each backbone with and without RASF---i.e., that the test/train splits are the same and architectures only differ in the embedding layer. It also might be helpful to include some error bars across different random seeds.\n\nHave you tried using your approach on some more recent state-of-the-art 3D learning methods? The ones compared against are fairly representative but a bit outdated, so I wonder if latest advances might make this method obsolete.\n\nWhile the paper is generally clear, there are typos and minor issues in exposition, some of which are listed below. In particular, I would encourage the authors to make some adjustments to some of the figures so that they are easier to parse just from the illustrations and caption contents.\n\n\nFigure 1 is difficult to parse. Some description of the actual contents of the plot in the caption would make it more clear.\n\nThe introduction has very many citations that are not particularly central to the paper and are mostly cited again in the related works section. It might be worth removing some of them from the introduction, because currently it makes the text appear somewhat cluttered and difficult to read.\n\nPage 1: \"In recent computer vision and deep learning community...\" sentence needs some restructuring\n\nPage 1: \"coordinate lacks geometric information\" not entirely clear what this means at this point in the paper\n\nPage 2: \"indices by\" -> \"indexed by\"\n\nDGCNN is mentioned several times but with the wrong citation---the actual paper is [Wang et al. 2018].\n\n\"HodgeNet: Learning Spectral Geometry on Triangle Meshes\" [Smirnov and Solomon 2021] should be mentioned in the related work on learning on meshes.\n\nThe description of the point extraction and normalization in 3.1 is a bit hard to follow.\n\nPage 5: \"by visualize\" -> \"by visualizing\"\n\nFigure 3 is very small, and it is not immediately clear what it's trying to show. It would help to add some more labeling.\n\nFigure 3 caption: \"geometrically-various\" -> \"geometrically-varying\"",
          "summary_of_the_review": "I think this paper proposes a simple yet effective framework for pertaining embedding for 3D deep learning tasks. Despite some minor questions and issues in exposition, I think it would be a nice addition to ICLR.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Reviewer_Y3fs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Reviewer_Y3fs"
        ]
      },
      {
        "id": "pyM-29n3nVL",
        "original": null,
        "number": 1,
        "cdate": 1637478072297,
        "mdate": 1637478072297,
        "ddate": null,
        "tcdate": 1637478072297,
        "tmdate": 1637478072297,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "QHoVl8D6LpS",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer uSYH (Part 1/2)",
          "comment": "We thank you for your detailed comments. We address your questions as follows:\n\n**Q1**:\n> The first concern is on the motivation of the design. Though the paper claims \"generalizable\", the method simply re-samples points for mesh and voxel inputs, so that we can obtain points for any kind of inputs and then use these points for local shape extraction and embedding. So, it seems to me that it is too strong to claim that the method is generalizable.\n\n**A1**: Please note that there are two kinds of \u201cpoint\u201d in our study: 1) \u201cpoint cloud\u201d as a data structure in 3D geometry, and 2) \u201cpoint-based graphics\u201d [1] as a way to solve problem in geometric processing. We use a \u201cpoint-based\u201d approach to improve the performance of 3D deep learning, which is \u201cgeneralizable\u201d across various shape representation including voxel, mesh and \u201cpoint cloud\u201d. The term \u201cgeneralizable\u201d refers to: 1) The obtained RASF embeddings are used as additional features for the original data representations, so that the data could still be processed by their corresponding backbones, such as MeshCNN for meshes, VoxNet for voxels. We do not transform meshes or voxel into point clouds to feed into a point-cloud-based backbone. Each shape representation has unique advantages on specific scenarios, while the flexibility of the RASF makes it easy to be integrated with any representation. 2) The RASF is proven to be effective for different representations. It is pre-trained on ShapeNetPart, a shape dataset based on point clouds while used for various 3D representations, tasks and datasets, not only point clouds.\n\n[1] Gross, Markus, and Hanspeter Pfister, eds. Point-based graphics. Elsevier, 2011.\n\n**Q2**:\n> From the results shown in Section 4, the method seems to be effective mainly for earlier networks such as PointNet (2018) and MeshCNN (2019), which explore very local information in the input.\n\n**A2**: We are not sure of the meaning of the term \u201clocal\u201d here. To our understand, PointNet and MeshCNN have very different mechanisms. PointNet aggregates the point feature by a single global pooling layer, while MeshCNN aggregates neighbor feature by repeatedly pooling in local space. PointNet should be more global and MeshCNN should be more local. RASF is effective in both mechanisms. Besides, based on Reviewer ESBS\u2019s suggestion, we additionally experiment on a recent state-of-the- art point cloud backbone, PCT [1], which learns both local and global geometry. We run PCT without RASF and with RASF under the same setting, and the ACCs are 92.93% and 93.18%, respectively, where RASF improves over PCT by 0.25%. It demonstrates that RASF could consistently make improvements over diverse backbones, no matter it is more local or global. We highlight the added value of our method lies in the generalizability, usability and low cost regardless of shape representation, tasks, datasets and network backbones for 3D deep learning.\n\n**Q3**:\n> The idea of finding K nearest neighbors for feature extraction sounds very similar to EdgeConv in DGCNN, which improves over PointNet with features from local neighborhood of K nearest points. From Table 3, the improvement of the proposed method over DGCNN is quite marginal.\n\n**A3**: We agree that both EdgeConv in DGCNN and RASF make use of finding K nearest neighbors. However, KNN is indeed a common method in 3D shape analysis, which is used in many 3D models. Beyond the KNN as an implementation detail in our framework, the novelty of the proposed RASF is representing local 3D geometry by discrete 3D grids; therefore, the semantic feature for various shape representations can be extracted from this simple design. What\u2019s more, we have empirically proved that EdgeConv does **NOT** generalize to meshes in Section 5.1 (Table 7), where the RASF does.\n\n**Q4**:\n> I am also concerned about the effectiveness of the design. As shown in Table 8, the randomly-initialized RASF achieves comparable performance with the pre-trained one. If the backbone is changed from PointNet to DGCNN, it may be hard to see such improvement.\n\n**A4**: Thanks for your comment. To address your concern, we additionally experiment on the DGCNN with a randomly-initialized RASF. We list the results below:\n\n| Setting                                   | ACC on ModelNet40 |\n|----                                       |       ----        |\n|DGCNN without RASF                         |92.71              |\n|DGCNN with pre-trained RASF                |92.95              |\n|DGCNN with randomly-initialized RASF       |92.52              |\n\nIt shows that randomly-initialized RASF still degrades the performance, even if the backbone is DGCNN. This result is consistent with the results in Table 8, where the backbone is PointNet. We reckon that a randomly-initialized RASF that is jointly trained with the backbone would overfit to the training set, while a pre-trained RASF is more generalizable.\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Authors"
        ]
      },
      {
        "id": "0ifDZOHfqLp",
        "original": null,
        "number": 2,
        "cdate": 1637478148602,
        "mdate": 1637478148602,
        "ddate": null,
        "tcdate": 1637478148602,
        "tmdate": 1637478148602,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "pyM-29n3nVL",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer uSYH (Part 2/2) ",
          "comment": "\n**Q5**:\n>  I am not sure how sensitive the method is to K and the density of the point samples in the inputs.\n\n**A5**: The number K in KNN and the density of point samples could both be converted to the number of the neighbor points, since we linearly scale up K as a function of the total point number in our experiments, such as K=32 when the sample has 1024 point and K=64 when the sample has 2048 points. We analyzed the sensitivity of K in Section A.5 in the appendix (the 1st, 4th and 5th columns in Figure 5). We tried different numbers of K (16, 32, 64), and find that they yield results between 90.2%-90.9%.\n\n**A6**: Thanks for pointing out the typos. We have fixed them."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Authors"
        ]
      },
      {
        "id": "DDACplQxpXL",
        "original": null,
        "number": 3,
        "cdate": 1637478212744,
        "mdate": 1637478212744,
        "ddate": null,
        "tcdate": 1637478212744,
        "tmdate": 1637478212744,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "6pzEIxdvk6",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer quxB",
          "comment": "We thank you for your comments and positive feedback. We address the comment as follows:\n\n**Q1**:\n> Though the performance can be boosted using the proposed method, the increase of the quantitative measurement seems to be a bit marginal.\n\n**A1**: We highlight the added value of our method lies in the generalizability, usability and low cost regardless of shape representation, tasks, datasets and network backbones for 3D deep learning. Besides, the improvements in some experiments are notable, such as mesh classification and segmentation. We believe this \u201csimple yet effective\u201d idea can be an addition to geometric deep leaning community. "
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Authors"
        ]
      },
      {
        "id": "FR5sWA291pN",
        "original": null,
        "number": 4,
        "cdate": 1637478365037,
        "mdate": 1637478365037,
        "ddate": null,
        "tcdate": 1637478365037,
        "tmdate": 1637478365037,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "hFcLCvET5Eh",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer ESBS",
          "comment": "\nWe thank you for your detailed and constructive comments. We address each of your concerns as follows:\n\n**Q1**:\n> Implementation details are unclear sometimes. Are RASF embedding layers used for all layers in pointnet for classification? I think the reader would benefit from an appendix where implementation details are clear.\n\n**A1**: The RASF embedding layers are only used as additional features, before the first layer in all backbones, which is straightforward to implement. We have added more implementation details in the appendix according to your suggestions. Besides, the code will be open-source upon acceptance.\n\n**Q2**:\n> The baselines used to add RASF on top might be outdated. It would be good if the authors could provide at least a couple of comparisons where baselines are state-of-the-art. Specially, those architectures using transformers (eg. https://arxiv.org/abs/2012.09688), which can learn both local and global geometry. I would like to see these comparisons before recommending this paper to be accepted.\n\n**A2**: Thanks for your comments. According to your suggestion, we have added PCT in the experiment. Due to the limit of time, so far we only have results on the classification task on ModelNet40. Based on the official code (https://github.com/Strawberry-Eat-Mango/PCT_Pytorch), we run the PCT without RASF and PCT with RASF under the same setting. The ACCs are 92.93% and 93.18% respectively, where RASF improves over PCT by 0.25%. We have included these results and cited the reference in the revised manuscript. \n\n**Q3**:\n> Have you thought about using RASF at different scales? One could easily do this by having multiple RASF embeddings which are queried by neighborhoods at different scales?\n\n**A3**: We have implemented a na\u00efve multi-scale extension of RASF  by concatenating multiple RASF embeddings queried by neighborhoods at different scales, in three settings, as listed in the table:\n\n| Number of neighbors in RASF concatenation | ACC on ModelNet40 |\n|----                                       |       ----        |\n|16,32                                      |88.73              |\n|16,32,64                                   |87.07              |\n|32,64                                      |90.84              |\n\nIt implies that with the existence of 16-neighbors RASF, the performance would degrades obviously. It could be explained by that the scale of neighbors is fixed during the pre-training of RASF; therefore, it could not adapt to various scales in down-stream tasks. Nevertheless, the multi-scale idea could further improve the performance with sophisticated implementation. In future work, we will try different neighbor scales in multiple RASF pre-training and use them together in down-stream tasks. Besides, adaptive scales are also in our considerations.\n\n**Q4**:\n> What would be the expected performance of a transfer learning task? Where you train RASF on point clouds and then test it on meshes? This could be an interesting direction for future work.\n\n**A4**: There might be a misunderstanding. RASF is pre-trained on ShapeNetPart, a point clouds dataset, as stated in the last paragraph in Section 3.3. Once RASF is pre-trained, it is fixed in **ALL** down-stream tasks, including point clouds, meshes and voxel grids. Therefore, we claim that RASF is an off-the-shelf module to improve performance at negligible cost in adaptation of down-stream tasks."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Authors"
        ]
      },
      {
        "id": "Rau2sFCNUZ8",
        "original": null,
        "number": 5,
        "cdate": 1637482116780,
        "mdate": 1637482116780,
        "ddate": null,
        "tcdate": 1637482116780,
        "tmdate": 1637482116780,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "waWqYodR9_1",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Comment",
        "content": {
          "title": "Response to Reviewer Y3fs",
          "comment": "\nWe thank you for your detailed comments and positive rating for the added value of our method in 3D deep learning. We address your questions as follows:\n\n**Q1**: \n> It is shown in the appendix that increasing the resolution and number channels in the embedding grid degrades quality, which is surprising. The authors claim that this is due to overfitting, but is this quantitatively validated? It would be nice to see metrics on the training set if this is indeed the case.\n\n**A1**: The term \u201coverfitting\u201d in the pre-resived manuscript might not be precise, since the setting of our experiments is not a standard train-test setting: we first pre-train the RASF on a large-scale shape dataset (ShapeNetPart), and use it on various downstream datasets where the RASF is fixed and other models with the RASF are trained in a standard train-test setting. We have conducted experiments where we pre-train and use RASF on the same dataset (ModelNet40), and find that increasing the resolution and number channels still degrades the performance. Therefore, wWe have changed the \u201coverfitting\u201d explanation in the manuscript into a more precise statement: The RASF needs a proper number of channels and resolution to achieve optimal performance.\n\n**Q2**:\n>The authors should explicitly confirm that the learning set-ups are identical for each backbone with and without RASF---i.e., that the test/train splits are the same and architectures only differ in the embedding layer. It also might be helpful to include some error bars across different random seeds.\n\n**A2**: We confirm that all settings are identical for each backbone with and without the RASF, including hyper-parameters and train-test splits. We have clarified this in the revised manuscript. The error bars have not been included in the current version due to the limit of computation resources. We will add it in the final manuscript by repeated experiments.\n\n**Q3**:\n>Have you tried using your approach on some more recent state-of-the-art 3D learning methods? The ones compared against are fairly representative but a bit outdated, so I wonder if latest advances might make this method obsolete.\n\n**A3**: Thanks for your suggestion. As suggested by other reviewer (Reviewer ESBS), we experiment on a more recent point cloud backbone based on transformer, PCT [1], which achieves SOTA performance in many tasks. We run PCT without RASF and with RASF under the same setting. The ACCs are 92.93% and 93.18% respectively, where RASF improves over PCT by 0.25%. We have included these results in the revised manuscript.\n\n**Q4**:\n>Figure 1 is difficult to parse. Some description of the actual contents of the plot in the caption would make it more clear. Figure 3 is very small, and it is not immediately clear what it's trying to show. It would help to add some more labeling.\n\n**A4**: We have improved the clarity of captions and sizes of Figure 1 and Figure 3 for better presentation. \n\n**A5**: All typos and the mistaken reference have been fixed in the revised manuscript. The redundant references in introduction have been removed. HodgeNet [2] has been added to the related work.\n\n\n[1] Guo, Meng-Hao, et al. \"PCT: Point cloud transformer.\" Computational Visual Media 7.2 (2021): 187-199.\n\n[2] Smirnov, Dmitriy, and Justin Solomon. \"HodgeNet: Learning Spectral Geometry on Triangle Meshes.\" SIGGRAPH (2021).\n\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Authors"
        ]
      },
      {
        "id": "BCwj-yt3y2I",
        "original": null,
        "number": 1,
        "cdate": 1642696833972,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696833972,
        "tmdate": 1642696833972,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "-ngwPqanCEZ",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "This paper proposes an \"embedding layer\" in which points on a model are mapped into a feature space, trained using a reconstruction-based pretext task.  Then, the resulting embedding layer can be applied to shape data before using different learning architectures for modalities like meshes and point clouds.  The work is particularly interesting in its attempt to derive a learned shape representation that is agnostic to modality.  Some questions remained about experiments (e.g. baselines), but these are relatively minor and partially addressed in the rebuttal phase; also, sometimes the improvement seems to be marginal in practice.  \n\nTwo reviewers championed this work during the discussion phase.  The AC tends to agree this work is an interesting direction for future work and contains insight that the vision/learning communities might be able to use in other settings."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "QHoVl8D6LpS",
        "original": null,
        "number": 1,
        "cdate": 1634959073935,
        "mdate": null,
        "ddate": null,
        "tcdate": 1634959073935,
        "tmdate": 1634959073935,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "-ngwPqanCEZ",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper presents a method for 3D shape analysis by means of local shape embedding.  The key idea is to use a learnable multi-channel 3D grid to embed local shapes in the input 3D object, similar to word embedding in NLP; hence, the method can be pre-trained and applied for various downstream tasks.\n\nThe procedure is:\n(1) If the input is mesh-based or voxel-based, first generate sample points on mesh or in voxels; if input is point-based, no need to have this pre-processing;\n(2) For each point p, find a set of K nearest neighbors points, i.e., P_neigh; and\n(3) normalize { p, P_neigh } and take the result as an index to retrieve shape features from the 3D grid, etc.\n\nThe paper claims that the proposed method is generalizable (i.e., could be used in different 3D representations, backbones and downstream tasks) and computation-efficient shape embedding layer for 3D deep learning.",
          "main_review": "Strengths:\n\n(1) This work has a good motivation of trying to unify various 3D representations: meshes, point clouds, and voxels.\n\n(2) The design is simple and brings certain improvements to most (basic) networks employed in the experiments, and the method can be used a plugin for various method\n\n(3) The paper is well written and quite clear\n\n\nWeaknesses:\n\n(1) The first concern is on the motivation of the design. Though the paper claims \"generalizable\", the method simply re-samples points for mesh and voxel inputs, so that we can obtain points for any kind of inputs and then use these points for local shape extraction and embedding.  So, it seems to me that it is too strong to claim that the method is generalizable.\n\n(2) From the results shown in Section 4, the method seems to be effective mainly for earlier networks such as PointNet (2018) and MeshCNN (2019), which explore very local information in the input.\n\n(3) The idea of finding K nearest neighbors for feature extraction sounds very similar to EdgeConv in DGCNN, which improves over PointNet with features from local neighborhood of K nearest points. From Table 3, the improvement of the proposed method over DGCNN is quite marginal.\n\n(4) I am also concerned about the effectiveness of the design.  As shown in Table 8, the randomly-initialized RASF achieves comparable performance with the pre-trained one. If the backbone is changed from PointNet to DGCNN, it may be hard to see such improvement.",
          "summary_of_the_review": "Overall, I am lukewarm for this paper and slightly more on the negative side.\n\n\n--------------------------------------\n\n\nOther issues:\n\nI am not sure how sensitive the method is to K and the density of the point samples in the inputs.\n\n\nP.3:\n\ndescriptors( -> descriptors (\n\n(Sun et al., 2009) , -> (Sun et al., 2009),\n\n\nP.6:\n\na accuracy -> an accuracy\n\nShapenetPart -> ShapeNetPart\n\nrepresentation, We -> representation, we \n\nclassfication -> classification\n\n\nP.7:\n\nAdam(Kingma -> Adam (Kingma",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "Nil.",
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Reviewer_uSYH"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Reviewer_uSYH"
        ]
      },
      {
        "id": "6pzEIxdvk6",
        "original": null,
        "number": 2,
        "cdate": 1635530781093,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635530781093,
        "tmdate": 1635530781093,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "-ngwPqanCEZ",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper presents a generalizable shape embedding layer for 3D deep learning. The proposed shape embedding can be used for a number of 3D shape representations including point cloud, mesh, and voxel. The shape embeddings can be obtained via self-supervised learning, where the paper has experimented with shape reconstruction and normal estimation as the pre-training tasks. The pre-training would enable the proposed method to learn general shape embeddings. At deploy time, the 3D priors are retrieved using coordinate indexing. The experimental results have shown that the proposed method can provide a boost on various representations in different applications.",
          "main_review": "### Strength \n- The proposed shape embedding is general and can be plug-and-play in various 3D representations. \n- The proposed method has been extensively evaluated with different representations in a range of downstream applications, including classification, part segmentation, and semantic segmentation. All the experiments have witnessed a boost in performance when incorporating the proposed shape embedding, which indicates the effectiveness of the proposed approach.\n- The paper is well written and easy to follow.\n\n### Weakness\n- Though the performance can be boosted using the proposed method, the increase of the quantitative measurement seems to be a bit marginal. ",
          "summary_of_the_review": "The idea of introducing an embedding layer to the 3D shape analysis just as the NLP community is interesting and novel. \nThe proposed framework is general to many mainstream 3D representations and does not require many additional changes to existing 3D learning frameworks in order to be deployed. The experimental results are positive and have shown the effectiveness of the proposed method in a number of applications. Though the performance boost has been verified, the magnitude of the increase seems to be a bit marginal. \n\n",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "details_of_ethics_concerns": "No.",
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Reviewer_quxB"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Reviewer_quxB"
        ]
      },
      {
        "id": "hFcLCvET5Eh",
        "original": null,
        "number": 3,
        "cdate": 1635795800424,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635795800424,
        "tmdate": 1635795800424,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "-ngwPqanCEZ",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This submissions introduces RASF (Representation Agnostic Shape Fields), an embedding layer that encodes local geometry and can be used for 3D deep learning with different input domains: point clouds, meshes or voxels. RASF is inspired by the idea of embedding layers in language models, where representations of tokens are indexed by one-hot vectors. \n\nIn this submissions the authors propose to implement RASF as volumetric latent embedding that is indexed via tri-linear sampling. For example, given an input pointcloud the process first extracts a local neighborhood around a point p and normalizes it, so that the point p becomes the origin. This pointcloud is then used to sample the volumetric latent embedding and afterwards a max-pooling operation is applied. The max-pooled feature becomes the representation of point p.\n\nThe experimental results show improvements when RASF is used on top of several baselines while incurring on a small additional computational complexity.",
          "main_review": "Overall I found the idea behind RASF compelling. The concept is simple and yet it does boost accuracy while keeping the added computational cost to a reasonable degree. \n\nStrengths:\n- Clear idea and exposition. I like that the authors didn't try to over-complicate the exposition of the idea.\n\n- Good evaluation on multiple geometry representations for different problems (reconstruction, normal estimation, segmentation, classification). It seems that adding RASF improves the accuracy for all the methods.\n\n\nWeaknesses:\n- Implementation details are unclear sometimes. Are RASF embedding layers used for all layers in pointnet for classification? I think the reader would benefit from an appendix where implementation details are clear.\n\n- The baselines used to add RASF on top might be outdated. It would be good if the authors could provide at least a couple of comparisons where baselines are state-of-the-art. Specially, those architectures using transformers (eg. https://arxiv.org/abs/2012.09688), which can learn both local and global geometry. I would like to see these comparisons before recommending this paper to be accepted.\n\n\nFinally, I have a couple of curiosities that I would like the authors to provide some feedback if possible:\n\n- Have you though about using RASF at different scales? One could easily do this by having multiple RASF embeddings which are queried by neigborhoods at different scales?\n\n- What would be the expected performance of a transfer learning task? Where you train RASF on point clouds and then test it on meshes? This could be an interesting direction for future work,",
          "summary_of_the_review": "This paper presents a conceptually simple approach that seems to boost performance for most baseline models for 3D deep learning. I like the exposition of the idea and the extensive experimental results. However, I'm concerned that the baselines chosen for comparison might be outdated. If the authors can provide results with updated baselines (specially those that use transformers as backbones) I would be happy to upgrade my current score.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Reviewer_ESBS"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Reviewer_ESBS"
        ]
      },
      {
        "id": "waWqYodR9_1",
        "original": null,
        "number": 4,
        "cdate": 1635906079541,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635906079541,
        "tmdate": 1635906256557,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "-ngwPqanCEZ",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper proposes a method for learning a latent spatial embedding of points in space to a feature space by pre-training on a pretext task (e.g., reconstruction). This embedding can the be used as part of any deep learning pipeline that inputs point coordinates as part of its architecture by first mapping the coordinates to their embedding. The authors demonstrate that utilizing these embeddings improves results of state-of-the-art learning methods on point clouds, meshes, and voxel grids.",
          "main_review": "This paper takes inspiration from word embeddings in NLP and applies the idea in a novel way to 3D deep learning. It is a simple idea, and it is exciting that it appears to be effective across various tasks and modalities. I think moving towards \"universal\" shape representations is an interesting direction for 3D deep learning, and this paper makes a nice step in that vein.\n\nIt is shown in the appendix that increasing the resolution and number channels in the embedding grid degrades quality, which is surprising. The authors claim that this is due to overfitting, but is this quantitatively validated? It would be nice to see metrics on the training set if this is indeed the case.\n\nThe authors should explicitly confirm that the learning set-ups are identical for each backbone with and without RASF---i.e., that the test/train splits are the same and architectures only differ in the embedding layer. It also might be helpful to include some error bars across different random seeds.\n\nHave you tried using your approach on some more recent state-of-the-art 3D learning methods? The ones compared against are fairly representative but a bit outdated, so I wonder if latest advances might make this method obsolete.\n\nWhile the paper is generally clear, there are typos and minor issues in exposition, some of which are listed below. In particular, I would encourage the authors to make some adjustments to some of the figures so that they are easier to parse just from the illustrations and caption contents.\n\n\nFigure 1 is difficult to parse. Some description of the actual contents of the plot in the caption would make it more clear.\n\nThe introduction has very many citations that are not particularly central to the paper and are mostly cited again in the related works section. It might be worth removing some of them from the introduction, because currently it makes the text appear somewhat cluttered and difficult to read.\n\nPage 1: \"In recent computer vision and deep learning community...\" sentence needs some restructuring\n\nPage 1: \"coordinate lacks geometric information\" not entirely clear what this means at this point in the paper\n\nPage 2: \"indices by\" -> \"indexed by\"\n\nDGCNN is mentioned several times but with the wrong citation---the actual paper is [Wang et al. 2018].\n\n\"HodgeNet: Learning Spectral Geometry on Triangle Meshes\" [Smirnov and Solomon 2021] should be mentioned in the related work on learning on meshes.\n\nThe description of the point extraction and normalization in 3.1 is a bit hard to follow.\n\nPage 5: \"by visualize\" -> \"by visualizing\"\n\nFigure 3 is very small, and it is not immediately clear what it's trying to show. It would help to add some more labeling.\n\nFigure 3 caption: \"geometrically-various\" -> \"geometrically-varying\"",
          "summary_of_the_review": "I think this paper proposes a simple yet effective framework for pertaining embedding for 3D deep learning tasks. Despite some minor questions and issues in exposition, I think it would be a nice addition to ICLR.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper39/Reviewer_Y3fs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper39/Reviewer_Y3fs"
        ]
      },
      {
        "id": "BCwj-yt3y2I",
        "original": null,
        "number": 1,
        "cdate": 1642696833972,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696833972,
        "tmdate": 1642696833972,
        "tddate": null,
        "forum": "-ngwPqanCEZ",
        "replyto": "-ngwPqanCEZ",
        "invitation": "ICLR.cc/2022/Conference/Paper39/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "This paper proposes an \"embedding layer\" in which points on a model are mapped into a feature space, trained using a reconstruction-based pretext task.  Then, the resulting embedding layer can be applied to shape data before using different learning architectures for modalities like meshes and point clouds.  The work is particularly interesting in its attempt to derive a learned shape representation that is agnostic to modality.  Some questions remained about experiments (e.g. baselines), but these are relatively minor and partially addressed in the rebuttal phase; also, sometimes the improvement seems to be marginal in practice.  \n\nTwo reviewers championed this work during the discussion phase.  The AC tends to agree this work is an interesting direction for future work and contains insight that the vision/learning communities might be able to use in other settings."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}