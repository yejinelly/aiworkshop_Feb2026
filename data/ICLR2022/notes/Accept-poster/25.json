{
  "id": "dDjSKKA5TP1",
  "original": "U-NczbO38_J",
  "number": 25,
  "cdate": 1632875423289,
  "pdate": 1643407560000,
  "odate": 1633539600000,
  "mdate": null,
  "tcdate": 1632875423289,
  "tmdate": 1697934973310,
  "ddate": null,
  "content": {
    "title": "Incremental False Negative Detection for Contrastive Learning",
    "authorids": [
      "~Tsai-Shien_Chen1",
      "~Wei-Chih_Hung1",
      "~Hung-Yu_Tseng2",
      "~Shao-Yi_Chien1",
      "~Ming-Hsuan_Yang1"
    ],
    "authors": [
      "Tsai-Shien Chen",
      "Wei-Chih Hung",
      "Hung-Yu Tseng",
      "Shao-Yi Chien",
      "Ming-Hsuan Yang"
    ],
    "keywords": [
      "Self-supervised learning",
      "Contrastive learning",
      "Representation learning",
      "Clustering-based learning"
    ],
    "abstract": "Self-supervised learning has recently shown great potential in vision tasks through contrastive learning, which aims to discriminate each image, or instance, in the dataset. However, such instance-level learning ignores the semantic relationship among instances and sometimes undesirably repels the anchor from the semantically similar samples, termed as \"false negatives\". In this work, we show that the unfavorable effect from false negatives is more significant for the large-scale datasets with more semantic concepts. To address the issue, we propose a novel self-supervised contrastive learning framework that incrementally detects and explicitly removes the false negative samples. Specifically, following the training process, our method dynamically detects increasing high-quality false negatives considering that the encoder gradually improves and the embedding space becomes more semantically structural. Next, we discuss two strategies to explicitly remove the detected false negatives during contrastive learning. Extensive experiments show that our framework outperforms other self-supervised contrastive learning methods on multiple benchmarks in a limited resource setup.",
    "pdf": "/pdf/bb320db0548af9f9dda16d602425fa7632befbb0.pdf",
    "one-sentence_summary": "This paper explores the effect of false negative samples in self-supervised contrastive learning and introduce a framework to incrementally detect and explicitly remove the false negatives.",
    "code_of_ethics": "",
    "submission_guidelines": "",
    "resubmission": "",
    "student_author": "",
    "serve_as_reviewer": "",
    "paperhash": "chen|incremental_false_negative_detection_for_contrastive_learning",
    "data": "",
    "code": "",
    "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2106.03719/code)",
    "_bibtex": "@inproceedings{\nchen2022incremental,\ntitle={Incremental False Negative Detection for Contrastive Learning},\nauthor={Tsai-Shien Chen and Wei-Chih Hung and Hung-Yu Tseng and Shao-Yi Chien and Ming-Hsuan Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dDjSKKA5TP1}\n}",
    "venue": "ICLR 2022 Poster",
    "venueid": "ICLR.cc/2022/Conference"
  },
  "forum": "dDjSKKA5TP1",
  "referent": null,
  "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission",
  "replyto": null,
  "readers": [
    "everyone"
  ],
  "nonreaders": [],
  "signatures": [
    "ICLR.cc/2022/Conference"
  ],
  "writers": [
    "ICLR.cc/2022/Conference"
  ],
  "details": {
    "overwriting": [],
    "tags": [],
    "writable": false,
    "replyCount": 14,
    "directReplyCount": 5,
    "revisions": true,
    "replies": [
      {
        "id": "I2FmeR4KrUt",
        "original": null,
        "number": 1,
        "cdate": 1635112260598,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635112260598,
        "tmdate": 1638201675929,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "dDjSKKA5TP1",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper deals with the very important topic of \u201cfalse negatives\u201d in SSL and shows that \u201cfalse negatives\u201d have a huge impact on performance of SSL methods. One of the most interesting experiments they show is that as they increase the number of classes in the dataset the performance drops more and more. They propose a clustering based method which reduces the  \u201cfalse negatives\u201d and shows improved performances as compared to other state-of-the-art baselines.\n",
          "main_review": "Strengths: \n1) Well written and easy to follow paper with a very solid hypothesis and analysis of effects of \u201cfalse negatives\u201d on contrastive learning.\n2) They have shown good results specially under classification settings in both supervised learning and semi-supervised learning setups.\n3) Table4 shows really good analysis and shows how increasing the depth results in more and more poor performances. \n4) Better clustering results than SWaV: Table 3 results are very promising as they show that IFND achieves better NMI results than SWaV and MoCo-V2. This means that IFND is really able to form better clusters and show improved performances.\n\nWeakness:\n1) Results on BYOL and other non-contrastive methods: The method would be much more comprehensive if this method can be extended to other non-contrastive methods such BYOL as well. Recently BYOL and other non-contrastive methods are showing much more promise than contrastive methods. Hence changing the loss function using similar motivation from IFND would be really good to see. \n2) Object-Detection and Semantic Segmentation results on COCO/VOC: I couldn't find any results on object detection or segmentation. Adding those results would make the paper more complete.\n3) Another interesting result would be to see the performance of IFND by pre-training on the COCO dataset. ImageNet in a way is already clustered and finding close \u201cfalse negatives\u201d would be easier on ImageNEt as compared to COCO. Results by pre-training on COCO would make the method much more convincing.\n",
          "summary_of_the_review": "I like the idea used in the paper and analysis shown in the paper. However lack of results on object detection and semantic segmentation coupled with lack of results using BYOL method, I'm leaning towards borderline rejection. Adding these results would make the paper stronger.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Reviewer_qg7u"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Reviewer_qg7u"
        ]
      },
      {
        "id": "urcnYCQAOkM",
        "original": null,
        "number": 2,
        "cdate": 1636388675334,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636388675334,
        "tmdate": 1636388675334,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "dDjSKKA5TP1",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes incremental false-negative detection (IFND), which has several improvements over prototypical contrastive learning (PCL). The key idea of PCL is to alternatively update (a) the pseudo-labels based on the current encoder and (b) update the encoder based on the current pseudo-labels. However, PCL often converges to the local minima since the bad encoder produces falsy pseudo-labels, further harming the encoder. To tackle the issue, INFD suggests including the confident pseudo-labels while uncertain ones incrementally. IFND consistently improves the PCL in all considered scenarios.",
          "main_review": "### Pros\n\n- Clear motivation over the prior work and a simple yet effective solution.\n- Nice details on the method, e.g., linear scheduling of the acceptance rate, to be hyperparameter-free.\n- Nice experimental setup, e.g., comparing with SimCLR and SupCon to provide lower and upper bounds.\n\n\n### Cons/concerns\n\n1. Limited novelty and inappropriate credit for PCL\n\nThe proposed method is somewhat incremental over the prior work, PCL. As PCL develops the core concept of pseudo-label-based contrastive learning, the paper should clearly illustrate the contribution of prior work and its novel innovations. While the paper briefly introduces PCL in the related work section, the paper should clearly state PCL as the preliminary in the main method section.\n\nFor example, the \"hierarchical semantic definition\" part of the paper (page 4) suggests using multiple clusters to consider the different hierarchy levels. However, this technique was already introduced in the original PCL paper (Eq. (11)).\n\n\n2. Ablation study on the difference from PCL\n\nPutting aside omitting uncertain pseudo-labels, IFND has several minor differences from the PCL. After clearly stating the differences, the paper could provide the ablation study for each modified component, e.g.,\n- Confidence of pseudo-label: One may use Eq. (9) of PCL instead of the proposed Eq. (2), although the latter seems to be better as the former does not consider different classes.\n- Number of clusters: The original PCL used {25000,50000,100000} but INFD uses {1000,3000,10000}. It can be unfair since the desired number of clusters are 1000 for ImageNet.\n\n\n3. Comparison with other cluster-based (and other self-supervised) methods\n\nTable 1 suggests that SwAV, another cluster-based method, performs the best. While the paper only verifies the merit over its predecessor, PCL, the empirical contribution could be limited if it underperforms than other self-supervised methods. Thus, the paper should compare IFND with SwAV for all experiments, under the same setup (e.g., 200 epochs).\n\nWhile BYOL is not cluster-based method, it does not suffer from the suggested false negative issue, which weaken the motivation of this work. The paper could also verify the advantages over the BYOL to strengthen the desirability of proposed method.\n\n\nMinor comments:\n- While the paper introduces two losses: elimination and attraction, the paper concludes that elimination is better for considered scenarios. The paper could mainly introduce the single final method and leave the inferior one for the discussion.\n- The definition of false negative $\\mathcal{N}(i)$ could be introduced before it is used in Eq. (3).",
          "summary_of_the_review": "The paper proposes a solid yet somewhat incremental modification from the prior work, PCL. Overall, I think the paper is on the boarderline. I'm willing to raise my score if my concerns: (a) improper credit for PCL, (b) ablation study over PCL, amd (c) comparison with SwAV/BYOL are addressed.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Reviewer_T8CE"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Reviewer_T8CE"
        ]
      },
      {
        "id": "hw0g1V0WZPg",
        "original": null,
        "number": 3,
        "cdate": 1636484803387,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636484803387,
        "tmdate": 1638187184657,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "dDjSKKA5TP1",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Review",
        "content": {
          "summary_of_the_paper": "Briefly, this paper presents a simple yet effective method (i.e., IFND) for self-supervised contrastive learning by effectively handling the false-negative samples. Specifically, the authors propose to cluster samples in the embedding space and assign the cluster indices as pseudo labels in an incremental manner during training. The experimental results are extensive and demonstrate the effectiveness of the proposed method.",
          "main_review": "Strengths:\n1) The paper is easy to follow with clear motivation. The discussion and analysis about \u201cfalse negatives\u201d on contrastive learning in Fig1 is quite valuable. \n2) The authors justify the effectiveness of the proposed method according to the extensive evaluations. Besides, the ablation study also validates the contribution of the main component of the proposed IFND.\n\nWeaknesses:\n1) The technical novelty of the proposed method is somewhat marginal since some of its main components (i.e., hierarchical semantic definition) have already been mentioned in PCL (Li et al., 2021). The authors should present more detailed analyses and discussions to highlight the difference between PCL and their proposed IFND. For instance, it will be better if the authors can conduct experiments on the object detection task to demonstrate the consistently superior performance of IFND over PCL. \n2) The acceptance rate of the pseudo label is determined without careful consideration. According to Appendix B, the authors empirically set the acceptance rate in a hyperparameter-free manner (i.e., Epoch / Training Epoch). This might make the learning process unstable due to the accumulated pseudo-labeling errors. It might be better if the authors can explore some robust learning techniques (e.g., self-paced learning) to improve learning efficiency.\n3) It seems that the experiment results do not validate the effectiveness of the introduced attraction strategy according to Table 5.",
          "summary_of_the_review": "This paper introduces some interesting insights for contrastive learning. However, its presentation and experiment design could be improved. I will raise my rating if most of my concerns have been well addressed.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Reviewer_7ZtC"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Reviewer_7ZtC"
        ]
      },
      {
        "id": "orJ95nqeOwH",
        "original": null,
        "number": 3,
        "cdate": 1637607159298,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637607159298,
        "tmdate": 1637642053266,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "hw0g1V0WZPg",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Comment",
        "content": {
          "title": "Responses to Reviewer 7ZtC",
          "comment": "We thank the reviewer for the insightful and valuable comments. Here are our responses:\n\n---\n\n> \"More detailed analyses and discussions to highlight the difference between PCL and IFND.\"\n\nThe main contributions of this work consist of 1) highlighting the unfavorable effect of false negatives in self-supervised contrastive learning, 2) introducing a framework that removes the false negatives during contrastive learning, and 3) developing an incremental scheme to obtain more reliable pseudo-labels for detecting false negatives. Our method is significantly different from the PCL approach in several aspects. As described in Section 2, PCL adopts the instance-level contrastive loss that still suffers from the effect of false negatives. Although our method uses a similar hierarchical semantic concept, it is not our major contribution.\n\nMoreover, we present in-depth experimental evaluations between our and PCL frameworks in Table 6 of the originally submitted manuscript as below. In the experiments, we first compare $\\mathcal{L}_\\mathrm{ProtoNCE}$ and $\\mathcal{L}_\\mathrm{elim}$ to show the effectiveness of removing false negatives. Secondly, we also demonstrate the advantage of using pseudo labels in a linear strategy.\n\n| Method  | Objective | Pseudo Label Assignment | Top-1 Acc. (%) |\n| :-----: | :-----: | :-----: | :-----: |\n| `SimCLR` | $\\mathcal{L}_\\mathrm{inst}$ | Constant (0%) | 71.5 |\n| `PCL` | $\\mathcal{L}_\\mathrm{ProtoNCE}$ | Step (0%$\\rightarrow$100%) | 72.8 |\n| | $\\mathcal{L}_\\mathrm{elim}$ | Step (0%$\\rightarrow$100%) | 73.1 |\n| | $\\mathcal{L}_\\mathrm{ProtoNCE}$ | Linear (0%$\\rightarrow$100%) | 73.4 |\n| `IFND` | $\\mathcal{L}_\\mathrm{elim}$ | Linear (0%$\\rightarrow$100%) | **74.0** |\n\nAs suggested, we also provide more experimental results on the object detection and semantic segmentation tasks using the COCO dataset. The results are shown in the below table and have been added to Section 4.2 in the revised paper (revised text is marked in red).\n\n| Method  | AP$^\\mathrm{bb}$ | AP$^\\mathrm{bb}_\\mathrm{50}$ | AP$^\\mathrm{bb}_\\mathrm{75}$ | AP$^\\mathrm{mk}$ | AP$^\\mathrm{mk}_\\mathrm{50}$ | AP$^\\mathrm{mk}_\\mathrm{75}$ |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| `Supervise` | 40.0 | 59.9 | 43.1 | 34.7 | 56.5 | 36.9 |\n| `MoCo` | 40.7 | 60.5 | 44.1 | 35.4 | 57.3 | 37.6 |\n| `PCL` | 41.0 | 60.8 | 44.2 | 35.6 | 57.4 | 37.8 |\n| `IFND (Ours)` | **41.8** | **61.2** | **44.5** | **36.1** | **57.6** | **38.5** |\n\nOverall, the proposed IFND approach performs favorably against the PCL and supervised pre-training schemes on these two tasks.\n\n---\n\n> \"The acceptance rate of the pseudo label is determined without careful consideration.\"\n\nThe main idea of this work is to progressively remove false negatives for self-supervised contrastive learning. We find that even the simple linear assignment strategy can validate our concept and improve the performance as shown in Table 5. We appreciate the reviewer's suggestion and plan to explore more schemes to determine the acceptance rate in our future work. \n\n---\n\n> \"The experiment results do not validate the effectiveness of the introduced attraction strategy\"\n\nIn Section 3.3, we compare two options to remove the false negatives from the contrastive learning objective: attraction and elimination. We give both a theoretical analysis in Section 3.3 and a quantitative study in Table 5 to discuss and compare their behavior and demonstrate that the elimination strategy is more suitable for leveraging the false negatives detected by our approach. As a result, we use the elimination strategy in all other experiments."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ]
      },
      {
        "id": "YCtce44sFz",
        "original": null,
        "number": 4,
        "cdate": 1637611256059,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637611256059,
        "tmdate": 1638182575930,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "I2FmeR4KrUt",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Comment",
        "content": {
          "title": "Responses to Reviewer qg7u",
          "comment": "We thank the reviewer for recognizing the good motivation, analysis, and experiment results in our paper. Here are our responses to the reviewer's comments.\n\n---\n\n> \"More comprehensive if this method can be extended to other methods\"\n\nBYOL only minimizes the similarity between two representations extracted from the same image, so we cannot directly apply our concept to this approach. As a fallback solution, we extend the proposed idea of incremental false negative removal to the SwAV framework. The idea is to prevent that the images with the same semantic concept are assigned with different codes when SwAV computes the codes online. The experiment results are shown in the below table (the framework in the brackets represents the baseline model).\n\n| Method  | Top-1 Acc. |\n| :-----: | :-----: |\n| `MoCo v2` | 67.5 |\n| `IFND (MoCo v2)` | 69.7 (2.2%$\\uparrow$) |\n| `SwAV` | 72.0 |\n| `IFND (SwAV)` | 72.8 (0.8%$\\uparrow$) |\n\nBy using either MoCo v2 or SwAV as the baseline learning framework, IFND can consistently boost performance. The results demonstrate the advantage of progressively removing the false negative samples. More implementation details and discussions are added in Appendix F of the revised manuscript (revised text is marked in red).\n\n---\n\n> \"Cannot find any results on object detection or segmentation\"\n\nAs suggested, we finetune the model pre-trained with our method on the COCO train2017 and evaluate on val2017 dataset. The results are shown in the below table and have been added to Section 4.2 in the revised paper.\n\n| Method  | AP$^\\mathrm{bb}$ | AP$^\\mathrm{bb}_\\mathrm{50}$ | AP$^\\mathrm{bb}_\\mathrm{75}$ | AP$^\\mathrm{mk}$ | AP$^\\mathrm{mk}_\\mathrm{50}$ | AP$^\\mathrm{mk}_\\mathrm{75}$ |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| `Supervise` | 40.0 | 59.9 | 43.1 | 34.7 | 56.5 | 36.9 |\n| `MoCo` | 40.7 | 60.5 | 44.1 | 35.4 | 57.3 | 37.6 |\n| `PCL` | 41.0 | 60.8 | 44.2 | 35.6 | 57.4 | 37.8 |\n| `IFND (Ours)` | **41.8** | **61.2** | **44.5** | **36.1** | **57.6** | **38.5** |\n\nOverall, the proposed IFND approach performs favorably against the PCL and supervised pre-training schemes on both the object detection and semantic segmentation tasks.\n\n---\n\n> \"Results by pre-training on COCO\"\n\nMost of the existing self-supervised learning work (e.g., SimCLR, MoCo, SwAV, BYOL, SimSiam) focuses on learning the image representations on object-centric datasets, such as ImageNet. The representation learning on the complicated scene datasets, where the images contain multiple objects with complex relationships, remains a challenging problem. We leave this problem as one of our future research directions.\n\n[Update in 11/29]\n\nWe extend IFND to multi-object datasets using dense contrastive learning [1] as a baseline framework.\n \nIn [1], the authors define positive/negative samples for multi-object images in a contrastive learning framework. Specifically, they discard the avgpool layer in ResNet and define the \"instance\" as the feature extracted from a specific region of the image.\nAs such, the false negatives in this work are defined as: the features extracted from other regions which are semantically similar to the anchor region. With this definition, we can extend IFND to multi-object datasets.\n\nWe present the experiment for this idea as follows.\nWe perform clustering on the embedding space after the dense projection head, where \"instance\" is defined as a feature extracted from a specific region in the image.\nIn order to remove the false negative samples, we revise the instance-level loss (Eq. (2) in DenseCL) to our elimination loss.\nWe follow the training details described in their paper but only pre-train the model in 100 epochs due to time constraints.\nThe pre-training model is trained on train2017 and evaluated on val2017 dataset. The results are shown in the below table.\n\n| Method  | AP$^\\mathrm{bb}$ | AP$^\\mathrm{bb}_\\mathrm{50}$ | AP$^\\mathrm{bb}_\\mathrm{75}$ | AP$^\\mathrm{mk}$ | AP$^\\mathrm{mk}_\\mathrm{50}$ | AP$^\\mathrm{mk}_\\mathrm{75}$ |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| `Dense CL*` | 38.7 | 58.2 | 42.0 | 34.5 | 55.1 | 37.0 |\n| `IFND (Ours)` | **38.9** | **58.6** | **42.3** | **34.6** | **55.5** | **37.6** |\n\n*We reproduce DenseCL by training 100 epochs for a fair comparison.\n\nThe results illustrate that the proposed IFND can also be applied to multi-object datasets.\nWe will add more details and experiment results in the final paper.\nPlease let us know whether you have additional questions or not.\n\nThank you,\n\n[1] X. Wang et al., Dense Contrastive Learning for Self-Supervised Visual Pre-Training, CVPR 2021"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ]
      },
      {
        "id": "jOlJ5flf7C9",
        "original": null,
        "number": 6,
        "cdate": 1637646094788,
        "mdate": 1637646094788,
        "ddate": null,
        "tcdate": 1637646094788,
        "tmdate": 1637646094788,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "urcnYCQAOkM",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Comment",
        "content": {
          "title": "Responses to Reviewer T8CE (2/2)",
          "comment": "> \"Comparison with other cluster-based (and other self-supervised) methods\"\n\nThe SwAV method uses a different learning framework from SimCLR and our approach, and also uses additional training strategies such as multi-crop augmentation. Therefore, it is unfair to directly compare our and the SwAV schemes. To validate the effectiveness of the proposed concept, we apply our incremental false negative removal to the SwAV method. The idea is to prevent that the images with the same semantic concept are assigned with different codes when SwAV computes the codes online. The experiment results are shown in the below table.\n\n| Method  | Top-1 Acc. |\n| :-----: | :-----: |\n| `MoCo v2` | 67.5 |\n| `IFND (MoCo v2)` | 69.7 (2.2%$\\uparrow$) |\n| `SwAV` | 72.0 |\n| `IFND (SwAV)` | 72.8 (0.8%$\\uparrow$) |\n\nBy using either MoCo v2 or SwAV as the baseline learning framework, IFND can consistently boost performance.\nThe results demonstrate the advantage of progressively removing the false negatives during contrastive learning.\nMore implementation details and discussions are added in Appendix F of the revised manuscript.\n\nThe goal of this work is to address the false negative issue for self-supervised contrastive learning. Analyzing the (dis)advantages of contrastive (e.g. SimCLR) and non-contrastive-based (e.g. BYOL) algorithms is not the major focus of this work. Nevertheless, as shown in the above table, the proposed concept improves the SwAV approach, which demonstrates comparable performance to non-contrastive-based learning methods, such as BYOL. Moreover, non-contrastive-based schemes have been shown to have the model collapsing problem in some tasks, such as human pose estimation [1]. In contrast, the proposed approach improves the performance of contrastive learning approaches, which do not suffer from the model collapsing issue.\n\n[1] R. Xie et al., An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation, ICCV 2021\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ]
      },
      {
        "id": "kl2CIquvwqg",
        "original": null,
        "number": 7,
        "cdate": 1637646158014,
        "mdate": 1637646158014,
        "ddate": null,
        "tcdate": 1637646158014,
        "tmdate": 1637646158014,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "urcnYCQAOkM",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Comment",
        "content": {
          "title": "Responses to Reviewer T8CE (1/2)",
          "comment": "We thank the reviewer for the insightful comments and suggestions to this work. We address the raised issues below.\n\n---\n\n> \"Limited novelty and inappropriate credit for PCL\"\n\nThe main contributions of this work consist of 1) highlighting the unfavorable effect of false negatives in self-supervised contrastive learning, 2) introducing a framework that removes the false negatives during contrastive learning, and 3) developing an incremental scheme to obtain more reliable pseudo-labels for detecting false negatives. Our method is significantly different from the PCL approach in several aspects. As described in Section 2, the PCL method uses the instance-level contrastive loss that still suffers from the effect of false negatives. Although our method uses a similar hierarchical semantic concept, it is not our major contribution. We have added the citation to the PCL approach for the clustering strategy and hierarchical semantic definition in Section 3.2 of the revised paper.\n\n---\n\n> \"Ablation study on the difference from PCL\"\n\nWe present experimental evaluations between our and PCL frameworks in Table 6 of the originally submitted manuscript as below. In the experiments, we first compare $\\mathcal{L}_\\mathrm{ProtoNCE}$ and $\\mathcal{L}_\\mathrm{elim}$ to show the effectiveness of removing false negatives. Secondly, we also demonstrate the advantage of using pseudo labels in a linear strategy.\n\n| Method  | Objective | Pseudo Label Assignment | Top-1 Acc. (%) |\n| :-----: | :-----: | :-----: | :-----: |\n| `SimCLR` | $\\mathcal{L}_\\mathrm{inst}$ | Constant (0%) | 71.5 |\n| `PCL` | $\\mathcal{L}_\\mathrm{ProtoNCE}$ | Step (0%$\\rightarrow$100%) | 72.8 |\n| | $\\mathcal{L}_\\mathrm{elim}$ | Step (0%$\\rightarrow$100%) | 73.1 |\n| | $\\mathcal{L}_\\mathrm{ProtoNCE}$ | Linear (0%$\\rightarrow$100%) | 73.4 |\n| `IFND` | $\\mathcal{L}_\\mathrm{elim}$ | Linear (0%$\\rightarrow$100%) | **74.0** |\n\nAs suggested, we conduct the ablation study to understand the impact of the number of clusters. The experimental setup is the same as in Table 6 and Appendix E. The results are shown as follows. The performance of the PCL approach degrades significantly when the number of clusters becomes smaller. In contrast, the proposed method is more robust to the number of clusters.\n\n| Method  | Objective | Numbers of clusters | Top-1 Acc. (%) |\n| :-----: | :-----: | :-----: | :-----: |\n| `PCL` | $\\mathcal{L}_\\mathrm{ProtoNCE}$ | {2500, 5000, 10000} | 72.8 |\n| | $\\mathcal{L}_\\mathrm{ProtoNCE}$ | {100, 300, 1000} | 69.9 (2.9$\\downarrow$) |\n| | $\\mathcal{L}_\\mathrm{elim}$ | {2500, 5000, 10000} | 73.2 (0.8$\\downarrow$) |\n| `IFND` | $\\mathcal{L}_\\mathrm{elim}$ | {100, 300, 1000} | 74.0 |\n\nWe also conduct the ablation study to compare different strategies to acquire the confidence score and show the results as below. The representation learning with our confidence estimation performs better than the one in PCL. One reason is that the concentration estimation in PCL only assigns a single confidence score for the instances within the same cluster. In contrast, our method respectively computes the confidence score for each instance and also refers to the centroids of other clusters. Hence, it can better estimate the confidence of each individual pseudo label.\n\n| Method  | Objective | Confidence Estimation | Top-1 Acc. (%) |\n| :-----: | :-----: | :-----: | :-----: |\n| `IFND` | $\\mathcal{L}_\\mathrm{elim}$ | Eq. 2 in our paper | 74.0 |\n| | $\\mathcal{L}_\\mathrm{elim}$ | Eq. 9 in PCL | 71.2 |\n\nWe have added these two studies to Appendix E of the revised manuscript."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ]
      },
      {
        "id": "19MC10qWZcj",
        "original": null,
        "number": 8,
        "cdate": 1637840358385,
        "mdate": null,
        "ddate": null,
        "tcdate": 1637840358385,
        "tmdate": 1637840474989,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "urcnYCQAOkM",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Comment",
        "content": {
          "title": "Response to the Rebuttal",
          "comment": "I sincerely read all reviews and rebuttals. The rebuttal addressed some of my (and other reviewers') concerns, but it is unsatisfactory considering two weeks of rebuttal periods. The rebuttal was submitted right before the deadline, and thus we had no chance of a second iteration. For this reason, I keep my original rating of weak reject.\n\nI first appreciate that the authors added the detailed ablation study with PCL (concern 2, or simply C2), justifying the design choices of IFND. However, other concerns are not fully resolved, as stated below.\n\n---\n\n> C1 (also C1 of Reviewer 7ZtC): Novelty over PCL.\n\nI agree that the paper has some contribution over PCL, addressing the false negative issue. However, I respectively disagree that IFND is \"significantly different\" from PCL.\n\nFor example, the rebuttal claims to remove InfoNCE and only use ProtoNCE (original ProtoNCE contains InfoNCE, but I mean the right term of Eq. (11) of PCL) is the major difference over PCL. However, PCL could choose any InfoNCE + ProtoNCE combinations, and they use the InfoNCE term just for an empirical reason. It was a nice suggestion to drop InfoNCE, but I'm not sure it is a significant technical innovation over PCL. On the other hand, while the rebuttal claims IFND uses a \"similar\" hierarchical semantic concept of PCL, it is the \"same\" concept in my understanding.\n\nThe revised paper added two mentions that IFND components are built on PCL, but they are still under the \"proposed method\" section.\n\n---\n\n> C3 (also C1 of Reviewer qg7u): Comparison with non-contrastive methods.\n\nThe rebuttal included additional results of SwAV+IFND, which would extend the contribution of the paper. This result is interesting since SwAV is state-of-the-art on various benchmarks (on par with BYOL). Could IFND also gain for the larger setup, e.g., applied on the last row of Table 1? If then, the impact of this paper would be significantly raised. (p.s., while the rebuttal claim that comparison with SwAV is unfair due to the multi-crop, the reader may wonder the final performance of SwAV+multi-crop+IFND) By the way, the current description in Appendix F is somewhat ambiguous. Could you elaborate on the difference from the original SwAV?\n\n---\n\n> C3 of Reviewer qg7u: Pre-training on multi-object images.\n\nAs Reviewer qg7u pointed out, the definition of positive and negative samples are ambiguous for multi-object images, significantly limiting its applicability in real-world situations. It would be appreciated if the rebuttal suggested some ideas to extend IFND for those scenarios."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Reviewer_T8CE"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Reviewer_T8CE"
        ]
      },
      {
        "id": "4Ki81F411An",
        "original": null,
        "number": 9,
        "cdate": 1637928806510,
        "mdate": 1637928806510,
        "ddate": null,
        "tcdate": 1637928806510,
        "tmdate": 1637928806510,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "19MC10qWZcj",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Comment",
        "content": {
          "title": "Unsolved concerns from Reviewer T8CE",
          "comment": "We thank the reviewer for the additional comments. \n\nSorry for the late responses. Our paper receives a late review (Nov 10) and we need about a week to conduct all experiments and then revise the paper. We would continue to provide more details until the end of Nov 29 to address any concerns. Here are the responses to the reviewer's comments.\n\n---\n\n> Novelty over PCL.\n\nProtoNCE is the combination of (1) infoNCE (left term) and (2) prototypical contrastive (right term) losses.\nIn this paper, we provide in-depth analysis on both of them and show the importance to improve both of them:\n\n(1) Removing instance-level infoNCE loss requires recognizing that the false negatives have negative effects during contrastive learning.\nThe concept of false negatives is not introduced in PCL. In contrast, we extensively analyze this issue in the paper that leads to the solution of removing infoNCE.\n\n(2) Prototypical contrastive loss uses the attraction strategy to handle the pseudo labels (i.e., the right term of Eq. 11 in PCL takes a similar form to Eq. 5 in our paper).\nIn this paper, we provide a detailed comparison between the attraction and suggested elimination strategies using both the theoretical analysis described in Section 3.3 and the quantitative study shown in Table 5.\nBoth studies showed that compared to the elimination scheme, the attraction method is more sensitive to the noisy pseudo labels generated by clustering and performs unfavorably in the unsupervised setting.\n\nOverall, both removing false negatives and applying the elimination strategies require meticulous study and algorithmic designs to integrate all modules to achieve state-of-the-art performane. Also, both of the improvements show clear conceptual differences between our and PCL approaches. \n\nClustering and hierarchical semantic concepts are two components of our algorithm. Hence, we include them in Section 3. As suggested, we will revise the title of Section 3 to \"Methodology\". We certainly agree that we should give due credit to PCL. On the other hand, we believe we have emphasized the differences and importance of the proposed modules to achieve state-of-the-art results. \n\n---\n\n> Comparison with non-contrastive methods.\n\nWe can only train our method on a smaller setup due to limited computing resources and time constraints for the rebuttal. Please note that the performance in Table 8 is the number with multi-crop augmentation (i.e., 2\u00d7160 + 4\u00d796 as mentioned in Appendix F).\n\nRegarding the difference from the original SwAV, SwAV originally follows the equipartition constraint to compute code online which makes sure the codes for different images in a batch are distinct. To remove the false negatives, we force the codes of semantically similar images to be the same. Specifically, when computing code through Eq. (3), Z originally contains the features directly extracted from different images. In our approach, we use one representative feature to represent the features of semantically similar images by averaging them. Hence, Z includes some same features shared by the semantically similar images. Moreover, during contrastive learning (through minimizing Eq. (2)), we remove the false negatives from the negative sample set (i.e, the dominator).\n\nWe will add these details to the paper.\n\n---\n\n> Pre-training on multi-object images.\n\nIn [1], the authors define positive/negative samples for multi-object images in a contrastive learning framework. Specifically, they discard the avgpool layer in ResNet and define the \"instance\" as the feature extracted from a specific region of the image.\n\nAs such, the false negatives in this work are defined as: the features extracted from the regions which are semantically similar to the anchor region. With this definition, we can extend IFND to multi-object datasets.\n\n[1] X. Wang et al.,  Dense Contrastive Learning for Self-Supervised Visual Pre-Training, CVPR 2021\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ]
      },
      {
        "id": "mSqttR6HO24",
        "original": null,
        "number": 15,
        "cdate": 1638152586000,
        "mdate": 1638152586000,
        "ddate": null,
        "tcdate": 1638152586000,
        "tmdate": 1638152586000,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "4Ki81F411An",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Comment",
        "content": {
          "title": "Response to the 2nd Rebuttal",
          "comment": "Thank you for the additional response.\n\nFirst, I remark that I agree with the proposed two components: (1) removing false negatives and (2) suggestion of elimination strategy are nice improvements over PCL. While I agree with the novelty of the proposed components, I claimed it is *limited* as the overall framework (e.g., clustering and hierarchical concepts) follows the one of PCL.\n\nI think the newly proposed (yet not fully riped) (1) extension to SwAV (and other non-contrastive methods) and (2) extension to multi-object images would be a clear difference and novelty over PCL. I suggest the revised paper include them in the main text and provide supporting experiments, which would strengthen the impact of the paper rather than being an improvement over PCL."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Reviewer_T8CE"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Reviewer_T8CE"
        ]
      },
      {
        "id": "Orb14usGgE",
        "original": null,
        "number": 16,
        "cdate": 1638182059903,
        "mdate": 1638182059903,
        "ddate": null,
        "tcdate": 1638182059903,
        "tmdate": 1638182059903,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "mSqttR6HO24",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Comment",
        "content": {
          "title": "Responses to Reviewer T8CE",
          "comment": "Thanks for the further response!!\n\nHere, we would like to supplement the experimental results on the multi-object COCO dataset.\nIn the experiment, we extend our IFND to dense contrastive learning framework (abbreviated as DenseCL) [1] as mentioned in the previous response.\nSpecifically, we perform clustering on the embedding space after the dense projection head, where \"instance\" is defined as a feature extracted from a specific region in the image.\nIn order to remove the false negative samples, we revise the instance-level loss (Eq. (2) in DenseCL) to our elimination loss.\nWe follow the training details described in their paper but only pre-train the model in 100 epochs due to time constraints.\nThe pre-training model is trained on train2017 and evaluated on val2017 dataset. The results are shown in the below table.\n\n| Method  | AP$^\\mathrm{bb}$ | AP$^\\mathrm{bb}_\\mathrm{50}$ | AP$^\\mathrm{bb}_\\mathrm{75}$ | AP$^\\mathrm{mk}$ | AP$^\\mathrm{mk}_\\mathrm{50}$ | AP$^\\mathrm{mk}_\\mathrm{75}$ |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| `Dense CL*` | 38.7 | 58.2 | 42.0 | 34.5 | 55.1 | 37.0 |\n| `IFND (Ours)` | **38.9** | **58.6** | **42.3** | **34.6** | **55.5** | **37.6** |\n\n*We reproduce DenseCL by training 100 epochs for a fair comparison.\n\nThe results illustrate that the proposed IFND can also be applied to multi-object datasets.\n\nAs suggested, we will (1) move the extension to SwAV to the main paper and (2) provide more implementation details and complete experiment results on the multi-object datasets in the final paper.\n\n[1] X. Wang et al., Dense Contrastive Learning for Self-Supervised Visual Pre-Training, CVPR 2021\n"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ]
      },
      {
        "id": "DdO39VlGyA6",
        "original": null,
        "number": 17,
        "cdate": 1638184210222,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638184210222,
        "tmdate": 1638189954172,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "dDjSKKA5TP1",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Comment",
        "content": {
          "title": "Summary of the general concerns and author responses",
          "comment": "Near the end of the discussion, we thank the reviewer again for giving lots of insightful and valuable suggestions. Here, we summarize the general concerns raised by the reviewers and our corresponding responses as below.\nWe will carefully make sure all responses mentioned below will be added to the final paper to fully solve the concerns.\n\n---\n\n> Reviewers 7ZtC, T8CE: \"Novelty and credit to PCL\"\n\n+ We highlight three differences from PCL: 1) removing the false negatives, 2) applying the elimination strategy, and 3) developing an incremental scheme to use pseudo labels.\n\n+ We provide several studies to emphasize the importance to improve each proposed component. For 1), we explore the effect of false negatives for self-supervised contrastive learning in Section 4.3 and Table 5. For 2), we present the theoretical and quantitative comparison between the suggested elimination and the attraction (which is applied in PCL) strategies in Section 3.3 and Table 6. For 3), we show that following the training, the embedding space progressively becomes more semantically structural in Section 4.5 and Appendix G. These studies explain why the proposed IFND outperforms PCL with a clear gap and also achieves a state-of-the-art result.   \n\n+ Thanks to the suggestion from reviewers T8CE and 7ZtC. We also provide an in-depth comparison with PCL in terms of 1) objective function, 2)  pseudo label assignment, 3) numbers of clusters, and 4) confidence estimation in Appendix E of the revised paper.\nThe comparison not only shows the superiority of the proposed concepts but also illustrates that IFND is more robust to the number of clusters, which is important for an unsupervised learning framework.\n\n+ Regarding the concepts of clustering and hierarchical semantic definition, we have added the citation of PCL in Section 3.2. We will move the part of the hierarchical semantic definition to Section 4.1 Implementation Details, and revise the title of Section 3 to \"Methodology\" in the final paper.\n\n---\n\n> Reviewers T8CE, qg7u: \"Extension of IFND to other methods and multi-object datasets\"\n\n+ Thanks to the suggestion from reviewers T8CE and qg7u. In Appendix F of the revised paper, we have added the extension of IFND which uses SwAV as a baseline framework. We demonstrate a 0.8% performance gain from IFND compared to vanilla SwAV. We will move the result to Section 4 in the main paper and add more implementation details in the final paper.\n\n+ As suggested by reviewers qg7u and T8CE, we also extend IFND to the multi-object datasets by using dense contrastive learning [1] as a baseline framework. We show the potential of IFND on the multi-object images as in the supplemented experiment. We will provide the complete experiment results in the final paper.\n\n[1] X. Wang et al., Dense Contrastive Learning for Self-Supervised Visual Pre-Training, CVPR 2021\n\n---\n\n> Reviewers 7ZtC, qg7u: \"Evaluation on object-detection tasks\"\n\n+ We have added the experiments in Section 4.2 of the revised paper. The results show that the proposed IFND approach performs favorably against the PCL and supervised pre-training schemes on both object-detection and semantic segmentation.\n\nWe believe the corresponding responses and results have covered most of your concerns.\n\nPlease let us know whether you have additional questions or not.\n\nThanks,"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ]
      },
      {
        "id": "AKZqG39UvY0",
        "original": null,
        "number": 18,
        "cdate": 1638187142861,
        "mdate": 1638187142861,
        "ddate": null,
        "tcdate": 1638187142861,
        "tmdate": 1638187142861,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "orJ95nqeOwH",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Comment",
        "content": {
          "title": "Response to the Rebuttal ",
          "comment": "Thanks a lot for the updated results and explanations. \nMost of my concerns have been well addressed. I will give a higher rating."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Reviewer_7ZtC"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Reviewer_7ZtC"
        ]
      },
      {
        "id": "F2WRIjPXMzF",
        "original": null,
        "number": 1,
        "cdate": 1642696833242,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696833242,
        "tmdate": 1642696833242,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "dDjSKKA5TP1",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "While the reviewers were somewhat split on this paper, they all found some strengths, and pointed out some weaknesses. Among these the main seems to be the somewhat incremental nature of the work, in particular with respect to PCL. As the authors point out, the differences w.r.t. PCL are meaningful and include the main thrust of the paper (removal of false negatives), and the results do indicate usefulness of the proposed approach. Given the wide interest in self-supervision I think the paper is above bar for acceptance."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "directReplies": [
      {
        "id": "I2FmeR4KrUt",
        "original": null,
        "number": 1,
        "cdate": 1635112260598,
        "mdate": null,
        "ddate": null,
        "tcdate": 1635112260598,
        "tmdate": 1638201675929,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "dDjSKKA5TP1",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Review",
        "content": {
          "summary_of_the_paper": "This paper deals with the very important topic of \u201cfalse negatives\u201d in SSL and shows that \u201cfalse negatives\u201d have a huge impact on performance of SSL methods. One of the most interesting experiments they show is that as they increase the number of classes in the dataset the performance drops more and more. They propose a clustering based method which reduces the  \u201cfalse negatives\u201d and shows improved performances as compared to other state-of-the-art baselines.\n",
          "main_review": "Strengths: \n1) Well written and easy to follow paper with a very solid hypothesis and analysis of effects of \u201cfalse negatives\u201d on contrastive learning.\n2) They have shown good results specially under classification settings in both supervised learning and semi-supervised learning setups.\n3) Table4 shows really good analysis and shows how increasing the depth results in more and more poor performances. \n4) Better clustering results than SWaV: Table 3 results are very promising as they show that IFND achieves better NMI results than SWaV and MoCo-V2. This means that IFND is really able to form better clusters and show improved performances.\n\nWeakness:\n1) Results on BYOL and other non-contrastive methods: The method would be much more comprehensive if this method can be extended to other non-contrastive methods such BYOL as well. Recently BYOL and other non-contrastive methods are showing much more promise than contrastive methods. Hence changing the loss function using similar motivation from IFND would be really good to see. \n2) Object-Detection and Semantic Segmentation results on COCO/VOC: I couldn't find any results on object detection or segmentation. Adding those results would make the paper more complete.\n3) Another interesting result would be to see the performance of IFND by pre-training on the COCO dataset. ImageNet in a way is already clustered and finding close \u201cfalse negatives\u201d would be easier on ImageNEt as compared to COCO. Results by pre-training on COCO would make the method much more convincing.\n",
          "summary_of_the_review": "I like the idea used in the paper and analysis shown in the paper. However lack of results on object detection and semantic segmentation coupled with lack of results using BYOL method, I'm leaning towards borderline rejection. Adding these results would make the paper stronger.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "6: marginally above the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Reviewer_qg7u"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Reviewer_qg7u"
        ]
      },
      {
        "id": "urcnYCQAOkM",
        "original": null,
        "number": 2,
        "cdate": 1636388675334,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636388675334,
        "tmdate": 1636388675334,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "dDjSKKA5TP1",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Review",
        "content": {
          "summary_of_the_paper": "The paper proposes incremental false-negative detection (IFND), which has several improvements over prototypical contrastive learning (PCL). The key idea of PCL is to alternatively update (a) the pseudo-labels based on the current encoder and (b) update the encoder based on the current pseudo-labels. However, PCL often converges to the local minima since the bad encoder produces falsy pseudo-labels, further harming the encoder. To tackle the issue, INFD suggests including the confident pseudo-labels while uncertain ones incrementally. IFND consistently improves the PCL in all considered scenarios.",
          "main_review": "### Pros\n\n- Clear motivation over the prior work and a simple yet effective solution.\n- Nice details on the method, e.g., linear scheduling of the acceptance rate, to be hyperparameter-free.\n- Nice experimental setup, e.g., comparing with SimCLR and SupCon to provide lower and upper bounds.\n\n\n### Cons/concerns\n\n1. Limited novelty and inappropriate credit for PCL\n\nThe proposed method is somewhat incremental over the prior work, PCL. As PCL develops the core concept of pseudo-label-based contrastive learning, the paper should clearly illustrate the contribution of prior work and its novel innovations. While the paper briefly introduces PCL in the related work section, the paper should clearly state PCL as the preliminary in the main method section.\n\nFor example, the \"hierarchical semantic definition\" part of the paper (page 4) suggests using multiple clusters to consider the different hierarchy levels. However, this technique was already introduced in the original PCL paper (Eq. (11)).\n\n\n2. Ablation study on the difference from PCL\n\nPutting aside omitting uncertain pseudo-labels, IFND has several minor differences from the PCL. After clearly stating the differences, the paper could provide the ablation study for each modified component, e.g.,\n- Confidence of pseudo-label: One may use Eq. (9) of PCL instead of the proposed Eq. (2), although the latter seems to be better as the former does not consider different classes.\n- Number of clusters: The original PCL used {25000,50000,100000} but INFD uses {1000,3000,10000}. It can be unfair since the desired number of clusters are 1000 for ImageNet.\n\n\n3. Comparison with other cluster-based (and other self-supervised) methods\n\nTable 1 suggests that SwAV, another cluster-based method, performs the best. While the paper only verifies the merit over its predecessor, PCL, the empirical contribution could be limited if it underperforms than other self-supervised methods. Thus, the paper should compare IFND with SwAV for all experiments, under the same setup (e.g., 200 epochs).\n\nWhile BYOL is not cluster-based method, it does not suffer from the suggested false negative issue, which weaken the motivation of this work. The paper could also verify the advantages over the BYOL to strengthen the desirability of proposed method.\n\n\nMinor comments:\n- While the paper introduces two losses: elimination and attraction, the paper concludes that elimination is better for considered scenarios. The paper could mainly introduce the single final method and leave the inferior one for the discussion.\n- The definition of false negative $\\mathcal{N}(i)$ could be introduced before it is used in Eq. (3).",
          "summary_of_the_review": "The paper proposes a solid yet somewhat incremental modification from the prior work, PCL. Overall, I think the paper is on the boarderline. I'm willing to raise my score if my concerns: (a) improper credit for PCL, (b) ablation study over PCL, amd (c) comparison with SwAV/BYOL are addressed.",
          "correctness": "4: All of the claims and statements are well-supported and correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "5: marginally below the acceptance threshold",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Reviewer_T8CE"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Reviewer_T8CE"
        ]
      },
      {
        "id": "hw0g1V0WZPg",
        "original": null,
        "number": 3,
        "cdate": 1636484803387,
        "mdate": null,
        "ddate": null,
        "tcdate": 1636484803387,
        "tmdate": 1638187184657,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "dDjSKKA5TP1",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Review",
        "content": {
          "summary_of_the_paper": "Briefly, this paper presents a simple yet effective method (i.e., IFND) for self-supervised contrastive learning by effectively handling the false-negative samples. Specifically, the authors propose to cluster samples in the embedding space and assign the cluster indices as pseudo labels in an incremental manner during training. The experimental results are extensive and demonstrate the effectiveness of the proposed method.",
          "main_review": "Strengths:\n1) The paper is easy to follow with clear motivation. The discussion and analysis about \u201cfalse negatives\u201d on contrastive learning in Fig1 is quite valuable. \n2) The authors justify the effectiveness of the proposed method according to the extensive evaluations. Besides, the ablation study also validates the contribution of the main component of the proposed IFND.\n\nWeaknesses:\n1) The technical novelty of the proposed method is somewhat marginal since some of its main components (i.e., hierarchical semantic definition) have already been mentioned in PCL (Li et al., 2021). The authors should present more detailed analyses and discussions to highlight the difference between PCL and their proposed IFND. For instance, it will be better if the authors can conduct experiments on the object detection task to demonstrate the consistently superior performance of IFND over PCL. \n2) The acceptance rate of the pseudo label is determined without careful consideration. According to Appendix B, the authors empirically set the acceptance rate in a hyperparameter-free manner (i.e., Epoch / Training Epoch). This might make the learning process unstable due to the accumulated pseudo-labeling errors. It might be better if the authors can explore some robust learning techniques (e.g., self-paced learning) to improve learning efficiency.\n3) It seems that the experiment results do not validate the effectiveness of the introduced attraction strategy according to Table 5.",
          "summary_of_the_review": "This paper introduces some interesting insights for contrastive learning. However, its presentation and experiment design could be improved. I will raise my rating if most of my concerns have been well addressed.",
          "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
          "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
          "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
          "flag_for_ethics_review": [
            "NO."
          ],
          "recommendation": "8: accept, good paper",
          "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Reviewer_7ZtC"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Reviewer_7ZtC"
        ]
      },
      {
        "id": "DdO39VlGyA6",
        "original": null,
        "number": 17,
        "cdate": 1638184210222,
        "mdate": null,
        "ddate": null,
        "tcdate": 1638184210222,
        "tmdate": 1638189954172,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "dDjSKKA5TP1",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Official_Comment",
        "content": {
          "title": "Summary of the general concerns and author responses",
          "comment": "Near the end of the discussion, we thank the reviewer again for giving lots of insightful and valuable suggestions. Here, we summarize the general concerns raised by the reviewers and our corresponding responses as below.\nWe will carefully make sure all responses mentioned below will be added to the final paper to fully solve the concerns.\n\n---\n\n> Reviewers 7ZtC, T8CE: \"Novelty and credit to PCL\"\n\n+ We highlight three differences from PCL: 1) removing the false negatives, 2) applying the elimination strategy, and 3) developing an incremental scheme to use pseudo labels.\n\n+ We provide several studies to emphasize the importance to improve each proposed component. For 1), we explore the effect of false negatives for self-supervised contrastive learning in Section 4.3 and Table 5. For 2), we present the theoretical and quantitative comparison between the suggested elimination and the attraction (which is applied in PCL) strategies in Section 3.3 and Table 6. For 3), we show that following the training, the embedding space progressively becomes more semantically structural in Section 4.5 and Appendix G. These studies explain why the proposed IFND outperforms PCL with a clear gap and also achieves a state-of-the-art result.   \n\n+ Thanks to the suggestion from reviewers T8CE and 7ZtC. We also provide an in-depth comparison with PCL in terms of 1) objective function, 2)  pseudo label assignment, 3) numbers of clusters, and 4) confidence estimation in Appendix E of the revised paper.\nThe comparison not only shows the superiority of the proposed concepts but also illustrates that IFND is more robust to the number of clusters, which is important for an unsupervised learning framework.\n\n+ Regarding the concepts of clustering and hierarchical semantic definition, we have added the citation of PCL in Section 3.2. We will move the part of the hierarchical semantic definition to Section 4.1 Implementation Details, and revise the title of Section 3 to \"Methodology\" in the final paper.\n\n---\n\n> Reviewers T8CE, qg7u: \"Extension of IFND to other methods and multi-object datasets\"\n\n+ Thanks to the suggestion from reviewers T8CE and qg7u. In Appendix F of the revised paper, we have added the extension of IFND which uses SwAV as a baseline framework. We demonstrate a 0.8% performance gain from IFND compared to vanilla SwAV. We will move the result to Section 4 in the main paper and add more implementation details in the final paper.\n\n+ As suggested by reviewers qg7u and T8CE, we also extend IFND to the multi-object datasets by using dense contrastive learning [1] as a baseline framework. We show the potential of IFND on the multi-object images as in the supplemented experiment. We will provide the complete experiment results in the final paper.\n\n[1] X. Wang et al., Dense Contrastive Learning for Self-Supervised Visual Pre-Training, CVPR 2021\n\n---\n\n> Reviewers 7ZtC, qg7u: \"Evaluation on object-detection tasks\"\n\n+ We have added the experiments in Section 4.2 of the revised paper. The results show that the proposed IFND approach performs favorably against the PCL and supervised pre-training schemes on both object-detection and semantic segmentation.\n\nWe believe the corresponding responses and results have covered most of your concerns.\n\nPlease let us know whether you have additional questions or not.\n\nThanks,"
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference",
          "ICLR.cc/2022/Conference/Paper25/Authors"
        ]
      },
      {
        "id": "F2WRIjPXMzF",
        "original": null,
        "number": 1,
        "cdate": 1642696833242,
        "mdate": null,
        "ddate": null,
        "tcdate": 1642696833242,
        "tmdate": 1642696833242,
        "tddate": null,
        "forum": "dDjSKKA5TP1",
        "replyto": "dDjSKKA5TP1",
        "invitation": "ICLR.cc/2022/Conference/Paper25/-/Decision",
        "content": {
          "title": "Paper Decision",
          "decision": "Accept (Poster)",
          "comment": "While the reviewers were somewhat split on this paper, they all found some strengths, and pointed out some weaknesses. Among these the main seems to be the somewhat incremental nature of the work, in particular with respect to PCL. As the authors point out, the differences w.r.t. PCL are meaningful and include the main thrust of the paper (removal of false negatives), and the results do indicate usefulness of the proposed approach. Given the wide interest in self-supervision I think the paper is above bar for acceptance."
        },
        "signatures": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ],
        "readers": [
          "everyone"
        ],
        "nonreaders": [],
        "writers": [
          "ICLR.cc/2022/Conference/Program_Chairs"
        ]
      }
    ],
    "invitation": {
      "reply": {
        "writers": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "signatures": {
          "values": [
            "ICLR.cc/2022/Conference"
          ]
        },
        "content": {
          "title": {
            "value-regex": ".*",
            "order": 1
          },
          "authors": {
            "values": [
              "Anonymous"
            ],
            "order": 2
          },
          "authorids": {
            "value-regex": ".*",
            "order": 3
          },
          "keywords": {
            "value-regex": ".*",
            "order": 6
          },
          "abstract": {
            "value-regex": ".*",
            "order": 8
          },
          "pdf": {
            "value-regex": ".*",
            "order": 9
          },
          "one-sentence_summary": {
            "value-regex": ".*",
            "order": 10
          },
          "supplementary_material": {
            "value-regex": ".*",
            "order": 11
          },
          "code_of_ethics": {
            "value-regex": ".*",
            "order": 12
          },
          "submission_guidelines": {
            "value-regex": ".*",
            "order": 13
          },
          "resubmission": {
            "value-regex": ".*",
            "order": 14
          },
          "student_author": {
            "value-regex": ".*",
            "order": 15
          },
          "serve_as_reviewer": {
            "value-regex": ".*",
            "order": 16
          },
          "community_implementations": {
            "required": false,
            "description": "Optional link to open source implementations",
            "value-regex": ".*",
            "markdown": true,
            "order": 103
          }
        }
      },
      "replyForumViews": [],
      "expdate": 1682960118639,
      "signatures": [
        "ICLR.cc/2022/Conference"
      ],
      "readers": [
        "everyone"
      ],
      "writers": [
        "ICLR.cc/2022/Conference"
      ],
      "invitees": [
        "ICLR.cc/2022/Conference"
      ],
      "tcdate": 1632875419419,
      "tmdate": 1682960118686,
      "id": "ICLR.cc/2022/Conference/-/Blind_Submission",
      "type": "note"
    }
  },
  "tauthor": "OpenReview.net"
}