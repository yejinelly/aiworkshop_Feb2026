{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ë…¸íŠ¸ë¶ 2: Citation Crawler + Selector\n\n**SNU AI Psychology Workshop - February 2026**\n\n---\n\n## ì „ì²´ íë¦„\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  1. paperscraper    â†’   ì‹œë“œ í›„ë³´ ê²€ìƒ‰ (arXiv, PubMed)      â”‚\nâ”‚         â†“                                                   â”‚\nâ”‚  2. ì‹œë“œ ì„ íƒ       â†’   ì‹œì‘ì  ì§€ì • (ë²ˆí˜¸ or DOI)           â”‚\nâ”‚         â†“                                                   â”‚\nâ”‚  3. Crawler         â†’   ì¸ìš© ë„¤íŠ¸ì›Œí¬ í™•ì¥                  â”‚\nâ”‚      â”œâ”€ References  (ì‹œë“œê°€ ì¸ìš©í•œ ë…¼ë¬¸)                    â”‚\nâ”‚      â”œâ”€ Citations   (ì‹œë“œë¥¼ ì¸ìš©í•œ ë…¼ë¬¸)                    â”‚\nâ”‚      â””â”€ Related     (ì¸ìš© ë¬´ê´€, ìœ ì‚¬ ë…¼ë¬¸)                  â”‚\nâ”‚         â†“                                                   â”‚\nâ”‚  4. Selector        â†’   ì¿¼ë¦¬ ê¸°ë°˜ ê´€ë ¨ì„± í•„í„°ë§ (SPECTER2)  â”‚\nâ”‚         â†“                                                   â”‚\nâ”‚  5. ê²°ê³¼            â†’   Top 10 ë…¼ë¬¸ ì¶”ì²œ                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**ì˜ˆì‹œ ì£¼ì œ: Climate Anxiety (ê¸°í›„ ë¶ˆì•ˆ)**\n\n| ì¶”ì²œ í‚¤ì›Œë“œ | ì„¤ëª… |\n|-------------|------|\n| `\"climate anxiety\"` | êµ¬ë¬¸ ê²€ìƒ‰ (ì •í™•íˆ ì¼ì¹˜) |\n| `\"eco-anxiety\"` | ë™ì˜ì–´ |\n| `\"climate change\" AND \"mental health\"` | ë³µí•© ê²€ìƒ‰ |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Setup\n",
    "\n",
    "**í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ì„¸ìš”.** ëª¨ë“  í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ê°€ ì •ì˜ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆìƒ ì¶œë ¥:\n",
    "```\n",
    "âœ“ ë¡œì»¬ í™˜ê²½ / Colab í™˜ê²½\n",
    "âœ“ Semantic Scholar API Key ë¡œë“œ ì„±ê³µ\n",
    "âœ“ search_with_paperscraper() ì •ì˜ ì™„ë£Œ\n",
    "âœ“ SemanticScholarAPI ì¤€ë¹„ ì™„ë£Œ\n",
    "âœ“ CitationCrawler ì •ì˜ ì™„ë£Œ\n",
    "âœ“ SPECTER2 ë¡œë“œ ì™„ë£Œ (ì²˜ìŒ: ~500MB ë‹¤ìš´ë¡œë“œ)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup 1/6: í™˜ê²½ ì„¤ì •\n",
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    WORKSHOP_DIR = \"/content/drive/MyDrive/aiworkshop_Feb2026/\"\n",
    "    os.makedirs(WORKSHOP_DIR, exist_ok=True)\n",
    "    os.chdir(WORKSHOP_DIR)\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ“ Colab í™˜ê²½\")\n",
    "except ImportError:\n",
    "    if os.getcwd().endswith('notebooks'):\n",
    "        os.chdir('..')\n",
    "    WORKSHOP_DIR = os.getcwd()\n",
    "    IN_COLAB = False\n",
    "    print(\"âœ“ ë¡œì»¬ í™˜ê²½\")\n",
    "\n",
    "print(f\"  ì‘ì—… í´ë”: {WORKSHOP_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup 2/6: íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "if IN_COLAB:\n",
    "    print(\"íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\n",
    "    %pip install paperscraper requests pandas torch transformers adapters -q\n",
    "    print(\"âœ“ ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "else:\n",
    "    print(\"ë¡œì»¬: í•„ìš” ì‹œ ì•„ë˜ ëª…ë ¹ ì‹¤í–‰\")\n",
    "    print(\"  pip install paperscraper requests pandas torch transformers adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup 3/6: API Key & Imports\nimport requests\nimport time\nimport pandas as pd\nfrom typing import List, Dict, Set, Optional\nfrom collections import Counter\n\n# API Key ë¡œë”©\nSEMANTIC_SCHOLAR_API_KEY = None\n\nif IN_COLAB:\n    try:\n        from google.colab import userdata\n        SEMANTIC_SCHOLAR_API_KEY = userdata.get('SEMANTIC_SCHOLAR_API_KEY', None)\n    except:\n        pass\nelse:\n    try:\n        from dotenv import load_dotenv\n        # ëª…ì‹œì ìœ¼ë¡œ ì›Œí¬ìˆ ë£¨íŠ¸ì˜ .env íŒŒì¼ ê²½ë¡œ ì§€ì •\n        env_path = os.path.join(WORKSHOP_DIR, '.env')\n        load_dotenv(env_path)\n        SEMANTIC_SCHOLAR_API_KEY = os.getenv('SEMANTIC_SCHOLAR_API_KEY')\n    except:\n        pass\n\nif SEMANTIC_SCHOLAR_API_KEY:\n    print(\"âœ“ Semantic Scholar API Key ë¡œë“œ ì„±ê³µ\")\n    print(f\"  Key: {SEMANTIC_SCHOLAR_API_KEY[:10]}...\")\n    print(f\"  í—ˆìš©ëŸ‰: 5000 req/5ë¶„\")\nelse:\n    print(\"âœ— Semantic Scholar API Key ì—†ìŒ\")\n    print(\"  í—ˆìš©ëŸ‰: 100 req/5ë¶„ (ë¬´ë£Œ)\")\n    print(\"  ë°œê¸‰: https://www.semanticscholar.org/product/api#api-key-form\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup 4/6: paperscraper í•¨ìˆ˜ + API í´ë˜ìŠ¤ë“¤\n\ndef search_with_paperscraper(query: str, max_results: int = 5) -> pd.DataFrame:\n    \"\"\"paperscraperë¡œ arXiv, PubMed ê²€ìƒ‰\"\"\"\n    all_papers = []\n    \n    # arXiv\n    try:\n        from paperscraper.arxiv import get_arxiv_papers_api\n        print(\"[arXiv] ê²€ìƒ‰ ì¤‘...\")\n        df = get_arxiv_papers_api(query, max_results=max_results, verbose=False)\n        for _, row in df.iterrows():\n            all_papers.append({\n                'source': 'arXiv', 'title': row.get('title', ''),\n                'year': str(row.get('date', ''))[:4], 'doi': row.get('doi', ''),\n                'abstract': str(row.get('abstract', ''))[:200] + '...'\n            })\n        print(f\"  -> {len(df)}ê°œ\")\n    except Exception as e:\n        print(f\"[arXiv] ì˜¤ë¥˜: {e}\")\n    \n    # PubMed\n    try:\n        from paperscraper.pubmed import get_pubmed_papers\n        print(\"[PubMed] ê²€ìƒ‰ ì¤‘...\")\n        df = get_pubmed_papers(query, max_results=max_results)\n        for _, row in df.iterrows():\n            all_papers.append({\n                'source': 'PubMed', 'title': row.get('title', ''),\n                'year': str(row.get('date', ''))[:4], 'doi': row.get('doi', ''),\n                'abstract': str(row.get('abstract', ''))[:200] + '...'\n            })\n        print(f\"  -> {len(df)}ê°œ\")\n    except Exception as e:\n        print(f\"[PubMed] ì˜¤ë¥˜: {e}\")\n    \n    return pd.DataFrame(all_papers)\n\n\nclass OpenAlexAPI:\n    \"\"\"OpenAlex API ë˜í¼\"\"\"\n    BASE_URL = \"https://api.openalex.org\"\n    \n    def __init__(self, email: str = None):\n        self.params = {'mailto': email} if email else {}\n    \n    def _request(self, url: str, params: dict = None) -> dict:\n        all_params = {**self.params, **(params or {})}\n        try:\n            response = requests.get(url, params=all_params, timeout=30)\n            if response.status_code == 200:\n                return response.json()\n            return {}\n        except:\n            return {}\n    \n    def _extract_abstract(self, inv_idx: dict) -> str:\n        \"\"\"inverted indexì—ì„œ abstract ë³µì›\"\"\"\n        if not inv_idx:\n            return ''\n        try:\n            word_positions = [(word, pos) for word, positions in inv_idx.items() for pos in positions]\n            word_positions.sort(key=lambda x: x[1])\n            return ' '.join([wp[0] for wp in word_positions[:200]])\n        except:\n            return ''\n    \n    def get_paper_by_doi(self, doi: str) -> Dict:\n        \"\"\"DOIë¡œ ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ\"\"\"\n        result = self._request(f\"{self.BASE_URL}/works/doi:{doi}\")\n        if not result:\n            return {}\n        return {\n            'openalex_id': result.get('id', ''),\n            'title': result.get('title', ''),\n            'year': result.get('publication_year'),\n            'cited_by_count': result.get('cited_by_count', 0),\n            'abstract': self._extract_abstract(result.get('abstract_inverted_index')),\n            'referenced_works': result.get('referenced_works', []),\n        }\n    \n    def get_references(self, doi: str, limit: int = 50) -> List[Dict]:\n        \"\"\"ë…¼ë¬¸ì˜ references ì¡°íšŒ (cited_by ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©)\"\"\"\n        paper = self.get_paper_by_doi(doi)\n        if not paper or not paper.get('referenced_works'):\n            return []\n        \n        ref_ids = paper['referenced_works'][:limit]\n        if not ref_ids:\n            return []\n        \n        # ë°°ì¹˜ë¡œ ì¡°íšŒ (í•œ ë²ˆì˜ API í˜¸ì¶œë¡œ ìµœëŒ€ 50ê°œ)\n        short_ids = [oid.replace('https://openalex.org/', '') for oid in ref_ids if isinstance(oid, str)]\n        filter_str = \"|\".join(short_ids)\n        \n        params = {\n            'filter': f'openalex:{filter_str}',\n            'per_page': limit,\n            'select': 'id,title,publication_year,cited_by_count,abstract_inverted_index'\n        }\n        \n        result = self._request(f\"{self.BASE_URL}/works\", params)\n        if not result or 'results' not in result:\n            return []\n        \n        papers = []\n        for r in result.get('results', []):\n            papers.append({\n                'openalex_id': r.get('id', ''),\n                'paperId': r.get('id', '').replace('https://openalex.org/', ''),\n                'title': r.get('title', ''),\n                'year': r.get('publication_year'),\n                'citationCount': r.get('cited_by_count', 0),\n                'abstract': self._extract_abstract(r.get('abstract_inverted_index'))\n            })\n        \n        return papers\n\n\nclass SemanticScholarAPI:\n    \"\"\"Semantic Scholar API ë˜í¼\"\"\"\n    BASE_URL = \"https://api.semanticscholar.org/graph/v1\"\n    RECO_URL = \"https://api.semanticscholar.org/recommendations/v1\"\n    \n    def __init__(self, api_key: str = None):\n        self.headers = {'x-api-key': api_key} if api_key else {}\n        self.publisher_blocked = False\n    \n    def _request(self, url: str, params: dict = None) -> dict:\n        for attempt in range(3):\n            try:\n                response = requests.get(url, params=params, headers=self.headers, timeout=30)\n                if response.status_code == 200:\n                    return response.json()\n                elif response.status_code == 429:\n                    wait_time = 5 * (attempt + 1)\n                    print(f\"  Rate limit - {wait_time}ì´ˆ ëŒ€ê¸°...\")\n                    time.sleep(wait_time)\n                else:\n                    return {}\n            except:\n                return {}\n        return {}\n    \n    def get_paper(self, doi: str = None, title: str = None) -> Dict:\n        self.publisher_blocked = False\n        params = {\"fields\": \"paperId,title,abstract,year,citationCount,authors,externalIds\"}\n        if doi:\n            self.current_doi = doi\n            if doi.startswith(\"10.48550/arXiv.\"):\n                paper_id = f\"arXiv:{doi.replace('10.48550/arXiv.', '')}\"\n            else:\n                paper_id = f\"DOI:{doi}\"\n            result = self._request(f\"{self.BASE_URL}/paper/{paper_id}\", params)\n        elif title:\n            self.current_doi = None\n            params.update({\"query\": title, \"limit\": 1})\n            result = self._request(f\"{self.BASE_URL}/paper/search\", params)\n            result = (result.get('data') or [{}])[0] if result else {}\n        else:\n            return {}\n        \n        if result and result.get('externalIds'):\n            self.current_doi = result['externalIds'].get('DOI')\n        \n        return result if result else {}\n    \n    def get_references(self, paper_id: str, limit: int = 50) -> List[Dict]:\n        params = {\"fields\": \"paperId,title,abstract,year,citationCount\", \"limit\": limit}\n        result = self._request(f\"{self.BASE_URL}/paper/{paper_id}/references\", params)\n        data = (result.get('data') or []) if result else []\n        refs = [i['citedPaper'] for i in data if i.get('citedPaper', {}).get('paperId')]\n        \n        # ì¶œíŒì‚¬ ì°¨ë‹¨ ê°ì§€\n        if not refs and result:\n            self.publisher_blocked = True\n        \n        return refs\n    \n    def get_citations(self, paper_id: str, limit: int = 50) -> List[Dict]:\n        params = {\"fields\": \"paperId,title,abstract,year,citationCount\", \"limit\": limit}\n        result = self._request(f\"{self.BASE_URL}/paper/{paper_id}/citations\", params)\n        data = (result.get('data') or []) if result else []\n        return [i['citingPaper'] for i in data if i.get('citingPaper', {}).get('paperId')]\n    \n    def get_related(self, paper_id: str, limit: int = 20) -> List[Dict]:\n        params = {\"fields\": \"paperId,title,abstract,year,citationCount\", \"limit\": limit}\n        result = self._request(f\"{self.RECO_URL}/papers/forpaper/{paper_id}\", params)\n        return (result.get('recommendedPapers') or []) if result else []\n\n\n# API ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\napi = SemanticScholarAPI(api_key=SEMANTIC_SCHOLAR_API_KEY)\nopenalex = OpenAlexAPI()\n\nprint(\"âœ“ search_with_paperscraper() ì •ì˜ ì™„ë£Œ\")\nprint(\"âœ“ SemanticScholarAPI ì¤€ë¹„ ì™„ë£Œ\")\nprint(\"âœ“ OpenAlexAPI ì¤€ë¹„ ì™„ë£Œ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup 5/6: CitationCrawler í´ë˜ìŠ¤\n\nclass CitationCrawler:\n    \"\"\"ì¸ìš© ë„¤íŠ¸ì›Œí¬ + ê´€ë ¨ ë…¼ë¬¸ íƒìƒ‰ (Semantic Scholar + OpenAlex)\"\"\"\n    \n    def __init__(self, api: SemanticScholarAPI, openalex_api: OpenAlexAPI = None):\n        self.api = api\n        self.openalex = openalex_api\n        self.visited: Set[str] = set()\n        self.papers: Dict[str, Dict] = {}\n    \n    def crawl(self, seed_paper: Dict, seed_doi: str = None, refs_limit: int = 15, \n              cites_limit: int = 15, related_limit: int = 10, max_total: int = 50) -> List[Dict]:\n        self.visited.clear()\n        self.papers.clear()\n        \n        paper_id = seed_paper.get('paperId')\n        if not paper_id:\n            print(\"ì˜¤ë¥˜: paperId ì—†ìŒ\")\n            return []\n        \n        self.visited.add(paper_id)\n        seed_paper['source'] = 'seed'\n        self.papers[paper_id] = seed_paper\n        print(f\"ì‹œë“œ: {seed_paper.get('title', 'N/A')[:50]}...\")\n        \n        # References (Semantic Scholar)\n        print(\"\\n[References] ìˆ˜ì§‘ ì¤‘...\")\n        refs = self.api.get_references(paper_id, limit=refs_limit)\n        \n        # Semantic Scholar ì‹¤íŒ¨ ì‹œ OpenAlex ì‹œë„\n        if not refs and self.openalex and seed_doi:\n            print(\"  â†’ OpenAlexì—ì„œ ì‹œë„...\")\n            refs = self.openalex.get_references(seed_doi, limit=refs_limit)\n        \n        for p in refs:\n            pid = p.get('paperId') or p.get('openalex_id', '')\n            if pid and pid not in self.visited and len(self.papers) < max_total:\n                self.visited.add(pid)\n                p['source'] = 'reference'\n                self.papers[pid] = p\n        print(f\"  -> {len([p for p in self.papers.values() if p.get('source') == 'reference'])}ê°œ\")\n        time.sleep(0.3)\n        \n        # Citations (Semantic Scholarë§Œ)\n        print(\"[Citations] ìˆ˜ì§‘ ì¤‘...\")\n        for p in self.api.get_citations(paper_id, limit=cites_limit):\n            pid = p.get('paperId')\n            if pid and pid not in self.visited and len(self.papers) < max_total:\n                self.visited.add(pid)\n                p['source'] = 'citation'\n                self.papers[pid] = p\n        print(f\"  -> {len([p for p in self.papers.values() if p.get('source') == 'citation'])}ê°œ\")\n        time.sleep(0.3)\n        \n        # Related (Semantic Scholarë§Œ)\n        print(\"[Related] ìˆ˜ì§‘ ì¤‘...\")\n        for p in self.api.get_related(paper_id, limit=related_limit):\n            pid = p.get('paperId')\n            if pid and pid not in self.visited and len(self.papers) < max_total:\n                self.visited.add(pid)\n                p['source'] = 'related'\n                self.papers[pid] = p\n        print(f\"  -> {len([p for p in self.papers.values() if p.get('source') == 'related'])}ê°œ\")\n        \n        print(f\"\\nì´ {len(self.papers)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ\")\n        return list(self.papers.values())\n\nprint(\"âœ“ CitationCrawler ì •ì˜ ì™„ë£Œ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup 6/6: SPECTER2Selector (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ~500MB)\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "class SPECTER2Selector:\n",
    "    def __init__(self, device='cpu'):\n",
    "        print(\"SPECTER2 ë¡œë”© ì¤‘... (ì²˜ìŒ: ~500MB ë‹¤ìš´ë¡œë“œ)\")\n",
    "        from transformers import AutoTokenizer\n",
    "        from adapters import AutoAdapterModel\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "        self.model = AutoAdapterModel.from_pretrained('allenai/specter2_base')\n",
    "        self.model.load_adapter(\"allenai/specter2\", source=\"hf\", load_as=\"proximity\", set_active=True)\n",
    "        self.model.to(device).eval()\n",
    "        self.device = device\n",
    "        print(f\"âœ“ SPECTER2 ë¡œë“œ ì™„ë£Œ\")\n",
    "    \n",
    "    def get_embedding(self, title: str, abstract: str = \"\") -> torch.Tensor:\n",
    "        text = title + self.tokenizer.sep_token + (abstract or \"\")\n",
    "        inputs = self.tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.model(**inputs).last_hidden_state[:, 0, :]\n",
    "    \n",
    "    def select_top_papers(self, query: str, papers: List[Dict], top_k: int = 10) -> List[Dict]:\n",
    "        if not papers:\n",
    "            return []\n",
    "        print(f\"\\n{len(papers)}ê°œ ë…¼ë¬¸ í‰ê°€ ì¤‘...\")\n",
    "        query_emb = self.get_embedding(query)\n",
    "        for i, paper in enumerate(papers):\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  {i + 1}/{len(papers)}\")\n",
    "            paper_emb = self.get_embedding(paper.get('title', ''), paper.get('abstract', ''))\n",
    "            paper['relevance_score'] = round(cosine_similarity(query_emb, paper_emb).item(), 4)\n",
    "        papers.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)\n",
    "        print(f\"âœ“ ìƒìœ„ {top_k}ê°œ ì„ íƒ ì™„ë£Œ\")\n",
    "        return papers[:top_k]\n",
    "\n",
    "# ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ëª¨ë¸ ë¡œë“œ)\n",
    "selector = SPECTER2Selector(device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: ì‹¤í–‰\n",
    "\n",
    "Setup ì™„ë£Œ í›„, ì•„ë˜ Stepë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: í‚¤ì›Œë“œ ê²€ìƒ‰ (paperscraper)\n\n### paperscraperë€?\n[paperscraper](https://github.com/jannisborn/paperscraper)ëŠ” ì—¬ëŸ¬ í•™ìˆ  ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í†µí•© ê²€ìƒ‰í•˜ëŠ” Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n\n**ì§€ì› ë°ì´í„°ë² ì´ìŠ¤:**\n| DB | ë¶„ì•¼ | íŠ¹ì§• |\n|---|---|---|\n| **arXiv** | CS, ë¬¼ë¦¬, ìˆ˜í•™, í†µê³„, AI/ML | í”„ë¦¬í”„ë¦°íŠ¸ ì¤‘ì‹¬, ìµœì‹  ì—°êµ¬ ë¹ ë¦„ |\n| **PubMed** | ì˜í•™, ìƒëª…ê³¼í•™, ì‹¬ë¦¬í•™ | í”¼ì–´ë¦¬ë·° ì €ë„ ì¤‘ì‹¬, ì„ìƒ ì—°êµ¬ |\n| bioRxiv | ìƒëª…ê³¼í•™ í”„ë¦¬í”„ë¦°íŠ¸ | (ë³„ë„ dump í•„ìš”) |\n| medRxiv | ì˜í•™ í”„ë¦¬í”„ë¦°íŠ¸ | (ë³„ë„ dump í•„ìš”) |\n\n> ğŸ’¡ **ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” arXiv + PubMedë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.**\n\n**ì™œ Google Scholar/Scopusê°€ ì•„ë‹Œê°€?**\n- Google Scholar: ê³µì‹ API ì—†ìŒ (ìŠ¤í¬ë˜í•‘ í•„ìš” â†’ ì°¨ë‹¨ ìœ„í—˜)\n- Scopus: ìœ ë£Œ API (ê¸°ê´€ êµ¬ë… í•„ìš”)\n- Semantic Scholar: **ë¬´ë£Œ ê³µì‹ API** ì œê³µ â†’ Crawlerì—ì„œ ì‚¬ìš©\n\n---\n\n### ì¿¼ë¦¬ í˜•ì‹\n- ë‹¨ìˆœ: `climate anxiety`\n- AND: `climate AND anxiety`\n- OR: `climate OR weather`\n- êµ¬ë¬¸: `\"eco-anxiety\"`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: í‚¤ì›Œë“œ ê²€ìƒ‰\nQUERY = '\"climate anxiety\"'  # <- ìˆ˜ì • ê°€ëŠ¥! (ë”°ì˜´í‘œë¡œ êµ¬ë¬¸ ê²€ìƒ‰)\n\nprint(f\"ì¿¼ë¦¬: {QUERY}\")\nprint(\"=\" * 50)\ncandidates_df = search_with_paperscraper(QUERY, max_results=5)\nprint(f\"\\nì´ {len(candidates_df)}ê°œ í›„ë³´ ë°œê²¬\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í›„ë³´ ëª©ë¡ ë³´ê¸°\nfrom IPython.display import display, HTML\n\nif len(candidates_df) > 0:\n    candidates_df.insert(0, 'No', range(1, len(candidates_df) + 1))\n    print(\"\\ní›„ë³´ ë…¼ë¬¸ ëª©ë¡:\")\n    print(\"=\" * 70)\n    \n    html_rows = []\n    for _, row in candidates_df.iterrows():\n        title = row['title'][:55] + '...' if len(str(row['title'])) > 55 else row['title']\n        doi = row['doi']\n        doi_link = f'<a href=\"https://doi.org/{doi}\" target=\"_blank\">{doi}</a>' if doi else \"N/A\"\n        \n        html_rows.append(f\"\"\"\n        <tr>\n            <td style=\"vertical-align:top; padding:5px;\"><b>{row['No']}</b></td>\n            <td style=\"vertical-align:top; padding:5px;\">[{row['source']}]</td>\n            <td style=\"padding:5px;\">\n                {title}<br>\n                <small>ì—°ë„: {row['year']} | DOI: {doi_link}</small>\n            </td>\n        </tr>\n        \"\"\")\n    \n    html_table = f\"\"\"\n    <table style=\"border-collapse:collapse; width:100%;\">\n        {''.join(html_rows)}\n    </table>\n    \"\"\"\n    display(HTML(html_table))\nelse:\n    print(\"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ. ì¿¼ë¦¬ë¥¼ ë³€ê²½í•˜ì„¸ìš”.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ì‹œë“œ ì„ íƒ\n",
    "\n",
    "ìœ„ ëª©ë¡ì—ì„œ **ë²ˆí˜¸**ë¡œ ì„ íƒí•˜ê±°ë‚˜, **DOIë¥¼ ì§ì ‘ ì…ë ¥**í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: ì‹œë“œ ì„ íƒ\n",
    "SEED_INDEX = 1        # <- ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)\n",
    "SEED_DOI = None       # <- DOI ì…ë ¥ ì‹œ ë²ˆí˜¸ ë¬´ì‹œ\n",
    "\n",
    "# ì‹œë“œ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "if SEED_DOI:\n",
    "    print(f\"DOIë¡œ ê²€ìƒ‰: {SEED_DOI}\")\n",
    "    seed_paper = api.get_paper(doi=SEED_DOI)\n",
    "elif len(candidates_df) >= SEED_INDEX:\n",
    "    row = candidates_df.iloc[SEED_INDEX - 1]\n",
    "    print(f\"{SEED_INDEX}ë²ˆ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘...\")\n",
    "    seed_paper = api.get_paper(doi=row['doi']) if row.get('doi') else api.get_paper(title=row['title'])\n",
    "else:\n",
    "    print(f\"ì˜¤ë¥˜: {SEED_INDEX}ë²ˆ ë…¼ë¬¸ ì—†ìŒ\")\n",
    "    seed_paper = {}\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "if seed_paper.get('paperId'):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ“ ì‹œë“œ ë…¼ë¬¸ ì„ íƒ ì™„ë£Œ\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ì œëª©: {seed_paper.get('title', 'N/A')}\")\n",
    "    print(f\"ì—°ë„: {seed_paper.get('year', 'N/A')} | ì¸ìš©: {seed_paper.get('citationCount', 0)}\")\n",
    "    authors = seed_paper.get('authors', [])\n",
    "    if authors:\n",
    "        print(f\"ì €ì: {', '.join([a.get('name','') for a in authors[:3]])}\" + (\" ì™¸\" if len(authors) > 3 else \"\"))\n",
    "else:\n",
    "    print(\"\\nâœ— ì‹œë“œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Crawler ì‹¤í–‰\n\n### Citation Crawlerë€?\nì‹œë“œ ë…¼ë¬¸ì—ì„œ ì¶œë°œí•˜ì—¬ **3ê°€ì§€ ë°©í–¥**ìœ¼ë¡œ ê´€ë ¨ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n\n| ë°©í–¥ | ì„¤ëª… | API |\n|------|------|-----|\n| **References** | ì‹œë“œê°€ ì¸ìš©í•œ ë…¼ë¬¸ (ê³¼ê±° ì—°êµ¬) | Semantic Scholar â†’ OpenAlex |\n| **Citations** | ì‹œë“œë¥¼ ì¸ìš©í•œ ë…¼ë¬¸ (í›„ì† ì—°êµ¬) | Semantic Scholar |\n| **Related** | ì¸ìš© ê´€ê³„ ì—†ì´ ìœ ì‚¬í•œ ë…¼ë¬¸ | Semantic Scholar |\n\n```\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    ê³¼ê±° â†â”€â”€ â”‚  Seed Paper â”‚ â”€â”€â†’ ìµœì‹ \n  References â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  Citations\n                    â”‚\n                    â–¼\n                 Related\n              (ì¸ìš© ë¬´ê´€ ìœ ì‚¬)\n```\n\n> ğŸ’¡ **Relatedê°€ ì¤‘ìš”í•œ ì´ìœ **: ì„œë¡œ ì¸ìš©í•˜ì§€ ì•Šì§€ë§Œ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆëŠ” ë…¼ë¬¸ì„ ë†“ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n---\n\n### ì‚¬ìš© API\n\n| API | ì„¤ëª… | íŠ¹ì§• |\n|-----|------|------|\n| **[Semantic Scholar](https://api.semanticscholar.org/)** | Allen AIì˜ í•™ìˆ  ê²€ìƒ‰ API | ì¶”ì²œ/ê´€ë ¨ ë…¼ë¬¸ ê¸°ëŠ¥ ê°•ì  |\n| **[OpenAlex](https://openalex.org/)** | ì™„ì „ ë¬´ë£Œ ì˜¤í”ˆ í•™ìˆ  DB | ì¶œíŒì‚¬ ì œí•œ ì—†ìŒ, References ê°•ì  |\n\n> âš ï¸ ì¼ë¶€ ì¶œíŒì‚¬ëŠ” Semantic Scholarì— ë°ì´í„°ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ê²½ìš° **OpenAlex**ì—ì„œ Referencesë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Crawler ì‹¤í–‰\nif seed_paper.get('paperId'):\n    crawler = CitationCrawler(api, openalex_api=openalex)\n    \n    # ì‹œë“œ DOI ê°€ì ¸ì˜¤ê¸° (OpenAlex fallbackìš©)\n    seed_doi = None\n    if hasattr(api, 'current_doi'):\n        seed_doi = api.current_doi\n    elif 'candidates_df' in dir() and SEED_INDEX <= len(candidates_df):\n        seed_doi = candidates_df.iloc[SEED_INDEX - 1].get('doi')\n    \n    crawled_papers = crawler.crawl(\n        seed_paper=seed_paper,\n        seed_doi=seed_doi,\n        refs_limit=15, cites_limit=15, related_limit=10, max_total=50\n    )\n    \n    # ê²°ê³¼ ê²€ì¦\n    ref_count = len([p for p in crawled_papers if p.get('source') == 'reference'])\n    cite_count = len([p for p in crawled_papers if p.get('source') == 'citation'])\n    \n    if ref_count == 0:\n        print(\"\\nâš ï¸  References = 0: ë¹„ì •ìƒì…ë‹ˆë‹¤.\")\n    if cite_count == 0 and seed_paper.get('year', 2020) < 2024:\n        print(\"   (ì˜¤ë˜ëœ ë…¼ë¬¸ì¸ë° Citations = 0ì´ë©´ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\nelse:\n    print(\"ì‹œë“œ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. Step 2ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n    crawled_papers = []"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í¬ë¡¤ë§ í†µê³„\n",
    "if crawled_papers:\n",
    "    counts = Counter(p.get('source', '?') for p in crawled_papers)\n",
    "    print(\"=\" * 40)\n",
    "    print(\"í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½\")\n",
    "    print(\"=\" * 40)\n",
    "    for src, cnt in counts.items():\n",
    "        icon = {'seed':'ğŸŒ±', 'reference':'ğŸ“š', 'citation':'ğŸ“', 'related':'ğŸ”—'}.get(src, 'ğŸ“„')\n",
    "        print(f\"  {icon} {src}: {cnt}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Selector ì‹¤í–‰\n\n### SPECTER2ë€?\n[SPECTER2](https://github.com/allenai/SPECTER2)ëŠ” Allen AIì—ì„œ ê°œë°œí•œ **í•™ìˆ  ë…¼ë¬¸ ì„ë² ë”© ëª¨ë¸**ì…ë‹ˆë‹¤.\n\n- **ê¸°ë°˜**: SciBERT (ê³¼í•™ ë¬¸í—Œ íŠ¹í™” BERT)\n- **í•™ìŠµ ë°ì´í„°**: 700ë§Œ+ ë…¼ë¬¸ì˜ ì¸ìš© ê´€ê³„\n- **íŠ¹ì§•**: ì œëª©+ì´ˆë¡ë§Œìœ¼ë¡œ ë…¼ë¬¸ì˜ ì˜ë¯¸ë¥¼ 768ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„\n\n**ì›ë¦¬:**\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Query     â”‚     â”‚   Paper     â”‚\nâ”‚ \"climate    â”‚     â”‚ title +     â”‚\nâ”‚  anxiety\"   â”‚     â”‚ abstract    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n       â”‚                   â”‚\n       â–¼                   â–¼\n   SPECTER2            SPECTER2\n       â”‚                   â”‚\n       â–¼                   â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Query      â”‚     â”‚  Paper      â”‚\nâ”‚  Embedding  â”‚     â”‚  Embedding  â”‚\nâ”‚  (768-dim)  â”‚     â”‚  (768-dim)  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n       â”‚                   â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â–¼\n        Cosine Similarity\n         (0.0 ~ 1.0)\n               â”‚\n               â–¼\n        Relevance Score\n```\n\n> ğŸ’¡ **Selectorì˜ ì—­í• **: Crawlerê°€ ìˆ˜ì§‘í•œ ë…¼ë¬¸ë“¤ ì¤‘ì—ì„œ **ì¿¼ë¦¬ì™€ ì˜ë¯¸ì ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ** ë…¼ë¬¸ì„ ì„ ë³„í•©ë‹ˆë‹¤.\n\n> âš ï¸ **ì¿¼ë¦¬ê°€ ë°”ë€Œë©´ ê²°ê³¼ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤**: ê°™ì€ ë…¼ë¬¸ í’€ì´ë¼ë„ ì¿¼ë¦¬ê°€ ë‹¤ë¥´ë©´ Query Embeddingì´ ë‹¬ë¼ì§€ê³ , ê° ë…¼ë¬¸ê³¼ì˜ ìœ ì‚¬ë„ ì ìˆ˜ê°€ ë°”ë€ë‹ˆë‹¤. â†’ **ì‹¤í—˜ B**ì—ì„œ ì§ì ‘ í™•ì¸í•´ë³´ì„¸ìš”!\n\n**ì°¸ê³  ìë£Œ:**\n- ë…¼ë¬¸: [SciRepEval: A Multi-Format Benchmark for Scientific Document Representations](https://arxiv.org/abs/2211.13308)\n- GitHub: [allenai/SPECTER2](https://github.com/allenai/SPECTER2)\n- HuggingFace: [allenai/specter2_base](https://huggingface.co/allenai/specter2_base)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Selector ì‹¤í–‰\n",
    "if crawled_papers:\n",
    "    selected_papers = selector.select_top_papers(\n",
    "        query=QUERY,\n",
    "        papers=crawled_papers.copy(),\n",
    "        top_k=10\n",
    "    )\n",
    "else:\n",
    "    print(\"í¬ë¡¤ë§ ê²°ê³¼ ì—†ìŒ. Step 3ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    selected_papers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: ìµœì¢… ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: ìµœì¢… ê²°ê³¼\nfrom IPython.display import display, HTML\n\nif selected_papers:\n    print(\"=\" * 70)\n    print(f\"Top 10 ê´€ë ¨ ë…¼ë¬¸\")\n    print(f\"ì¿¼ë¦¬: {QUERY}\")\n    print(\"=\" * 70)\n    \n    html_rows = []\n    for i, p in enumerate(selected_papers, 1):\n        icon = {'seed':'ğŸŒ±', 'reference':'ğŸ“š', 'citation':'ğŸ“', 'related':'ğŸ”—'}.get(p.get('source',''), 'ğŸ“„')\n        title = p.get('title', 'N/A')\n        score = p.get('relevance_score', 0)\n        year = p.get('year', '?')\n        citations = p.get('citationCount', 0)\n        source = p.get('source', '')\n        \n        # DOI ë˜ëŠ” Semantic Scholar ë§í¬\n        paper_id = p.get('paperId', '')\n        ext_ids = p.get('externalIds', {}) or {}\n        doi = ext_ids.get('DOI', '') if isinstance(ext_ids, dict) else ''\n        \n        if doi:\n            link = f'<a href=\"https://doi.org/{doi}\" target=\"_blank\">{doi}</a>'\n        elif paper_id:\n            link = f'<a href=\"https://www.semanticscholar.org/paper/{paper_id}\" target=\"_blank\">SS:{paper_id[:12]}...</a>'\n        else:\n            link = \"N/A\"\n        \n        html_rows.append(f\"\"\"\n        <tr>\n            <td style=\"padding:5px;\"><b>{i}</b></td>\n            <td style=\"padding:5px;\">[{score:.3f}]</td>\n            <td style=\"padding:5px;\">{icon}</td>\n            <td style=\"padding:5px;\">\n                {title[:60]}{'...' if len(title) > 60 else ''}<br>\n                <small>ì—°ë„: {year} | ì¸ìš©: {citations} | {source} | {link}</small>\n            </td>\n        </tr>\n        \"\"\")\n    \n    html_table = f\"\"\"\n    <table style=\"border-collapse:collapse; width:100%;\">\n        {''.join(html_rows)}\n    </table>\n    <p style=\"color:#666; font-size:12px;\">ğŸ’¡ DOIë¥¼ ë³µì‚¬í•´ì„œ Part 3 ì‹¤í—˜ì— ì‹œë“œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n    \"\"\"\n    display(HTML(html_table))\nelse:\n    print(\"ì„ íƒëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. Step 4ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Part 3: ë¹„êµ ì‹¤í—˜\n\n**ë‘ ê°€ì§€ ì‹¤í—˜ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì†Œë¥¼ íƒêµ¬í•©ë‹ˆë‹¤.**\n\n| ì‹¤í—˜ | ê³ ì • | ë³€ê²½ | ì§ˆë¬¸ |\n|------|------|------|------|\n| **ì‹¤í—˜ A** | ì¿¼ë¦¬ | ì‹œë“œ | ì‹œë“œê°€ ë‹¤ë¥´ë©´ ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ë‹¬ë¼ì§ˆê¹Œ? |\n| **ì‹¤í—˜ B** | ì‹œë“œ | ì¿¼ë¦¬ | ì¿¼ë¦¬ê°€ ë‹¤ë¥´ë©´ ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ë‹¬ë¼ì§ˆê¹Œ? |\n\n---\n\n## ì‹¤í—˜ A: ì‹œë“œ ë¹„êµ (ê°™ì€ ì¿¼ë¦¬, ë‹¤ë¥¸ ì‹œë“œ)\n\n### ì‹œë“œ ì„ íƒ ë°©ë²•\n\n**Step 1 (cell-11)ì—ì„œ ê²€ìƒ‰ëœ ë…¼ë¬¸ ëª©ë¡**ì„ ì°¸ê³ í•˜ì—¬ 2ê°œì˜ ì‹œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”.\n\n```\nì˜ˆ: Step 1 ê²°ê³¼ê°€ ë‹¤ìŒê³¼ ê°™ë‹¤ë©´\n   1. [PubMed] Climate change and mental health... (DOI: 10.1016/j.ajp.2026.104859)\n   2. [PubMed] The generational happiness switch... (DOI: 10.3389/fpsyg.2025.1706473)\n   3. [arXiv] Climate anxiety in adolescents...   (DOI: 10.48550/arXiv.2401.12345)\n   \n   â†’ SEED_1_DOI = \"10.1016/j.ajp.2026.104859\"  (1ë²ˆ ì„ íƒ)\n   â†’ SEED_2_DOI = \"10.3389/fpsyg.2025.1706473\" (2ë²ˆ ì„ íƒ)\n```\n\n> ğŸ’¡ **íŒ**: ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì˜ ë…¼ë¬¸ì„ ì„ íƒí•˜ë©´ ë¹„êµê°€ ë” í¥ë¯¸ë¡œì›Œì§‘ë‹ˆë‹¤.  \n> ì˜ˆ: ì„ìƒ ì—°êµ¬ vs ì´ë¡  ì—°êµ¬, ì²­ì†Œë…„ ëŒ€ìƒ vs ì„±ì¸ ëŒ€ìƒ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜\n",
    "def run_pipeline(seed_doi=None, seed_title=None, query=\"climate anxiety\", max_papers=30, top_k=10):\n",
    "    result = {'seed': None, 'crawled': [], 'selected': []}\n",
    "    \n",
    "    seed = api.get_paper(doi=seed_doi, title=seed_title)\n",
    "    if not seed.get('paperId'):\n",
    "        print(\"âœ— ì‹œë“œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return result\n",
    "    result['seed'] = seed\n",
    "    print(f\"ì‹œë“œ: {seed.get('title', 'N/A')[:50]}...\")\n",
    "    \n",
    "    crawler = CitationCrawler(api)\n",
    "    result['crawled'] = crawler.crawl(seed, refs_limit=10, cites_limit=10, related_limit=5, max_total=max_papers)\n",
    "    result['selected'] = selector.select_top_papers(query, result['crawled'].copy(), top_k)\n",
    "    return result\n",
    "\n",
    "print(\"âœ“ run_pipeline() ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ì‹¤í—˜ A: ì‹œë“œ ë¹„êµ ì‹¤í–‰\n# ============================================================\n# Step 1 (cell-11) ê²°ê³¼ì—ì„œ DOI 2ê°œë¥¼ ë³µì‚¬í•´ì„œ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”!\n# ============================================================\n\nSEED_1_DOI = \"\"  # <- cell-11ì—ì„œ 1ë²ˆ ë…¼ë¬¸ì˜ DOI\nSEED_2_DOI = \"\"  # <- cell-11ì—ì„œ 2ë²ˆ ë…¼ë¬¸ì˜ DOI\n\nCOMPARE_QUERY = '\"climate anxiety\"'\n\n# ============================================================\n# ì…ë ¥ ê²€ì¦\n# ============================================================\nif not COMPARE_QUERY or COMPARE_QUERY.strip() == \"\":\n    print(\"âš ï¸  ê²½ê³ : COMPARE_QUERYê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!\")\n    print(\"   ë¹„êµì— ì‚¬ìš©í•  ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n    print('   ì˜ˆ: COMPARE_QUERY = \\'\"climate anxiety\"\\'')\n    result_1, result_2 = {'selected': []}, {'selected': []}\nelif not SEED_1_DOI or not SEED_2_DOI:\n    print(\"âš ï¸  ê²½ê³ : ì‹œë“œ DOIê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!\")\n    print(\"   Step 1 (cell-11) ê²°ê³¼ì—ì„œ DOI 2ê°œë¥¼ ë³µì‚¬í•˜ì„¸ìš”.\")\n    print('   ì˜ˆ: SEED_1_DOI = \"10.1016/j.ajp.2026.104859\"')\n    result_1, result_2 = {'selected': []}, {'selected': []}\nelse:\n    print(f\"ì¿¼ë¦¬: {COMPARE_QUERY}\")\n    print(\"=\" * 60)\n    print(\"1ï¸âƒ£ ì‹œë“œ 1 ì‹¤í–‰\")\n    print(\"=\" * 60)\n    result_1 = run_pipeline(seed_doi=SEED_1_DOI, query=COMPARE_QUERY)\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"2ï¸âƒ£ ì‹œë“œ 2 ì‹¤í–‰\")\n    print(\"=\" * 60)\n    result_2 = run_pipeline(seed_doi=SEED_2_DOI, query=COMPARE_QUERY)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ì‹¤í—˜ A: ê²°ê³¼ ë¹„êµ í…Œì´ë¸”\nimport pandas as pd\nfrom IPython.display import display, HTML\n\nif result_1.get('selected') and result_2.get('selected'):\n    # ì‹œë“œ ì •ë³´\n    seed_1_title = result_1['seed'].get('title', 'N/A')[:40] + \"...\"\n    seed_2_title = result_2['seed'].get('title', 'N/A')[:40] + \"...\"\n    \n    # Top 10 ë¹„êµ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n    def make_result_df(result, label):\n        rows = []\n        for i, p in enumerate(result['selected'], 1):\n            rows.append({\n                'ìˆœìœ„': i,\n                'ì ìˆ˜': f\"{p.get('relevance_score', 0):.3f}\",\n                'ì œëª©': p.get('title', 'N/A')[:45] + \"...\",\n                'ì—°ë„': p.get('year', '?'),\n                'ì¸ìš©': p.get('citationCount', 0),\n                'ì¶œì²˜': p.get('source', ''),\n                'paperId': p.get('paperId', '')\n            })\n        return pd.DataFrame(rows)\n    \n    df_1 = make_result_df(result_1, '1')\n    df_2 = make_result_df(result_2, '2')\n    \n    # ê³µí†µ ë…¼ë¬¸ ì°¾ê¸°\n    ids_1 = set(df_1['paperId'])\n    ids_2 = set(df_2['paperId'])\n    common_ids = ids_1 & ids_2\n    \n    # ê²°ê³¼ ì¶œë ¥\n    print(\"=\" * 70)\n    print(\"ğŸ“Š ì‹¤í—˜ A: ì‹œë“œ ë¹„êµ ê²°ê³¼\")\n    print(\"=\" * 70)\n    print(f\"\\n1ï¸âƒ£ ì‹œë“œ 1: {seed_1_title}\")\n    print(f\"2ï¸âƒ£ ì‹œë“œ 2: {seed_2_title}\")\n    print(f\"\\nğŸ”— ê³µí†µ ë…¼ë¬¸: {len(common_ids)}ê°œ / 10ê°œ\")\n    \n    # ë¹„êµ í…Œì´ë¸” (ë‚˜ë€íˆ)\n    print(\"\\n\" + \"=\" * 70)\n    print(\"1ï¸âƒ£ ì‹œë“œ 1 ê²°ê³¼                      2ï¸âƒ£ ì‹œë“œ 2 ê²°ê³¼\")\n    print(\"=\" * 70)\n    \n    comparison_rows = []\n    for i in range(10):\n        s1_title = df_1.iloc[i]['ì œëª©'][:30] if i < len(df_1) else \"\"\n        s2_title = df_2.iloc[i]['ì œëª©'][:30] if i < len(df_2) else \"\"\n        comparison_rows.append({\n            '#': i+1,\n            'ì‹œë“œ1 ì ìˆ˜': df_1.iloc[i]['ì ìˆ˜'],\n            'ì‹œë“œ1 ë…¼ë¬¸': s1_title,\n            '': 'â”‚',\n            'ì‹œë“œ2 ì ìˆ˜': df_2.iloc[i]['ì ìˆ˜'],\n            'ì‹œë“œ2 ë…¼ë¬¸': s2_title,\n            'ê³µí†µ': \"â­\" if df_1.iloc[i]['paperId'] in common_ids or df_2.iloc[i]['paperId'] in common_ids else \"\"\n        })\n    \n    comparison_df = pd.DataFrame(comparison_rows)\n    display(comparison_df.style.hide(axis='index'))\n    \n    # ê³µí†µ ë…¼ë¬¸ ìƒì„¸\n    if common_ids:\n        print(\"\\n\" + \"=\" * 70)\n        print(\"â­ ê³µí†µìœ¼ë¡œ ì„ íƒëœ ë…¼ë¬¸\")\n        print(\"=\" * 70)\n        for p in result_1['selected']:\n            if p.get('paperId') in common_ids:\n                print(f\"  â€¢ {p.get('title', 'N/A')[:60]}...\")\n                print(f\"    ì—°ë„: {p.get('year', '?')} | ì¸ìš©: {p.get('citationCount', 0)}\")\n    else:\n        print(\"\\nâš ï¸  ê³µí†µ ë…¼ë¬¸ ì—†ìŒ! ì‹œë“œ ì„ íƒì´ ê²°ê³¼ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\")\n    \n    # ê²°ê³¼ ì €ì¥ìš© ë³€ìˆ˜\n    comparison_result = {\n        'query': COMPARE_QUERY,\n        'seed_1': result_1['seed'],\n        'seed_2': result_2['seed'],\n        'top10_1': result_1['selected'],\n        'top10_2': result_2['selected'],\n        'common_count': len(common_ids),\n        'common_ids': list(common_ids)\n    }\nelse:\n    print(\"âš ï¸  ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ì‹¤í—˜ B: ì¿¼ë¦¬ ë¹„êµ (ê°™ì€ ì‹œë“œ, ë‹¤ë¥¸ ì¿¼ë¦¬)\n\n**ê°™ì€ ë…¼ë¬¸ í’€ì—ì„œ ì¿¼ë¦¬ë§Œ ë°”ê¾¸ë©´ Top 10ì´ ì–´ë–»ê²Œ ë‹¬ë¼ì§ˆê¹Œìš”?**\n\nì˜ˆì‹œ:\n- ì¿¼ë¦¬ 1: `\"climate anxiety\"` â†’ ë¶ˆì•ˆ ì¤‘ì‹¬\n- ì¿¼ë¦¬ 2: `\"eco-paralysis\"` â†’ ë¬´ë ¥ê°/í–‰ë™ ì–µì œ ì¤‘ì‹¬",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ì‹¤í—˜ B: ì¿¼ë¦¬ ë¹„êµ ì‹¤í–‰\n# ============================================================\n# ê°™ì€ ì‹œë“œì—ì„œ ì¿¼ë¦¬ë§Œ ë°”ê¾¸ë©´ ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§ˆê¹Œ?\n# ============================================================\n\nFIXED_SEED_DOI = \"\"  # <- Step 1 (cell-11) ë˜ëŠ” Step 5 (cell-20) ê²°ê³¼ì—ì„œ DOI ë³µì‚¬\n\nQUERY_1 = '\"climate anxiety\"'    # ë¶ˆì•ˆ ì¤‘ì‹¬\nQUERY_2 = '\"eco-paralysis\"'      # ë¬´ë ¥ê°/í–‰ë™ ì–µì œ ì¤‘ì‹¬\n\n# ============================================================\n# ì…ë ¥ ê²€ì¦\n# ============================================================\nif not FIXED_SEED_DOI:\n    print(\"âš ï¸  ê²½ê³ : FIXED_SEED_DOIê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!\")\n    print(\"   Step 1 (cell-11) ë˜ëŠ” Step 5 (cell-20) ê²°ê³¼ì—ì„œ DOIë¥¼ ë³µì‚¬í•˜ì„¸ìš”.\")\n    print('   ì˜ˆ: FIXED_SEED_DOI = \"10.1016/S2542-5196(21)00278-3\"')\n    query_result_1, query_result_2 = {'selected': [], 'crawled': []}, {'selected': [], 'crawled': []}\nelif not QUERY_1 or not QUERY_2:\n    print(\"âš ï¸  ê²½ê³ : ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!\")\n    print(\"   QUERY_1ê³¼ QUERY_2ë¥¼ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.\")\n    query_result_1, query_result_2 = {'selected': [], 'crawled': []}, {'selected': [], 'crawled': []}\nelse:\n    print(f\"ğŸ”’ ê³ ì • ì‹œë“œ: {FIXED_SEED_DOI}\")\n    print(f\"ğŸ“ ì¿¼ë¦¬ 1: {QUERY_1}\")\n    print(f\"ğŸ“ ì¿¼ë¦¬ 2: {QUERY_2}\")\n    print(\"=\" * 60)\n    \n    # ì‹œë“œ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸° (í•œ ë²ˆë§Œ)\n    print(\"ì‹œë“œ ë…¼ë¬¸ ë¡œë“œ ì¤‘...\")\n    fixed_seed = api.get_paper(doi=FIXED_SEED_DOI)\n    \n    if not fixed_seed.get('paperId'):\n        print(\"âœ— ì‹œë“œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n        query_result_1, query_result_2 = {'selected': [], 'crawled': []}, {'selected': [], 'crawled': []}\n    else:\n        print(f\"âœ“ ì‹œë“œ: {fixed_seed.get('title', 'N/A')[:50]}...\")\n        \n        # Crawler 1íšŒë§Œ ì‹¤í–‰ (ë…¼ë¬¸ í’€ì€ ë™ì¼)\n        print(\"\\n[Crawler] ë…¼ë¬¸ í’€ ìˆ˜ì§‘ ì¤‘...\")\n        crawler = CitationCrawler(api)\n        shared_papers = crawler.crawl(fixed_seed, refs_limit=15, cites_limit=15, related_limit=10, max_total=50)\n        \n        # Selector: ì¿¼ë¦¬ 1\n        print(\"\\n\" + \"=\" * 60)\n        print(f\"ğŸ” ì¿¼ë¦¬ 1: {QUERY_1}\")\n        print(\"=\" * 60)\n        selected_1 = selector.select_top_papers(QUERY_1, [p.copy() for p in shared_papers], top_k=10)\n        query_result_1 = {'seed': fixed_seed, 'crawled': shared_papers, 'selected': selected_1}\n        \n        # Selector: ì¿¼ë¦¬ 2\n        print(\"\\n\" + \"=\" * 60)\n        print(f\"ğŸ” ì¿¼ë¦¬ 2: {QUERY_2}\")\n        print(\"=\" * 60)\n        selected_2 = selector.select_top_papers(QUERY_2, [p.copy() for p in shared_papers], top_k=10)\n        query_result_2 = {'seed': fixed_seed, 'crawled': shared_papers, 'selected': selected_2}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ì‹¤í—˜ B: ì¿¼ë¦¬ ë¹„êµ ê²°ê³¼ í…Œì´ë¸”\nif query_result_1.get('selected') and query_result_2.get('selected'):\n    # Top 10 ë¹„êµ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n    def make_query_result_df(selected):\n        rows = []\n        for i, p in enumerate(selected, 1):\n            rows.append({\n                'ìˆœìœ„': i,\n                'ì ìˆ˜': f\"{p.get('relevance_score', 0):.3f}\",\n                'ì œëª©': p.get('title', 'N/A')[:45] + \"...\",\n                'ì—°ë„': p.get('year', '?'),\n                'ì¸ìš©': p.get('citationCount', 0),\n                'paperId': p.get('paperId', '')\n            })\n        return pd.DataFrame(rows)\n    \n    qdf_1 = make_query_result_df(query_result_1['selected'])\n    qdf_2 = make_query_result_df(query_result_2['selected'])\n    \n    # ê³µí†µ ë…¼ë¬¸ ì°¾ê¸°\n    qids_1 = set(qdf_1['paperId'])\n    qids_2 = set(qdf_2['paperId'])\n    q_common_ids = qids_1 & qids_2\n    \n    # ê²°ê³¼ ì¶œë ¥\n    print(\"=\" * 70)\n    print(\"ğŸ“Š ì¿¼ë¦¬ ë¹„êµ ê²°ê³¼\")\n    print(\"=\" * 70)\n    print(f\"\\nğŸ”’ ê³ ì • ì‹œë“œ: {query_result_1['seed'].get('title', 'N/A')[:50]}...\")\n    print(f\"\\nğŸ“ ì¿¼ë¦¬ 1: {QUERY_1}\")\n    print(f\"ğŸ“ ì¿¼ë¦¬ 2: {QUERY_2}\")\n    print(f\"\\nğŸ”— ê³µí†µ ë…¼ë¬¸: {len(q_common_ids)}ê°œ / 10ê°œ\")\n    print(f\"ğŸ“Š ë…¼ë¬¸ í’€ í¬ê¸°: {len(query_result_1['crawled'])}ê°œ (Crawler ê²°ê³¼)\")\n    \n    # ë¹„êµ í…Œì´ë¸” (ë‚˜ë€íˆ)\n    print(\"\\n\" + \"=\" * 70)\n    print(f\"ğŸ” ì¿¼ë¦¬ 1 ê²°ê³¼                      ğŸ” ì¿¼ë¦¬ 2 ê²°ê³¼\")\n    print(\"=\" * 70)\n    \n    q_comparison_rows = []\n    for i in range(10):\n        q1_title = qdf_1.iloc[i]['ì œëª©'][:28] if i < len(qdf_1) else \"\"\n        q2_title = qdf_2.iloc[i]['ì œëª©'][:28] if i < len(qdf_2) else \"\"\n        q_comparison_rows.append({\n            '#': i+1,\n            'Q1 ì ìˆ˜': qdf_1.iloc[i]['ì ìˆ˜'] if i < len(qdf_1) else \"\",\n            'Q1 ë…¼ë¬¸': q1_title,\n            '': 'â”‚',\n            'Q2 ì ìˆ˜': qdf_2.iloc[i]['ì ìˆ˜'] if i < len(qdf_2) else \"\",\n            'Q2 ë…¼ë¬¸': q2_title,\n            'ê³µí†µ': \"â­\" if (i < len(qdf_1) and qdf_1.iloc[i]['paperId'] in q_common_ids) else \"\"\n        })\n    \n    q_comparison_df = pd.DataFrame(q_comparison_rows)\n    display(q_comparison_df.style.hide(axis='index'))\n    \n    # ì¸ì‚¬ì´íŠ¸\n    print(\"\\n\" + \"=\" * 70)\n    print(\"ğŸ’¡ ì¸ì‚¬ì´íŠ¸\")\n    print(\"=\" * 70)\n    \n    if len(q_common_ids) >= 7:\n        print(\"  â†’ ë‘ ì¿¼ë¦¬ê°€ ë§¤ìš° ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (ê³µí†µ 70%+)\")\n        print(\"  â†’ ì¿¼ë¦¬ ê°„ ì˜ë¯¸ì  ì°¨ì´ê°€ ì‘ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n    elif len(q_common_ids) >= 4:\n        print(\"  â†’ ë‘ ì¿¼ë¦¬ê°€ ë¶€ë¶„ì ìœ¼ë¡œ ì¤‘ë³µë©ë‹ˆë‹¤ (ê³µí†µ 40-60%)\")\n        print(\"  â†’ ê° ì¿¼ë¦¬ê°€ ë‹¤ë¥¸ ê´€ì ì˜ ë…¼ë¬¸ì„ ë°œê²¬í•©ë‹ˆë‹¤.\")\n    else:\n        print(\"  â†’ ë‘ ì¿¼ë¦¬ê°€ ìƒë‹¹íˆ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (ê³µí†µ 30% ë¯¸ë§Œ)\")\n        print(\"  â†’ ì¿¼ë¦¬ ì„ íƒì´ Selector ê²°ê³¼ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤!\")\n    \n    # ì ìˆ˜ ì°¨ì´ ë¶„ì„\n    avg_score_1 = sum(p.get('relevance_score', 0) for p in query_result_1['selected']) / 10\n    avg_score_2 = sum(p.get('relevance_score', 0) for p in query_result_2['selected']) / 10\n    print(f\"\\n  ğŸ“ˆ í‰ê·  ê´€ë ¨ì„± ì ìˆ˜:\")\n    print(f\"     ì¿¼ë¦¬ 1 ({QUERY_1}): {avg_score_1:.3f}\")\n    print(f\"     ì¿¼ë¦¬ 2 ({QUERY_2}): {avg_score_2:.3f}\")\n    \n    if abs(avg_score_1 - avg_score_2) > 0.05:\n        higher = QUERY_1 if avg_score_1 > avg_score_2 else QUERY_2\n        print(f\"     â†’ '{higher}' ì¿¼ë¦¬ê°€ ìˆ˜ì§‘ëœ ë…¼ë¬¸ í’€ê³¼ ë” ì˜ ë§ìŠµë‹ˆë‹¤.\")\nelse:\n    print(\"âš ï¸  ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Part 4: ê²°ê³¼ ê³µìœ  & í† ë¡ \n\n## ğŸ”— ê²°ê³¼ ì œì¶œ ë°©ë²•\n\n### ì°¸ê°€ì ì›Œí¬í”Œë¡œìš°\n1. **ì‹¤í—˜ Aì™€ B ëª¨ë‘ ì‹¤í–‰** (ìœ„ ì…€ë“¤)\n2. ì•„ë˜ ì…€ì—ì„œ **ì´ë¦„** ì…ë ¥\n3. ì…€ ì‹¤í–‰ â†’ **íŒŒë€ ë²„íŠ¼ í´ë¦­**\n4. í¼ì—ì„œ **í† ë¡  ì§ˆë¬¸ 4ê°œ ì‘ì„±** í›„ ì œì¶œ\n\n### ğŸ“ ê²°ê³¼ í™•ì¸\n- **[ê³µìœ  ì‹œíŠ¸ ë³´ê¸°](https://docs.google.com/spreadsheets/d/15jyTrqGY7Po5iLcXFrv_kwyUNkCC9YMX6kypPMs-bAc/edit?usp=sharing)** - ëª¨ë“  ì°¸ê°€ì ê²°ê³¼\n- **[ì§ì ‘ ì œì¶œ (í¼)](https://forms.gle/dYNbvMeeBMqxSmLa7)** - ìˆ˜ë™ ì…ë ¥ìš©\n\n---\n\n### ğŸ’¡ í† ë¡  ì§ˆë¬¸ (í¼ì—ì„œ ì‘ì„±)\n1. **ì‹œë“œ ì„ íƒì˜ ì˜í–¥**: ê°™ì€ ì¿¼ë¦¬ì—¬ë„ ì‹œë“œì— ë”°ë¼ ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ë‹¬ë¼ì¡Œë‚˜ìš”?\n2. **ì¿¼ë¦¬ ì„ íƒì˜ ì˜í–¥**: ê°™ì€ ë…¼ë¬¸ í’€ì—ì„œ ì¿¼ë¦¬ì— ë”°ë¼ Top 10ì´ ì–¼ë§ˆë‚˜ ë°”ë€Œì—ˆë‚˜ìš”?\n3. **ê³µí†µ ë…¼ë¬¸ì˜ íŠ¹ì§•**: ì–´ë–¤ ë…¼ë¬¸ì´ ê³µí†µìœ¼ë¡œ ì„ íƒë˜ì—ˆë‚˜ìš”? ì™œ ê·¸ëŸ´ê¹Œìš”?\n4. **ë‚´ ì—°êµ¬ì— ì ìš©í•œë‹¤ë©´**: ì–´ë–¤ ì‹œë“œì™€ ì¿¼ë¦¬ ì¡°í•©ì´ íš¨ê³¼ì ì¼ê¹Œìš”?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ê²°ê³¼ ì œì¶œ\n# ============================================================\n# ì´ë¦„ë§Œ ì…ë ¥í•˜ì„¸ìš”! ë‚˜ë¨¸ì§€ëŠ” ìë™ìœ¼ë¡œ ì±„ì›Œì§‘ë‹ˆë‹¤.\n# ============================================================\n\nPARTICIPANT_NAME = \"\"  # <- ë³¸ì¸ ì´ë¦„\n\n# ============================================================\n# ì‹¤í—˜ ê²°ê³¼ ìë™ ìˆ˜ì§‘\n# ============================================================\nimport urllib.parse\nfrom datetime import datetime\nfrom IPython.display import display, HTML\n\ndef format_top10(selected_papers):\n    \"\"\"Top 10 ë…¼ë¬¸ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…\"\"\"\n    if not selected_papers:\n        return \"(ê²°ê³¼ ì—†ìŒ)\"\n    lines = []\n    for i, p in enumerate(selected_papers[:10], 1):\n        title = p.get('title', 'N/A')[:50]\n        score = p.get('relevance_score', 0)\n        lines.append(f\"{i}. [{score:.3f}] {title}...\")\n    return \"\\n\".join(lines)\n\ndef format_common_papers(papers_a, papers_b):\n    \"\"\"ê³µí†µ ë…¼ë¬¸ ì°¾ê¸° ë° í¬ë§·íŒ…\"\"\"\n    if not papers_a or not papers_b:\n        return \"(ì‹¤í—˜ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”)\", 0\n    \n    ids_a = {p.get('paperId') for p in papers_a if p.get('paperId')}\n    ids_b = {p.get('paperId') for p in papers_b if p.get('paperId')}\n    common_ids = ids_a & ids_b\n    \n    if not common_ids:\n        return \"(ê³µí†µ ë…¼ë¬¸ ì—†ìŒ)\", 0\n    \n    lines = [f\"[{len(common_ids)}ê°œ ê³µí†µ]\"]\n    for p in papers_a:\n        if p.get('paperId') in common_ids:\n            lines.append(f\"â€¢ {p.get('title', 'N/A')[:50]}...\")\n    return \"\\n\".join(lines), len(common_ids)\n\n# ì‹¤í—˜ A ë°ì´í„° ìˆ˜ì§‘\na_query = COMPARE_QUERY if 'COMPARE_QUERY' in dir() else \"\"\na_seeds = \"\"\na_results = \"\"\nif 'result_1' in dir() and 'result_2' in dir():\n    if result_1.get('seed') and result_2.get('seed'):\n        a_seeds = f\"ì‹œë“œ1: {result_1['seed'].get('title', 'N/A')[:40]}...\\nì‹œë“œ2: {result_2['seed'].get('title', 'N/A')[:40]}...\"\n    a_results = format_top10(result_1.get('selected', []))\n\n# ì‹¤í—˜ B ë°ì´í„° ìˆ˜ì§‘\nb_seed = \"\"\nb_queries = \"\"\nb_results = \"\"\nif 'query_result_1' in dir() and 'query_result_2' in dir():\n    if query_result_1.get('seed'):\n        b_seed = query_result_1['seed'].get('title', 'N/A')[:50] + \"...\"\n    b_queries = f\"ì¿¼ë¦¬1: {QUERY_1 if 'QUERY_1' in dir() else ''}\\nì¿¼ë¦¬2: {QUERY_2 if 'QUERY_2' in dir() else ''}\"\n    b_results = format_top10(query_result_1.get('selected', []))\n\n# ê³µí†µ ë…¼ë¬¸ (ì‹¤í—˜ A ê¸°ì¤€)\ncommon_text = \"(ì‹¤í—˜ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”)\"\nif 'result_1' in dir() and 'result_2' in dir():\n    common_text, _ = format_common_papers(\n        result_1.get('selected', []), \n        result_2.get('selected', [])\n    )\n\n# ============================================================\n# ì…ë ¥ ê²€ì¦ ë° ì œì¶œ ë§í¬ ìƒì„±\n# ============================================================\nif not PARTICIPANT_NAME:\n    print(\"âš ï¸  PARTICIPANT_NAMEì„ ì…ë ¥í•˜ì„¸ìš”!\")\nelif a_results == \"\" and b_results == \"\":\n    print(\"âš ï¸  ì‹¤í—˜ A ë˜ëŠ” Bë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”!\")\nelse:\n    # Pre-filled Google Form URL ìƒì„±\n    FORM_BASE = \"https://docs.google.com/forms/d/e/1FAIpQLSfcDpL_hUwM0L_wzP3AwDiN0_EUTO4sqIZWTZ_ehVhQkJxkTg/viewform?usp=pp_url\"\n    \n    params = {\n        \"entry.1560815365\": PARTICIPANT_NAME,      # ì´ë¦„\n        \"entry.1494848871\": a_query,               # A_ì¿¼ë¦¬\n        \"entry.268560000\": a_seeds,                # A_ì‹œë“œë“¤\n        \"entry.1308825609\": a_results,             # Aê²°ê³¼_Top10\n        \"entry.604042982\": b_seed,                 # B_ì‹œë“œ\n        \"entry.767507188\": b_queries,              # B_ì¿¼ë¦¬ë“¤\n        \"entry.1614784864\": b_results,             # Bê²°ê³¼_Top10\n        \"entry.1054173665\": common_text,           # ê³µí†µë…¼ë¬¸\n        # Q1~Q4ëŠ” ë¹„ì›Œë‘ê³  í¼ì—ì„œ ì§ì ‘ ì‘ì„±\n    }\n    \n    form_url = FORM_BASE + \"&\" + urllib.parse.urlencode(params)\n    \n    # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n    print(\"=\" * 70)\n    print(\"ğŸ“‹ ì œì¶œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°\")\n    print(\"=\" * 70)\n    print(f\"\\nğŸ‘¤ ì´ë¦„: {PARTICIPANT_NAME}\")\n    print(f\"\\nğŸ…°ï¸ ì‹¤í—˜ A\")\n    print(f\"   ì¿¼ë¦¬: {a_query}\")\n    print(f\"   ì‹œë“œ: {a_seeds[:50]}...\")\n    print(f\"\\nğŸ…±ï¸ ì‹¤í—˜ B\")\n    print(f\"   ì‹œë“œ: {b_seed[:50]}...\")\n    print(f\"   ì¿¼ë¦¬: {b_queries}\")\n    print(f\"\\nğŸ“š ê³µí†µ ë…¼ë¬¸: {common_text[:50]}...\")\n    \n    # í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ ì¶œë ¥\n    print(\"\\n\" + \"=\" * 70)\n    print(\"ğŸ”— ì•„ë˜ ë²„íŠ¼ í´ë¦­ â†’ í† ë¡  ì§ˆë¬¸ 4ê°œ ì‘ì„± â†’ ì œì¶œ\")\n    print(\"=\" * 70)\n    \n    display(HTML(f'''\n    <a href=\"{form_url}\" target=\"_blank\" \n       style=\"display:inline-block; font-size:16px; padding:12px 24px; \n              background:#4285f4; color:white; text-decoration:none; \n              border-radius:5px; margin:10px 0;\">\n       ğŸ“ ê²°ê³¼ ì œì¶œí•˜ê¸° (í´ë¦­)\n    </a>\n    <p style=\"color:#666; font-size:12px;\">â€» ì‹¤í—˜ ê²°ê³¼ëŠ” ìë™ ì…ë ¥ë©ë‹ˆë‹¤. í† ë¡  ì§ˆë¬¸ë§Œ ì‘ì„±í•˜ì„¸ìš”!</p>\n    '''))\n    \n    # ë¡œì»¬ ë°±ì—… ì €ì¥\n    save_data = {\n        'participant': PARTICIPANT_NAME,\n        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        'experiment_A': {'query': a_query, 'seeds': a_seeds, 'results': a_results},\n        'experiment_B': {'seed': b_seed, 'queries': b_queries, 'results': b_results},\n        'common_papers': common_text\n    }\n    \n    import json\n    if IN_COLAB:\n        save_folder = \"/content/drive/MyDrive/aiworkshop_Feb2026/shared_results/\"\n    else:\n        save_folder = os.path.join(WORKSHOP_DIR, \"outputs\", \"comparison_results\")\n    \n    os.makedirs(save_folder, exist_ok=True)\n    filename = f\"{PARTICIPANT_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n    filepath = os.path.join(save_folder, filename)\n    \n    with open(filepath, 'w', encoding='utf-8') as f:\n        json.dump(save_data, f, ensure_ascii=False, indent=2)\n    \n    print(f\"\\nâœ“ ë¡œì»¬ ë°±ì—…: {filepath}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# (ê°•ì‚¬ìš©) ì „ì²´ ì°¸ê°€ì ê²°ê³¼ ì¡°íšŒ\n# ============================================================\n# Google Sheetsì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²°ê³¼ ì½ê¸°\n# ============================================================\n\nSHEET_URL = \"https://docs.google.com/spreadsheets/d/15jyTrqGY7Po5iLcXFrv_kwyUNkCC9YMX6kypPMs-bAc/export?format=csv\"\n\ntry:\n    sheet_df = pd.read_csv(SHEET_URL)\n    \n    if len(sheet_df) > 0:\n        print(\"=\" * 70)\n        print(f\"ğŸ“Š ì „ì²´ ì°¸ê°€ì ê²°ê³¼ ({len(sheet_df)}ëª…)\")\n        print(\"=\" * 70)\n        \n        # ì»¬ëŸ¼ëª… ê°„ì†Œí™”\n        new_cols = ['íƒ€ì„ìŠ¤íƒ¬í”„', 'ì´ë¦„', 'A_ì¿¼ë¦¬', 'A_ì‹œë“œë“¤', 'Aê²°ê³¼', \n                    'B_ì‹œë“œ', 'B_ì¿¼ë¦¬ë“¤', 'Bê²°ê³¼', 'ê³µí†µë…¼ë¬¸',\n                    'Q1_ì‹œë“œì˜í–¥', 'Q2_ì¿¼ë¦¬ì˜í–¥', 'Q3_ê³µí†µíŠ¹ì§•', 'Q4_ì ìš©ì „ëµ']\n        sheet_df.columns = new_cols[:len(sheet_df.columns)]\n        \n        # ìš”ì•½ í…Œì´ë¸” (ì£¼ìš” í•„ë“œë§Œ)\n        summary_cols = ['ì´ë¦„', 'A_ì¿¼ë¦¬', 'B_ì¿¼ë¦¬ë“¤', 'ê³µí†µë…¼ë¬¸']\n        summary_df = sheet_df[[c for c in summary_cols if c in sheet_df.columns]].copy()\n        \n        # ê¸´ í…ìŠ¤íŠ¸ ìë¥´ê¸°\n        for col in summary_df.columns:\n            if col != 'ì´ë¦„':\n                summary_df[col] = summary_df[col].apply(\n                    lambda x: str(x)[:30] + \"...\" if pd.notna(x) and len(str(x)) > 30 else x\n                )\n        \n        print(\"\\nğŸ“‹ ìš”ì•½ (ì¿¼ë¦¬ & ê³µí†µë…¼ë¬¸)\")\n        display(summary_df.style.hide(axis='index'))\n        \n        # í† ë¡  ì‘ë‹µ ë³´ê¸°\n        discussion_cols = ['ì´ë¦„', 'Q1_ì‹œë“œì˜í–¥', 'Q2_ì¿¼ë¦¬ì˜í–¥', 'Q3_ê³µí†µíŠ¹ì§•', 'Q4_ì ìš©ì „ëµ']\n        if all(c in sheet_df.columns for c in discussion_cols):\n            print(\"\\n\" + \"=\" * 70)\n            print(\"ğŸ’¬ í† ë¡  ì‘ë‹µ\")\n            print(\"=\" * 70)\n            \n            for _, row in sheet_df.iterrows():\n                print(f\"\\nğŸ‘¤ {row['ì´ë¦„']}\")\n                print(f\"   Q1: {str(row.get('Q1_ì‹œë“œì˜í–¥', ''))[:60]}...\")\n                print(f\"   Q2: {str(row.get('Q2_ì¿¼ë¦¬ì˜í–¥', ''))[:60]}...\")\n                print(f\"   Q3: {str(row.get('Q3_ê³µí†µíŠ¹ì§•', ''))[:60]}...\")\n                print(f\"   Q4: {str(row.get('Q4_ì ìš©ì „ëµ', ''))[:60]}...\")\n        \n        print(f\"\\nâœ“ ì´ {len(sheet_df)}ëª… ì œì¶œ\")\n    else:\n        print(\"âš ï¸  ì•„ì§ ì œì¶œëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n        \nexcept Exception as e:\n    print(f\"âš ï¸  ì‹œíŠ¸ ì½ê¸° ì˜¤ë¥˜: {e}\")\n    print(\"   Form ì‘ë‹µ ì‹œíŠ¸ê°€ ì—°ê²°ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n    print(f\"\\nğŸ’¡ ì§ì ‘ í™•ì¸: https://docs.google.com/spreadsheets/d/15jyTrqGY7Po5iLcXFrv_kwyUNkCC9YMX6kypPMs-bAc/\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Part 5: DIY\n\në³¸ì¸ì˜ ì—°êµ¬ ì£¼ì œë¡œ ì‹¤í—˜í•´ë³´ì„¸ìš”!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DIY - ë³¸ì¸ì˜ ì—°êµ¬ ì£¼ì œë¡œ ì‹¤í—˜!\nMY_DOI = None  # <- DOI ì…ë ¥ (ê¶Œì¥)\nMY_TITLE = None  # <- DOI ì—†ìœ¼ë©´ ì œëª©ìœ¼ë¡œ ê²€ìƒ‰\nMY_QUERY = None  # <- ê²€ìƒ‰ ì¿¼ë¦¬ (í•„ìˆ˜!)\n\n# ì…ë ¥ ê²€ì¦\nif not MY_QUERY or MY_QUERY.strip() == \"\":\n    print(\"âš ï¸  ê²½ê³ : MY_QUERYê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!\")\n    print(\"   ê²€ìƒ‰ì— ì‚¬ìš©í•  ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n    print(\"   ì˜ˆ: MY_QUERY = \\\"depression detection machine learning\\\"\")\n    my_result = {'selected': []}\nelif not MY_DOI and not MY_TITLE:\n    print(\"âš ï¸  ê²½ê³ : ì‹œë“œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n    print(\"   MY_DOI ë˜ëŠ” MY_TITLE ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n    print(\"   ì˜ˆ: MY_DOI = \\\"10.1000/your.paper.doi\\\"\")\n    print(\"   ì˜ˆ: MY_TITLE = \\\"Your paper title here\\\"\")\n    my_result = {'selected': []}\nelse:\n    my_result = run_pipeline(\n        seed_doi=MY_DOI, \n        seed_title=MY_TITLE if not MY_DOI else None, \n        query=MY_QUERY\n    )\n    \n    if my_result['selected']:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"ë‚´ ì‹œë“œ ê¸°ë°˜ Top 10\")\n        print(\"=\" * 60)\n        for i, p in enumerate(my_result['selected'], 1):\n            print(f\"{i}. [{p.get('relevance_score', 0):.3f}] {p.get('title', 'N/A')[:50]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ê²°ë¡ \n\n### ë°°ìš´ ê²ƒ\n| ë„êµ¬ | ì—­í•  | ë§í¬ |\n|------|------|------|\n| **paperscraper** | í‚¤ì›Œë“œë¡œ ì‹œë“œ í›„ë³´ ê²€ìƒ‰ | [GitHub](https://github.com/jannisborn/paperscraper) |\n| **Semantic Scholar** | ì¸ìš©/ì°¸ì¡°/ì¶”ì²œ ë°ì´í„° | [API Docs](https://api.semanticscholar.org/api-docs/) |\n| **SPECTER2** | ë…¼ë¬¸ ì„ë² ë”© â†’ ê´€ë ¨ì„± ì ìˆ˜ | [GitHub](https://github.com/allenai/SPECTER2) \\| [ë…¼ë¬¸](https://arxiv.org/abs/2211.13308) |\n\n### í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n- **Crawler**: ì–‘ì  í™•ì¥ (References + Citations + Related)\n- **Selector**: ì§ˆì  í•„í„°ë§ (ì¿¼ë¦¬-ë…¼ë¬¸ ìœ ì‚¬ë„)\n- **ì‹œë“œ ì„ íƒì´ ì¤‘ìš”**: ì¶œë°œì ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§\n\n### ì´ ì ‘ê·¼ë²•ì˜ í•œê³„\n- paperscraper: arXiv/PubMedë§Œ ê²€ìƒ‰ (Google Scholar ì»¤ë²„ë¦¬ì§€ë³´ë‹¤ ì¢ìŒ)\n- Crawler: 1-hopë§Œ í™•ì¥ (ê¹Šì´ ì œí•œ)\n- Selector: ì´ˆë¡ ì—†ëŠ” ë…¼ë¬¸ì€ ì ìˆ˜ê°€ ë‚®ì•„ì§\n\n### ì¶”ê°€ í•™ìŠµ ìë£Œ\n- [PaSa: An LLM Agent for Comprehensive Academic Paper Search](https://arxiv.org/abs/2501.10120) - ì´ ë…¸íŠ¸ë¶ì˜ ì˜ê°ì´ ëœ ë…¼ë¬¸\n- [OpenAlex](https://openalex.org/) - ì™„ì „ ë¬´ë£Œ í•™ìˆ  ë°ì´í„°ë² ì´ìŠ¤ API\n- [Semantic Scholar Open Research Corpus](https://allenai.org/data/s2orc) - ëŒ€ê·œëª¨ ë…¼ë¬¸ ë°ì´í„°ì…‹"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}