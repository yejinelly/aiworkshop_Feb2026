{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Citation Crawler + Selector\n",
    "\n",
    "**SNU AI Psychology Workshop - February 2026**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1ê³¼ì˜ ì°¨ì´\n",
    "\n",
    "| Part 1 (API ê²€ìƒ‰) | Part 2 (Crawler + Selector) |\n",
    "|-------------------|-----------------------------|\n",
    "| í‚¤ì›Œë“œ â†’ ë…¼ë¬¸ ëª©ë¡ | **ì‹œë“œ ë…¼ë¬¸ ì„ íƒ** â†’ ì¸ìš© ë„¤íŠ¸ì›Œí¬ í™•ì¥ |\n",
    "| ë‹¨ì¼ ê²€ìƒ‰ | ë‹¤ì¤‘ í™‰ íƒìƒ‰ (multi-hop) |\n",
    "| ê²€ìƒ‰ ê²°ê³¼ë§Œ | ê´€ë ¨ ë…¼ë¬¸ í•„í„°ë§ (Selector) |\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "1. **paperscraper**ë¡œ ë‹¤ì¤‘ DB ê²€ìƒ‰ (arXiv, PubMed, bioRxiv, chemRxiv)\n",
    "2. **ì‹œë“œ ë…¼ë¬¸ ì„ íƒ**: ë²ˆí˜¸ ë˜ëŠ” DOI ì§ì ‘ ì…ë ¥\n",
    "3. **Citation Crawler**: ì‹œë“œì—ì„œ ì¸ìš© ë„¤íŠ¸ì›Œí¬ í™•ì¥\n",
    "4. **SPECTER2 Selector**: ê´€ë ¨ì„± ë†’ì€ ë…¼ë¬¸ í•„í„°ë§\n",
    "5. **ì‹œë“œ ë¹„êµ**: ë‹¤ë¥¸ ì‹œë“œë¡œ ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: í™˜ê²½ ì„¤ì •\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Colab í™˜ê²½ ê°ì§€\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    WORKSHOP_DIR = \"/content/drive/MyDrive/aiworkshop_Feb2026/\"\n",
    "    os.makedirs(WORKSHOP_DIR, exist_ok=True)\n",
    "    os.chdir(WORKSHOP_DIR)\n",
    "    IN_COLAB = True\n",
    "    print(\"Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘\")\n",
    "except ImportError:\n",
    "    current_dir = os.getcwd()\n",
    "    if current_dir.endswith('notebooks'):\n",
    "        os.chdir('..')\n",
    "    WORKSHOP_DIR = os.getcwd()\n",
    "    IN_COLAB = False\n",
    "    print(\"ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘\")\n",
    "\n",
    "print(f\"ì‘ì—… í´ë”: {WORKSHOP_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "if IN_COLAB:\n",
    "    print(\"íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\n",
    "    %pip install paperscraper requests pandas torch transformers adapters -q\n",
    "    print(\"ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(\"ë¡œì»¬ í™˜ê²½: í•„ìš” ì‹œ ì•„ë˜ ëª…ë ¹ ì‹¤í–‰\")\n",
    "    print(\"  pip install paperscraper requests pandas torch transformers adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: API Key ë¡œë”© (ì„ íƒì‚¬í•­)\n",
    "import os\n",
    "\n",
    "SEMANTIC_SCHOLAR_API_KEY = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        SEMANTIC_SCHOLAR_API_KEY = userdata.get('SEMANTIC_SCHOLAR_API_KEY', None)\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        SEMANTIC_SCHOLAR_API_KEY = os.getenv('SEMANTIC_SCHOLAR_API_KEY')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if SEMANTIC_SCHOLAR_API_KEY:\n",
    "    print(\"âœ“ Semantic Scholar API Key ë¡œë“œ ì„±ê³µ!\")\n",
    "    print(f\"  Key: {SEMANTIC_SCHOLAR_API_KEY[:10]}...\")\n",
    "    print(f\"  í—ˆìš©ëŸ‰: 5000 req/5ë¶„\")\n",
    "else:\n",
    "    print(\"âœ— Semantic Scholar API Key ì—†ìŒ (ì„ íƒì‚¬í•­)\")\n",
    "    print(\"  í—ˆìš©ëŸ‰: 100 req/5ë¶„ (ë¬´ë£Œ)\")\n",
    "    print(\"  ë°œê¸‰ ê¶Œì¥: https://www.semanticscholar.org/product/api\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: ê³µí†µ import\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Set, Optional\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "print(\"ê³µí†µ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. paperscraperë¡œ í›„ë³´ ë…¼ë¬¸ ê²€ìƒ‰\n",
    "\n",
    "[paperscraper](https://github.com/jannisborn/paperscraper)ëŠ” ì—¬ëŸ¬ í•™ìˆ  DBë¥¼ í†µí•© ê²€ìƒ‰í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ì…ë‹ˆë‹¤.\n",
    "\n",
    "| ì§€ì› DB | íŠ¹ì§• |\n",
    "|---------|------|\n",
    "| arXiv | CS, Physics í”„ë¦¬í”„ë¦°íŠ¸ |\n",
    "| PubMed | ì˜í•™, ì‹¬ë¦¬í•™ |\n",
    "| bioRxiv | ìƒë¬¼í•™ í”„ë¦¬í”„ë¦°íŠ¸ |\n",
    "| chemRxiv | í™”í•™ í”„ë¦¬í”„ë¦°íŠ¸ |\n",
    "\n",
    "### ê²€ìƒ‰ ì¿¼ë¦¬ ì‘ì„±ë²•\n",
    "\n",
    "| í˜•ì‹ | ì˜ˆì‹œ | ì˜ë¯¸ |\n",
    "|------|------|------|\n",
    "| ë‹¨ìˆœ í‚¤ì›Œë“œ | `depression detection` | ë‘ ë‹¨ì–´ ëª¨ë‘ í¬í•¨ |\n",
    "| AND ì—°ì‚° | `depression AND anxiety` | ë‘˜ ë‹¤ í¬í•¨ (ëª…ì‹œì ) |\n",
    "| OR ì—°ì‚° | `depression OR anxiety` | ë‘˜ ì¤‘ í•˜ë‚˜ |\n",
    "| ë³µí•© ì¡°ê±´ | `(depression OR anxiety) AND NLP` | ê·¸ë£¹í™” |\n",
    "| êµ¬ë¬¸ ê²€ìƒ‰ | `\"major depressive disorder\"` | ì •í™•í•œ êµ¬ë¬¸ |\n",
    "\n",
    "**íŒ**: comma(`,`)ë³´ë‹¤ `AND`/`OR` í‚¤ì›Œë“œê°€ ë” ëª…í™•í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: paperscraper ê²€ìƒ‰ í•¨ìˆ˜\n",
    "\n",
    "def search_with_paperscraper(query: str, max_results: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    paperscraperë¡œ ì—¬ëŸ¬ DB ê²€ìƒ‰\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: source, title, authors, year, doi, abstract\n",
    "    \"\"\"\n",
    "    all_papers = []\n",
    "    \n",
    "    # arXiv ê²€ìƒ‰\n",
    "    try:\n",
    "        from paperscraper.arxiv import get_arxiv_papers_api\n",
    "        print(\"[arXiv] ê²€ìƒ‰ ì¤‘...\")\n",
    "        arxiv_df = get_arxiv_papers_api(query, max_results=max_results, verbose=False)\n",
    "        for _, row in arxiv_df.iterrows():\n",
    "            all_papers.append({\n",
    "                'source': 'arXiv',\n",
    "                'title': row.get('title', ''),\n",
    "                'authors': row.get('authors', []),\n",
    "                'year': str(row.get('date', ''))[:4] if row.get('date') else '',\n",
    "                'doi': row.get('doi', ''),\n",
    "                'abstract': str(row.get('abstract', ''))[:200] + '...' if row.get('abstract') else ''\n",
    "            })\n",
    "        print(f\"  -> {len(arxiv_df)}ê°œ ë°œê²¬\")\n",
    "    except Exception as e:\n",
    "        print(f\"[arXiv] ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    # PubMed ê²€ìƒ‰\n",
    "    try:\n",
    "        from paperscraper.pubmed import get_pubmed_papers\n",
    "        print(\"[PubMed] ê²€ìƒ‰ ì¤‘...\")\n",
    "        pubmed_df = get_pubmed_papers(query, max_results=max_results)\n",
    "        for _, row in pubmed_df.iterrows():\n",
    "            all_papers.append({\n",
    "                'source': 'PubMed',\n",
    "                'title': row.get('title', ''),\n",
    "                'authors': row.get('authors', []),\n",
    "                'year': str(row.get('date', ''))[:4] if row.get('date') else '',\n",
    "                'doi': row.get('doi', ''),\n",
    "                'abstract': str(row.get('abstract', ''))[:200] + '...' if row.get('abstract') else ''\n",
    "            })\n",
    "        print(f\"  -> {len(pubmed_df)}ê°œ ë°œê²¬\")\n",
    "    except Exception as e:\n",
    "        print(f\"[PubMed] ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    # bioRxiv ê²€ìƒ‰ (ì„ íƒì‚¬í•­)\n",
    "    try:\n",
    "        from paperscraper.get_dumps import biorxiv\n",
    "        print(\"[bioRxiv] ê²€ìƒ‰ ì¤‘... (ì‹œê°„ ì†Œìš”)\")\n",
    "        # bioRxivëŠ” dump ê¸°ë°˜ì´ë¼ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ\n",
    "        # ì›Œí¬ìƒµì—ì„œëŠ” ìŠ¤í‚µ ê°€ëŠ¥\n",
    "        print(\"  -> ìŠ¤í‚µ (ì‹œê°„ ì ˆì•½)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[bioRxiv] ìŠ¤í‚µ\")\n",
    "    \n",
    "    return pd.DataFrame(all_papers)\n",
    "\n",
    "print(\"search_with_paperscraper() ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: í›„ë³´ ë…¼ë¬¸ ê²€ìƒ‰ ì‹¤í–‰\n",
    "QUERY = \"depression detection natural language processing\"  # <- ìˆ˜ì • ê°€ëŠ¥!\n",
    "\n",
    "print(f\"ê²€ìƒ‰ ì¿¼ë¦¬: {QUERY}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "candidates_df = search_with_paperscraper(QUERY, max_results=5)\n",
    "\n",
    "print(f\"\\nì´ {len(candidates_df)}ê°œ í›„ë³´ ë…¼ë¬¸ ë°œê²¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: í›„ë³´ ë…¼ë¬¸ í…Œì´ë¸”ë¡œ ë³´ê¸°\n",
    "if len(candidates_df) > 0:\n",
    "    # ë²ˆí˜¸ ì¶”ê°€\n",
    "    candidates_df.insert(0, 'No.', range(1, len(candidates_df) + 1))\n",
    "    \n",
    "    # ì œëª© ì¶•ì•½\n",
    "    candidates_df['title_short'] = candidates_df['title'].apply(\n",
    "        lambda x: x[:50] + '...' if len(str(x)) > 50 else x\n",
    "    )\n",
    "    \n",
    "    # í‘œì‹œìš© ì»¬ëŸ¼ ì„ íƒ\n",
    "    display_cols = ['No.', 'source', 'title_short', 'year', 'doi']\n",
    "    print(\"\\ní›„ë³´ ë…¼ë¬¸ ëª©ë¡:\")\n",
    "    print(\"=\" * 80)\n",
    "    for _, row in candidates_df[display_cols].iterrows():\n",
    "        print(f\"{row['No.']:2}. [{row['source']:6}] {row['title_short']}\")\n",
    "        print(f\"    ì—°ë„: {row['year']} | DOI: {row['doi']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¿¼ë¦¬ë¥¼ ë³€ê²½í•´ë³´ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ì‹œë“œ ë…¼ë¬¸ ì„ íƒ\n",
    "\n",
    "**ë‘ ê°€ì§€ ë°©ë²• ì¤‘ ì„ íƒ:**\n",
    "\n",
    "1. **ë²ˆí˜¸ë¡œ ì„ íƒ**: ìœ„ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë²ˆí˜¸ ì…ë ¥\n",
    "2. **DOI ì§ì ‘ ì…ë ¥**: ì´ë¯¸ ì•Œê³  ìˆëŠ” ë…¼ë¬¸ì˜ DOI ì…ë ¥\n",
    "\n",
    "DOIê°€ ì…ë ¥ë˜ë©´ ë²ˆí˜¸ëŠ” ë¬´ì‹œë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: ì‹œë“œ ë…¼ë¬¸ ì„ íƒ ì„¤ì •\n",
    "\n",
    "# ============================================\n",
    "# ë°©ë²• 1: ìœ„ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë²ˆí˜¸ë¡œ ì„ íƒ\n",
    "SEED_INDEX = 1  # <- 1ë¶€í„° ì‹œì‘í•˜ëŠ” ë²ˆí˜¸\n",
    "\n",
    "# ë°©ë²• 2: DOI ì§ì ‘ ì…ë ¥ (ì´ ê°’ì´ ìˆìœ¼ë©´ SEED_INDEX ë¬´ì‹œ)\n",
    "SEED_DOI = None  # <- DOI ì…ë ¥ ì‹œ ë²ˆí˜¸ ë¬´ì‹œ\n",
    "# ì˜ˆì‹œ: SEED_DOI = \"10.1038/s41591-023-02325-4\"\n",
    "# ============================================\n",
    "\n",
    "print(\"ì‹œë“œ ì„ íƒ ì„¤ì • ì™„ë£Œ\")\n",
    "if SEED_DOI:\n",
    "    print(f\"  -> DOI ì§ì ‘ ì…ë ¥: {SEED_DOI}\")\n",
    "else:\n",
    "    print(f\"  -> ê²€ìƒ‰ ê²°ê³¼ {SEED_INDEX}ë²ˆ ì„ íƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Semantic Scholar API Helper\n",
    "\n",
    "class SemanticScholarAPI:\n",
    "    \"\"\"Semantic Scholar API ë˜í¼\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.semanticscholar.org/graph/v1\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        self.api_key = api_key\n",
    "        self.headers = {}\n",
    "        if api_key:\n",
    "            self.headers['x-api-key'] = api_key\n",
    "    \n",
    "    def _request(self, endpoint: str, params: dict = None) -> dict:\n",
    "        \"\"\"API ìš”ì²­ (rate limit ì²˜ë¦¬ í¬í•¨)\"\"\"\n",
    "        url = f\"{self.BASE_URL}{endpoint}\"\n",
    "        \n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                response = requests.get(url, params=params, headers=self.headers)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    return response.json()\n",
    "                elif response.status_code == 429:\n",
    "                    wait_time = 5 * (attempt + 1)\n",
    "                    print(f\"  Rate limit - {wait_time}ì´ˆ ëŒ€ê¸°...\")\n",
    "                    time.sleep(wait_time)\n",
    "                elif response.status_code == 404:\n",
    "                    # ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ì •ìƒì ì¸ ê²½ìš°)\n",
    "                    return {}\n",
    "                else:\n",
    "                    print(f\"  API ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    return {}\n",
    "            except Exception as e:\n",
    "                print(f\"  ìš”ì²­ ì˜¤ë¥˜: {e}\")\n",
    "                return {}\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def get_paper_by_doi(self, doi: str) -> Dict:\n",
    "        \"\"\"DOIë¡œ ë…¼ë¬¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "        params = {\n",
    "            \"fields\": \"paperId,title,abstract,year,citationCount,authors\"\n",
    "        }\n",
    "        result = self._request(f\"/paper/DOI:{doi}\", params)\n",
    "        return result if result else {}\n",
    "    \n",
    "    def get_paper_by_title(self, title: str) -> Dict:\n",
    "        \"\"\"ì œëª©ìœ¼ë¡œ ë…¼ë¬¸ ê²€ìƒ‰ (ì²« ë²ˆì§¸ ê²°ê³¼ ë°˜í™˜)\"\"\"\n",
    "        params = {\n",
    "            \"query\": title,\n",
    "            \"limit\": 1,\n",
    "            \"fields\": \"paperId,title,abstract,year,citationCount,authors\"\n",
    "        }\n",
    "        result = self._request(\"/paper/search\", params)\n",
    "        if not result:\n",
    "            return {}\n",
    "        data = result.get('data', [])\n",
    "        return data[0] if data else {}\n",
    "    \n",
    "    def get_references(self, paper_id: str, limit: int = 50) -> List[Dict]:\n",
    "        \"\"\"ë…¼ë¬¸ì´ ì¸ìš©í•œ ì°¸ì¡°ë¬¸í—Œ (References)\"\"\"\n",
    "        params = {\n",
    "            \"fields\": \"paperId,title,abstract,year,citationCount\",\n",
    "            \"limit\": limit\n",
    "        }\n",
    "        result = self._request(f\"/paper/{paper_id}/references\", params)\n",
    "        \n",
    "        if not result:\n",
    "            return []\n",
    "        \n",
    "        papers = []\n",
    "        for item in result.get('data', []):\n",
    "            cited = item.get('citedPaper')\n",
    "            if cited and cited.get('paperId'):\n",
    "                papers.append(cited)\n",
    "        return papers\n",
    "    \n",
    "    def get_citations(self, paper_id: str, limit: int = 50) -> List[Dict]:\n",
    "        \"\"\"ë…¼ë¬¸ì„ ì¸ìš©í•œ ë…¼ë¬¸ë“¤ (Citations)\"\"\"\n",
    "        params = {\n",
    "            \"fields\": \"paperId,title,abstract,year,citationCount\",\n",
    "            \"limit\": limit\n",
    "        }\n",
    "        result = self._request(f\"/paper/{paper_id}/citations\", params)\n",
    "        \n",
    "        if not result:\n",
    "            return []\n",
    "        \n",
    "        papers = []\n",
    "        for item in result.get('data', []):\n",
    "            citing = item.get('citingPaper')\n",
    "            if citing and citing.get('paperId'):\n",
    "                papers.append(citing)\n",
    "        return papers\n",
    "\n",
    "# API ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "api = SemanticScholarAPI(api_key=SEMANTIC_SCHOLAR_API_KEY)\n",
    "print(\"âœ“ SemanticScholarAPI ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: ì‹œë“œ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "seed_paper = None\n",
    "\n",
    "if SEED_DOI:\n",
    "    # DOIë¡œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°\n",
    "    print(f\"DOIë¡œ ë…¼ë¬¸ ê²€ìƒ‰: {SEED_DOI}\")\n",
    "    seed_paper = api.get_paper_by_doi(SEED_DOI)\n",
    "else:\n",
    "    # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì„ íƒ\n",
    "    if len(candidates_df) >= SEED_INDEX:\n",
    "        selected_row = candidates_df.iloc[SEED_INDEX - 1]\n",
    "        \n",
    "        # DOIê°€ ìˆìœ¼ë©´ DOIë¡œ, ì—†ìœ¼ë©´ ì œëª©ìœ¼ë¡œ ê²€ìƒ‰\n",
    "        if selected_row.get('doi'):\n",
    "            print(f\"DOIë¡œ ë…¼ë¬¸ ê²€ìƒ‰: {selected_row['doi']}\")\n",
    "            seed_paper = api.get_paper_by_doi(selected_row['doi'])\n",
    "        else:\n",
    "            print(f\"ì œëª©ìœ¼ë¡œ ë…¼ë¬¸ ê²€ìƒ‰: {selected_row['title'][:50]}...\")\n",
    "            seed_paper = api.get_paper_by_title(selected_row['title'])\n",
    "    else:\n",
    "        print(f\"ì˜¤ë¥˜: {SEED_INDEX}ë²ˆ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. 1-{len(candidates_df)} ì‚¬ì´ ê°’ì„ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "if seed_paper and seed_paper.get('paperId'):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ì„ íƒëœ ì‹œë“œ ë…¼ë¬¸\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ì œëª©: {seed_paper.get('title', 'N/A')}\")\n",
    "    print(f\"ì—°ë„: {seed_paper.get('year', 'N/A')}\")\n",
    "    print(f\"ì¸ìš©ìˆ˜: {seed_paper.get('citationCount', 0)}\")\n",
    "    print(f\"Paper ID: {seed_paper.get('paperId')}\")\n",
    "    \n",
    "    # ì €ì ì •ë³´\n",
    "    authors = seed_paper.get('authors', [])\n",
    "    if authors:\n",
    "        author_names = [a.get('name', '') for a in authors[:3]]\n",
    "        print(f\"ì €ì: {', '.join(author_names)}\" + (\" ì™¸\" if len(authors) > 3 else \"\"))\n",
    "else:\n",
    "    print(\"\\nì‹œë“œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"DOIë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ë…¼ë¬¸ì„ ì„ íƒí•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Citation Crawler ì‹¤í–‰\n",
    "\n",
    "ì‹œë“œ ë…¼ë¬¸ì—ì„œ ì‹œì‘í•´ì„œ:\n",
    "1. **References**: ì´ ë…¼ë¬¸ì´ ì¸ìš©í•œ ë…¼ë¬¸ë“¤\n",
    "2. **Citations**: ì´ ë…¼ë¬¸ì„ ì¸ìš©í•œ ë…¼ë¬¸ë“¤\n",
    "\n",
    "```\n",
    "         [ì¸ìš©í•œ ë…¼ë¬¸ë“¤]      [ì¸ìš©ëœ ë…¼ë¬¸ë“¤]\n",
    "              â†‘                    â†“\n",
    "              â””â”€â”€â”€â”€ ì‹œë“œ ë…¼ë¬¸ â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Citation Crawler í´ë˜ìŠ¤\n",
    "\n",
    "class CitationCrawler:\n",
    "    \"\"\"\n",
    "    ì¸ìš© ë„¤íŠ¸ì›Œí¬ë¥¼ íƒìƒ‰í•˜ëŠ” Crawler\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api: SemanticScholarAPI):\n",
    "        self.api = api\n",
    "        self.visited: Set[str] = set()\n",
    "        self.papers: Dict[str, Dict] = {}\n",
    "    \n",
    "    def crawl(\n",
    "        self,\n",
    "        seed_paper: Dict,\n",
    "        max_hops: int = 1,\n",
    "        refs_per_paper: int = 20,\n",
    "        cites_per_paper: int = 20,\n",
    "        max_total: int = 100\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ì‹œë“œ ë…¼ë¬¸ì—ì„œ ì¸ìš© ë„¤íŠ¸ì›Œí¬ íƒìƒ‰\n",
    "        \"\"\"\n",
    "        self.visited.clear()\n",
    "        self.papers.clear()\n",
    "        \n",
    "        # ì‹œë“œ ë…¼ë¬¸ ì¶”ê°€\n",
    "        paper_id = seed_paper.get('paperId')\n",
    "        if not paper_id:\n",
    "            print(\"ì˜¤ë¥˜: ì‹œë“œ ë…¼ë¬¸ì— paperIdê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return []\n",
    "        \n",
    "        self.visited.add(paper_id)\n",
    "        seed_paper['hop'] = 0\n",
    "        seed_paper['source'] = 'seed'\n",
    "        self.papers[paper_id] = seed_paper\n",
    "        \n",
    "        current_level = [paper_id]\n",
    "        print(f\"ì‹œë“œ ë…¼ë¬¸: {seed_paper.get('title', 'N/A')[:50]}...\")\n",
    "        \n",
    "        # Multi-hop íƒìƒ‰\n",
    "        for hop in range(1, max_hops + 1):\n",
    "            if len(self.papers) >= max_total:\n",
    "                print(f\"ìµœëŒ€ ë…¼ë¬¸ ìˆ˜ ë„ë‹¬: {len(self.papers)}ê°œ\")\n",
    "                break\n",
    "            \n",
    "            print(f\"\\nHop {hop}: {len(current_level)}ê°œ ë…¼ë¬¸ì—ì„œ í™•ì¥ ì¤‘...\")\n",
    "            next_level = []\n",
    "            \n",
    "            for pid in current_level:\n",
    "                if len(self.papers) >= max_total:\n",
    "                    break\n",
    "                \n",
    "                # References ê°€ì ¸ì˜¤ê¸°\n",
    "                refs = self.api.get_references(pid, limit=refs_per_paper)\n",
    "                for ref in refs:\n",
    "                    ref_id = ref.get('paperId')\n",
    "                    if ref_id and ref_id not in self.visited:\n",
    "                        self.visited.add(ref_id)\n",
    "                        ref['hop'] = hop\n",
    "                        ref['source'] = 'reference'\n",
    "                        self.papers[ref_id] = ref\n",
    "                        next_level.append(ref_id)\n",
    "                \n",
    "                # Citations ê°€ì ¸ì˜¤ê¸°\n",
    "                cites = self.api.get_citations(pid, limit=cites_per_paper)\n",
    "                for cite in cites:\n",
    "                    cite_id = cite.get('paperId')\n",
    "                    if cite_id and cite_id not in self.visited:\n",
    "                        self.visited.add(cite_id)\n",
    "                        cite['hop'] = hop\n",
    "                        cite['source'] = 'citation'\n",
    "                        self.papers[cite_id] = cite\n",
    "                        next_level.append(cite_id)\n",
    "                \n",
    "                time.sleep(0.3)  # Rate limit ë°©ì§€\n",
    "            \n",
    "            print(f\"  -> References: {len([p for p in self.papers.values() if p.get('source') == 'reference'])}ê°œ\")\n",
    "            print(f\"  -> Citations: {len([p for p in self.papers.values() if p.get('source') == 'citation'])}ê°œ\")\n",
    "            print(f\"  -> í˜„ì¬ ì´: {len(self.papers)}ê°œ\")\n",
    "            current_level = next_level[:50]\n",
    "        \n",
    "        return list(self.papers.values())\n",
    "\n",
    "print(\"CitationCrawler í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Crawler ì‹¤í–‰\n",
    "if seed_paper and seed_paper.get('paperId'):\n",
    "    crawler = CitationCrawler(api)\n",
    "    \n",
    "    print(\"Citation Crawling ì‹œì‘...\")\n",
    "    print(\"(API rate limit ë•Œë¬¸ì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\\n\")\n",
    "    \n",
    "    crawled_papers = crawler.crawl(\n",
    "        seed_paper=seed_paper,\n",
    "        max_hops=1,           # 1-hop (ì§ì ‘ ì¸ìš©ë§Œ)\n",
    "        refs_per_paper=15,    # ë…¼ë¬¸ë‹¹ references 15ê°œ\n",
    "        cites_per_paper=15,   # ë…¼ë¬¸ë‹¹ citations 15ê°œ\n",
    "        max_total=50          # ìµœëŒ€ 50ê°œ\n",
    "    )\n",
    "    \n",
    "    print(f\"\\ní¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(crawled_papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘\")\n",
    "else:\n",
    "    print(\"ì‹œë“œ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. Cell 10ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "    crawled_papers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: í¬ë¡¤ë§ ê²°ê³¼ í†µê³„\n",
    "if crawled_papers:\n",
    "    source_counts = Counter(p.get('source', 'unknown') for p in crawled_papers)\n",
    "    years = [p.get('year') for p in crawled_papers if p.get('year')]\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"í¬ë¡¤ë§ ê²°ê³¼ í†µê³„\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nì¶œì²˜ë³„ ë…¼ë¬¸ ìˆ˜:\")\n",
    "    for source, count in source_counts.items():\n",
    "        emoji = {\"seed\": \"ğŸŒ±\", \"reference\": \"ğŸ“š\", \"citation\": \"ğŸ“\"}.get(source, \"ğŸ“„\")\n",
    "        print(f\"  {emoji} {source}: {count}ê°œ\")\n",
    "    \n",
    "    if years:\n",
    "        print(f\"\\nì—°ë„ ë²”ìœ„: {min(years)} - {max(years)}\")\n",
    "        print(f\"í‰ê·  ì—°ë„: {sum(years) / len(years):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. SPECTER2 Selectorë¡œ í•„í„°ë§\n",
    "\n",
    "Crawlerê°€ ìˆ˜ì§‘í•œ ë…¼ë¬¸ ì¤‘ì—ì„œ **ì¿¼ë¦¬ì™€ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë…¼ë¬¸**ë§Œ ì„ íƒí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: SPECTER2 Selector ì •ì˜\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "class SPECTER2Selector:\n",
    "    \"\"\"\n",
    "    SPECTER2ë¥¼ ì‚¬ìš©í•œ ë…¼ë¬¸ ê´€ë ¨ì„± í‰ê°€\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        print(\"SPECTER2 ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "        print(\"(ì²˜ìŒ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œ ~500MB)\")\n",
    "        \n",
    "        from transformers import AutoTokenizer\n",
    "        from adapters import AutoAdapterModel\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "        self.model = AutoAdapterModel.from_pretrained('allenai/specter2_base')\n",
    "        self.model.load_adapter(\n",
    "            \"allenai/specter2\", \n",
    "            source=\"hf\", \n",
    "            load_as=\"proximity\", \n",
    "            set_active=True\n",
    "        )\n",
    "        \n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"SPECTER2 ë¡œë“œ ì™„ë£Œ (device: {device})\")\n",
    "    \n",
    "    def get_embedding(self, title: str, abstract: str = \"\") -> torch.Tensor:\n",
    "        \"\"\"ë…¼ë¬¸ ì„ë² ë”© ê³„ì‚°\"\"\"\n",
    "        text = title + self.tokenizer.sep_token + (abstract or \"\")\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            [text],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model(**inputs)\n",
    "        \n",
    "        return output.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    def select_top_papers(\n",
    "        self, \n",
    "        query: str, \n",
    "        papers: List[Dict], \n",
    "        top_k: int = 10\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ì¿¼ë¦¬ì™€ ê´€ë ¨ì„± ë†’ì€ ìƒìœ„ Kê°œ ë…¼ë¬¸ ì„ íƒ\n",
    "        \"\"\"\n",
    "        if not papers:\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\n{len(papers)}ê°œ ë…¼ë¬¸ í‰ê°€ ì¤‘...\")\n",
    "        \n",
    "        query_emb = self.get_embedding(query)\n",
    "        \n",
    "        for i, paper in enumerate(papers):\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  {i + 1}/{len(papers)} ì™„ë£Œ\")\n",
    "            \n",
    "            paper_emb = self.get_embedding(\n",
    "                paper.get('title', ''),\n",
    "                paper.get('abstract', '')\n",
    "            )\n",
    "            score = cosine_similarity(query_emb, paper_emb).item()\n",
    "            paper['relevance_score'] = round(score, 4)\n",
    "        \n",
    "        papers.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)\n",
    "        \n",
    "        print(f\"í‰ê°€ ì™„ë£Œ! ìƒìœ„ {top_k}ê°œ ì„ íƒ\")\n",
    "        return papers[:top_k]\n",
    "\n",
    "print(\"SPECTER2Selector í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Selector ì‹¤í–‰\n",
    "if crawled_papers:\n",
    "    selector = SPECTER2Selector(device='cpu')\n",
    "    \n",
    "    selected_papers = selector.select_top_papers(\n",
    "        query=QUERY,\n",
    "        papers=crawled_papers.copy(),\n",
    "        top_k=10\n",
    "    )\n",
    "else:\n",
    "    print(\"í¬ë¡¤ë§ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    selected_papers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "if selected_papers:\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SPECTER2 Selector ê²°ê³¼: Top 10\")\n",
    "    print(f\"ì‹œë“œ: {seed_paper.get('title', 'N/A')[:40]}...\")\n",
    "    print(f\"ì¿¼ë¦¬: {QUERY}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, paper in enumerate(selected_papers, 1):\n",
    "        title = paper.get('title', 'N/A')\n",
    "        year = paper.get('year', 'N/A')\n",
    "        score = paper.get('relevance_score', 0)\n",
    "        source = paper.get('source', 'unknown')\n",
    "        cites = paper.get('citationCount', 0)\n",
    "        \n",
    "        emoji = {\"seed\": \"ğŸŒ±\", \"reference\": \"ğŸ“š\", \"citation\": \"ğŸ“\"}.get(source, \"ğŸ“„\")\n",
    "        \n",
    "        print(f\"\\n{i}. [{score:.3f}] {emoji} {title}\")\n",
    "        print(f\"   ì—°ë„: {year} | ì¸ìš©: {cites} | ì¶œì²˜: {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ì‹œë“œ ë¹„êµ ì‹¤í—˜\n",
    "\n",
    "**ë‹¤ë¥¸ ì‹œë“œ ë…¼ë¬¸ì„ ì„ íƒí•˜ë©´ ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§ˆê¹Œìš”?**\n",
    "\n",
    "ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í•¨ìˆ˜ë¡œ ë¬¶ì–´ì„œ, ì‹œë“œë§Œ ë°”ê¿”ê°€ë©° ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "def run_pipeline(\n",
    "    api: SemanticScholarAPI,\n",
    "    selector: SPECTER2Selector,\n",
    "    seed_doi: str = None,\n",
    "    seed_title: str = None,\n",
    "    query: str = \"depression detection\",\n",
    "    max_papers: int = 30,\n",
    "    top_k: int = 10\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "    \n",
    "    Parameters:\n",
    "    - seed_doi: ì‹œë“œ ë…¼ë¬¸ DOI (ìš°ì„ )\n",
    "    - seed_title: ì‹œë“œ ë…¼ë¬¸ ì œëª© (DOI ì—†ì„ ë•Œ)\n",
    "    - query: ê´€ë ¨ì„± í‰ê°€ìš© ì¿¼ë¦¬\n",
    "    \n",
    "    Returns:\n",
    "    - Dict with seed_paper, crawled_papers, selected_papers\n",
    "    \"\"\"\n",
    "    result = {'seed': None, 'crawled': [], 'selected': []}\n",
    "    \n",
    "    # 1. ì‹œë“œ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "    if seed_doi:\n",
    "        seed = api.get_paper_by_doi(seed_doi)\n",
    "    elif seed_title:\n",
    "        seed = api.get_paper_by_title(seed_title)\n",
    "    else:\n",
    "        print(\"ì˜¤ë¥˜: seed_doi ë˜ëŠ” seed_titleì„ ì œê³µí•˜ì„¸ìš”.\")\n",
    "        return result\n",
    "    \n",
    "    if not seed or not seed.get('paperId'):\n",
    "        print(\"ì‹œë“œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return result\n",
    "    \n",
    "    result['seed'] = seed\n",
    "    print(f\"ì‹œë“œ: {seed.get('title', 'N/A')[:50]}...\")\n",
    "    \n",
    "    # 2. Crawler ì‹¤í–‰\n",
    "    crawler = CitationCrawler(api)\n",
    "    crawled = crawler.crawl(\n",
    "        seed_paper=seed,\n",
    "        max_hops=1,\n",
    "        refs_per_paper=10,\n",
    "        cites_per_paper=10,\n",
    "        max_total=max_papers\n",
    "    )\n",
    "    result['crawled'] = crawled\n",
    "    \n",
    "    # 3. Selector ì‹¤í–‰\n",
    "    selected = selector.select_top_papers(\n",
    "        query=query,\n",
    "        papers=crawled.copy(),\n",
    "        top_k=top_k\n",
    "    )\n",
    "    result['selected'] = selected\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"run_pipeline() ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: ë‘ ì‹œë“œ ë¹„êµ ì‹¤í—˜\n",
    "\n",
    "# ============================================\n",
    "# ë¹„êµí•  ë‘ ì‹œë“œ ì„¤ì •\n",
    "# DOI ë˜ëŠ” ì œëª© ì¤‘ í•˜ë‚˜ë§Œ ì…ë ¥\n",
    "# ============================================\n",
    "\n",
    "# ì‹œë“œ A: (ì˜ˆ: ìœ ëª…í•œ ì´ˆê¸° ë…¼ë¬¸)\n",
    "SEED_A_DOI = None  # DOI ì…ë ¥ ë˜ëŠ”\n",
    "SEED_A_TITLE = \"depression detection social media\"  # ì œëª© ì…ë ¥\n",
    "\n",
    "# ì‹œë“œ B: (ì˜ˆ: ìµœì‹  ë…¼ë¬¸)\n",
    "SEED_B_DOI = None\n",
    "SEED_B_TITLE = \"large language model mental health\"\n",
    "\n",
    "COMPARE_QUERY = \"depression detection natural language processing\"\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ì‹œë“œ A íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\")\n",
    "print(\"=\" * 60)\n",
    "result_A = run_pipeline(\n",
    "    api=api,\n",
    "    selector=selector,\n",
    "    seed_doi=SEED_A_DOI,\n",
    "    seed_title=SEED_A_TITLE,\n",
    "    query=COMPARE_QUERY,\n",
    "    max_papers=30,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ì‹œë“œ B íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\")\n",
    "print(\"=\" * 60)\n",
    "result_B = run_pipeline(\n",
    "    api=api,\n",
    "    selector=selector,\n",
    "    seed_doi=SEED_B_DOI,\n",
    "    seed_title=SEED_B_TITLE,\n",
    "    query=COMPARE_QUERY,\n",
    "    max_papers=30,\n",
    "    top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: ê²°ê³¼ ë¹„êµ ë¶„ì„\n",
    "\n",
    "if result_A['selected'] and result_B['selected']:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ì‹œë“œ ë¹„êµ ê²°ê³¼\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # ì‹œë“œ ì •ë³´\n",
    "    print(f\"\\nì‹œë“œ A: {result_A['seed'].get('title', 'N/A')[:50]}...\")\n",
    "    print(f\"        ì—°ë„: {result_A['seed'].get('year')}, ì¸ìš©: {result_A['seed'].get('citationCount', 0)}\")\n",
    "    print(f\"\\nì‹œë“œ B: {result_B['seed'].get('title', 'N/A')[:50]}...\")\n",
    "    print(f\"        ì—°ë„: {result_B['seed'].get('year')}, ì¸ìš©: {result_B['seed'].get('citationCount', 0)}\")\n",
    "    \n",
    "    # í†µê³„ ë¹„êµ\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"{'í•­ëª©':<20} {'ì‹œë“œ A':>15} {'ì‹œë“œ B':>15}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'í¬ë¡¤ë§ ë…¼ë¬¸ ìˆ˜':<20} {len(result_A['crawled']):>15} {len(result_B['crawled']):>15}\")\n",
    "    \n",
    "    # í‰ê·  ì—°ë„\n",
    "    years_A = [p.get('year') for p in result_A['crawled'] if p.get('year')]\n",
    "    years_B = [p.get('year') for p in result_B['crawled'] if p.get('year')]\n",
    "    avg_year_A = sum(years_A) / len(years_A) if years_A else 0\n",
    "    avg_year_B = sum(years_B) / len(years_B) if years_B else 0\n",
    "    print(f\"{'í‰ê·  ì—°ë„':<20} {avg_year_A:>15.0f} {avg_year_B:>15.0f}\")\n",
    "    \n",
    "    # Top 10 ê³µí†µ ë…¼ë¬¸\n",
    "    ids_A = set(p.get('paperId') for p in result_A['selected'])\n",
    "    ids_B = set(p.get('paperId') for p in result_B['selected'])\n",
    "    common = ids_A & ids_B\n",
    "    print(f\"{'Top 10 ê³µí†µ ë…¼ë¬¸':<20} {len(common):>15} {'ê°œ':>14}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # ê³µí†µ ë…¼ë¬¸ ëª©ë¡\n",
    "    if common:\n",
    "        print(\"\\nê³µí†µìœ¼ë¡œ ì„ íƒëœ ë…¼ë¬¸:\")\n",
    "        for p in result_A['selected']:\n",
    "            if p.get('paperId') in common:\n",
    "                print(f\"  - {p.get('title', 'N/A')[:60]}...\")\n",
    "    else:\n",
    "        print(\"\\nê³µí†µ ë…¼ë¬¸ ì—†ìŒ - ì‹œë“œ ì„ íƒì´ ê²°ê³¼ì— í° ì˜í–¥ì„ ë¯¸ì¹¨!\")\n",
    "else:\n",
    "    print(\"ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: ì‹œë“œë³„ Top 5 ë‚˜ë€íˆ ë³´ê¸°\n",
    "\n",
    "if result_A['selected'] and result_B['selected']:\n",
    "    print(\"=\" * 90)\n",
    "    print(\"ì‹œë“œë³„ Top 5 ë¹„êµ\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    print(f\"\\n{'ìˆœìœ„':<4} {'ì‹œë“œ A Top 5':<40} {'ì‹œë“œ B Top 5':<40}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for i in range(5):\n",
    "        title_A = result_A['selected'][i].get('title', 'N/A')[:35] + \"...\" if i < len(result_A['selected']) else \"-\"\n",
    "        title_B = result_B['selected'][i].get('title', 'N/A')[:35] + \"...\" if i < len(result_B['selected']) else \"-\"\n",
    "        print(f\"{i+1:<4} {title_A:<40} {title_B:<40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. DIY: ë³¸ì¸ ë…¼ë¬¸ìœ¼ë¡œ ì‹¤í—˜\n",
    "\n",
    "ë³¸ì¸ì´ ì˜ ì•„ëŠ” ë…¼ë¬¸ì˜ DOIë¥¼ ì…ë ¥í•´ì„œ ì‹¤í—˜í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIY Cell: ë³¸ì¸ ë…¼ë¬¸ìœ¼ë¡œ ì‹¤í—˜\n",
    "\n",
    "# ============================================\n",
    "# ë³¸ì¸ì´ ì˜ ì•„ëŠ” ë…¼ë¬¸ì˜ DOI ì…ë ¥\n",
    "MY_DOI = \"10.xxxx/your-paper-doi\"  # <- ìˆ˜ì •!\n",
    "# ë˜ëŠ” ì œëª©ìœ¼ë¡œ ê²€ìƒ‰\n",
    "MY_TITLE = None  # <- DOI ëŒ€ì‹  ì œëª© ì…ë ¥ ê°€ëŠ¥\n",
    "\n",
    "MY_QUERY = \"your research topic\"  # <- ì—°êµ¬ ì£¼ì œ ì…ë ¥\n",
    "# ============================================\n",
    "\n",
    "print(\"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...\\n\")\n",
    "\n",
    "my_result = run_pipeline(\n",
    "    api=api,\n",
    "    selector=selector,\n",
    "    seed_doi=MY_DOI if MY_DOI and not MY_DOI.startswith(\"10.xxxx\") else None,\n",
    "    seed_title=MY_TITLE,\n",
    "    query=MY_QUERY,\n",
    "    max_papers=30,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "if my_result['selected']:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ë‚´ ì‹œë“œ ë…¼ë¬¸ ê¸°ë°˜ Top 10 ì¶”ì²œ\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, p in enumerate(my_result['selected'], 1):\n",
    "        score = p.get('relevance_score', 0)\n",
    "        print(f\"{i}. [{score:.3f}] {p.get('title', 'N/A')[:55]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. ê²°ë¡ \n",
    "\n",
    "### ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œ ë°°ìš´ ê²ƒ\n",
    "\n",
    "1. **paperscraper**: ë‹¤ì¤‘ DB (arXiv, PubMed, bioRxiv) í†µí•© ê²€ìƒ‰\n",
    "2. **ì‹œë“œ ì„ íƒ**: ë²ˆí˜¸ ë˜ëŠ” DOIë¡œ ì‹œì‘ì  ì§€ì •\n",
    "3. **Citation Crawler**: References + Citations ë„¤íŠ¸ì›Œí¬ í™•ì¥\n",
    "4. **SPECTER2 Selector**: ê´€ë ¨ì„± ê¸°ë°˜ í•„í„°ë§\n",
    "5. **ì‹œë“œ ë¹„êµ**: ì‹œë“œ ì„ íƒì´ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ í™•ì¸\n",
    "\n",
    "### í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n",
    "\n",
    "- **ì‹œë“œ ì„ íƒì´ ì¤‘ìš”**: ì–´ë–¤ ë…¼ë¬¸ì—ì„œ ì‹œì‘í•˜ëŠëƒì— ë”°ë¼ ì™„ì „íˆ ë‹¤ë¥¸ ë…¼ë¬¸ë“¤ì´ ì¶”ì²œë¨\n",
    "- **ì˜¤ë˜ëœ ì‹œë“œ vs ìµœì‹  ì‹œë“œ**: ì¸ìš© íŒ¨í„´ì´ ë‹¤ë¦„\n",
    "- **Crawler + Selector ì¡°í•©**: ì–‘(Crawler)ê³¼ ì§ˆ(Selector)ì˜ ê· í˜•\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "- **Part 3**: LitLLMìœ¼ë¡œ Related Work ì´ˆì•ˆ ìƒì„±\n",
    "- **Part 4**: AgentReviewë¡œ ë…¼ë¬¸ í”¼ë“œë°± ë°›ê¸°\n",
    "\n",
    "### ì°¸ê³  ìë£Œ\n",
    "- [paperscraper GitHub](https://github.com/jannisborn/paperscraper)\n",
    "- [Semantic Scholar API](https://api.semanticscholar.org/api-docs/)\n",
    "- [SPECTER2 GitHub](https://github.com/allenai/SPECTER2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "yejin-venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
